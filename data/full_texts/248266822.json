{"id": 248266822, "updated": "2023-10-05 15:05:14.656", "metadata": {"title": "A Fast Post-Training Pruning Framework for Transformers", "authors": "[{\"first\":\"Woosuk\",\"last\":\"Kwon\",\"middle\":[]},{\"first\":\"Sehoon\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Mahoney\",\"middle\":[\"W.\"]},{\"first\":\"Joseph\",\"last\":\"Hassoun\",\"middle\":[]},{\"first\":\"Kurt\",\"last\":\"Keutzer\",\"middle\":[]},{\"first\":\"Amir\",\"last\":\"Gholami\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Pruning is an effective way to reduce the huge inference cost of Transformer models. However, prior work on pruning Transformers requires retraining the models. This can add high training cost and high complexity to model deployment, making it difficult to use in many practical situations. To address this, we propose a fast post-training pruning framework for Transformers that does not require any retraining. Given a resource constraint and a sample dataset, our framework automatically prunes the Transformer model using structured sparsity methods. To retain high accuracy without retraining, we introduce three novel techniques: (i) a lightweight mask search algorithm that finds which heads and filters to prune based on the Fisher information; (ii) mask rearrangement that complements the search algorithm; and (iii) mask tuning that reconstructs the output activations for each layer. We apply our method to BERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x speedup in inference latency, while maintaining<1% loss in accuracy. Importantly, our framework prunes Transformers in less than 3 minutes on a single GPU, which is over two orders of magnitude faster than existing pruning approaches that retrain the models.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2204.09656", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/KwonKMHKG22", "doi": "10.48550/arxiv.2204.09656"}}, "content": {"source": {"pdf_hash": "fb145e1e49d3269d8223c7710e22b45438613ff0", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2204.09656v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "51a7711b8a4f4a687c35431b1e27550a48b1abf6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fb145e1e49d3269d8223c7710e22b45438613ff0.txt", "contents": "\nA Fast Post-Training Pruning Framework for Transformers\n\n\nWoosuk Kwon woosuk.kwon@berkeley.edu \nSehoon Kim sehoonkim@berkeley.edu \nMichael W Mahoney mahoneymw@berkeley.edu \nJoseph Hassoun j.hassoun@samsung.com \nKurt Keutzer keutzer@berkeley.edu \nAmir Gholami amirgh@berkeley.edu \n\nUC Berkeley\nBerkeley\n\n\nUC Berkeley\nICSI\n& LBNL\n\n\nSamsung Semiconductor, Inc\nBerkeley, Berkeley\n\nA Fast Post-Training Pruning Framework for Transformers\n\nPruning is an effective way to reduce the huge inference cost of Transformer models. However, prior work on pruning Transformers requires retraining the models. This can add high training cost and high complexity to model deployment, making it difficult to use in many practical situations. To address this, we propose a fast posttraining pruning framework for Transformers that does not require any retraining. Given a resource constraint and a sample dataset, our framework automatically prunes the Transformer model using structured sparsity methods. To retain high accuracy without retraining, we introduce three novel techniques: (i) a lightweight mask search algorithm that finds which heads and filters to prune based on the Fisher information; (ii) mask rearrangement that complements the search algorithm; and (iii) mask tuning that reconstructs the output activations for each layer. We apply our method to BERT BASE and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD benchmarks. Our framework achieves up to 2.0\u00d7 reduction in FLOPs and 1.56\u00d7 speedup in inference latency, while maintaining < 1% loss in accuracy. Importantly, our framework prunes Transformers in less than 3 minutes on a single GPU, which is over two orders of magnitude faster than existing pruning approaches that retrain the models. 1\n\nIntroduction\n\nIn recent years, Transformer [76] has become a de facto standard model architecture in Natural Language Processing [4,12,46], and it is becoming common in many domains including Computer Vision [14,48,75] and Speech Recognition [2,7,26]. However, efficient deployment of Transformer architectures has been challenging due to their large model size and high inference latency. As a promising way to tackle this challenge, structured pruning of Transformers has been widely studied.\n\nWhile prior work on pruning Transformers substantially reduces the inference time, it is often difficult to use in practice for several reasons. First, previous approaches require retraining the pruned model and/or jointly learning the pruning configurations during training. This increases the training time by up to 10\u00d7 [38,88], adding significant computational overhead. Second, previous methods add many moving parts to the model deployment process. That is, the pruning pipelines are often complex and require additional hyperparameter tuning. Such techniques demand significant engineering efforts for implementation and debugging, which impedes their adoption in production pipelines. Third, these previous methods do not directly adapt to the users' constraints. They either rely on vague regularization hyperparameters to control the model sparsity or use fixed model architectures selected independently of the user settings. This can result in sub-optimally pruned models that are not tailored to the users' constraints and hardware.\n\nTo address the above limitations, we propose a fast post-training pruning framework for Transformers that does not require any retraining of the models. As illustrated in Figure 1, our framework takes as input a Transformer model, a sample dataset, and a FLOPs/latency constraint. It then outputs a pruned Transformer model that can be deployed immediately. By avoiding expensive retraining, the end-to-end pruning pipeline can be extremely fast and simplified, which typically takes a few minutes without any user interventions that complicate the whole process.\n\nIndeed, post-training compression has been widely studied for quantization, and it gained considerable attention in both academia and industry [3,27,97]. Although quantization-aware training methods achieve higher compression rates in general, post-training quantization (PTQ) has often been more preferred in practice due to its retraining-free advantages. Importantly, PTQ allows quantization to happen seamlessly at the model deployment time using the tools such as TensorRT [56], TFLite [19], and OpenVINO [29]. Similar to the PTQ methods, our framework provides an out-of-the-box tool that enables pruning of Transformers without engineering efforts.\n\nOur contributions can be summarized as follow:\n\n\u2022 We propose a novel post-training pruning framework for Transformers that does not require model retraining. To retain accuracy without retraining, our framework consists of three stages: (i) the mask search process guided by the Fisher information matrix to select which heads/filters to prune (Section 4.1); (ii) the mask rearrangement process that reselects the heads/filters to prune by capturing intra-layer interactions (Section 4.2); and (iii) the mask tuning process that adjusts the mask variables to ensure that the output signal is recovered for each layer (Section 4.3).\n\n\u2022 We extensively test our framework by applying it to BERT BASE and DistilBERT on GLUE and SQuAD tasks (Section 5.2). Within 1% of accuracy drop, our framework reduces 30-50% of the original FLOPs ( Figure 5), resulting in up to 1.56\u00d7 speedup on an NVIDIA V100 GPU (Table 1).\n\n\u2022 We show that our method achieves comparable or even better FLOPs-accuracy trade-off than prior structured pruning methods without retraining (Section 5.3, Figure 6). Our end-to-end pruning pipeline finishes in only 39 and 135 seconds on average for GLUE and SQuAD (Section 5.4, Table 5), which is over 100\u00d7 faster than the retraining-based methods.\n\n\nRelated Work\n\nEfficient Transformers. In order to improve the inference speed and reduce the memory footprint of Transformers, multiple different approaches have been proposed. These can be broadly categorized as follows: (i) efficient architecture design [28,35,39,72,81,87]; (ii) hardware-software codesign [21,22,73,80]; (iii) knowledge distillation [31,63,71,82]; (iv) quantization [33,66,95,96];\n\n(v) neural architecture search [6,67,68,79,90,93]; and (vi) pruning. In this paper, we focus on pruning and briefly discuss the related works.\n\nTransformers Pruning. Pruning has been a promising way to remove unimportant weights in neural networks. Pruning can be largely categorized into unstructured and structured pruning. For unstructured pruning, magnitude-based [18], first-order [64], and second-order [36] pruning methods, and the lottery ticket hypothesis [9,10,16,58] have been explored for Transformers. While these methods can substantially compress the model size, commodity hardware such as GPUs can hardly take advantage of the unstructured sparse patterns for model inference speedup.\n\nFor this reason, a number of structured pruning methods have been introduced to remove coarsegrained sets of parameters in Transformers. For example, to prune structured sets of parameters in weight matrices, low-rank factorization [83], block-wise sparsity [43], and tile-wise sparsity [20] were studied. Furthermore, as more coarse-grained methods, attention head pruning [51,77] and layer dropping [15,62] have been popularly used. Taking a step further, recent approaches [8,32,38,44,47,88,92] have explored jointly pruning Transformers with different pruning granularity and principles, maximizing the model efficiency in every dimension. Orthogonally, another thread of work [15,25,49,89,98] has shown that Transformers can be dynamically pruned at inference time.\n\nUnfortunately, while the structured pruning methods can achieve high compression rates and speedups, they are often difficult to use in practice. One reason for this is the high computational cost of additional training during or after pruning, which can be up to 10\u00d7 [38,88] compared to that of the original model training. Another reason is the high complexity of the pruning pipelines [25,38,47,92], where each pruning stage often requires rewriting the training code and introduces additional hyperparameters to tune.\n\nPost-training Model Compression. Post-training compression methods have been widely studied in quantization. These methods, categorized as post-training quantization (PTQ), perform quantization without any retraining, thereby avoiding the additional training cost and user intervention. Multiple PTQ techniques have been proposed to effectively mitigate the accuracy degradation without retraining [3,27,54,97].\n\nAlthough not as much as for quantization, post-training schemes have also been explored for unstructured [17,40,53] and structured pruning of CNNs. For structured pruning, [34,70,94] proposed ways to group and merge similar neurons in a CNN. However, we find it difficult to extend those techniques to pruning Transformers because they require the model to have a repeating structure of a linear layer and an element-wise nonlinearity, which is not the case for the multi-head attention layers of Transformers. Even for the feed-forward network layers of Transformers, [34,70] can hardly be used because they rely on a certain characteristic of ReLU while many Transformers [4,12,91] use GELU [24] instead of ReLU.\n\nMotivated by the fact that the existing post-training CNN pruning techniques cannot be applied to Transformers, in this paper we propose a novel post-training pruning method with a focus on Transformers. However, we would like to note that the underlying principles in our approach are general enough to be extended to pruning other types of model architectures such as CNNs.\n\n\nOverview\n\n\nBackground\n\nTransformer Architecture. In this paper, we focus on the pruning of encoder-based Transformer [76] models, especially the BERT [12] architecture family. BERT is a stack of homogeneous Transformer encoder blocks, each of which consists of a multi-head attention (MHA) layer followed by a pointwise Feed-Forward Network (FFN) layer. Specifically, an MHA layer consists of H independently parameterized attention heads:\nMHA(x) = H i=1 Att i (x), x MHA = LayerNorm x + MHA(x) ,\nwhere Att is a dot product attention head, and x is the input sequence. The output of the MHA layer is then fed into the FFN layer, which consists of N filters:\nFFN(x) = N i=1 W (2) :,i \u03c3(W (1) i,: x + b (1) i ) + b (2) , x out = LayerNorm x MHA + FFN(x MHA ) ,\nwhere W (1) , W (2) , b (1) and b (2) are the FFN parameters, and \u03c3 is the activation function, typically GELU [24]. Note that (H, N ) is (12,3072) for BERT BASE , and (16, 4096) for BERT LARGE . We also denote L as the number of Transformer layers (e.g., 12 for BERT BASE ). Granularity of Pruning. Our framework considers the structured pruning of both heads in MHA and filters in FFN layers. We do not prune the embedding and the final classifier, as computation of those layers takes a negligible portion of the total inference latency. Since our pruning framework always produces a smaller dense architecture, the model can be readily accelerated without the need of specialized hardware logic, which is often required for unstructured sparsity to gain latency speedup.\n\nNotations. We pose the pruning problem as finding a sparse mask for the heads and filters. To formalize this, we introduce mask variables associated with the outputs of heads and filters:\nMHA(x; m MHA l ) = H i=1 m MHA l,i \u2022 Att i (x), FFN(x; m FFN l ) = N i=1 m FFN l,i \u2022 W (2) :,i \u03c3(W (1) i,: x + b (1) i ) + b (2) ,\nwhere m MHA l \u2208 R H and m FFN l \u2208 R N are the mask variables for MHA and FFN in the l-th layer, respectively, and m MHA l,i and m FFN l,i are their i-th elements. Furthermore, \u2022 denotes the Hadamard product. Originally, the mask variables are all initialized to 1, which does not change the model outputs. After pruning, the mask variables become zero or any nonzero values, affecting the model accuracy and sparsity. Especially, setting m MHA l,i and m MHA l,i as zero is equivalent to pruning the i-th head and filter, respectively.\n\nOverall, there are LH head mask variables and LN filter mask variables, summing up to L(H + N ) number of total mask variables in a Transformer model. To simplify notations, we additionally define m MHA \u2208 R LH , m FFN \u2208 R LN , and m \u2208 R L(H+N ) as flattened vectors of the head, filter, and total mask variables, respectively, across all layers. In what follows, we discuss how to find the optimal sparse masks under a given cost constraint and how to adjust their values to recover accuracy. Inputs. Our framework has 3 inputs: a Transformer model; a sample dataset; and a resource constraint. The input Transformer model should contain weights fine-tuned for a downstream task. The sample dataset is a small partition of the training dataset (typically 1-2K examples) for the downstream task. The resource constraint can be given either as the number of floating point operations (FLOPs) or as an actual latency on target hardware. In the later case, we further assume that a latency lookup table for the target hardware is provided.\n\n\nFramework Overview\n\nCompression Pipeline. As illustrated in Figure 2, our framework consists of 3 stages: Fisher-based mask search; Fisher-based mask rearrangement; and mask tuning. During the Fisher-based mask search stage (Section 4.1), we search for a binary mask applied to the heads and filters based on the Fisher information of the mask variables. Intuitively, the mask variables with relatively higher Fisher information are considered more important, and they should be less likely to be pruned [41,45,52]. As finding the optimal mask that minimizes the Fisher information loss is intractable due to the large size of the full Fisher matrix, we propose a lightweight search algorithm that finds the optimal mask under reasonable approximations. Second, in the Fisher-based mask rearrangement stage (Section 4.2), the framework adjusts the searched mask patterns to better take into account the intra-layer interactions between the mask variables. Lastly, in the mask tuning stage (Section 4.3), the framework tunes the nonzero mask variables to recover the accuracy drop by reconstructing the layer-wise output signal.\n\n\nMethodology\n\nThe pruning problem can be seen as finding an optimal mask under a sparsity constraint. However, without retraining, the problem becomes intractable. To address this, we decompose Transformer pruning into three sub-problems, each of which can be efficiently solved; the first two stages of our pipeline address the problems of finding an optimal binary mask, and the last stage further optimizes it into a real-valued mask.\n\nNote that the number of the mask variables is much less than the number of the parameters in a Transformer (e.g., 37K vs. 110M in case of BERT BASE ). This allows the framework to use only a small number of examples without overfitting to the sample dataset, and thus to be extremely faster than the retraining-based pruning methods which typically use the entire dataset. As the framework keeps the model \"as is\" and only decides the mask variables, we henceforth regard the model parameters as constants and consider the mask variables as the only parameters for our pruning problem.\n\nProblem Formulation. We formulate Transformer pruning as a constrained optimization problem on the mask m:\narg min m L(m) s.t. Cost(m) \u2264 C(1)\nwhere L denotes the loss function, Cost is the FLOPs/latency of the architecture pruned by the mask, and C is the given FLOPs/latency constraint. Unfortunately, such a problem is generally intractable as Cost is usually a function of l 0 -norm of the mask m, which is non-differentiable. Thus, in what follows, we introduce several assumptions and approximations to simplify the problem.\n\nWe start by approximating the loss function using the second-order Taylor expansion around the initial mask 1:\nL(m) \u2248 L(1) \u2212 g (1 \u2212 m) + 1 2 (1 \u2212 m) H(1 \u2212 m) (2) \u2248 L(1) + 1 2 (1 \u2212 m) H(1 \u2212 m),(3)where g = E[ \u2202 \u2202m L(1)] and H = E[ \u2202 2 \u2202m 2 L(1)].\nEq. 3 is deduced from an assumption that the model has converged to a local minima, where the gradient term is close to 0 [41]. As L(1) is a constant, we can rewrite the optimization objective as follows:\narg min m L(m) \u2248 arg min m (1 \u2212 m) H(1 \u2212 m).(4)\nEq. 4 shows that the optimal mask is determined by the Hessian of the loss with respect to the mask variables. Since forming the exact Hessian matrix explicitly is infeasible, we approximate the Hessian H with the (empirical) Fisher information matrix I of the mask variables:\nI := 1 |D| (x,y)\u2208D \u2202 \u2202m L(x, y; 1) \u2202 \u2202m L(x, y; 1) ,(5)\nwhere D is the sample dataset and (x, y) is a tuple of an input example and its label.\n\n\nFisher-based Mask Search\n\nDiagonal Approximation of the Fisher Information Matrix. It is intractable to solve the optimization objective in Eq. 4 using the full Fisher information matrix I. Thus, we first make a simple assumption that I is diagonal. This further simplifies Eq. 4 as follows:\narg min m L(m) \u2248 arg min m i (1 \u2212 m i ) 2 I ii ,(6)\nSince we restrict the possible mask values to either 0 or 1, the following can be derived from Eq. 6:\narg min m L(m) \u2248 arg min m i\u2208Z(m) I ii where Z(m) := {i | m i = 0}.(7)\nWe can interpret each diagonal element of I as the importance score of the head/filter associated with the mask variable, and Eq. 7 as a process of minimizing the total importance scores of the pruned heads and filters. Such an importance score has also been introduced in [52,74] to guide pruning.\n\n\nAlgorithm 1 Mask Search with a FLOPs Constraint\n\nInput: FLOPs constraint C, diagonal Fisher information matrix I 1: for n = 0 to LH do # remaining heads 2: Solving FLOPs-constrained Problem. We need to solve Eq. 7 given a cost constraint. For a given target FLOPs cost, denoted by C, we can formulate the binary mask search problem as follows:\nk 1 = LH \u2212 n #arg min m i\u2208Z(m) I ii s.t. F head ||m MHA || 0 + F filter ||m FFN || 0 \u2264 C,(8)\nwhere F head \u2208 R and F filter \u2208 R are the FLOPs for computing a head and a filter, respectively. Note that the number of FLOPs of a head/filter is constant across all layers. While such an optimization problem can be generally solved by a knapsack algorithm [1,65], the following observations allow a faster polynomial-time solution: (1) having more heads and filters unpruned always optimizes Eq. 8 since the diagonal elements of I are non-negative; and (2) if a certain number of heads needs to be pruned, they should be the ones with the lowest importance scores because each head accounts for the same amount of FLOPs. The same statement also holds for pruning filters. The two observations lead to our mask search algorithm described in Algorithm 1.\n\nAlgorithm 1 partitions the solution space by the total number of remaining heads in the pruned architecture (n in line 1). For each n, the number of remaining filters should be the largest possible number that satisfies the cost constraint by observation (1), which can be described as f in line 4. Then by observation (2), the heads/filters with the lowest important scores are selected to be pruned. Therefore, S[n] is a solution of Eq. 8 under additional constraint of fixing the number of remaining heads to be n. When the loop terminates, the output is the mask that minimizes S[n] across all possible n (line 10 and 11). In Section A.1, we prove that the output mask m * of Algorithm 1 is optimal. That is, any other mask m satisfying the given FLOPs constraint will result in a higher loss:\ni\u2208Z(m * ) I ii \u2264 i\u2208Z(m) I ii .(9)\nSolving Latency-constrained Problem. If the cost constraint is given in terms of latency on target hardware, we have a new optimization problem with a different cost constraint than Eq. 8:\narg min m i\u2208Z(m) I s.t. L l=1 LAT(m MHA l ) + L l=1 LAT(m FFN l ) \u2264 C,(10)\nwhere the function LAT indicates the latency of a MHA/FFN layer after pruning. We assume that a latency lookup table on the target hardware is provided so that evaluating LAT takes negligible time.\n\nUnfortunately, the latency constraint makes the problem more challenging as directly applying Algorithm 1 is no longer possible. This is because LAT is not linear to the number of remaining heads or filters after pruning [59], as shown in Figure 3 (Left). We can interpret this as follows: (1) with a sufficient number of heads/filters in a layer, the hardware resources such as parallel cores can be fully utilized, resulting in latency roughly proportional to the number of heads/filters; and (2) otherwise, the hardware is underutilized and a constant overhead dominates the latency [37,50]. Thus, pruning more heads/filters below a certain threshold does not translate into actual speedup. Based on the above analysis, we approximate LAT as a piece-wise linear function as in Figure 3 (\nRight) such that LAT(m l ) is 0 if ||m l || 0 = 0, c if 0 < ||m l || 0 \u2264 T , and a(||m l || 0 \u2212 T ) + c if ||m l || 0 > T ,\nwhere c \u2208 R is the constant overhead, T \u2208 N is the threshold number of heads/filters that the latency starts to become linear, and a \u2208 R is the slope of the linear part. This can be easily obtained by fitting the actual latency data in the lookup table with the minimum mean squared error.\n\nThe piece-wise linear approximation of LAT allows us to extend Algorithm 1 to the setting with latency constraints. The core idea is to separately consider the constant part and the linear part of LAT; after handling the constant part, we can apply the Algorithm 1 to the linear part. The detailed modification to Algorithm 1 is described in Section A.2.\n\n\nFisher-based Mask Rearrangement\n\nBlock Diagonal Approximation of the Fisher Information Matrix. Although it simplifies the problem, the diagonal assumption in Section 4.1 alone might not find the best solution, as it does not take into account the interactions between different mask variables. For example, if there are two attention heads playing a similar role in a layer, pruning only one of them might not affect the model accuracy. However, when both of them are pruned, the model accuracy can be significantly degraded. Such interactions are captured by the non-diagonal elements of the Fisher information matrix, which were ignored in the previous stage. Thus, we can better consider the interactions in our pruning problem by using a block diagonal approximation to the Fisher matrix, where a block corresponds to a MHA layer or a FFN layer as illustrated in Figure 4.\n\nHowever, the block diagonal approximation results in an intractable optimization problem over the binary mask. To alleviate this, we use the results from the previous stage to warm start the optimization problem. First, we constrain the number of heads/filters to prune for each layer to be the same as the binary mask we obtained in the first stage. In other words, given the mask m * obtained in Section 4.1, we constrain ||m l || 0 to be equal to ||m * l || 0 for each layer l. Second, we use the mask m * as the starting point of the greedy search to solve the new optimization problem.\n\nGiven the two assumptions that (i) there is no interaction between the mask variables in different layers (i.e., the block diagonal approximation), and (ii) the number of heads/filters to prune are pre-determined for each layer (i.e., warm-start), Eq. 4 breaks down to a set of layer-wise optimization problems, as follows based on the derivation in Section A.3:\nm l = arg min m l (1 \u2212 m l ) I l (1 \u2212 m l ),(11)\nwhere I l is the l-th diagonal block of I. We approximately solve this problem with a greedy algorithm.\n\nAfter initializing the mask m l as m * l (i.e., warm-start), we pick for every round a pruned head (or filter) with the highest Fisher information and exchange it with an unpruned head (or filter) in the current mask if that can further optimize Eq. 11. After every pruned head/filter goes through one round, we obtain an approximate solution to Eq. 11.\n\nBecause this process does not change the number of heads/filters in each layer, the obtained mask m l results in the same FLOPs/latency as that of the mask m * l searched in Section 4.1. In effect, this process rearranges the binary mask variables of each layer to find a better arrangement of pruning locations and capture the intra-layer interactions. Note that these results can be achieved in only 39 and 135 seconds for GLUE and SQuAD benchmarks, respectively, on a single GPU system, as described in Table 5 (in Section A.10). \n\n\nMask Tuning\n\nIn the previous two stages, the possible mask values are restricted to either 0 or 1 in order to simplify the search process. In this stage, we further relax this restriction. The nonzero variables in the mask m from Section 4.2 are tuned to any real values such that the pruned model recovers its accuracy.\n\nLayer-wise Reconstruction via Linear Least Squares. We tune the mask variables toward minimizing the layer-wise reconstruction error, similarly to [23]. From the first to the last layer, we reconstruct the output activation of the original model with the remaining heads/filters in the pruned model. This can be formally written as follows:\narg min m l ||x + layer(x; m l ) \u2212 x + layer(x ; 1) || 2 2 ,(12)\nwhere layer is either MHA or FFN, and x and x are the inputs to the layer of the pruned model and the original model, respectively. Here we compare the activations after the residual connection. Note that this stage does not incur any change in model FLOPs/latency, as we only tune the nonzero mask variables. We show in Section A.4 that Eq. 12 can be reduced to a linear least squares problem of arg min m l ||Am l \u2212 b|| 2 2 , where the matrix A denotes head/filter-wise output activations of the model pruned by the binary mask and the vector b is the difference between the output activations of the two models. Concretely, when there are T tokens in the sample dataset and D is the hidden size of the model, the size of the matrix A is T D \u00d7 H for head masks and T D \u00d7 N for filter masks.\n\nDue to the large size of the matrix A, naively solving the least squares problem can lead to numerically unstable results. To address this, our framework uses the LSMR solver in CuPy [55] with a regularization hyperparameter (i.e., damp). Concretely, we re-parameterize the least squares problem as arg min r l ||Ar l + A \u00b7 1 \u2212 b|| 2 2 where m l = 1 + r l , and solve it with the damp value fixed to 1. Then, to prevent the case in which the tuned mask rather hurts the accuracy, we restrict the acceptable range of the tuned mask variables to [-10, 10]. When the solver finds a layer mask that exceeds this range, we discard the mask for that layer and stop mask tuning. In our experiments, we find that the aforementioned heuristics make the mask tuning process highly stable across different models, tasks, and seeds. Furthermore, while the use of the heuristics involves the two hyperparameters (i.e., damp and the acceptable range), we empirically find that these need not be tuned for different tasks and models. In all of our experiments, we fixed the two hyperparameter values as we mentioned here.  Figure 6: Amount of accuracy degradation from the baseline when pruning BERT BASE using our method and the prior structured pruning methods with different relative FLOPs. Note that our method does not require retraining, whereas all the other methods involve significant retraining overheads as described in Table 2.\n\n\nEvaluation\n\n\nExperimental Setup\n\nOur framework is implemented on top of PyTorch [57] and the HuggingFace Transformers [86] library. We evaluate the effectiveness of our approach using BERT BASE [12] and DistilBERT [63] on GLUE [78] and SQuAD [60,61] benchmarks. We use 2K examples from the training sets for pruning, and we evaluate the resulting models on the development sets. All of the results are averaged over the runs with 10 different seeds. More details on the experimental setup can be found in Section A.5.\n\n\nPerformance Evaluation\n\nFLOPs. Figure 5 shows the accuracy of BERT BASE and DistilBERT with different FLOPs constraints on GLUE and SQuAD datasets. As can be seen in the plots, with only 1% of accuracy drop, BERT BASE achieves 60-70% of the original FLOPs for all tasks. DistilBERT also shows a similar pattern and achieves up to 50% FLOPs reduction (in STS-B and MRPC) even though it is already a compressed architecture. More results using larger sample datasets are provided in Section A.6.\n\nLatency. We further measure the latency on real hardware by pruning BERT BASE with latency constraints and deploying the resulting models on an NVIDIA V100 GPU. Table 1 lists the latency speedup with maximum accuracy drop of 1% for GLUE and SQuAD datasets. With batch size of 256, we achieve speedup of 1.47\u00d7 on average and up to 1.56\u00d7.\n\n\nComparison with the Prior Methods\n\nFLOPs and Accuracy Comparison. Here, we compare our method with the prior structured pruning methods for Transformers including Flop [83], SLIP [44], Sajjad et al. [62], DynaBERT [25], EBERT [49], Block Movement Pruning (BMP) [38], and CoFi [88] by the FLOPs-accuracy tradeoff of BERT BASE on GLUE tasks. We use the results without knowledge distillation and data augmentation reported in each paper. Since the baseline accuracy differs slightly from paper to paper, we compare the amount of the accuracy drop from the baseline instead of the absolute accuracy. The results are plotted as Figure 6. We include the comparison details and full table in Section A.7.\n\nInterestingly, our method exhibits comparable or better results than the prior methods without any model retraining and with substantially lower pruning costs. This empirically demonstrates that retraining and a complex pruning pipeline are not necessary for moderate level of pruning of Transformers. For high sparsity, we find that our framework with retraining works comparably to or better than the prior methods at the same pruning cost (See Section A.8).\n\nRetraining Cost. We select DynaBERT [25], EBERT [49], BMP [38], and CoFi [88] that achieve comparably good accuracy in Figure 6, and we systematically analyze their end-to-end retraining costs on MNLI dataset. As shown in Table 2, these methods require 5\u221233 hours of retraining.\n\nOn the other hand, our method finishes in less than a minute, which is 2\u22123 orders of magnitude faster. We also highlight that this training latency analysis only accounts for a single hyperparameter, and the entire cost should be multiplied by the size of the hyperparameter space. While the prior methods rely on a considerable number of hyperparameters, ours introduce only two hyperparameters (in Section 4.3) which we fix for all experiments. See Section A.9 for more details. Weight magnitude Gradient-based Ours Figure 7: Retraining-free accuracy without (dotted) and with (solid) mask tuning. Table 3: Ablation of our mask search, rearrangement, and tuning methods, described in Section 4. We use BERT BASE as the baseline model, and we prune it with a 60% FLOPs constraint. \n\n\nDiscussion\n\nAblation Studies. Table 3 lists an ablation of the mask rearrangement (Section 4.2) and tuning (Section 4.3) stages for pruned BERT BASE with 60% of FLOPs. We find that both stages help recover the baseline accuracy, and that mask tuning is in particular critical, recovering up to 2.88% accuracy.\n\nTo further investigate the importance of the mask search and rearrangement, we compare the retraining-free performance of the binary masks obtained by our method and other pruning criteria: weight magnitude and the gradient-based method used in DynaBERT. We uniformly pruned the layers using the two criteria with different width multipliers. Figure 7 shows that the two methods significantly degrade the accuracy under the low sparsity regimes. Even with mask tuning, the accuracy is not fully recovered. The results demonstrate that our mask search and re-arrangement are necessary to get optimal binary masks, and that mask tuning is only effective when the binary mask preserves high accuracy. More ablation studies can be found in Section A.11 and Section A.12.\n\nTime Breakdown. We break down our pruning pipeline into 4 parts-gradient computation, mask search, rearrangement, and tuning-and we measure the latency for each stage as Table 5 (Section A.10). For GLUE and SQuAD tasks, our framework finishes in 39 and 135 seconds on average.\n\n\nConclusion\n\nIn this work, we have proposed a novel post-training pruning framework for Transformers that does not require model retraining. The core of our framework is the three-stage decomposition of the pruning process. It uses a fast Fisher-based mask search algorithm to decide which heads/filters to prune, rearranges the pruned heads/filters, and tunes the mask variables to recover the output signal for each layer. We empirically evaluate our framework using BERT BASE and DistilBERT, where our pruning method achieves up to 50% FLOPs reduction within only 1% accuracy degradation on GLUE and SQuAD datasets. This results in up to 1.56\u00d7 latency speedup on an NVIDIA V100 GPU. Importantly, our end-to-end pruning pipeline only needs 39 and 135 seconds for GLUE and SQuAD, which is 2\u22123 orders of magnitude faster than the prior methods. \n\n\nA.3 Derivation of Equation 11\n\nBased on Eq. 5 and the warm-start constraint in Section 4.2, the optimization problem Eq. 4 is written as follows:\narg min m (1 \u2212 m) I(1 \u2212 m),(20)\ns.t. ||m l || 0 = ||m * l || 0 for l = 1, 2, . . . , L,\n\nwhere m * is the mask searched in Section 4.1 using the diagonal approximation of I. Under the block diagonal assumption, Eq. 20 can be reduced as follows:\narg min m (1 \u2212 m) I(1 \u2212 m) \u2248 arg min m L l=1 (1 \u2212 m l ) I l (1 \u2212 m l ),(22)\nwhere I l is the l-th diagonal block of I. Here what we want to show is that the problem Eq. 22 can be solved by independently solving the optimization problem Eq. 11 for each layer. We prove this by contradiction. Suppose thatm = (m 1 , . . . ,m L ) is the mask obtained by solving Eq. 11 for each layer. If there exists a mask m that strictly better optimizes Eq. 22 thanm:\nL l=1 (1 \u2212 m l ) I l (1 \u2212 m l ) < L l=1 (1 \u2212m l ) I l (1 \u2212m l ),(23)\nwhile also satisfying the constraint Eq. 21, then there must exist a layer k such that\n(1 \u2212 m k ) I k (1 \u2212 m k ) < (1 \u2212m k ) I k (1 \u2212m k ).(24)\nHowever, this contradicts the assumption thatm k is the minimizer of Eq. 11 for layer k. Therefore, such a mask as m cannot exist, andm k is the optimal solution for Eq. 22.\n\n\nA.4 Formulating Equation 12 as a Linear Least Squares Problem\n\nFor a MHA layer, the problem of minimizing reconstruction error can be written as follows:\n\narg min\nm MHA l || x + MHA(x; m MHA l ) \u2212 x + MHA(x ; 1) || 2 2 ,(25)s.t. Z(m MHA l ) = Z(m MHA l ),(26)\nwherem is the mask obtained as the result of mask rearrangement (Section 4.2) and Z(m) denotes the indices of zero entries in m. Eq. 26 is the constraint we impose in Section 4.3 that the zero-valued mask variables inm are fixed to 0 so that the tuned mask also satisfies the FLOPs/latency constraint. Then we rewrite the problem as the following linear least squares problem: \narg min m l ||Am l \u2212 b|| 2 2 ,(27)= x + H h=1 Att h (x ) \u2212 x.(28)\nHere, the elements ofm MHA l are multiplied to the matrix A to ensure that the output activations of the pruned heads are not used to reconstruct the original output. Although Eq. 27 has a closed form solution (A A) \u22121 A B, we use the numerical solver in CuPy for higher stability.\n\n\nA.5 Experimental Details\n\n\nA.5.1 Experimental Setup\n\nOur framework is implemented on top of PyTorch v1.9.1 [57] and HuggingFace Transformers v4.12.0 [86]. For the baseline, we downloaded the pre-trained checkpoints from the HuggingFace Transformers repository, and we fine-tuned them on GLUE [78] and SQuAD [61,60] datasets with the standard training recipe. We then use 2K examples from the training sets to prune the baseline models. We report accuracy for GLUE tasks, except for STS-B that we report Spearman Correlation, and F1 score for SQuAD tasks on the development sets. All experiments in this paper are conducted on an AWS p3.2xlarge instance which has an NVIDIA V100 GPU. We used seed numbers from 0 to 9, and we reported the averaged results.\n\n\nA.5.2 Datasets\n\nGLUE tasks [78] include sentence similarity (QQP [30], MRPC [13], STS-B [5]), sentiment classification (SST-2 [69]), textual entailment (RTE [11]) and natural language inference (MNLI [85], QNLI [61]). There are 364K, 4K, 6K, 67K, 3K, 392K, 105K training examples, respectively. We exclude CoLA [84] and WLNI [42] due to their unstable behaviors.   There is a trade-off between sample dataset size and accuracy. Figure 8 shows the correlation between sample size, pruning time, and accuracy for 4 GLUE datasets (with more than 64K training examples). Note that for simplicity we used 2K samples in all our experiments in Section 5. Figure 8 demonstrates that using more examples can improve our accuracy results by up to 0.4% with 2-4\u00d7 longer pruning time (which is still less than 3 minutes).\n\n\nA.7 Details for the Comparison with the Prior Methods\n\nWe compare the FLOPs-accuracy trade-off of our method to the prior structured pruning methods for Transformers in Flop [83], SLIP [44], Sajjad et al. [62], DynaBERT [25], EBERT [49], and Block Movement Pruning (BMP) [38] on 4 GLUE tasks, QQP, QNLI, SST-2, and MRPC. We use the FLOPs and accuracy values reported in each paper (for Flop, we use the values reported in SLIP). To make a fair comparison, we use the experimental results without any additional knowledge distillation or data augmentation in each paper. Because all of these papers have slight differences in their baseline accuracy, it is difficult to directly compare the absolute accuracy for the pruned models. Therefore, we use the amount of the accuracy drop (i.e., accuracy of the pruned model subtracted by the accuracy of the original model) instead. For Flop and SLIP, the results for MRPC are reported as F1 score instead of accuracy. For these cases, we report that the amount of the F1 score drop instead, assuming that it is similar to the amount of the accuracy drop. Since BMP only reports the parameter counts reduction, we directly use this value as FLOPs reduction, under an assumption that the head pruning ratio is similar to the filter pruning ratio. We include the full table in Table 4.\n\n\nA.8 Performance at high sparsity\n\nFor high sparsity, our framework can be used with retraining to recover the accuracy. In this experiment, our framework skips Mask Tuning (Section 4.3) and retrains the model parameters with the binary mask fixed. Specifically, it retrains the pruned BERT BASE model for 3 epochs with the full training dataset with learning rate 2e \u22125 . The retraining cost can be considered the same as that of Sajjad et al. and much lower than DynaBERT [25], EBERT [49], and BMP [38] (more details in Section A.9). Figure 9 shows the FLOPs-accuracy comparison between our method and the prior structured pruning methods at high sparsity. For fair comparison, we used the results without knowledge distillation and data augmentation reported in each paper. For the SST-2 dataset, our method performs comparably to DynaBERT and outperforms all other methods. For the MRPC dataset, our framework consistently outperforms all of the baselines. These results imply that our Mask Search (Section 4.1) and Mask Rearrangement (Section 4.2) find more optimal binary masks than other methods even at high sparsity, which is consistent with Figure 7.\n\n\nA.9 Retraining Costs\n\nDynaBERT [25] consists of 2-stage training, 1 epoch of width and depth-adaptive training followed by additional 3 epochs of final fine-tuning. For the first stage, it jointly trains 12 different width and depth configurations, roughly adding up 12\u00d7 training costs than the normal training. EBERT [49] consists of 2-stage training, 3 epochs that jointly trains the pruning parameters and the model weights, followed by 3 additional epochs of final fine-tuning. BMP [38] requires 20 epochs of training to search for the optimal pruning configuration and retrain the model weights. Similarly, CoFi [88] requires 20 epochs of pruning and 20 epochs of post-pruning fine-tuning. We measure end-to-end retraining latency on an NVIDIA V100 GPU using a batch size of 32 for all experiments.\n\nA.10 Pruning Time Breakdown  Weight magnitude Gradient-based Fisher Figure 11: Pruning performance of our pipeline when different importance metrics are used. Note that the mask re-arrangement stage is included in this experiment.\n\nTo demonstrate the efficacy of the Fisher-based importance score, we compare its performance with two other importance metrics (i.e., weight magnitude and gradient-based), which can be plugged into the mask search algorithm. However, since the mask re-arrangement algorithm needs signals that capture the interaction between mask variables, the mask re-arrangement technique can only be used with the Fisher information matrix. Hence, we designed two experiments: without and with the mask re-arrangement stage. Figure 10 shows the results when the mask re-arrangement is skipped. Figure 11 shows the results when the Fisher-based mask re-arrangement is applied to all. In both experiments, our Fisher importance metric significantly outperforms the two other metrics. To show the effectiveness of the mask search and re-arrangement stages, we compare the performance of our method with uniform Fisher pruning, which prunes every layer with the same sparsity. Mask tuning is applied to both methods. Figure 12 shows that the accuracy of our method significantly outperforms that of uniform pruning by up to 8.6%. The result demonstrates the necessity of our mask search and re-arrangement techniques in finding quality binary masks.\n\n\nA.13 Societal Impacts\n\nWe believe our work would not bring immediate negative impacts on the society, as it aims to accelerate the model inference without affecting the output quality. Our work can partly relieve the environmental concern due to DNN training, as it eliminates the need of re-training after pruning.\n\nFigure 1 :\n1(a) Prior pruning frameworks require additional training on the entire training set and involve user intervention for hyperparameter tuning. This complicates the pruning process and requires a large amount of time (e.g., \u223c30 hours). (b) Our pruning framework does not require retraining. It outputs pruned Transformer models satisfying the FLOPs/latency constraints within considerably less time (e.g., \u223c3 minutes), without user intervention.\n\nFigure 2 :\n2Overview of our pruning framework. (a) The mask variables are initialized as 1. Then they undergo the three-stage pipeline of (b) mask search (Section 4.1), (c) rearrangement (Section 4.2), and (d) tuning (Section 4.3).\n\nFigure 1 (\n1b) andFigure 2illustrate the overview of our framework.\n\n\nn] = (HI, FI) 9: end for 10: n * = arg min n S[n] optimal # remaining heads 11: HI * , FI * = R[n * ] indicies of heads/filters to prune 12: Initialize m MHA and m FFN as 1 13: m MHA [HI * ] = 0 prune the selected heads 14: m FFN [FI * ] = 0 prune the selected filters Output: m * = (m MHA , m FFN )\n\nFigure 3 :Figure 4 :\n34(Left) Real latency of a single FFN layer with different numbers of remaining filters. (Right) Schematic plot for the approximated latency as a piece-wise linear function. Illustration of the block diagonal Fisher matrix.\n\nFigure 5 :\n5Accuracy of our pruning method applied to BERT BASE and DistilBERT with different FLOPs constraints. The dashed horizontal lines indicate 1% accuracy drop from the baseline models.\n\n\nLatency constraint C, LAT function parameters (ahead,chead,Thead),(afilter,cfilter,Tfilter), diagonal Fisher information matrix I 1: HI = indicies of Thead most important heads in each layer 2: FI = indicies of Tfilter most important filters in each layer 3: H = H \u2212 Thead 4: N = N \u2212 Tfilter 5: C = C \u2212 L(chead + cfilter) 6: for n = 0 to LH do 7: I = indicies of n most important heads not in HI 8: f = (C \u2212 nahead)/afilter 9: J = indicies of f most important filters not in FI 10: HI = HI \u222a I 11: FI = FI \u222a J 12: S[n] = i\u2208HI \u222aFI Iii 13: R[n] = (HI , FI ) 14: end for 15: n * = arg max n S[n] 16: HI * , FI * = R[n * ] 17: Initialize m MHA and m FFN as 0 18: m MHA [HI * ] = 1 19: m FFN [FI * ] = 1 Output: m * = (m MHA , m FFN )\n\n\nwhere A := [m MHA l,1 Att 1 (x), . . . ,m MHA l,H Att H (x)] and b :\n\n\nSQuAD 1.1[61] and SQuAD 2.0[60] are question and answering tasks, each of which contains 88K and 130K training examples. SQuAD 2.0 is an extension of SQuAD 1.1 by including unanswerable questions whose answers are not stated in the given contexts.\n\nFigure 8 :\n8Accuracy and pruning time with 2K, 4K, 8K samples. The FLOPs constraint is 60%.\n\nFigure 9 :\n9Performance comparison at high sparsity. Each method (including ours) retrains the pruned models without data augmentation and knowledge distillation.\n\nFigure 10 :\n10Pruning performance of our pipeline when different importance metrics are used. Note that the mask re-arrangement stage is skipped in this experiment.\n\nFigure 12 :\n12Performance of our method and Fisher-based uniform pruning. Mask tuning is applied to both methods.\n\nTable 1 :\n1Latency speedup of BERT BASE on a single NVIDIA V100 GPU with different batch sizes. The latency is measured using PyTorch. We constrain the accuracy degradation to be at most 1% from the baseline accuracy, and we report the largest speedup among those that satisfy the constraint.Batch size MNLI QQP QNLI SST-2 STS-B MRPC SQuAD1.1 SQuAD2.0 Geo. mean \n\n32 \n1.27\u00d7 1.42\u00d7 1.42\u00d7 1.23\u00d7 1.34\u00d7 1.36\u00d7 \n1.33\u00d7 \n1.37\u00d7 \n1.34\u00d7 \n256 \n1.34\u00d7 1.54\u00d7 1.53\u00d7 1.56\u00d7 1.54\u00d7 1.55\u00d7 \n1.34\u00d7 \n1.40\u00d7 \n1.47\u00d7 \n\n\n\nTable 2 :\n2Pruning cost comparison between the prior structured pruning methods and ours. We compare the number of training epochs and the end-to-end (E2E) time required for pruning.# Epochs E2E time (hr) \n\nDynaBERT [25] \n4 \n12 \nEBERT [49] \n6 \n5 \nBMP [38] \n20 \n17 \nCoFi [88] \n40 \n33 \n\nOurs \n0 \n0.01 \n\n0.50 \n0.58 \n0.66 \n0.75 \n0.83 \n\nRelative FLOPs \n\n60 \n\n70 \n\n80 \n\n90 \n\nAccuracy \n\nQQP (BERT-base) \n\n\n\nTable 4 :\n4The absolute accuracy and the amount of accuracy degradation from the baseline after pruning BERT BASE using our method and the prior structured pruning methods with different relative FLOPs. \u2020 Reported as F1 score instead of accuracy. FLOPs QQP QNLI SST-2 MRPC QQP QNLI SST-2 MRPCMethod \nRel. \nAccuracy \nAccuracy Diff \nFlop [83] \nBaseline \n-\n91.6 \n92.7 \n90.9  \u2020 \n-\n0 \n0 \n0 \n66.7 \n-\n89.0 \n92.1 \n88.6  \u2020 \n-\n-2.6 \n-0.6 \n-2.3 \n\nSLIP [44] \nBaseline 90.6 \n91.6 \n92.7 \n90.9  \u2020 \n0 \n0 \n0 \n0 \n65.6 \n89.7 \n90.7 \n91.7 \n89.9  \u2020 \n-0.9 \n-0.9 \n-1.0 \n-1.0 \n61.5 \n88.9 \n89.5 \n91.8 \n88.1  \u2020 \n-1.7 \n-2.1 \n-0.9 \n-2.8 \n\nSajjad et al. [62] Baseline 91.1 \n91.1 \n92.4 \n88.0 \n0 \n0 \n0 \n0 \n66.7 \n90.6 \n89.7 \n90.6 \n79.4 \n-0.4 \n-1.4 \n-1.8 \n-8.6 \n50.0 \n90.4 \n87.6 \n90.3 \n80.2 \n-0.7 \n-3.5 \n-2.2 \n-7.8 \n\nDynaBERT [25] Baseline \n-\n-\n92.9 \n87.7 \n-\n-\n0 \n0 \n75.0 \n-\n-\n92.3 \n86.0 \n-\n-\n-0.6 \n-1.7 \n50.0 \n-\n-\n91.9 \n86.0 \n-\n-\n-1.0 \n-1.7 \n\nEBERT [49] \nBaseline 87.9 \n91.5 \n93.2 \n-\n0 \n0 \n0 \n-\n60.0 \n87.5 \n90.2 \n92.2 \n-\n-0.4 \n-1.3 \n-1.0 \n-\n50.0 \n87.0 \n89.6 \n91.6 \n-\n-0.7 \n-1.9 \n-1.6 \n-\n\nBMP [38] \nBaseline 91.1 \n-\n92.7 \n-\n0 \n-\n0 \n-\n50.0 \n90.4 \n-\n90.7 \n-\n-0.7 \n-\n-2.0 \n-\n\nBMP [38] \nBaseline \n-\n-\n92.7 \n-\n-\n-\n0 \n-\n(Reproduced) \n72.6 \n-\n-\n92.1 \n-\n-\n-\n-0.6 \n-\n56.5 \n-\n-\n91.5 \n-\n-\n-\n-1.2 \n-\n46.7 \n-\n-\n90.8 \n-\n-\n-\n-1.9 \n-\n\nOurs \nBaseline 91.0 \n91.4 \n93.6 \n86.3 \n0 \n0 \n0 \n0 \n70.0 \n90.7 \n90.9 \n93.0 \n86.1 \n-0.3 \n-0.5 \n-0.6 \n-0.2 \n60.0 \n90.4 \n90.0 \n92.5 \n85.3 \n-0.6 \n-1.4 \n-1.1 \n-1.0 \n50.0 \n89.5 \n88.7 \n91.6 \n83.2 \n-1.5 \n-2.7 \n-2.0 \n-3.1 \n\n\n\nTable 5 :\n5Time breakdown (in percentage) of our pruning framework on a single NVIDIA V100 GPU. It consists of Gradient Computation (GC), Mask Search (MS, Section 4.1), Mask Rearrangement (MR, Section 4.2), and Mask Tuning (MT, Section 4.3). In the last column, we provide the total amount of time for end-to-end pruning, in seconds. SQuAD 20.5% 0.1% 3.5% 76.0% 135.1A.11 Efficacy of the Fisher-based Importance MetricGC \nMS \nMR \nMT \nTotal (s) \n\nGLUE \n23.8% 0.3% 9.4% 66.5% \n39.3 \n0.5 \n0.6 \n0.7 \n0.8 \n0.9 \n\nRelative FLOPs \n\n\nAcknowledgments and Disclosure of FundingThe authors would like to thank Suhong Moon who helped with brainstorming. We also acknowledge gracious support from Google Cloud, Google TRC team, and specifically Jonathan Caton, Prof. David Patterson, and Jing Li. Prof. Keutzer's lab is sponsored by Samsung, Intel corporation, Intel VLAB team, Intel One-API center of excellence, as well as funding through BDD and BAIR. Woosuk Kwon and Sehoon Kim acknowledge the support from Korea Foundation for Advanced Studies. Amir Gholami was supported through funding from Samsung SAIT. Michael W. Mahoney would also like to acknowledge the UC Berkeley CLTC, ARO, NSF, and ONR. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred.A AppendixA.1 Proof of Equation 9We prove Eq. 9 by contradiction. Suppose that there exists a mask m such that i\u2208Z(m)Iii,(13)where the mask m * is the output of Algorithm 1. Let h = ||m MHA || 0 , i.e. the total number of heads in the architecture pruned by the mask m. Then we construct a new mask m as follows:1. Keep the mask for MHA layers. That is, m MHA = m MHA .2. Construct m FFN as in the inner statements of the for loop (line 1 of Algorithm 1). That is, given a mask initialized to 1, we zero out the k filter mask variables with the least importantObviously, the mask m satisfies the FLOPs constraint Eq. 8. Moreover, as the mask m prunes the least important filters, the following two inequalities hold:i\u2208Z(m )Then we construct another mask m from m such that:i\u2208Z(m )Essentially, m is the mask when n (in line 1 of Algorithm 1) is h. As Algorithm 1 finds the minimum by iterating different values of n, the following inequalities hold:Finally, the above inequalities are combined as follows:which contradicts Eq. 13. Thus, the output mask m * of Algorithm 1 is the minimizer of Eq. 8.A.2 Latency-aware Search AlgorithmAlgorithm 2 is our proposed algorithm for latency-aware mask search, which extends Algorithm 1. It takes as inputs the given latency constraint, approximated LAT functions for MHA and FFN layers, and the diagonal Fisher information matrix I. Overall, Algorithm 2 has the same structure as Algorithm 1. A notable difference between the two is that Algorithm 2 separately considers the constant part (i.e., when the number of heads/filters is below the threshold T ) in line 1-5. Another difference is that Algorithm 2 uses a head and a filter instead of F head and F filter in Algorithm 1.\nKnapsack pruning with inner distillation. Yonathan Aflalo, Asaf Noy, Ming Lin, Itamar Friedman, Lihi Zelnik, arXiv:2002.08258arXiv preprintYonathan Aflalo, Asaf Noy, Ming Lin, Itamar Friedman, and Lihi Zelnik. Knapsack pruning with inner distillation. arXiv preprint arXiv:2002.08258, 2020.\n\nAbdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. Alexei Baevski, Henry Zhou, arXiv:2006.11477arXiv preprintAlexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. arXiv preprint arXiv:2006.11477, 2020.\n\nPost-training 4-bit quantization of convolution networks for rapid-deployment. Ron Banner, Yury Nahshan, Elad Hoffer, Daniel Soudry, arXiv:1810.05723arXiv preprintRon Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry. Post-training 4-bit quantization of convolution networks for rapid-deployment. arXiv preprint arXiv:1810.05723, 2018.\n\nLanguage models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:2005.14165arXiv preprintTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n\nSemeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, Lucia Specia, arXiv:1708.00055arXiv preprintDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017.\n\nDaoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding, Hongbo Deng, Jun Huang, Wei Lin, Jingren Zhou, arXiv:2001.04246Adabert: Task-adaptive bert compression with differentiable neural architecture search. arXiv preprintDaoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding, Hongbo Deng, Jun Huang, Wei Lin, and Jingren Zhou. Adabert: Task-adaptive bert compression with differentiable neural architecture search. arXiv preprint arXiv:2001.04246, 2020.\n\nLarge-scale self-supervised pre-training for full stack speech processing. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, arXiv:2110.13900arXiv preprintSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. arXiv preprint arXiv:2110.13900, 2021.\n\nChasing sparsity in vision transformers: An end-to-end exploration. Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, Zhangyang Wang, Advances in Neural Information Processing Systems. 34Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, and Zhangyang Wang. Chasing sparsity in vision transformers: An end-to-end exploration. Advances in Neural Information Processing Systems, 34:19974-19988, 2021.\n\nThe lottery ticket hypothesis for pre-trained BERT networks. Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, Michael Carbin, arXiv:2007.12223arXiv preprintTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and Michael Carbin. The lottery ticket hypothesis for pre-trained BERT networks. arXiv preprint arXiv:2007.12223, 2020.\n\nXiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, Jingjing Liu, Earlybert, arXiv:2101.00063Efficient bert training via early-bird lottery tickets. arXiv preprintXiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, and Jingjing Liu. Earlybert: Efficient bert training via early-bird lottery tickets. arXiv preprint arXiv:2101.00063, 2020.\n\nThe pascal recognising textual entailment challenge. Oren Ido Dagan, Bernardo Glickman, Magnini, Machine Learning Challenges Workshop. SpringerIdo Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, pages 177-190. Springer, 2005.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec- tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nAutomatically constructing a corpus of sentential paraphrases. B William, Chris Dolan, Brockett, Proceedings of the Third International Workshop on Paraphrasing (IWP2005). the Third International Workshop on Paraphrasing (IWP2005)William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, arXiv:2010.11929An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprintAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nAngela Fan, Edouard Grave, Armand Joulin, arXiv:1909.11556Reducing transformer depth on demand with structured dropout. arXiv preprintAngela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. arXiv preprint arXiv:1909.11556, 2019.\n\nJonathan Frankle, Michael Carbin, arXiv:1803.03635The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprintJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.\n\nElias Frantar, Dan Alistarh, arXiv:2201.13096Spdy: Accurate pruning with speedup guarantees. arXiv preprintElias Frantar and Dan Alistarh. Spdy: Accurate pruning with speedup guarantees. arXiv preprint arXiv:2201.13096, 2022.\n\nTrevor Gale, Erich Elsen, Sara Hooker, arXiv:1902.09574The state of sparsity in deep neural networks. arXiv preprintTrevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574, 2019.\n\n. Google Tensorflow Lite, Google. Tensorflow Lite: https://www.tensorflow.org/lite, 2017.\n\nAccelerating sparse dnn models without hardware-support via tile-wise sparsity. Cong Guo, Bo Yang Hsueh, Jingwen Leng, Yuxian Qiu, Yue Guan, Zehuan Wang, Xiaoying Jia, Xipeng Li, Minyi Guo, Yuhao Zhu, SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEECong Guo, Bo Yang Hsueh, Jingwen Leng, Yuxian Qiu, Yue Guan, Zehuan Wang, Xiaoying Jia, Xipeng Li, Minyi Guo, and Yuhao Zhu. Accelerating sparse dnn models without hardware-support via tile-wise sparsity. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-15. IEEE, 2020.\n\nA\u02c63: Accelerating attention mechanisms in neural networks with approximation. Tae Jun Ham, Sung Jun Jung, Seonghak Kim, H Young, Yeonhong Oh, Yoonho Park, Jung-Hun Song, Sanghee Park, Kyoung Lee, Jae W Park, Lee, 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEETae Jun Ham, Sung Jun Jung, Seonghak Kim, Young H Oh, Yeonhong Park, Yoonho Song, Jung-Hun Park, Sanghee Lee, Kyoung Park, Jae W Lee, et al. A\u02c63: Accelerating attention mechanisms in neural networks with approximation. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA), pages 328-341. IEEE, 2020.\n\nElsa: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks. Yejin Tae Jun Ham, Lee, Soosung Seong Hoon Seo, Hyunji Kim, Sung Jun Choi, Jae W Jung, Lee, 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA). IEEETae Jun Ham, Yejin Lee, Seong Hoon Seo, Soosung Kim, Hyunji Choi, Sung Jun Jung, and Jae W Lee. Elsa: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks. In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA), pages 692-705. IEEE, 2021.\n\nChannel pruning for accelerating very deep neural networks. Yihui He, Xiangyu Zhang, Jian Sun, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionYihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In Proceedings of the IEEE international conference on computer vision, pages 1389-1397, 2017.\n\nGaussian error linear units (gelus). Dan Hendrycks, Kevin Gimpel, arXiv:1606.08415arXiv preprintDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.\n\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu, Dynabert, arXiv:2004.04037Dynamic bert with adaptive width and depth. arXiv preprintLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Dynabert: Dynamic bert with adaptive width and depth. arXiv preprint arXiv:2004.04037, 2020.\n\nHubert: Self-supervised speech representation learning by masked prediction of hidden units. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, arXiv:2106.07447arXiv preprintWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. arXiv preprint arXiv:2106.07447, 2021.\n\nImproving post training neural quantization: Layer-wise calibration and integer programming. Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, Daniel Soudry, arXiv:2006.10518arXiv preprintItay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Improving post training neural quantization: Layer-wise calibration and integer programming. arXiv preprint arXiv:2006.10518, 2020.\n\nN Forrest, Albert E Iandola, Ravi Shaw, Kurt W Krishna, Keutzer, arXiv:2006.11316Squeezebert: What can computer vision teach nlp about efficient neural networks. arXiv preprintForrest N Iandola, Albert E Shaw, Ravi Krishna, and Kurt W Keutzer. Squeezebert: What can computer vision teach nlp about efficient neural networks? arXiv preprint arXiv:2006.11316, 2020.\n\n. Intel, Openvino, Intel. OpenVINO: https://docs.openvino.ai/latest/index.html, 2021.\n\nURL https://data. quora. com/First-Quora-Dataset-Release-Question-Pairs. Shankar Iyer, Nikhil Dandekar, Kornl Csernai, First quora dataset release: Question pairsShankar Iyer, Nikhil Dandekar, and Kornl Csernai. First quora dataset release: Question pairs.(2017). URL https://data. quora. com/First-Quora-Dataset-Release-Question-Pairs, 2017.\n\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu, Tinybert, arXiv:1909.10351Distilling bert for natural language understanding. arXiv preprintXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351, 2019.\n\nAshish Khetan, Zohar Karnin, Schubert, arXiv:2005.06628Optimizing elements of bert. arXiv preprintAshish Khetan and Zohar Karnin. schubert: Optimizing elements of bert. arXiv preprint arXiv:2005.06628, 2020.\n\nSehoon Kim, Amir Gholami, Zhewei Yao, W Michael, Kurt Mahoney, Keutzer, arXiv:2101.01321Integer-only bert quantization. arXiv preprintSehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer-only bert quantization. arXiv preprint arXiv:2101.01321, 2021.\n\nNeuron merging: Compensating for pruned neurons. Woojeong Kim, Suhyun Kim, Mincheol Park, Geonseok Jeon, arXiv:2010.13160arXiv preprintWoojeong Kim, Suhyun Kim, Mincheol Park, and Geonseok Jeon. Neuron merging: Compensating for pruned neurons. arXiv preprint arXiv:2010.13160, 2020.\n\nReformer: The efficient transformer. Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya, International Conference on Learning Representations. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019.\n\nThe optimal bert surgeon: Scalable and accurate second-order pruning for large language models. Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael Goin, Dan Alistarh, arXiv:2203.07259arXiv preprintEldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael Goin, and Dan Alistarh. The optimal bert surgeon: Scalable and accurate second-order pruning for large language models. arXiv preprint arXiv:2203.07259, 2022.\n\nWoosuk Kwon, Gyeong-In Yu, Eunji Jeong, Byung-Gon Chun, arXiv:2012.02732Nimble: Lightweight and parallel gpu task scheduling for deep learning. arXiv preprintWoosuk Kwon, Gyeong-In Yu, Eunji Jeong, and Byung-Gon Chun. Nimble: Lightweight and parallel gpu task scheduling for deep learning. arXiv preprint arXiv:2012.02732, 2020.\n\nFran\u00e7ois Lagunas, Ella Charlaix, Victor Sanh, Alexander M Rush, arXiv:2109.04838Block pruning for faster transformers. arXiv preprintFran\u00e7ois Lagunas, Ella Charlaix, Victor Sanh, and Alexander M Rush. Block pruning for faster transformers. arXiv preprint arXiv:2109.04838, 2021.\n\nAlbert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, arXiv:1909.11942arXiv preprintZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Al- bert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\n\nPost-training deep neural network pruning via layer-wise calibration. Ivan Lazarevich, Alexander Kozlov, Nikita Malinin, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionIvan Lazarevich, Alexander Kozlov, and Nikita Malinin. Post-training deep neural network pruning via layer-wise calibration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 798-805, 2021.\n\nOptimal brain damage. Yann Lecun, S John, Sara A Denker, Solla, Advances in neural information processing systems. Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural information processing systems, pages 598-605, 1990.\n\nThe winograd schema challenge. Hector Levesque, Ernest Davis, Leora Morgenstern, Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning. CiteseerHector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning. Citeseer, 2012.\n\nEfficient transformer-based large scale language representations using hardware-friendly block structured pruning. Bingbing Li, Zhenglun Kong, Tianyun Zhang, Ji Li, Zhengang Li, Hang Liu, Caiwen Ding, arXiv:2009.08065arXiv preprintBingbing Li, Zhenglun Kong, Tianyun Zhang, Ji Li, Zhengang Li, Hang Liu, and Caiwen Ding. Efficient transformer-based large scale language representations using hardware-friendly block structured pruning. arXiv preprint arXiv:2009.08065, 2020.\n\nPruning redundant mappings in transformer models via spectral-normalized identity prior. Zi Lin, Jeremiah Zhe Liu, Zi Yang, Nan Hua, Dan Roth, arXiv:2010.01791arXiv preprintZi Lin, Jeremiah Zhe Liu, Zi Yang, Nan Hua, and Dan Roth. Pruning redundant mappings in transformer models via spectral-normalized identity prior. arXiv preprint arXiv:2010.01791, 2020.\n\nGroup fisher pruning for practical network compression. Liyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun Zhou, Jing-Hao Xue, Xinjiang Wang, Yimin Chen, Wenming Yang, Qingmin Liao, Wayne Zhang, International Conference on Machine Learning. PMLRLiyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun Zhou, Jing-Hao Xue, Xinjiang Wang, Yimin Chen, Wenming Yang, Qingmin Liao, and Wayne Zhang. Group fisher pruning for practical network compression. In International Conference on Machine Learning, pages 7021-7032. PMLR, 2021.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nRosita: Refined bert compression with integrated techniques. Yuanxin Liu, Zheng Lin, Fengcheng Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Yuanxin Liu, Zheng Lin, and Fengcheng Yuan. Rosita: Refined bert compression with integrated techniques. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8715-8722, 2021.\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, arXiv:2103.14030arXiv preprintZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.\n\nEbert: Efficient bert inference with dynamic structured pruning. Zejian Liu, Fanrong Li, Gang Li, Jian Cheng, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Zejian Liu, Fanrong Li, Gang Li, and Jian Cheng. Ebert: Efficient bert inference with dynamic structured pruning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4814- 4823, 2021.\n\nRammer: Enabling holistic deep learning compiler optimizations with rtasks. Lingxiao Ma, Zhiqiang Xie, Zhi Yang, Jilong Xue, Youshan Miao, Wei Cui, Wenxiang Hu, Fan Yang, Lintao Zhang, Lidong Zhou, 14th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 20). Lingxiao Ma, Zhiqiang Xie, Zhi Yang, Jilong Xue, Youshan Miao, Wei Cui, Wenxiang Hu, Fan Yang, Lintao Zhang, and Lidong Zhou. Rammer: Enabling holistic deep learning compiler optimizations with rtasks. In 14th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 20), pages 881-897, 2020.\n\nAre sixteen heads really better than one?. Paul Michel, Omer Levy, Graham Neubig, arXiv:1905.10650arXiv preprintPaul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? arXiv preprint arXiv:1905.10650, 2019.\n\nImportance estimation for neural network pruning. Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, Jan Kautz, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11264-11272, 2019.\n\nBen Mussay, Margarita Osadchy, Vladimir Braverman, Samson Zhou, Dan Feldman, arXiv:1907.04018Data-independent neural pruning via coresets. arXiv preprintBen Mussay, Margarita Osadchy, Vladimir Braverman, Samson Zhou, and Dan Feldman. Data-independent neural pruning via coresets. arXiv preprint arXiv:1907.04018, 2019.\n\nUp or down? adaptive rounding for post-training quantization. Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, Tijmen Blankevoort, International Conference on Machine Learning. PMLRMarkus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197-7206. PMLR, 2020.\n\nCupy: A numpy-compatible library for nvidia gpu calculations. 31st confernce on neural information processing systems. Royud Nishino, Shohei Hido Crissman Loomis, 151ROYUD Nishino and Shohei Hido Crissman Loomis. Cupy: A numpy-compatible library for nvidia gpu calculations. 31st confernce on neural information processing systems, 151, 2017.\n\n. Nvidia, Tensorrt, NVIDIA. TensorRT: https://developer.nvidia.com/tensorrt, 2018.\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. 32Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32:8026-8037, 2019.\n\nWhen BERT plays the lottery, all tickets are winning. Sai Prasanna, Anna Rogers, Anna Rumshisky, arXiv:2005.00561arXiv preprintSai Prasanna, Anna Rogers, and Anna Rumshisky. When BERT plays the lottery, all tickets are winning. arXiv preprint arXiv:2005.00561, 2020.\n\nPerformance aware convolutional neural network channel pruning for embedded gpus. Valentin Radu, Kuba Kaszyk, Yuan Wen, Jack Turner, Jos\u00e9 Cano, J Elliot, Bj\u00f6rn Crowley, Amos Franke, Michael O&apos; Storkey, Boyle, 2019 IEEE International Symposium on Workload Characterization (IISWC). IEEEValentin Radu, Kuba Kaszyk, Yuan Wen, Jack Turner, Jos\u00e9 Cano, Elliot J Crowley, Bj\u00f6rn Franke, Amos Storkey, and Michael O'Boyle. Performance aware convolutional neural network channel pruning for embedded gpus. In 2019 IEEE International Symposium on Workload Characterization (IISWC), pages 24-34. IEEE, 2019.\n\nPranav Rajpurkar, Robin Jia, Percy Liang, arXiv:1806.03822Know what you don't know: Unanswerable questions for squad. arXiv preprintPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.\n\nSQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, arXiv:1606.05250arXiv preprintPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n\nOn the effect of dropping layers of pre-trained transformer models. Hassan Sajjad, Fahim Dalvi, Nadir Durrani, Preslav Nakov, arXiv:2004.03844arXiv preprintHassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. On the effect of dropping layers of pre-trained transformer models. arXiv preprint arXiv:2004.03844, 2020.\n\nDistilbert, a distilled version of bert: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, arXiv:1910.01108arXiv preprintVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n\nVictor Sanh, Thomas Wolf, Alexander M Rush, arXiv:2005.07683Movement pruning: Adaptive sparsity by fine-tuning. arXiv preprintVictor Sanh, Thomas Wolf, and Alexander M Rush. Movement pruning: Adaptive sparsity by fine-tuning. arXiv preprint arXiv:2005.07683, 2020.\n\nMaying Shen, Hongxu Yin, Pavlo Molchanov, Lei Mao, Jianna Liu, Jose M Alvarez, arXiv:2110.10811Halp: Hardwareaware latency pruning. arXiv preprintMaying Shen, Hongxu Yin, Pavlo Molchanov, Lei Mao, Jianna Liu, and Jose M Alvarez. Halp: Hardware- aware latency pruning. arXiv preprint arXiv:2110.10811, 2021.\n\nQ-bert: Hessian based ultra low precision quantization of bert. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, W Michael, Kurt Mahoney, Keutzer, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8815-8821, 2020.\n\nThe evolved transformer. David So, Quoc Le, Chen Liang, International Conference on Machine Learning. PMLRDavid So, Quoc Le, and Chen Liang. The evolved transformer. In International Conference on Machine Learning, pages 5877-5886. PMLR, 2019.\n\nPrimer: Searching for efficient transformers for language modeling. Wojciech David R So, Hanxiao Ma\u0144ke, Zihang Liu, Noam Dai, Quoc V Shazeer, Le, arXiv:2109.08668arXiv preprintDavid R So, Wojciech Ma\u0144ke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V Le. Primer: Searching for efficient transformers for language modeling. arXiv preprint arXiv:2109.08668, 2021.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, D Christopher, Manning, Y Andrew, Christopher Ng, Potts, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processingRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631-1642, 2013.\n\nSuraj Srinivas, Venkatesh Babu, arXiv:1507.06149Data-free parameter pruning for deep neural networks. arXiv preprintSuraj Srinivas and R Venkatesh Babu. Data-free parameter pruning for deep neural networks. arXiv preprint arXiv:1507.06149, 2015.\n\nSiqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu, arXiv:1908.09355Patient knowledge distillation for bert model compression. arXiv preprintSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model compression. arXiv preprint arXiv:1908.09355, 2019.\n\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou, arXiv:2004.02984Mobilebert: a compact task-agnostic bert for resource-limited devices. arXiv preprintZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert: a compact task-agnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984, 2020.\n\nEdgebert: Sentence-level energy optimizations for latency-aware multi-task nlp inference. Thierry Tambe, Coleman Hooper, Lillian Pentecost, Tianyu Jia, En-Yu Yang, Marco Donato, Victor Sanh, Paul Whatmough, Alexander M Rush, David Brooks, MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture. Thierry Tambe, Coleman Hooper, Lillian Pentecost, Tianyu Jia, En-Yu Yang, Marco Donato, Victor Sanh, Paul Whatmough, Alexander M Rush, David Brooks, et al. Edgebert: Sentence-level energy optimiza- tions for latency-aware multi-task nlp inference. In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture, pages 830-844, 2021.\n\nFaster gaze prediction with dense networks and fisher pruning. Lucas Theis, Iryna Korshunova, Alykhan Tejani, Ferenc Husz\u00e1r, arXiv:1801.05787arXiv preprintLucas Theis, Iryna Korshunova, Alykhan Tejani, and Ferenc Husz\u00e1r. Faster gaze prediction with dense networks and fisher pruning. arXiv preprint arXiv:1801.05787, 2018.\n\nTraining data-efficient image transformers & distillation through attention. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou, International Conference on Machine Learning. PMLRHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347-10357. PMLR, 2021.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.\n\nAnalyzing multi-head selfattention: Specialized heads do the heavy lifting, the rest can be pruned. Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov, arXiv:1905.09418arXiv preprintElena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self- attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418, 2019.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, arXiv:1804.07461GLUE: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprintAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\n\nHat: Hardware-aware transformers for efficient natural language processing. Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, Song Han, arXiv:2005.14187arXiv preprintHanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, and Song Han. Hat: Hardware-aware transformers for efficient natural language processing. arXiv preprint arXiv:2005.14187, 2020.\n\nSpatten: Efficient sparse attention architecture with cascade token and head pruning. Hanrui Wang, Zhekai Zhang, Song Han, 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEEHanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cas- cade token and head pruning. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pages 97-110. IEEE, 2021.\n\nLinformer: Self-attention with linear complexity. Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, Hao Ma, arXiv:2006.04768arXiv preprintSinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.\n\nMinilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, Ming Zhou, arXiv:2002.10957arXiv preprintWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. arXiv preprint arXiv:2002.10957, 2020.\n\nStructured pruning of large language models. Ziheng Wang, Jeremy Wohlwend, Tao Lei, arXiv:1910.04732arXiv preprintZiheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models. arXiv preprint arXiv:1910.04732, 2019.\n\nNeural network acceptability judgments. Alex Warstadt, Amanpreet Singh, Samuel R Bowman, Transactions of the Association for Computational Linguistics. 7Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625-641, 2019.\n\nA broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel R Bowman, arXiv:1704.05426arXiv preprintAdina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.\n\nTransformers: State-of-the-art natural language processing. Thomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony Moi, Pierric Cistac, Morgan Funtowicz, Joe Davison, Sam Shleifer, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsThomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony Moi, Pierric Cistac, Morgan Funtowicz, Joe Davison, Sam Shleifer, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, 2020.\n\nLite transformer with long-short range attention. Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, Song Han, arXiv:2004.11886arXiv preprintZhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite transformer with long-short range attention. arXiv preprint arXiv:2004.11886, 2020.\n\nStructured pruning learns compact and accurate models. Mengzhou Xia, Zexuan Zhong, Danqi Chen, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsLong Papers1Mengzhou Xia, Zexuan Zhong, and Danqi Chen. Structured pruning learns compact and accurate models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1513-1528, 2022.\n\nDeebert: Dynamic early exiting for accelerating bert inference. Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early exiting for accelerating bert inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2246-2251, 2020.\n\nJin Xu, Xu Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, Tie-Yan Liu, arXiv:2105.14444Nas-bert: Task-agnostic and adaptive-size bert compression with neural architecture search. arXiv preprintJin Xu, Xu Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, and Tie-Yan Liu. Nas-bert: Task-agnostic and adaptive-size bert compression with neural architecture search. arXiv preprint arXiv:2105.14444, 2021.\n\nXlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, R Russ, Quoc V Salakhutdinov, Le, Advances in neural information processing systems. 32Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019.\n\nMlpruning: A multilevel structured pruning framework for transformer-based models. Zhewei Yao, Linjian Ma, Sheng Shen, Kurt Keutzer, Michael W Mahoney, arXiv:2105.14636arXiv preprintZhewei Yao, Linjian Ma, Sheng Shen, Kurt Keutzer, and Michael W Mahoney. Mlpruning: A multilevel structured pruning framework for transformer-based models. arXiv preprint arXiv:2105.14636, 2021.\n\nAutotinybert: Automatic hyper-parameter optimization for efficient pre-trained language models. Yichun Yin, Cheng Chen, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu, arXiv:2107.13686arXiv preprintYichun Yin, Cheng Chen, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Autotinybert: Automatic hyper-parameter optimization for efficient pre-trained language models. arXiv preprint arXiv:2107.13686, 2021.\n\nRed: Looking for redundancies for data-freestructured compression of deep neural networks. Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, Kevin Bailly, Advances in Neural Information Processing Systems. 34Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Red: Looking for redundancies for data-freestructured compression of deep neural networks. Advances in Neural Information Processing Systems, 34:20863-20873, 2021.\n\nGobo: Quantizing attentionbased nlp models for low latency and energy efficient inference. Ali Hadi Zadeh, Isak Edo, 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEEOmar Mohamed Awad, and Andreas MoshovosAli Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. Gobo: Quantizing attention- based nlp models for low latency and energy efficient inference. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 811-824. IEEE, 2020.\n\nOfir Zafrir, Guy Boudoukh, Peter Izsak, Moshe Wasserblat, arXiv:1910.06188Q8bert: Quantized 8bit bert. arXiv preprintOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. arXiv preprint arXiv:1910.06188, 2019.\n\nImproving neural network quantization without retraining using outlier channel splitting. Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang, Proceedings of Machine Learning Research. Machine Learning ResearchRitchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. Proceedings of Machine Learning Research, 2019.\n\nBert loses patience: Fast and robust inference with early exit. Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian Mcauley, Ke Xu, Furu Wei, Advances in Neural Information Processing Systems. 33Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and Furu Wei. Bert loses patience: Fast and robust inference with early exit. Advances in Neural Information Processing Systems, 33:18330-18341, 2020.\n", "annotations": {"author": "[{\"end\":96,\"start\":59},{\"end\":131,\"start\":97},{\"end\":173,\"start\":132},{\"end\":211,\"start\":174},{\"end\":246,\"start\":212},{\"end\":280,\"start\":247},{\"end\":303,\"start\":281},{\"end\":329,\"start\":304},{\"end\":377,\"start\":330}]", "publisher": null, "author_last_name": "[{\"end\":70,\"start\":66},{\"end\":107,\"start\":104},{\"end\":149,\"start\":142},{\"end\":188,\"start\":181},{\"end\":224,\"start\":217},{\"end\":259,\"start\":252}]", "author_first_name": "[{\"end\":65,\"start\":59},{\"end\":103,\"start\":97},{\"end\":139,\"start\":132},{\"end\":141,\"start\":140},{\"end\":180,\"start\":174},{\"end\":216,\"start\":212},{\"end\":251,\"start\":247}]", "author_affiliation": "[{\"end\":302,\"start\":282},{\"end\":328,\"start\":305},{\"end\":376,\"start\":331}]", "title": "[{\"end\":56,\"start\":1},{\"end\":433,\"start\":378}]", "venue": null, "abstract": "[{\"end\":1767,\"start\":435}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b75\"},\"end\":1816,\"start\":1812},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1901,\"start\":1898},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1904,\"start\":1901},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":1907,\"start\":1904},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1981,\"start\":1977},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":1984,\"start\":1981},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":1987,\"start\":1984},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2014,\"start\":2011},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2016,\"start\":2014},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2019,\"start\":2016},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2591,\"start\":2587},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":2594,\"start\":2591},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4022,\"start\":4019},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4025,\"start\":4022},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":4028,\"start\":4025},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":4358,\"start\":4354},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4371,\"start\":4367},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4390,\"start\":4386},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6056,\"start\":6052},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6059,\"start\":6056},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6062,\"start\":6059},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":6065,\"start\":6062},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":6068,\"start\":6065},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":6071,\"start\":6068},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6109,\"start\":6105},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6112,\"start\":6109},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":6115,\"start\":6112},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":6118,\"start\":6115},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6153,\"start\":6149},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":6156,\"start\":6153},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":6159,\"start\":6156},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":6162,\"start\":6159},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6186,\"start\":6182},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":6189,\"start\":6186},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":6192,\"start\":6189},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":6195,\"start\":6192},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6232,\"start\":6229},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":6235,\"start\":6232},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":6238,\"start\":6235},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":6241,\"start\":6238},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":6244,\"start\":6241},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":6247,\"start\":6244},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6570,\"start\":6566},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":6588,\"start\":6584},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6611,\"start\":6607},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6666,\"start\":6663},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6669,\"start\":6666},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6672,\"start\":6669},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":6675,\"start\":6672},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":7136,\"start\":7132},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7162,\"start\":7158},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7191,\"start\":7187},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7278,\"start\":7274},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":7281,\"start\":7278},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7305,\"start\":7301},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":7308,\"start\":7305},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7379,\"start\":7376},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7382,\"start\":7379},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7385,\"start\":7382},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7388,\"start\":7385},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7391,\"start\":7388},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":7394,\"start\":7391},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":7397,\"start\":7394},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7585,\"start\":7581},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7588,\"start\":7585},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7591,\"start\":7588},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":7594,\"start\":7591},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":7597,\"start\":7594},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7944,\"start\":7940},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":7947,\"start\":7944},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8064,\"start\":8060},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8067,\"start\":8064},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8070,\"start\":8067},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":8073,\"start\":8070},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8596,\"start\":8593},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8599,\"start\":8596},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8602,\"start\":8599},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":8605,\"start\":8602},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8717,\"start\":8713},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8720,\"start\":8717},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8723,\"start\":8720},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8784,\"start\":8780},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":8787,\"start\":8784},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":8790,\"start\":8787},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9181,\"start\":9177},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":9184,\"start\":9181},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9285,\"start\":9282},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9288,\"start\":9285},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":9291,\"start\":9288},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9305,\"start\":9301},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":9823,\"start\":9819},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9856,\"start\":9852},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10472,\"start\":10469},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10480,\"start\":10477},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10488,\"start\":10485},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10498,\"start\":10495},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10576,\"start\":10572},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10603,\"start\":10599},{\"end\":10608,\"start\":10603},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13638,\"start\":13634},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13641,\"start\":13638},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":13644,\"start\":13641},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16188,\"start\":16184},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":17531,\"start\":17527},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":17534,\"start\":17531},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18253,\"start\":18250},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":18256,\"start\":18253},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":20268,\"start\":20264},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20633,\"start\":20629},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":20636,\"start\":20633},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24958,\"start\":24954},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":26194,\"start\":26190},{\"end\":26560,\"start\":26551},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":27518,\"start\":27514},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":27556,\"start\":27552},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27632,\"start\":27628},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":27652,\"start\":27648},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":27665,\"start\":27661},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":27680,\"start\":27676},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":27683,\"start\":27680},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":28960,\"start\":28956},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":28971,\"start\":28967},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":28991,\"start\":28987},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29006,\"start\":29002},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":29018,\"start\":29014},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":29053,\"start\":29049},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":29068,\"start\":29064},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29990,\"start\":29986},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":30002,\"start\":29998},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":30012,\"start\":30008},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":30027,\"start\":30023},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":35551,\"start\":35547},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":35593,\"start\":35589},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":35736,\"start\":35732},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":35751,\"start\":35747},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":35754,\"start\":35751},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":36228,\"start\":36224},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":36266,\"start\":36262},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":36277,\"start\":36273},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":36288,\"start\":36285},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":36327,\"start\":36323},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":36358,\"start\":36354},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":36401,\"start\":36397},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":36412,\"start\":36408},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":36512,\"start\":36508},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":36526,\"start\":36522},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":37187,\"start\":37183},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":37198,\"start\":37194},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":37218,\"start\":37214},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":37233,\"start\":37229},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":37245,\"start\":37241},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":37284,\"start\":37280},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":38815,\"start\":38811},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":38827,\"start\":38823},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":38841,\"start\":38837},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":39535,\"start\":39531},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":39822,\"start\":39818},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":39990,\"start\":39986},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":40121,\"start\":40117},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":44406,\"start\":44402},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":44424,\"start\":44420}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":42543,\"start\":42088},{\"attributes\":{\"id\":\"fig_1\"},\"end\":42776,\"start\":42544},{\"attributes\":{\"id\":\"fig_2\"},\"end\":42845,\"start\":42777},{\"attributes\":{\"id\":\"fig_3\"},\"end\":43147,\"start\":42846},{\"attributes\":{\"id\":\"fig_4\"},\"end\":43393,\"start\":43148},{\"attributes\":{\"id\":\"fig_5\"},\"end\":43587,\"start\":43394},{\"attributes\":{\"id\":\"fig_7\"},\"end\":44319,\"start\":43588},{\"attributes\":{\"id\":\"fig_8\"},\"end\":44390,\"start\":44320},{\"attributes\":{\"id\":\"fig_9\"},\"end\":44640,\"start\":44391},{\"attributes\":{\"id\":\"fig_10\"},\"end\":44733,\"start\":44641},{\"attributes\":{\"id\":\"fig_11\"},\"end\":44897,\"start\":44734},{\"attributes\":{\"id\":\"fig_12\"},\"end\":45063,\"start\":44898},{\"attributes\":{\"id\":\"fig_13\"},\"end\":45178,\"start\":45064},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":45670,\"start\":45179},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":46070,\"start\":45671},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":47571,\"start\":46071},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":48097,\"start\":47572}]", "paragraph": "[{\"end\":2263,\"start\":1783},{\"end\":3309,\"start\":2265},{\"end\":3874,\"start\":3311},{\"end\":4531,\"start\":3876},{\"end\":4579,\"start\":4533},{\"end\":5164,\"start\":4581},{\"end\":5441,\"start\":5166},{\"end\":5793,\"start\":5443},{\"end\":6196,\"start\":5810},{\"end\":6340,\"start\":6198},{\"end\":6898,\"start\":6342},{\"end\":7670,\"start\":6900},{\"end\":8193,\"start\":7672},{\"end\":8606,\"start\":8195},{\"end\":9322,\"start\":8608},{\"end\":9699,\"start\":9324},{\"end\":10141,\"start\":9725},{\"end\":10359,\"start\":10199},{\"end\":11235,\"start\":10461},{\"end\":11424,\"start\":11237},{\"end\":12090,\"start\":11556},{\"end\":13127,\"start\":12092},{\"end\":14257,\"start\":13150},{\"end\":14696,\"start\":14273},{\"end\":15283,\"start\":14698},{\"end\":15391,\"start\":15285},{\"end\":15814,\"start\":15427},{\"end\":15926,\"start\":15816},{\"end\":16266,\"start\":16062},{\"end\":16591,\"start\":16315},{\"end\":16734,\"start\":16648},{\"end\":17028,\"start\":16763},{\"end\":17182,\"start\":17081},{\"end\":17552,\"start\":17254},{\"end\":17898,\"start\":17604},{\"end\":18746,\"start\":17992},{\"end\":19545,\"start\":18748},{\"end\":19768,\"start\":19580},{\"end\":20041,\"start\":19844},{\"end\":20833,\"start\":20043},{\"end\":21247,\"start\":20958},{\"end\":21603,\"start\":21249},{\"end\":22483,\"start\":21639},{\"end\":23075,\"start\":22485},{\"end\":23439,\"start\":23077},{\"end\":23592,\"start\":23489},{\"end\":23947,\"start\":23594},{\"end\":24482,\"start\":23949},{\"end\":24805,\"start\":24498},{\"end\":25147,\"start\":24807},{\"end\":26005,\"start\":25213},{\"end\":27431,\"start\":26007},{\"end\":27951,\"start\":27467},{\"end\":28447,\"start\":27978},{\"end\":28785,\"start\":28449},{\"end\":29486,\"start\":28823},{\"end\":29948,\"start\":29488},{\"end\":30228,\"start\":29950},{\"end\":31012,\"start\":30230},{\"end\":31324,\"start\":31027},{\"end\":32092,\"start\":31326},{\"end\":32370,\"start\":32094},{\"end\":33217,\"start\":32385},{\"end\":33365,\"start\":33251},{\"end\":33453,\"start\":33398},{\"end\":33610,\"start\":33455},{\"end\":34062,\"start\":33687},{\"end\":34218,\"start\":34132},{\"end\":34449,\"start\":34276},{\"end\":34605,\"start\":34515},{\"end\":34614,\"start\":34607},{\"end\":35089,\"start\":34712},{\"end\":35437,\"start\":35156},{\"end\":36194,\"start\":35493},{\"end\":37006,\"start\":36213},{\"end\":38335,\"start\":37064},{\"end\":39497,\"start\":38372},{\"end\":40303,\"start\":39522},{\"end\":40535,\"start\":40305},{\"end\":41769,\"start\":40537},{\"end\":42087,\"start\":41795}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10198,\"start\":10142},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10460,\"start\":10360},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11555,\"start\":11425},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15426,\"start\":15392},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16011,\"start\":15927},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16061,\"start\":16011},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16314,\"start\":16267},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16647,\"start\":16592},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17080,\"start\":17029},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17253,\"start\":17183},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17913,\"start\":17899},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17991,\"start\":17913},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19579,\"start\":19546},{\"attributes\":{\"id\":\"formula_13\"},\"end\":19843,\"start\":19769},{\"attributes\":{\"id\":\"formula_14\"},\"end\":20957,\"start\":20834},{\"attributes\":{\"id\":\"formula_15\"},\"end\":23488,\"start\":23440},{\"attributes\":{\"id\":\"formula_16\"},\"end\":25212,\"start\":25148},{\"attributes\":{\"id\":\"formula_17\"},\"end\":33397,\"start\":33366},{\"attributes\":{\"id\":\"formula_19\"},\"end\":33686,\"start\":33611},{\"attributes\":{\"id\":\"formula_20\"},\"end\":34131,\"start\":34063},{\"attributes\":{\"id\":\"formula_21\"},\"end\":34275,\"start\":34219},{\"attributes\":{\"id\":\"formula_22\"},\"end\":34676,\"start\":34615},{\"attributes\":{\"id\":\"formula_23\"},\"end\":34711,\"start\":34676},{\"attributes\":{\"id\":\"formula_24\"},\"end\":35124,\"start\":35090},{\"attributes\":{\"id\":\"formula_25\"},\"end\":35155,\"start\":35124}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":5439,\"start\":5431},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":5730,\"start\":5723},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":24462,\"start\":24455},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27430,\"start\":27423},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28617,\"start\":28610},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30179,\"start\":30172},{\"end\":30837,\"start\":30830},{\"end\":31052,\"start\":31045},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":32271,\"start\":32264},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":38334,\"start\":38327}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1781,\"start\":1769},{\"attributes\":{\"n\":\"2\"},\"end\":5808,\"start\":5796},{\"attributes\":{\"n\":\"3\"},\"end\":9710,\"start\":9702},{\"attributes\":{\"n\":\"3.1\"},\"end\":9723,\"start\":9713},{\"attributes\":{\"n\":\"3.2\"},\"end\":13148,\"start\":13130},{\"attributes\":{\"n\":\"4\"},\"end\":14271,\"start\":14260},{\"attributes\":{\"n\":\"4.1\"},\"end\":16761,\"start\":16737},{\"end\":17602,\"start\":17555},{\"attributes\":{\"n\":\"4.2\"},\"end\":21637,\"start\":21606},{\"attributes\":{\"n\":\"4.3\"},\"end\":24496,\"start\":24485},{\"attributes\":{\"n\":\"5\"},\"end\":27444,\"start\":27434},{\"attributes\":{\"n\":\"5.1\"},\"end\":27465,\"start\":27447},{\"attributes\":{\"n\":\"5.2\"},\"end\":27976,\"start\":27954},{\"attributes\":{\"n\":\"5.3\"},\"end\":28821,\"start\":28788},{\"attributes\":{\"n\":\"5.4\"},\"end\":31025,\"start\":31015},{\"attributes\":{\"n\":\"6\"},\"end\":32383,\"start\":32373},{\"end\":33249,\"start\":33220},{\"end\":34513,\"start\":34452},{\"end\":35464,\"start\":35440},{\"end\":35491,\"start\":35467},{\"end\":36211,\"start\":36197},{\"end\":37062,\"start\":37009},{\"end\":38370,\"start\":38338},{\"end\":39520,\"start\":39500},{\"end\":41793,\"start\":41772},{\"end\":42099,\"start\":42089},{\"end\":42555,\"start\":42545},{\"end\":42788,\"start\":42778},{\"end\":43169,\"start\":43149},{\"end\":43405,\"start\":43395},{\"end\":44652,\"start\":44642},{\"end\":44745,\"start\":44735},{\"end\":44910,\"start\":44899},{\"end\":45076,\"start\":45065},{\"end\":45189,\"start\":45180},{\"end\":45681,\"start\":45672},{\"end\":46081,\"start\":46072},{\"end\":47582,\"start\":47573}]", "table": "[{\"end\":45670,\"start\":45472},{\"end\":46070,\"start\":45854},{\"end\":47571,\"start\":46364},{\"end\":48097,\"start\":47991}]", "figure_caption": "[{\"end\":42543,\"start\":42101},{\"end\":42776,\"start\":42557},{\"end\":42845,\"start\":42790},{\"end\":43147,\"start\":42848},{\"end\":43393,\"start\":43172},{\"end\":43587,\"start\":43407},{\"end\":44319,\"start\":43590},{\"end\":44390,\"start\":44322},{\"end\":44640,\"start\":44393},{\"end\":44733,\"start\":44654},{\"end\":44897,\"start\":44747},{\"end\":45063,\"start\":44913},{\"end\":45178,\"start\":45079},{\"end\":45472,\"start\":45191},{\"end\":45854,\"start\":45683},{\"end\":46364,\"start\":46083},{\"end\":47991,\"start\":47584}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3490,\"start\":3482},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":5373,\"start\":5365},{\"end\":5608,\"start\":5600},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13198,\"start\":13190},{\"end\":20290,\"start\":20282},{\"end\":20831,\"start\":20823},{\"end\":22482,\"start\":22474},{\"end\":27123,\"start\":27115},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27993,\"start\":27985},{\"end\":29420,\"start\":29412},{\"end\":30077,\"start\":30069},{\"end\":30756,\"start\":30748},{\"end\":31677,\"start\":31669},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":36633,\"start\":36625},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":36853,\"start\":36845},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":38881,\"start\":38873},{\"end\":39496,\"start\":39488},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40382,\"start\":40373},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41058,\"start\":41049},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41127,\"start\":41118},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41546,\"start\":41537}]", "bib_author_first_name": "[{\"end\":50666,\"start\":50658},{\"end\":50679,\"start\":50675},{\"end\":50689,\"start\":50685},{\"end\":50701,\"start\":50695},{\"end\":50716,\"start\":50712},{\"end\":51034,\"start\":51028},{\"end\":51049,\"start\":51044},{\"end\":51356,\"start\":51353},{\"end\":51369,\"start\":51365},{\"end\":51383,\"start\":51379},{\"end\":51398,\"start\":51392},{\"end\":51661,\"start\":51653},{\"end\":51679,\"start\":51675},{\"end\":51693,\"start\":51686},{\"end\":51706,\"start\":51701},{\"end\":51724,\"start\":51716},{\"end\":51739,\"start\":51733},{\"end\":51756,\"start\":51750},{\"end\":51776,\"start\":51770},{\"end\":51790,\"start\":51784},{\"end\":52183,\"start\":52177},{\"end\":52193,\"start\":52189},{\"end\":52205,\"start\":52200},{\"end\":52219,\"start\":52214},{\"end\":52239,\"start\":52234},{\"end\":52500,\"start\":52493},{\"end\":52514,\"start\":52507},{\"end\":52526,\"start\":52519},{\"end\":52536,\"start\":52532},{\"end\":52549,\"start\":52543},{\"end\":52559,\"start\":52554},{\"end\":52572,\"start\":52566},{\"end\":52582,\"start\":52579},{\"end\":52593,\"start\":52590},{\"end\":52606,\"start\":52599},{\"end\":53065,\"start\":53058},{\"end\":53079,\"start\":53072},{\"end\":53095,\"start\":53086},{\"end\":53104,\"start\":53102},{\"end\":53115,\"start\":53109},{\"end\":53125,\"start\":53121},{\"end\":53137,\"start\":53132},{\"end\":53149,\"start\":53142},{\"end\":53163,\"start\":53157},{\"end\":53179,\"start\":53174},{\"end\":53549,\"start\":53541},{\"end\":53558,\"start\":53556},{\"end\":53569,\"start\":53566},{\"end\":53577,\"start\":53575},{\"end\":53587,\"start\":53584},{\"end\":53604,\"start\":53595},{\"end\":53949,\"start\":53941},{\"end\":53964,\"start\":53956},{\"end\":53979,\"start\":53974},{\"end\":53992,\"start\":53987},{\"end\":54002,\"start\":53998},{\"end\":54019,\"start\":54010},{\"end\":54033,\"start\":54026},{\"end\":54285,\"start\":54278},{\"end\":54294,\"start\":54292},{\"end\":54310,\"start\":54302},{\"end\":54320,\"start\":54317},{\"end\":54335,\"start\":54326},{\"end\":54350,\"start\":54342},{\"end\":54699,\"start\":54695},{\"end\":54719,\"start\":54711},{\"end\":54964,\"start\":54959},{\"end\":54981,\"start\":54973},{\"end\":54995,\"start\":54989},{\"end\":55009,\"start\":55001},{\"end\":55019,\"start\":55010},{\"end\":55386,\"start\":55385},{\"end\":55401,\"start\":55396},{\"end\":55742,\"start\":55736},{\"end\":55761,\"start\":55756},{\"end\":55778,\"start\":55769},{\"end\":55795,\"start\":55791},{\"end\":55816,\"start\":55809},{\"end\":55829,\"start\":55823},{\"end\":55850,\"start\":55843},{\"end\":55869,\"start\":55861},{\"end\":55885,\"start\":55880},{\"end\":55902,\"start\":55895},{\"end\":56319,\"start\":56313},{\"end\":56332,\"start\":56325},{\"end\":56346,\"start\":56340},{\"end\":56603,\"start\":56595},{\"end\":56620,\"start\":56613},{\"end\":56889,\"start\":56884},{\"end\":56902,\"start\":56899},{\"end\":57117,\"start\":57111},{\"end\":57129,\"start\":57124},{\"end\":57141,\"start\":57137},{\"end\":57365,\"start\":57359},{\"end\":57532,\"start\":57528},{\"end\":57540,\"start\":57538},{\"end\":57545,\"start\":57541},{\"end\":57560,\"start\":57553},{\"end\":57573,\"start\":57567},{\"end\":57582,\"start\":57579},{\"end\":57595,\"start\":57589},{\"end\":57610,\"start\":57602},{\"end\":57622,\"start\":57616},{\"end\":57632,\"start\":57627},{\"end\":57643,\"start\":57638},{\"end\":58165,\"start\":58158},{\"end\":58175,\"start\":58171},{\"end\":58179,\"start\":58176},{\"end\":58194,\"start\":58186},{\"end\":58201,\"start\":58200},{\"end\":58217,\"start\":58209},{\"end\":58228,\"start\":58222},{\"end\":58243,\"start\":58235},{\"end\":58257,\"start\":58250},{\"end\":58270,\"start\":58264},{\"end\":58279,\"start\":58276},{\"end\":58281,\"start\":58280},{\"end\":58826,\"start\":58821},{\"end\":58852,\"start\":58845},{\"end\":58875,\"start\":58869},{\"end\":58885,\"start\":58881},{\"end\":58889,\"start\":58886},{\"end\":58899,\"start\":58896},{\"end\":58901,\"start\":58900},{\"end\":59381,\"start\":59376},{\"end\":59393,\"start\":59386},{\"end\":59405,\"start\":59401},{\"end\":59767,\"start\":59764},{\"end\":59784,\"start\":59779},{\"end\":59934,\"start\":59932},{\"end\":59945,\"start\":59940},{\"end\":59959,\"start\":59953},{\"end\":59970,\"start\":59967},{\"end\":59982,\"start\":59978},{\"end\":59992,\"start\":59989},{\"end\":60347,\"start\":60339},{\"end\":60361,\"start\":60353},{\"end\":60384,\"start\":60369},{\"end\":60397,\"start\":60391},{\"end\":60414,\"start\":60408},{\"end\":60441,\"start\":60430},{\"end\":60827,\"start\":60823},{\"end\":60840,\"start\":60836},{\"end\":60854,\"start\":60850},{\"end\":60866,\"start\":60863},{\"end\":60881,\"start\":60875},{\"end\":61125,\"start\":61124},{\"end\":61141,\"start\":61135},{\"end\":61143,\"start\":61142},{\"end\":61157,\"start\":61153},{\"end\":61168,\"start\":61164},{\"end\":61170,\"start\":61169},{\"end\":61656,\"start\":61649},{\"end\":61669,\"start\":61663},{\"end\":61685,\"start\":61680},{\"end\":61926,\"start\":61920},{\"end\":61939,\"start\":61933},{\"end\":61951,\"start\":61945},{\"end\":61962,\"start\":61959},{\"end\":61974,\"start\":61970},{\"end\":61987,\"start\":61981},{\"end\":61996,\"start\":61992},{\"end\":62006,\"start\":62003},{\"end\":62308,\"start\":62302},{\"end\":62322,\"start\":62317},{\"end\":62517,\"start\":62511},{\"end\":62527,\"start\":62523},{\"end\":62543,\"start\":62537},{\"end\":62550,\"start\":62549},{\"end\":62564,\"start\":62560},{\"end\":62857,\"start\":62849},{\"end\":62869,\"start\":62863},{\"end\":62883,\"start\":62875},{\"end\":62898,\"start\":62890},{\"end\":63127,\"start\":63121},{\"end\":63142,\"start\":63136},{\"end\":63157,\"start\":63151},{\"end\":63475,\"start\":63470},{\"end\":63490,\"start\":63484},{\"end\":63503,\"start\":63499},{\"end\":63517,\"start\":63512},{\"end\":63531,\"start\":63527},{\"end\":63547,\"start\":63539},{\"end\":63564,\"start\":63557},{\"end\":63574,\"start\":63571},{\"end\":63876,\"start\":63870},{\"end\":63892,\"start\":63883},{\"end\":63902,\"start\":63897},{\"end\":63919,\"start\":63910},{\"end\":64208,\"start\":64200},{\"end\":64222,\"start\":64218},{\"end\":64239,\"start\":64233},{\"end\":64257,\"start\":64246},{\"end\":64567,\"start\":64558},{\"end\":64579,\"start\":64573},{\"end\":64595,\"start\":64586},{\"end\":64610,\"start\":64605},{\"end\":64625,\"start\":64619},{\"end\":64638,\"start\":64634},{\"end\":64966,\"start\":64962},{\"end\":64988,\"start\":64979},{\"end\":65003,\"start\":64997},{\"end\":65391,\"start\":65387},{\"end\":65400,\"start\":65399},{\"end\":65411,\"start\":65407},{\"end\":65413,\"start\":65412},{\"end\":65660,\"start\":65654},{\"end\":65677,\"start\":65671},{\"end\":65690,\"start\":65685},{\"end\":66134,\"start\":66126},{\"end\":66147,\"start\":66139},{\"end\":66161,\"start\":66154},{\"end\":66171,\"start\":66169},{\"end\":66184,\"start\":66176},{\"end\":66193,\"start\":66189},{\"end\":66205,\"start\":66199},{\"end\":66578,\"start\":66576},{\"end\":66592,\"start\":66584},{\"end\":66596,\"start\":66593},{\"end\":66604,\"start\":66602},{\"end\":66614,\"start\":66611},{\"end\":66623,\"start\":66620},{\"end\":66909,\"start\":66903},{\"end\":66922,\"start\":66915},{\"end\":66938,\"start\":66930},{\"end\":66951,\"start\":66946},{\"end\":66966,\"start\":66958},{\"end\":66980,\"start\":66972},{\"end\":66992,\"start\":66987},{\"end\":67006,\"start\":66999},{\"end\":67020,\"start\":67013},{\"end\":67032,\"start\":67027},{\"end\":67372,\"start\":67366},{\"end\":67382,\"start\":67378},{\"end\":67393,\"start\":67388},{\"end\":67408,\"start\":67401},{\"end\":67419,\"start\":67413},{\"end\":67432,\"start\":67427},{\"end\":67443,\"start\":67439},{\"end\":67454,\"start\":67450},{\"end\":67466,\"start\":67462},{\"end\":67487,\"start\":67480},{\"end\":67886,\"start\":67879},{\"end\":67897,\"start\":67892},{\"end\":67912,\"start\":67903},{\"end\":68311,\"start\":68309},{\"end\":68323,\"start\":68317},{\"end\":68332,\"start\":68329},{\"end\":68341,\"start\":68338},{\"end\":68352,\"start\":68346},{\"end\":68363,\"start\":68358},{\"end\":68378,\"start\":68371},{\"end\":68391,\"start\":68384},{\"end\":68703,\"start\":68697},{\"end\":68716,\"start\":68709},{\"end\":68725,\"start\":68721},{\"end\":68734,\"start\":68730},{\"end\":69120,\"start\":69112},{\"end\":69133,\"start\":69125},{\"end\":69142,\"start\":69139},{\"end\":69155,\"start\":69149},{\"end\":69168,\"start\":69161},{\"end\":69178,\"start\":69175},{\"end\":69192,\"start\":69184},{\"end\":69200,\"start\":69197},{\"end\":69213,\"start\":69207},{\"end\":69227,\"start\":69221},{\"end\":69676,\"start\":69672},{\"end\":69689,\"start\":69685},{\"end\":69702,\"start\":69696},{\"end\":69921,\"start\":69916},{\"end\":69937,\"start\":69933},{\"end\":69953,\"start\":69946},{\"end\":69965,\"start\":69961},{\"end\":69977,\"start\":69974},{\"end\":70372,\"start\":70369},{\"end\":70390,\"start\":70381},{\"end\":70408,\"start\":70400},{\"end\":70426,\"start\":70420},{\"end\":70436,\"start\":70433},{\"end\":70757,\"start\":70751},{\"end\":70769,\"start\":70765},{\"end\":70773,\"start\":70770},{\"end\":70785,\"start\":70781},{\"end\":70806,\"start\":70798},{\"end\":70822,\"start\":70816},{\"end\":71240,\"start\":71235},{\"end\":71270,\"start\":71250},{\"end\":71618,\"start\":71614},{\"end\":71630,\"start\":71627},{\"end\":71647,\"start\":71638},{\"end\":71659,\"start\":71655},{\"end\":71672,\"start\":71667},{\"end\":71690,\"start\":71683},{\"end\":71705,\"start\":71699},{\"end\":71721,\"start\":71715},{\"end\":71734,\"start\":71727},{\"end\":71751,\"start\":71747},{\"end\":72165,\"start\":72162},{\"end\":72180,\"start\":72176},{\"end\":72193,\"start\":72189},{\"end\":72466,\"start\":72458},{\"end\":72477,\"start\":72473},{\"end\":72490,\"start\":72486},{\"end\":72500,\"start\":72496},{\"end\":72513,\"start\":72509},{\"end\":72521,\"start\":72520},{\"end\":72535,\"start\":72530},{\"end\":72549,\"start\":72545},{\"end\":72573,\"start\":72558},{\"end\":72984,\"start\":72978},{\"end\":73001,\"start\":72996},{\"end\":73012,\"start\":73007},{\"end\":73323,\"start\":73317},{\"end\":73339,\"start\":73335},{\"end\":73357,\"start\":73347},{\"end\":73372,\"start\":73367},{\"end\":73652,\"start\":73646},{\"end\":73666,\"start\":73661},{\"end\":73679,\"start\":73674},{\"end\":73696,\"start\":73689},{\"end\":73989,\"start\":73983},{\"end\":74004,\"start\":73996},{\"end\":74018,\"start\":74012},{\"end\":74035,\"start\":74029},{\"end\":74260,\"start\":74254},{\"end\":74273,\"start\":74267},{\"end\":74291,\"start\":74280},{\"end\":74526,\"start\":74520},{\"end\":74539,\"start\":74533},{\"end\":74550,\"start\":74545},{\"end\":74565,\"start\":74562},{\"end\":74577,\"start\":74571},{\"end\":74589,\"start\":74583},{\"end\":74897,\"start\":74892},{\"end\":74908,\"start\":74904},{\"end\":74920,\"start\":74915},{\"end\":74932,\"start\":74925},{\"end\":74943,\"start\":74937},{\"end\":74953,\"start\":74949},{\"end\":74964,\"start\":74963},{\"end\":74978,\"start\":74974},{\"end\":75411,\"start\":75406},{\"end\":75420,\"start\":75416},{\"end\":75429,\"start\":75425},{\"end\":75702,\"start\":75694},{\"end\":75722,\"start\":75715},{\"end\":75736,\"start\":75730},{\"end\":75746,\"start\":75742},{\"end\":75758,\"start\":75752},{\"end\":76078,\"start\":76071},{\"end\":76091,\"start\":76087},{\"end\":76107,\"start\":76103},{\"end\":76117,\"start\":76112},{\"end\":76127,\"start\":76126},{\"end\":76151,\"start\":76150},{\"end\":76171,\"start\":76160},{\"end\":76655,\"start\":76650},{\"end\":76901,\"start\":76897},{\"end\":76909,\"start\":76907},{\"end\":76920,\"start\":76917},{\"end\":76934,\"start\":76926},{\"end\":77182,\"start\":77175},{\"end\":77195,\"start\":77188},{\"end\":77207,\"start\":77200},{\"end\":77220,\"start\":77214},{\"end\":77232,\"start\":77226},{\"end\":77244,\"start\":77239},{\"end\":77640,\"start\":77633},{\"end\":77655,\"start\":77648},{\"end\":77671,\"start\":77664},{\"end\":77689,\"start\":77683},{\"end\":77700,\"start\":77695},{\"end\":77712,\"start\":77707},{\"end\":77727,\"start\":77721},{\"end\":77738,\"start\":77734},{\"end\":77759,\"start\":77750},{\"end\":77761,\"start\":77760},{\"end\":77773,\"start\":77768},{\"end\":78277,\"start\":78272},{\"end\":78290,\"start\":78285},{\"end\":78310,\"start\":78303},{\"end\":78325,\"start\":78319},{\"end\":78614,\"start\":78610},{\"end\":78632,\"start\":78624},{\"end\":78647,\"start\":78639},{\"end\":78664,\"start\":78655},{\"end\":78681,\"start\":78672},{\"end\":78701,\"start\":78696},{\"end\":79053,\"start\":79047},{\"end\":79067,\"start\":79063},{\"end\":79081,\"start\":79077},{\"end\":79095,\"start\":79090},{\"end\":79112,\"start\":79107},{\"end\":79125,\"start\":79120},{\"end\":79127,\"start\":79126},{\"end\":79141,\"start\":79135},{\"end\":79155,\"start\":79150},{\"end\":79554,\"start\":79549},{\"end\":79567,\"start\":79562},{\"end\":79581,\"start\":79576},{\"end\":79595,\"start\":79591},{\"end\":79610,\"start\":79606},{\"end\":79867,\"start\":79863},{\"end\":79883,\"start\":79874},{\"end\":79897,\"start\":79891},{\"end\":79912,\"start\":79907},{\"end\":79923,\"start\":79919},{\"end\":79938,\"start\":79930},{\"end\":80361,\"start\":80355},{\"end\":80376,\"start\":80368},{\"end\":80388,\"start\":80381},{\"end\":80397,\"start\":80394},{\"end\":80409,\"start\":80403},{\"end\":80421,\"start\":80415},{\"end\":80431,\"start\":80427},{\"end\":80761,\"start\":80755},{\"end\":80774,\"start\":80768},{\"end\":80786,\"start\":80782},{\"end\":81179,\"start\":81173},{\"end\":81193,\"start\":81186},{\"end\":81204,\"start\":81198},{\"end\":81216,\"start\":81213},{\"end\":81226,\"start\":81223},{\"end\":81519,\"start\":81513},{\"end\":81530,\"start\":81526},{\"end\":81538,\"start\":81536},{\"end\":81551,\"start\":81545},{\"end\":81560,\"start\":81557},{\"end\":81571,\"start\":81567},{\"end\":81868,\"start\":81862},{\"end\":81881,\"start\":81875},{\"end\":81895,\"start\":81892},{\"end\":82103,\"start\":82099},{\"end\":82123,\"start\":82114},{\"end\":82139,\"start\":82131},{\"end\":82471,\"start\":82466},{\"end\":82488,\"start\":82482},{\"end\":82505,\"start\":82497},{\"end\":82782,\"start\":82776},{\"end\":82795,\"start\":82789},{\"end\":82814,\"start\":82806},{\"end\":82828,\"start\":82822},{\"end\":82842,\"start\":82835},{\"end\":82860,\"start\":82853},{\"end\":82873,\"start\":82866},{\"end\":82888,\"start\":82882},{\"end\":82903,\"start\":82900},{\"end\":82916,\"start\":82913},{\"end\":83542,\"start\":83534},{\"end\":83554,\"start\":83547},{\"end\":83562,\"start\":83560},{\"end\":83573,\"start\":83568},{\"end\":83583,\"start\":83579},{\"end\":83831,\"start\":83823},{\"end\":83843,\"start\":83837},{\"end\":83856,\"start\":83851},{\"end\":84341,\"start\":84339},{\"end\":84354,\"start\":84347},{\"end\":84367,\"start\":84361},{\"end\":84381,\"start\":84373},{\"end\":84391,\"start\":84386},{\"end\":84803,\"start\":84800},{\"end\":84810,\"start\":84808},{\"end\":84823,\"start\":84816},{\"end\":84835,\"start\":84829},{\"end\":84846,\"start\":84842},{\"end\":84854,\"start\":84851},{\"end\":84867,\"start\":84860},{\"end\":85284,\"start\":85278},{\"end\":85297,\"start\":85291},{\"end\":85309,\"start\":85303},{\"end\":85321,\"start\":85316},{\"end\":85334,\"start\":85333},{\"end\":85347,\"start\":85341},{\"end\":85737,\"start\":85731},{\"end\":85750,\"start\":85743},{\"end\":85760,\"start\":85755},{\"end\":85771,\"start\":85767},{\"end\":85788,\"start\":85781},{\"end\":85790,\"start\":85789},{\"end\":86128,\"start\":86122},{\"end\":86139,\"start\":86134},{\"end\":86152,\"start\":86146},{\"end\":86163,\"start\":86160},{\"end\":86175,\"start\":86171},{\"end\":86185,\"start\":86182},{\"end\":86528,\"start\":86521},{\"end\":86543,\"start\":86537},{\"end\":86561,\"start\":86553},{\"end\":86573,\"start\":86568},{\"end\":86959,\"start\":86956},{\"end\":86976,\"start\":86972},{\"end\":87380,\"start\":87376},{\"end\":87392,\"start\":87389},{\"end\":87408,\"start\":87403},{\"end\":87421,\"start\":87416},{\"end\":87721,\"start\":87714},{\"end\":87733,\"start\":87728},{\"end\":87744,\"start\":87738},{\"end\":87764,\"start\":87753},{\"end\":87767,\"start\":87765},{\"end\":87777,\"start\":87772},{\"end\":88141,\"start\":88130},{\"end\":88154,\"start\":88148},{\"end\":88162,\"start\":88159},{\"end\":88173,\"start\":88167},{\"end\":88185,\"start\":88183},{\"end\":88194,\"start\":88190}]", "bib_author_last_name": "[{\"end\":50673,\"start\":50667},{\"end\":50683,\"start\":50680},{\"end\":50693,\"start\":50690},{\"end\":50710,\"start\":50702},{\"end\":50723,\"start\":50717},{\"end\":51042,\"start\":51035},{\"end\":51054,\"start\":51050},{\"end\":51363,\"start\":51357},{\"end\":51377,\"start\":51370},{\"end\":51390,\"start\":51384},{\"end\":51405,\"start\":51399},{\"end\":51673,\"start\":51662},{\"end\":51684,\"start\":51680},{\"end\":51699,\"start\":51694},{\"end\":51714,\"start\":51707},{\"end\":51731,\"start\":51725},{\"end\":51748,\"start\":51740},{\"end\":51768,\"start\":51757},{\"end\":51782,\"start\":51777},{\"end\":51797,\"start\":51791},{\"end\":51805,\"start\":51799},{\"end\":52187,\"start\":52184},{\"end\":52198,\"start\":52194},{\"end\":52212,\"start\":52206},{\"end\":52232,\"start\":52220},{\"end\":52246,\"start\":52240},{\"end\":52505,\"start\":52501},{\"end\":52517,\"start\":52515},{\"end\":52530,\"start\":52527},{\"end\":52541,\"start\":52537},{\"end\":52552,\"start\":52550},{\"end\":52564,\"start\":52560},{\"end\":52577,\"start\":52573},{\"end\":52588,\"start\":52583},{\"end\":52597,\"start\":52594},{\"end\":52611,\"start\":52607},{\"end\":53070,\"start\":53066},{\"end\":53084,\"start\":53080},{\"end\":53100,\"start\":53096},{\"end\":53107,\"start\":53105},{\"end\":53119,\"start\":53116},{\"end\":53130,\"start\":53126},{\"end\":53140,\"start\":53138},{\"end\":53155,\"start\":53150},{\"end\":53172,\"start\":53164},{\"end\":53184,\"start\":53180},{\"end\":53554,\"start\":53550},{\"end\":53564,\"start\":53559},{\"end\":53573,\"start\":53570},{\"end\":53582,\"start\":53578},{\"end\":53593,\"start\":53588},{\"end\":53609,\"start\":53605},{\"end\":53954,\"start\":53950},{\"end\":53972,\"start\":53965},{\"end\":53985,\"start\":53980},{\"end\":53996,\"start\":53993},{\"end\":54008,\"start\":54003},{\"end\":54024,\"start\":54020},{\"end\":54040,\"start\":54034},{\"end\":54290,\"start\":54286},{\"end\":54300,\"start\":54295},{\"end\":54315,\"start\":54311},{\"end\":54324,\"start\":54321},{\"end\":54340,\"start\":54336},{\"end\":54354,\"start\":54351},{\"end\":54365,\"start\":54356},{\"end\":54709,\"start\":54700},{\"end\":54728,\"start\":54720},{\"end\":54737,\"start\":54730},{\"end\":54971,\"start\":54965},{\"end\":54987,\"start\":54982},{\"end\":54999,\"start\":54996},{\"end\":55024,\"start\":55020},{\"end\":55394,\"start\":55387},{\"end\":55407,\"start\":55402},{\"end\":55417,\"start\":55409},{\"end\":55754,\"start\":55743},{\"end\":55767,\"start\":55762},{\"end\":55789,\"start\":55779},{\"end\":55807,\"start\":55796},{\"end\":55821,\"start\":55817},{\"end\":55841,\"start\":55830},{\"end\":55859,\"start\":55851},{\"end\":55878,\"start\":55870},{\"end\":55893,\"start\":55886},{\"end\":55908,\"start\":55903},{\"end\":56323,\"start\":56320},{\"end\":56338,\"start\":56333},{\"end\":56353,\"start\":56347},{\"end\":56611,\"start\":56604},{\"end\":56627,\"start\":56621},{\"end\":56897,\"start\":56890},{\"end\":56911,\"start\":56903},{\"end\":57122,\"start\":57118},{\"end\":57135,\"start\":57130},{\"end\":57148,\"start\":57142},{\"end\":57381,\"start\":57366},{\"end\":57536,\"start\":57533},{\"end\":57551,\"start\":57546},{\"end\":57565,\"start\":57561},{\"end\":57577,\"start\":57574},{\"end\":57587,\"start\":57583},{\"end\":57600,\"start\":57596},{\"end\":57614,\"start\":57611},{\"end\":57625,\"start\":57623},{\"end\":57636,\"start\":57633},{\"end\":57647,\"start\":57644},{\"end\":58169,\"start\":58166},{\"end\":58184,\"start\":58180},{\"end\":58198,\"start\":58195},{\"end\":58207,\"start\":58202},{\"end\":58220,\"start\":58218},{\"end\":58233,\"start\":58229},{\"end\":58248,\"start\":58244},{\"end\":58262,\"start\":58258},{\"end\":58274,\"start\":58271},{\"end\":58286,\"start\":58282},{\"end\":58291,\"start\":58288},{\"end\":58838,\"start\":58827},{\"end\":58843,\"start\":58840},{\"end\":58867,\"start\":58853},{\"end\":58879,\"start\":58876},{\"end\":58894,\"start\":58890},{\"end\":58906,\"start\":58902},{\"end\":58911,\"start\":58908},{\"end\":59384,\"start\":59382},{\"end\":59399,\"start\":59394},{\"end\":59409,\"start\":59406},{\"end\":59777,\"start\":59768},{\"end\":59791,\"start\":59785},{\"end\":59938,\"start\":59935},{\"end\":59951,\"start\":59946},{\"end\":59965,\"start\":59960},{\"end\":59976,\"start\":59971},{\"end\":59987,\"start\":59983},{\"end\":59996,\"start\":59993},{\"end\":60006,\"start\":59998},{\"end\":60351,\"start\":60348},{\"end\":60367,\"start\":60362},{\"end\":60389,\"start\":60385},{\"end\":60406,\"start\":60398},{\"end\":60428,\"start\":60415},{\"end\":60449,\"start\":60442},{\"end\":60834,\"start\":60828},{\"end\":60848,\"start\":60841},{\"end\":60861,\"start\":60855},{\"end\":60873,\"start\":60867},{\"end\":60888,\"start\":60882},{\"end\":61133,\"start\":61126},{\"end\":61151,\"start\":61144},{\"end\":61162,\"start\":61158},{\"end\":61178,\"start\":61171},{\"end\":61187,\"start\":61180},{\"end\":61496,\"start\":61491},{\"end\":61506,\"start\":61498},{\"end\":61661,\"start\":61657},{\"end\":61678,\"start\":61670},{\"end\":61693,\"start\":61686},{\"end\":61931,\"start\":61927},{\"end\":61943,\"start\":61940},{\"end\":61957,\"start\":61952},{\"end\":61968,\"start\":61963},{\"end\":61979,\"start\":61975},{\"end\":61990,\"start\":61988},{\"end\":62001,\"start\":61997},{\"end\":62010,\"start\":62007},{\"end\":62020,\"start\":62012},{\"end\":62315,\"start\":62309},{\"end\":62329,\"start\":62323},{\"end\":62339,\"start\":62331},{\"end\":62521,\"start\":62518},{\"end\":62535,\"start\":62528},{\"end\":62547,\"start\":62544},{\"end\":62558,\"start\":62551},{\"end\":62572,\"start\":62565},{\"end\":62581,\"start\":62574},{\"end\":62861,\"start\":62858},{\"end\":62873,\"start\":62870},{\"end\":62888,\"start\":62884},{\"end\":62903,\"start\":62899},{\"end\":63134,\"start\":63128},{\"end\":63149,\"start\":63143},{\"end\":63166,\"start\":63158},{\"end\":63482,\"start\":63476},{\"end\":63497,\"start\":63491},{\"end\":63510,\"start\":63504},{\"end\":63525,\"start\":63518},{\"end\":63537,\"start\":63532},{\"end\":63555,\"start\":63548},{\"end\":63569,\"start\":63565},{\"end\":63583,\"start\":63575},{\"end\":63881,\"start\":63877},{\"end\":63895,\"start\":63893},{\"end\":63908,\"start\":63903},{\"end\":63924,\"start\":63920},{\"end\":64216,\"start\":64209},{\"end\":64231,\"start\":64223},{\"end\":64244,\"start\":64240},{\"end\":64262,\"start\":64258},{\"end\":64571,\"start\":64568},{\"end\":64584,\"start\":64580},{\"end\":64603,\"start\":64596},{\"end\":64617,\"start\":64611},{\"end\":64632,\"start\":64626},{\"end\":64646,\"start\":64639},{\"end\":64977,\"start\":64967},{\"end\":64995,\"start\":64989},{\"end\":65011,\"start\":65004},{\"end\":65397,\"start\":65392},{\"end\":65405,\"start\":65401},{\"end\":65420,\"start\":65414},{\"end\":65427,\"start\":65422},{\"end\":65669,\"start\":65661},{\"end\":65683,\"start\":65678},{\"end\":65702,\"start\":65691},{\"end\":66137,\"start\":66135},{\"end\":66152,\"start\":66148},{\"end\":66167,\"start\":66162},{\"end\":66174,\"start\":66172},{\"end\":66187,\"start\":66185},{\"end\":66197,\"start\":66194},{\"end\":66210,\"start\":66206},{\"end\":66582,\"start\":66579},{\"end\":66600,\"start\":66597},{\"end\":66609,\"start\":66605},{\"end\":66618,\"start\":66615},{\"end\":66628,\"start\":66624},{\"end\":66913,\"start\":66910},{\"end\":66928,\"start\":66923},{\"end\":66944,\"start\":66939},{\"end\":66956,\"start\":66952},{\"end\":66970,\"start\":66967},{\"end\":66985,\"start\":66981},{\"end\":66997,\"start\":66993},{\"end\":67011,\"start\":67007},{\"end\":67025,\"start\":67021},{\"end\":67038,\"start\":67033},{\"end\":67376,\"start\":67373},{\"end\":67386,\"start\":67383},{\"end\":67399,\"start\":67394},{\"end\":67411,\"start\":67409},{\"end\":67425,\"start\":67420},{\"end\":67437,\"start\":67433},{\"end\":67448,\"start\":67444},{\"end\":67460,\"start\":67455},{\"end\":67478,\"start\":67467},{\"end\":67496,\"start\":67488},{\"end\":67505,\"start\":67498},{\"end\":67890,\"start\":67887},{\"end\":67901,\"start\":67898},{\"end\":67917,\"start\":67913},{\"end\":68315,\"start\":68312},{\"end\":68327,\"start\":68324},{\"end\":68336,\"start\":68333},{\"end\":68344,\"start\":68342},{\"end\":68356,\"start\":68353},{\"end\":68369,\"start\":68364},{\"end\":68382,\"start\":68379},{\"end\":68395,\"start\":68392},{\"end\":68707,\"start\":68704},{\"end\":68719,\"start\":68717},{\"end\":68728,\"start\":68726},{\"end\":68740,\"start\":68735},{\"end\":69123,\"start\":69121},{\"end\":69137,\"start\":69134},{\"end\":69147,\"start\":69143},{\"end\":69159,\"start\":69156},{\"end\":69173,\"start\":69169},{\"end\":69182,\"start\":69179},{\"end\":69195,\"start\":69193},{\"end\":69205,\"start\":69201},{\"end\":69219,\"start\":69214},{\"end\":69232,\"start\":69228},{\"end\":69683,\"start\":69677},{\"end\":69694,\"start\":69690},{\"end\":69709,\"start\":69703},{\"end\":69931,\"start\":69922},{\"end\":69944,\"start\":69938},{\"end\":69959,\"start\":69954},{\"end\":69972,\"start\":69966},{\"end\":69983,\"start\":69978},{\"end\":70379,\"start\":70373},{\"end\":70398,\"start\":70391},{\"end\":70418,\"start\":70409},{\"end\":70431,\"start\":70427},{\"end\":70444,\"start\":70437},{\"end\":70763,\"start\":70758},{\"end\":70779,\"start\":70774},{\"end\":70796,\"start\":70786},{\"end\":70814,\"start\":70807},{\"end\":70834,\"start\":70823},{\"end\":71248,\"start\":71241},{\"end\":71277,\"start\":71271},{\"end\":71468,\"start\":71462},{\"end\":71478,\"start\":71470},{\"end\":71625,\"start\":71619},{\"end\":71636,\"start\":71631},{\"end\":71653,\"start\":71648},{\"end\":71665,\"start\":71660},{\"end\":71681,\"start\":71673},{\"end\":71697,\"start\":71691},{\"end\":71713,\"start\":71706},{\"end\":71725,\"start\":71722},{\"end\":71745,\"start\":71735},{\"end\":71758,\"start\":71752},{\"end\":72174,\"start\":72166},{\"end\":72187,\"start\":72181},{\"end\":72203,\"start\":72194},{\"end\":72471,\"start\":72467},{\"end\":72484,\"start\":72478},{\"end\":72494,\"start\":72491},{\"end\":72507,\"start\":72501},{\"end\":72518,\"start\":72514},{\"end\":72528,\"start\":72522},{\"end\":72543,\"start\":72536},{\"end\":72556,\"start\":72550},{\"end\":72581,\"start\":72574},{\"end\":72588,\"start\":72583},{\"end\":72994,\"start\":72985},{\"end\":73005,\"start\":73002},{\"end\":73018,\"start\":73013},{\"end\":73333,\"start\":73324},{\"end\":73345,\"start\":73340},{\"end\":73365,\"start\":73358},{\"end\":73378,\"start\":73373},{\"end\":73659,\"start\":73653},{\"end\":73672,\"start\":73667},{\"end\":73687,\"start\":73680},{\"end\":73702,\"start\":73697},{\"end\":73994,\"start\":73990},{\"end\":74010,\"start\":74005},{\"end\":74027,\"start\":74019},{\"end\":74040,\"start\":74036},{\"end\":74265,\"start\":74261},{\"end\":74278,\"start\":74274},{\"end\":74296,\"start\":74292},{\"end\":74531,\"start\":74527},{\"end\":74543,\"start\":74540},{\"end\":74560,\"start\":74551},{\"end\":74569,\"start\":74566},{\"end\":74581,\"start\":74578},{\"end\":74597,\"start\":74590},{\"end\":74902,\"start\":74898},{\"end\":74913,\"start\":74909},{\"end\":74923,\"start\":74921},{\"end\":74935,\"start\":74933},{\"end\":74947,\"start\":74944},{\"end\":74961,\"start\":74954},{\"end\":74972,\"start\":74965},{\"end\":74986,\"start\":74979},{\"end\":74995,\"start\":74988},{\"end\":75414,\"start\":75412},{\"end\":75423,\"start\":75421},{\"end\":75435,\"start\":75430},{\"end\":75713,\"start\":75703},{\"end\":75728,\"start\":75723},{\"end\":75740,\"start\":75737},{\"end\":75750,\"start\":75747},{\"end\":75766,\"start\":75759},{\"end\":75770,\"start\":75768},{\"end\":76085,\"start\":76079},{\"end\":76101,\"start\":76092},{\"end\":76110,\"start\":76108},{\"end\":76124,\"start\":76118},{\"end\":76139,\"start\":76128},{\"end\":76148,\"start\":76141},{\"end\":76158,\"start\":76152},{\"end\":76174,\"start\":76172},{\"end\":76181,\"start\":76176},{\"end\":76664,\"start\":76656},{\"end\":76680,\"start\":76666},{\"end\":76905,\"start\":76902},{\"end\":76915,\"start\":76910},{\"end\":76924,\"start\":76921},{\"end\":76938,\"start\":76935},{\"end\":77186,\"start\":77183},{\"end\":77198,\"start\":77196},{\"end\":77212,\"start\":77208},{\"end\":77224,\"start\":77221},{\"end\":77237,\"start\":77233},{\"end\":77249,\"start\":77245},{\"end\":77646,\"start\":77641},{\"end\":77662,\"start\":77656},{\"end\":77681,\"start\":77672},{\"end\":77693,\"start\":77690},{\"end\":77705,\"start\":77701},{\"end\":77719,\"start\":77713},{\"end\":77732,\"start\":77728},{\"end\":77748,\"start\":77739},{\"end\":77766,\"start\":77762},{\"end\":77780,\"start\":77774},{\"end\":78283,\"start\":78278},{\"end\":78301,\"start\":78291},{\"end\":78317,\"start\":78311},{\"end\":78332,\"start\":78326},{\"end\":78622,\"start\":78615},{\"end\":78637,\"start\":78633},{\"end\":78653,\"start\":78648},{\"end\":78670,\"start\":78665},{\"end\":78694,\"start\":78682},{\"end\":78707,\"start\":78702},{\"end\":79061,\"start\":79054},{\"end\":79075,\"start\":79068},{\"end\":79088,\"start\":79082},{\"end\":79105,\"start\":79096},{\"end\":79118,\"start\":79113},{\"end\":79133,\"start\":79128},{\"end\":79148,\"start\":79142},{\"end\":79166,\"start\":79156},{\"end\":79560,\"start\":79555},{\"end\":79574,\"start\":79568},{\"end\":79589,\"start\":79582},{\"end\":79604,\"start\":79596},{\"end\":79616,\"start\":79611},{\"end\":79872,\"start\":79868},{\"end\":79889,\"start\":79884},{\"end\":79905,\"start\":79898},{\"end\":79917,\"start\":79913},{\"end\":79928,\"start\":79924},{\"end\":79945,\"start\":79939},{\"end\":80366,\"start\":80362},{\"end\":80379,\"start\":80377},{\"end\":80392,\"start\":80389},{\"end\":80401,\"start\":80398},{\"end\":80413,\"start\":80410},{\"end\":80425,\"start\":80422},{\"end\":80435,\"start\":80432},{\"end\":80766,\"start\":80762},{\"end\":80780,\"start\":80775},{\"end\":80790,\"start\":80787},{\"end\":81184,\"start\":81180},{\"end\":81196,\"start\":81194},{\"end\":81211,\"start\":81205},{\"end\":81221,\"start\":81217},{\"end\":81229,\"start\":81227},{\"end\":81524,\"start\":81520},{\"end\":81534,\"start\":81531},{\"end\":81543,\"start\":81539},{\"end\":81555,\"start\":81552},{\"end\":81565,\"start\":81561},{\"end\":81576,\"start\":81572},{\"end\":81873,\"start\":81869},{\"end\":81890,\"start\":81882},{\"end\":81899,\"start\":81896},{\"end\":82112,\"start\":82104},{\"end\":82129,\"start\":82124},{\"end\":82146,\"start\":82140},{\"end\":82480,\"start\":82472},{\"end\":82495,\"start\":82489},{\"end\":82512,\"start\":82506},{\"end\":82787,\"start\":82783},{\"end\":82804,\"start\":82796},{\"end\":82820,\"start\":82815},{\"end\":82833,\"start\":82829},{\"end\":82851,\"start\":82843},{\"end\":82864,\"start\":82861},{\"end\":82880,\"start\":82874},{\"end\":82898,\"start\":82889},{\"end\":82911,\"start\":82904},{\"end\":82925,\"start\":82917},{\"end\":83545,\"start\":83543},{\"end\":83558,\"start\":83555},{\"end\":83566,\"start\":83563},{\"end\":83577,\"start\":83574},{\"end\":83587,\"start\":83584},{\"end\":83835,\"start\":83832},{\"end\":83849,\"start\":83844},{\"end\":83861,\"start\":83857},{\"end\":84345,\"start\":84342},{\"end\":84359,\"start\":84355},{\"end\":84371,\"start\":84368},{\"end\":84384,\"start\":84382},{\"end\":84395,\"start\":84392},{\"end\":84806,\"start\":84804},{\"end\":84814,\"start\":84811},{\"end\":84827,\"start\":84824},{\"end\":84840,\"start\":84836},{\"end\":84849,\"start\":84847},{\"end\":84858,\"start\":84855},{\"end\":84871,\"start\":84868},{\"end\":85289,\"start\":85285},{\"end\":85301,\"start\":85298},{\"end\":85314,\"start\":85310},{\"end\":85331,\"start\":85322},{\"end\":85339,\"start\":85335},{\"end\":85361,\"start\":85348},{\"end\":85365,\"start\":85363},{\"end\":85741,\"start\":85738},{\"end\":85753,\"start\":85751},{\"end\":85765,\"start\":85761},{\"end\":85779,\"start\":85772},{\"end\":85798,\"start\":85791},{\"end\":86132,\"start\":86129},{\"end\":86144,\"start\":86140},{\"end\":86158,\"start\":86153},{\"end\":86169,\"start\":86164},{\"end\":86180,\"start\":86176},{\"end\":86189,\"start\":86186},{\"end\":86535,\"start\":86529},{\"end\":86551,\"start\":86544},{\"end\":86566,\"start\":86562},{\"end\":86580,\"start\":86574},{\"end\":86970,\"start\":86960},{\"end\":86980,\"start\":86977},{\"end\":87387,\"start\":87381},{\"end\":87401,\"start\":87393},{\"end\":87414,\"start\":87409},{\"end\":87432,\"start\":87422},{\"end\":87726,\"start\":87722},{\"end\":87736,\"start\":87734},{\"end\":87751,\"start\":87745},{\"end\":87770,\"start\":87768},{\"end\":87783,\"start\":87778},{\"end\":88146,\"start\":88142},{\"end\":88157,\"start\":88155},{\"end\":88165,\"start\":88163},{\"end\":88181,\"start\":88174},{\"end\":88188,\"start\":88186},{\"end\":88198,\"start\":88195}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2002.08258\",\"id\":\"b0\"},\"end\":50906,\"start\":50616},{\"attributes\":{\"doi\":\"arXiv:2006.11477\",\"id\":\"b1\"},\"end\":51272,\"start\":50908},{\"attributes\":{\"doi\":\"arXiv:1810.05723\",\"id\":\"b2\"},\"end\":51612,\"start\":51274},{\"attributes\":{\"doi\":\"arXiv:2005.14165\",\"id\":\"b3\"},\"end\":52075,\"start\":51614},{\"attributes\":{\"doi\":\"arXiv:1708.00055\",\"id\":\"b4\"},\"end\":52491,\"start\":52077},{\"attributes\":{\"doi\":\"arXiv:2001.04246\",\"id\":\"b5\"},\"end\":52981,\"start\":52493},{\"attributes\":{\"doi\":\"arXiv:2110.13900\",\"id\":\"b6\"},\"end\":53471,\"start\":52983},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":235367934},\"end\":53878,\"start\":53473},{\"attributes\":{\"doi\":\"arXiv:2007.12223\",\"id\":\"b8\"},\"end\":54276,\"start\":53880},{\"attributes\":{\"doi\":\"arXiv:2101.00063\",\"id\":\"b9\"},\"end\":54640,\"start\":54278},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":8587959},\"end\":54957,\"start\":54642},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b11\"},\"end\":55320,\"start\":54959},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":16639476},\"end\":55734,\"start\":55322},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b13\"},\"end\":56311,\"start\":55736},{\"attributes\":{\"doi\":\"arXiv:1909.11556\",\"id\":\"b14\"},\"end\":56593,\"start\":56313},{\"attributes\":{\"doi\":\"arXiv:1803.03635\",\"id\":\"b15\"},\"end\":56882,\"start\":56595},{\"attributes\":{\"doi\":\"arXiv:2201.13096\",\"id\":\"b16\"},\"end\":57109,\"start\":56884},{\"attributes\":{\"doi\":\"arXiv:1902.09574\",\"id\":\"b17\"},\"end\":57355,\"start\":57111},{\"attributes\":{\"id\":\"b18\"},\"end\":57446,\"start\":57357},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":221376946},\"end\":58078,\"start\":57448},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":211296403},\"end\":58713,\"start\":58080},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":235414966},\"end\":59314,\"start\":58715},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":20157893},\"end\":59725,\"start\":59316},{\"attributes\":{\"doi\":\"arXiv:1606.08415\",\"id\":\"b23\"},\"end\":59930,\"start\":59727},{\"attributes\":{\"doi\":\"arXiv:2004.04037\",\"id\":\"b24\"},\"end\":60244,\"start\":59932},{\"attributes\":{\"doi\":\"arXiv:2106.07447\",\"id\":\"b25\"},\"end\":60728,\"start\":60246},{\"attributes\":{\"doi\":\"arXiv:2006.10518\",\"id\":\"b26\"},\"end\":61122,\"start\":60730},{\"attributes\":{\"doi\":\"arXiv:2006.11316\",\"id\":\"b27\"},\"end\":61487,\"start\":61124},{\"attributes\":{\"id\":\"b28\"},\"end\":61574,\"start\":61489},{\"attributes\":{\"id\":\"b29\"},\"end\":61918,\"start\":61576},{\"attributes\":{\"doi\":\"arXiv:1909.10351\",\"id\":\"b30\"},\"end\":62300,\"start\":61920},{\"attributes\":{\"doi\":\"arXiv:2005.06628\",\"id\":\"b31\"},\"end\":62509,\"start\":62302},{\"attributes\":{\"doi\":\"arXiv:2101.01321\",\"id\":\"b32\"},\"end\":62798,\"start\":62511},{\"attributes\":{\"doi\":\"arXiv:2010.13160\",\"id\":\"b33\"},\"end\":63082,\"start\":62800},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":209315300},\"end\":63372,\"start\":63084},{\"attributes\":{\"doi\":\"arXiv:2203.07259\",\"id\":\"b35\"},\"end\":63868,\"start\":63374},{\"attributes\":{\"doi\":\"arXiv:2012.02732\",\"id\":\"b36\"},\"end\":64198,\"start\":63870},{\"attributes\":{\"doi\":\"arXiv:2109.04838\",\"id\":\"b37\"},\"end\":64478,\"start\":64200},{\"attributes\":{\"doi\":\"arXiv:1909.11942\",\"id\":\"b38\"},\"end\":64890,\"start\":64480},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":233476465},\"end\":65363,\"start\":64892},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":7785881},\"end\":65621,\"start\":65365},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":15710851},\"end\":66009,\"start\":65623},{\"attributes\":{\"doi\":\"arXiv:2009.08065\",\"id\":\"b42\"},\"end\":66485,\"start\":66011},{\"attributes\":{\"doi\":\"arXiv:2010.01791\",\"id\":\"b43\"},\"end\":66845,\"start\":66487},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":235825363},\"end\":67364,\"start\":66847},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b45\"},\"end\":67816,\"start\":67366},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":232307646},\"end\":68234,\"start\":67818},{\"attributes\":{\"doi\":\"arXiv:2103.14030\",\"id\":\"b47\"},\"end\":68630,\"start\":68236},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":236477807},\"end\":69034,\"start\":68632},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":228990152},\"end\":69627,\"start\":69036},{\"attributes\":{\"doi\":\"arXiv:1905.10650\",\"id\":\"b50\"},\"end\":69864,\"start\":69629},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":195657904},\"end\":70367,\"start\":69866},{\"attributes\":{\"doi\":\"arXiv:1907.04018\",\"id\":\"b52\"},\"end\":70687,\"start\":70369},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":216056295},\"end\":71114,\"start\":70689},{\"attributes\":{\"id\":\"b54\"},\"end\":71458,\"start\":71116},{\"attributes\":{\"id\":\"b55\"},\"end\":71542,\"start\":71460},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":202786778},\"end\":72106,\"start\":71544},{\"attributes\":{\"doi\":\"arXiv:2005.00561\",\"id\":\"b57\"},\"end\":72374,\"start\":72108},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":204796040},\"end\":72976,\"start\":72376},{\"attributes\":{\"doi\":\"arXiv:1806.03822\",\"id\":\"b59\"},\"end\":73254,\"start\":72978},{\"attributes\":{\"doi\":\"arXiv:1606.05250\",\"id\":\"b60\"},\"end\":73576,\"start\":73256},{\"attributes\":{\"doi\":\"arXiv:2004.03844\",\"id\":\"b61\"},\"end\":73902,\"start\":73578},{\"attributes\":{\"doi\":\"arXiv:1910.01108\",\"id\":\"b62\"},\"end\":74252,\"start\":73904},{\"attributes\":{\"doi\":\"arXiv:2005.07683\",\"id\":\"b63\"},\"end\":74518,\"start\":74254},{\"attributes\":{\"doi\":\"arXiv:2110.10811\",\"id\":\"b64\"},\"end\":74826,\"start\":74520},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":202565587},\"end\":75379,\"start\":74828},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":59523610},\"end\":75624,\"start\":75381},{\"attributes\":{\"doi\":\"arXiv:2109.08668\",\"id\":\"b67\"},\"end\":75990,\"start\":75626},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":990233},\"end\":76648,\"start\":75992},{\"attributes\":{\"doi\":\"arXiv:1507.06149\",\"id\":\"b69\"},\"end\":76895,\"start\":76650},{\"attributes\":{\"doi\":\"arXiv:1908.09355\",\"id\":\"b70\"},\"end\":77173,\"start\":76897},{\"attributes\":{\"doi\":\"arXiv:2004.02984\",\"id\":\"b71\"},\"end\":77541,\"start\":77175},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":237421361},\"end\":78207,\"start\":77543},{\"attributes\":{\"doi\":\"arXiv:1801.05787\",\"id\":\"b73\"},\"end\":78531,\"start\":78209},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":229363322},\"end\":79018,\"start\":78533},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":13756489},\"end\":79447,\"start\":79020},{\"attributes\":{\"doi\":\"arXiv:1905.09418\",\"id\":\"b76\"},\"end\":79861,\"start\":79449},{\"attributes\":{\"doi\":\"arXiv:1804.07461\",\"id\":\"b77\"},\"end\":80277,\"start\":79863},{\"attributes\":{\"doi\":\"arXiv:2005.14187\",\"id\":\"b78\"},\"end\":80667,\"start\":80279},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":229298088},\"end\":81121,\"start\":80669},{\"attributes\":{\"doi\":\"arXiv:2006.04768\",\"id\":\"b80\"},\"end\":81411,\"start\":81123},{\"attributes\":{\"doi\":\"arXiv:2002.10957\",\"id\":\"b81\"},\"end\":81815,\"start\":81413},{\"attributes\":{\"doi\":\"arXiv:1910.04732\",\"id\":\"b82\"},\"end\":82057,\"start\":81817},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":44072099},\"end\":82384,\"start\":82059},{\"attributes\":{\"doi\":\"arXiv:1704.05426\",\"id\":\"b84\"},\"end\":82714,\"start\":82386},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":208117506},\"end\":83482,\"start\":82716},{\"attributes\":{\"doi\":\"arXiv:2004.11886\",\"id\":\"b86\"},\"end\":83766,\"start\":83484},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":247922354},\"end\":84273,\"start\":83768},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":216552850},\"end\":84798,\"start\":84275},{\"attributes\":{\"doi\":\"arXiv:2105.14444\",\"id\":\"b89\"},\"end\":85202,\"start\":84800},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":195069387},\"end\":85646,\"start\":85204},{\"attributes\":{\"doi\":\"arXiv:2105.14636\",\"id\":\"b91\"},\"end\":86024,\"start\":85648},{\"attributes\":{\"doi\":\"arXiv:2107.13686\",\"id\":\"b92\"},\"end\":86428,\"start\":86026},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":235253836},\"end\":86863,\"start\":86430},{\"attributes\":{\"id\":\"b94\",\"matched_paper_id\":218571099},\"end\":87374,\"start\":86865},{\"attributes\":{\"doi\":\"arXiv:1910.06188\",\"id\":\"b95\"},\"end\":87622,\"start\":87376},{\"attributes\":{\"id\":\"b96\",\"matched_paper_id\":59413897},\"end\":88064,\"start\":87624},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":219531455},\"end\":88463,\"start\":88066}]", "bib_title": "[{\"end\":53539,\"start\":53473},{\"end\":54693,\"start\":54642},{\"end\":55383,\"start\":55322},{\"end\":57526,\"start\":57448},{\"end\":58156,\"start\":58080},{\"end\":58819,\"start\":58715},{\"end\":59374,\"start\":59316},{\"end\":63119,\"start\":63084},{\"end\":64960,\"start\":64892},{\"end\":65385,\"start\":65365},{\"end\":65652,\"start\":65623},{\"end\":66901,\"start\":66847},{\"end\":67877,\"start\":67818},{\"end\":68695,\"start\":68632},{\"end\":69110,\"start\":69036},{\"end\":69914,\"start\":69866},{\"end\":70749,\"start\":70689},{\"end\":71612,\"start\":71544},{\"end\":72456,\"start\":72376},{\"end\":74890,\"start\":74828},{\"end\":75404,\"start\":75381},{\"end\":76069,\"start\":75992},{\"end\":77631,\"start\":77543},{\"end\":78608,\"start\":78533},{\"end\":79045,\"start\":79020},{\"end\":80753,\"start\":80669},{\"end\":82097,\"start\":82059},{\"end\":82774,\"start\":82716},{\"end\":83821,\"start\":83768},{\"end\":84337,\"start\":84275},{\"end\":85276,\"start\":85204},{\"end\":86519,\"start\":86430},{\"end\":86954,\"start\":86865},{\"end\":87712,\"start\":87624},{\"end\":88128,\"start\":88066}]", "bib_author": "[{\"end\":50675,\"start\":50658},{\"end\":50685,\"start\":50675},{\"end\":50695,\"start\":50685},{\"end\":50712,\"start\":50695},{\"end\":50725,\"start\":50712},{\"end\":51044,\"start\":51028},{\"end\":51056,\"start\":51044},{\"end\":51365,\"start\":51353},{\"end\":51379,\"start\":51365},{\"end\":51392,\"start\":51379},{\"end\":51407,\"start\":51392},{\"end\":51675,\"start\":51653},{\"end\":51686,\"start\":51675},{\"end\":51701,\"start\":51686},{\"end\":51716,\"start\":51701},{\"end\":51733,\"start\":51716},{\"end\":51750,\"start\":51733},{\"end\":51770,\"start\":51750},{\"end\":51784,\"start\":51770},{\"end\":51799,\"start\":51784},{\"end\":51807,\"start\":51799},{\"end\":52189,\"start\":52177},{\"end\":52200,\"start\":52189},{\"end\":52214,\"start\":52200},{\"end\":52234,\"start\":52214},{\"end\":52248,\"start\":52234},{\"end\":52507,\"start\":52493},{\"end\":52519,\"start\":52507},{\"end\":52532,\"start\":52519},{\"end\":52543,\"start\":52532},{\"end\":52554,\"start\":52543},{\"end\":52566,\"start\":52554},{\"end\":52579,\"start\":52566},{\"end\":52590,\"start\":52579},{\"end\":52599,\"start\":52590},{\"end\":52613,\"start\":52599},{\"end\":53072,\"start\":53058},{\"end\":53086,\"start\":53072},{\"end\":53102,\"start\":53086},{\"end\":53109,\"start\":53102},{\"end\":53121,\"start\":53109},{\"end\":53132,\"start\":53121},{\"end\":53142,\"start\":53132},{\"end\":53157,\"start\":53142},{\"end\":53174,\"start\":53157},{\"end\":53186,\"start\":53174},{\"end\":53556,\"start\":53541},{\"end\":53566,\"start\":53556},{\"end\":53575,\"start\":53566},{\"end\":53584,\"start\":53575},{\"end\":53595,\"start\":53584},{\"end\":53611,\"start\":53595},{\"end\":53956,\"start\":53941},{\"end\":53974,\"start\":53956},{\"end\":53987,\"start\":53974},{\"end\":53998,\"start\":53987},{\"end\":54010,\"start\":53998},{\"end\":54026,\"start\":54010},{\"end\":54042,\"start\":54026},{\"end\":54292,\"start\":54278},{\"end\":54302,\"start\":54292},{\"end\":54317,\"start\":54302},{\"end\":54326,\"start\":54317},{\"end\":54342,\"start\":54326},{\"end\":54356,\"start\":54342},{\"end\":54367,\"start\":54356},{\"end\":54711,\"start\":54695},{\"end\":54730,\"start\":54711},{\"end\":54739,\"start\":54730},{\"end\":54973,\"start\":54959},{\"end\":54989,\"start\":54973},{\"end\":55001,\"start\":54989},{\"end\":55026,\"start\":55001},{\"end\":55396,\"start\":55385},{\"end\":55409,\"start\":55396},{\"end\":55419,\"start\":55409},{\"end\":55756,\"start\":55736},{\"end\":55769,\"start\":55756},{\"end\":55791,\"start\":55769},{\"end\":55809,\"start\":55791},{\"end\":55823,\"start\":55809},{\"end\":55843,\"start\":55823},{\"end\":55861,\"start\":55843},{\"end\":55880,\"start\":55861},{\"end\":55895,\"start\":55880},{\"end\":55910,\"start\":55895},{\"end\":56325,\"start\":56313},{\"end\":56340,\"start\":56325},{\"end\":56355,\"start\":56340},{\"end\":56613,\"start\":56595},{\"end\":56629,\"start\":56613},{\"end\":56899,\"start\":56884},{\"end\":56913,\"start\":56899},{\"end\":57124,\"start\":57111},{\"end\":57137,\"start\":57124},{\"end\":57150,\"start\":57137},{\"end\":57383,\"start\":57359},{\"end\":57538,\"start\":57528},{\"end\":57553,\"start\":57538},{\"end\":57567,\"start\":57553},{\"end\":57579,\"start\":57567},{\"end\":57589,\"start\":57579},{\"end\":57602,\"start\":57589},{\"end\":57616,\"start\":57602},{\"end\":57627,\"start\":57616},{\"end\":57638,\"start\":57627},{\"end\":57649,\"start\":57638},{\"end\":58171,\"start\":58158},{\"end\":58186,\"start\":58171},{\"end\":58200,\"start\":58186},{\"end\":58209,\"start\":58200},{\"end\":58222,\"start\":58209},{\"end\":58235,\"start\":58222},{\"end\":58250,\"start\":58235},{\"end\":58264,\"start\":58250},{\"end\":58276,\"start\":58264},{\"end\":58288,\"start\":58276},{\"end\":58293,\"start\":58288},{\"end\":58840,\"start\":58821},{\"end\":58845,\"start\":58840},{\"end\":58869,\"start\":58845},{\"end\":58881,\"start\":58869},{\"end\":58896,\"start\":58881},{\"end\":58908,\"start\":58896},{\"end\":58913,\"start\":58908},{\"end\":59386,\"start\":59376},{\"end\":59401,\"start\":59386},{\"end\":59411,\"start\":59401},{\"end\":59779,\"start\":59764},{\"end\":59793,\"start\":59779},{\"end\":59940,\"start\":59932},{\"end\":59953,\"start\":59940},{\"end\":59967,\"start\":59953},{\"end\":59978,\"start\":59967},{\"end\":59989,\"start\":59978},{\"end\":59998,\"start\":59989},{\"end\":60008,\"start\":59998},{\"end\":60353,\"start\":60339},{\"end\":60369,\"start\":60353},{\"end\":60391,\"start\":60369},{\"end\":60408,\"start\":60391},{\"end\":60430,\"start\":60408},{\"end\":60451,\"start\":60430},{\"end\":60836,\"start\":60823},{\"end\":60850,\"start\":60836},{\"end\":60863,\"start\":60850},{\"end\":60875,\"start\":60863},{\"end\":60890,\"start\":60875},{\"end\":61135,\"start\":61124},{\"end\":61153,\"start\":61135},{\"end\":61164,\"start\":61153},{\"end\":61180,\"start\":61164},{\"end\":61189,\"start\":61180},{\"end\":61498,\"start\":61491},{\"end\":61508,\"start\":61498},{\"end\":61663,\"start\":61649},{\"end\":61680,\"start\":61663},{\"end\":61695,\"start\":61680},{\"end\":61933,\"start\":61920},{\"end\":61945,\"start\":61933},{\"end\":61959,\"start\":61945},{\"end\":61970,\"start\":61959},{\"end\":61981,\"start\":61970},{\"end\":61992,\"start\":61981},{\"end\":62003,\"start\":61992},{\"end\":62012,\"start\":62003},{\"end\":62022,\"start\":62012},{\"end\":62317,\"start\":62302},{\"end\":62331,\"start\":62317},{\"end\":62341,\"start\":62331},{\"end\":62523,\"start\":62511},{\"end\":62537,\"start\":62523},{\"end\":62549,\"start\":62537},{\"end\":62560,\"start\":62549},{\"end\":62574,\"start\":62560},{\"end\":62583,\"start\":62574},{\"end\":62863,\"start\":62849},{\"end\":62875,\"start\":62863},{\"end\":62890,\"start\":62875},{\"end\":62905,\"start\":62890},{\"end\":63136,\"start\":63121},{\"end\":63151,\"start\":63136},{\"end\":63168,\"start\":63151},{\"end\":63484,\"start\":63470},{\"end\":63499,\"start\":63484},{\"end\":63512,\"start\":63499},{\"end\":63527,\"start\":63512},{\"end\":63539,\"start\":63527},{\"end\":63557,\"start\":63539},{\"end\":63571,\"start\":63557},{\"end\":63585,\"start\":63571},{\"end\":63883,\"start\":63870},{\"end\":63897,\"start\":63883},{\"end\":63910,\"start\":63897},{\"end\":63926,\"start\":63910},{\"end\":64218,\"start\":64200},{\"end\":64233,\"start\":64218},{\"end\":64246,\"start\":64233},{\"end\":64264,\"start\":64246},{\"end\":64573,\"start\":64558},{\"end\":64586,\"start\":64573},{\"end\":64605,\"start\":64586},{\"end\":64619,\"start\":64605},{\"end\":64634,\"start\":64619},{\"end\":64648,\"start\":64634},{\"end\":64979,\"start\":64962},{\"end\":64997,\"start\":64979},{\"end\":65013,\"start\":64997},{\"end\":65399,\"start\":65387},{\"end\":65407,\"start\":65399},{\"end\":65422,\"start\":65407},{\"end\":65429,\"start\":65422},{\"end\":65671,\"start\":65654},{\"end\":65685,\"start\":65671},{\"end\":65704,\"start\":65685},{\"end\":66139,\"start\":66126},{\"end\":66154,\"start\":66139},{\"end\":66169,\"start\":66154},{\"end\":66176,\"start\":66169},{\"end\":66189,\"start\":66176},{\"end\":66199,\"start\":66189},{\"end\":66212,\"start\":66199},{\"end\":66584,\"start\":66576},{\"end\":66602,\"start\":66584},{\"end\":66611,\"start\":66602},{\"end\":66620,\"start\":66611},{\"end\":66630,\"start\":66620},{\"end\":66915,\"start\":66903},{\"end\":66930,\"start\":66915},{\"end\":66946,\"start\":66930},{\"end\":66958,\"start\":66946},{\"end\":66972,\"start\":66958},{\"end\":66987,\"start\":66972},{\"end\":66999,\"start\":66987},{\"end\":67013,\"start\":66999},{\"end\":67027,\"start\":67013},{\"end\":67040,\"start\":67027},{\"end\":67378,\"start\":67366},{\"end\":67388,\"start\":67378},{\"end\":67401,\"start\":67388},{\"end\":67413,\"start\":67401},{\"end\":67427,\"start\":67413},{\"end\":67439,\"start\":67427},{\"end\":67450,\"start\":67439},{\"end\":67462,\"start\":67450},{\"end\":67480,\"start\":67462},{\"end\":67498,\"start\":67480},{\"end\":67507,\"start\":67498},{\"end\":67892,\"start\":67879},{\"end\":67903,\"start\":67892},{\"end\":67919,\"start\":67903},{\"end\":68317,\"start\":68309},{\"end\":68329,\"start\":68317},{\"end\":68338,\"start\":68329},{\"end\":68346,\"start\":68338},{\"end\":68358,\"start\":68346},{\"end\":68371,\"start\":68358},{\"end\":68384,\"start\":68371},{\"end\":68397,\"start\":68384},{\"end\":68709,\"start\":68697},{\"end\":68721,\"start\":68709},{\"end\":68730,\"start\":68721},{\"end\":68742,\"start\":68730},{\"end\":69125,\"start\":69112},{\"end\":69139,\"start\":69125},{\"end\":69149,\"start\":69139},{\"end\":69161,\"start\":69149},{\"end\":69175,\"start\":69161},{\"end\":69184,\"start\":69175},{\"end\":69197,\"start\":69184},{\"end\":69207,\"start\":69197},{\"end\":69221,\"start\":69207},{\"end\":69234,\"start\":69221},{\"end\":69685,\"start\":69672},{\"end\":69696,\"start\":69685},{\"end\":69711,\"start\":69696},{\"end\":69933,\"start\":69916},{\"end\":69946,\"start\":69933},{\"end\":69961,\"start\":69946},{\"end\":69974,\"start\":69961},{\"end\":69985,\"start\":69974},{\"end\":70381,\"start\":70369},{\"end\":70400,\"start\":70381},{\"end\":70420,\"start\":70400},{\"end\":70433,\"start\":70420},{\"end\":70446,\"start\":70433},{\"end\":70765,\"start\":70751},{\"end\":70781,\"start\":70765},{\"end\":70798,\"start\":70781},{\"end\":70816,\"start\":70798},{\"end\":70836,\"start\":70816},{\"end\":71250,\"start\":71235},{\"end\":71279,\"start\":71250},{\"end\":71470,\"start\":71462},{\"end\":71480,\"start\":71470},{\"end\":71627,\"start\":71614},{\"end\":71638,\"start\":71627},{\"end\":71655,\"start\":71638},{\"end\":71667,\"start\":71655},{\"end\":71683,\"start\":71667},{\"end\":71699,\"start\":71683},{\"end\":71715,\"start\":71699},{\"end\":71727,\"start\":71715},{\"end\":71747,\"start\":71727},{\"end\":71760,\"start\":71747},{\"end\":72176,\"start\":72162},{\"end\":72189,\"start\":72176},{\"end\":72205,\"start\":72189},{\"end\":72473,\"start\":72458},{\"end\":72486,\"start\":72473},{\"end\":72496,\"start\":72486},{\"end\":72509,\"start\":72496},{\"end\":72520,\"start\":72509},{\"end\":72530,\"start\":72520},{\"end\":72545,\"start\":72530},{\"end\":72558,\"start\":72545},{\"end\":72583,\"start\":72558},{\"end\":72590,\"start\":72583},{\"end\":72996,\"start\":72978},{\"end\":73007,\"start\":72996},{\"end\":73020,\"start\":73007},{\"end\":73335,\"start\":73317},{\"end\":73347,\"start\":73335},{\"end\":73367,\"start\":73347},{\"end\":73380,\"start\":73367},{\"end\":73661,\"start\":73646},{\"end\":73674,\"start\":73661},{\"end\":73689,\"start\":73674},{\"end\":73704,\"start\":73689},{\"end\":73996,\"start\":73983},{\"end\":74012,\"start\":73996},{\"end\":74029,\"start\":74012},{\"end\":74042,\"start\":74029},{\"end\":74267,\"start\":74254},{\"end\":74280,\"start\":74267},{\"end\":74298,\"start\":74280},{\"end\":74533,\"start\":74520},{\"end\":74545,\"start\":74533},{\"end\":74562,\"start\":74545},{\"end\":74571,\"start\":74562},{\"end\":74583,\"start\":74571},{\"end\":74599,\"start\":74583},{\"end\":74904,\"start\":74892},{\"end\":74915,\"start\":74904},{\"end\":74925,\"start\":74915},{\"end\":74937,\"start\":74925},{\"end\":74949,\"start\":74937},{\"end\":74963,\"start\":74949},{\"end\":74974,\"start\":74963},{\"end\":74988,\"start\":74974},{\"end\":74997,\"start\":74988},{\"end\":75416,\"start\":75406},{\"end\":75425,\"start\":75416},{\"end\":75437,\"start\":75425},{\"end\":75715,\"start\":75694},{\"end\":75730,\"start\":75715},{\"end\":75742,\"start\":75730},{\"end\":75752,\"start\":75742},{\"end\":75768,\"start\":75752},{\"end\":75772,\"start\":75768},{\"end\":76087,\"start\":76071},{\"end\":76103,\"start\":76087},{\"end\":76112,\"start\":76103},{\"end\":76126,\"start\":76112},{\"end\":76141,\"start\":76126},{\"end\":76150,\"start\":76141},{\"end\":76160,\"start\":76150},{\"end\":76176,\"start\":76160},{\"end\":76183,\"start\":76176},{\"end\":76666,\"start\":76650},{\"end\":76682,\"start\":76666},{\"end\":76907,\"start\":76897},{\"end\":76917,\"start\":76907},{\"end\":76926,\"start\":76917},{\"end\":76940,\"start\":76926},{\"end\":77188,\"start\":77175},{\"end\":77200,\"start\":77188},{\"end\":77214,\"start\":77200},{\"end\":77226,\"start\":77214},{\"end\":77239,\"start\":77226},{\"end\":77251,\"start\":77239},{\"end\":77648,\"start\":77633},{\"end\":77664,\"start\":77648},{\"end\":77683,\"start\":77664},{\"end\":77695,\"start\":77683},{\"end\":77707,\"start\":77695},{\"end\":77721,\"start\":77707},{\"end\":77734,\"start\":77721},{\"end\":77750,\"start\":77734},{\"end\":77768,\"start\":77750},{\"end\":77782,\"start\":77768},{\"end\":78285,\"start\":78272},{\"end\":78303,\"start\":78285},{\"end\":78319,\"start\":78303},{\"end\":78334,\"start\":78319},{\"end\":78624,\"start\":78610},{\"end\":78639,\"start\":78624},{\"end\":78655,\"start\":78639},{\"end\":78672,\"start\":78655},{\"end\":78696,\"start\":78672},{\"end\":78709,\"start\":78696},{\"end\":79063,\"start\":79047},{\"end\":79077,\"start\":79063},{\"end\":79090,\"start\":79077},{\"end\":79107,\"start\":79090},{\"end\":79120,\"start\":79107},{\"end\":79135,\"start\":79120},{\"end\":79150,\"start\":79135},{\"end\":79168,\"start\":79150},{\"end\":79562,\"start\":79549},{\"end\":79576,\"start\":79562},{\"end\":79591,\"start\":79576},{\"end\":79606,\"start\":79591},{\"end\":79618,\"start\":79606},{\"end\":79874,\"start\":79863},{\"end\":79891,\"start\":79874},{\"end\":79907,\"start\":79891},{\"end\":79919,\"start\":79907},{\"end\":79930,\"start\":79919},{\"end\":79947,\"start\":79930},{\"end\":80368,\"start\":80355},{\"end\":80381,\"start\":80368},{\"end\":80394,\"start\":80381},{\"end\":80403,\"start\":80394},{\"end\":80415,\"start\":80403},{\"end\":80427,\"start\":80415},{\"end\":80437,\"start\":80427},{\"end\":80768,\"start\":80755},{\"end\":80782,\"start\":80768},{\"end\":80792,\"start\":80782},{\"end\":81186,\"start\":81173},{\"end\":81198,\"start\":81186},{\"end\":81213,\"start\":81198},{\"end\":81223,\"start\":81213},{\"end\":81231,\"start\":81223},{\"end\":81526,\"start\":81513},{\"end\":81536,\"start\":81526},{\"end\":81545,\"start\":81536},{\"end\":81557,\"start\":81545},{\"end\":81567,\"start\":81557},{\"end\":81578,\"start\":81567},{\"end\":81875,\"start\":81862},{\"end\":81892,\"start\":81875},{\"end\":81901,\"start\":81892},{\"end\":82114,\"start\":82099},{\"end\":82131,\"start\":82114},{\"end\":82148,\"start\":82131},{\"end\":82482,\"start\":82466},{\"end\":82497,\"start\":82482},{\"end\":82514,\"start\":82497},{\"end\":82789,\"start\":82776},{\"end\":82806,\"start\":82789},{\"end\":82822,\"start\":82806},{\"end\":82835,\"start\":82822},{\"end\":82853,\"start\":82835},{\"end\":82866,\"start\":82853},{\"end\":82882,\"start\":82866},{\"end\":82900,\"start\":82882},{\"end\":82913,\"start\":82900},{\"end\":82927,\"start\":82913},{\"end\":83547,\"start\":83534},{\"end\":83560,\"start\":83547},{\"end\":83568,\"start\":83560},{\"end\":83579,\"start\":83568},{\"end\":83589,\"start\":83579},{\"end\":83837,\"start\":83823},{\"end\":83851,\"start\":83837},{\"end\":83863,\"start\":83851},{\"end\":84347,\"start\":84339},{\"end\":84361,\"start\":84347},{\"end\":84373,\"start\":84361},{\"end\":84386,\"start\":84373},{\"end\":84397,\"start\":84386},{\"end\":84808,\"start\":84800},{\"end\":84816,\"start\":84808},{\"end\":84829,\"start\":84816},{\"end\":84842,\"start\":84829},{\"end\":84851,\"start\":84842},{\"end\":84860,\"start\":84851},{\"end\":84873,\"start\":84860},{\"end\":85291,\"start\":85278},{\"end\":85303,\"start\":85291},{\"end\":85316,\"start\":85303},{\"end\":85333,\"start\":85316},{\"end\":85341,\"start\":85333},{\"end\":85363,\"start\":85341},{\"end\":85367,\"start\":85363},{\"end\":85743,\"start\":85731},{\"end\":85755,\"start\":85743},{\"end\":85767,\"start\":85755},{\"end\":85781,\"start\":85767},{\"end\":85800,\"start\":85781},{\"end\":86134,\"start\":86122},{\"end\":86146,\"start\":86134},{\"end\":86160,\"start\":86146},{\"end\":86171,\"start\":86160},{\"end\":86182,\"start\":86171},{\"end\":86191,\"start\":86182},{\"end\":86537,\"start\":86521},{\"end\":86553,\"start\":86537},{\"end\":86568,\"start\":86553},{\"end\":86582,\"start\":86568},{\"end\":86972,\"start\":86956},{\"end\":86982,\"start\":86972},{\"end\":87389,\"start\":87376},{\"end\":87403,\"start\":87389},{\"end\":87416,\"start\":87403},{\"end\":87434,\"start\":87416},{\"end\":87728,\"start\":87714},{\"end\":87738,\"start\":87728},{\"end\":87753,\"start\":87738},{\"end\":87772,\"start\":87753},{\"end\":87785,\"start\":87772},{\"end\":88148,\"start\":88130},{\"end\":88159,\"start\":88148},{\"end\":88167,\"start\":88159},{\"end\":88183,\"start\":88167},{\"end\":88190,\"start\":88183},{\"end\":88200,\"start\":88190}]", "bib_venue": "[{\"end\":50656,\"start\":50616},{\"end\":51026,\"start\":50908},{\"end\":51351,\"start\":51274},{\"end\":51651,\"start\":51614},{\"end\":52175,\"start\":52077},{\"end\":52715,\"start\":52629},{\"end\":53056,\"start\":52983},{\"end\":53660,\"start\":53611},{\"end\":53939,\"start\":53880},{\"end\":54437,\"start\":54383},{\"end\":54775,\"start\":54739},{\"end\":55116,\"start\":55042},{\"end\":55492,\"start\":55419},{\"end\":56000,\"start\":55926},{\"end\":56431,\"start\":56371},{\"end\":56717,\"start\":56645},{\"end\":56975,\"start\":56929},{\"end\":57211,\"start\":57166},{\"end\":57744,\"start\":57649},{\"end\":58375,\"start\":58293},{\"end\":58994,\"start\":58913},{\"end\":59478,\"start\":59411},{\"end\":59762,\"start\":59727},{\"end\":60066,\"start\":60024},{\"end\":60337,\"start\":60246},{\"end\":60821,\"start\":60730},{\"end\":61284,\"start\":61205},{\"end\":61647,\"start\":61576},{\"end\":62088,\"start\":62038},{\"end\":62384,\"start\":62357},{\"end\":62629,\"start\":62599},{\"end\":62847,\"start\":62800},{\"end\":63220,\"start\":63168},{\"end\":63468,\"start\":63374},{\"end\":64012,\"start\":63942},{\"end\":64317,\"start\":64280},{\"end\":64556,\"start\":64480},{\"end\":65084,\"start\":65013},{\"end\":65478,\"start\":65429},{\"end\":65799,\"start\":65704},{\"end\":66124,\"start\":66011},{\"end\":66574,\"start\":66487},{\"end\":67084,\"start\":67040},{\"end\":67569,\"start\":67523},{\"end\":67980,\"start\":67919},{\"end\":68307,\"start\":68236},{\"end\":68816,\"start\":68742},{\"end\":69316,\"start\":69234},{\"end\":69670,\"start\":69629},{\"end\":70066,\"start\":69985},{\"end\":70506,\"start\":70462},{\"end\":70880,\"start\":70836},{\"end\":71233,\"start\":71116},{\"end\":71809,\"start\":71760},{\"end\":72160,\"start\":72108},{\"end\":72660,\"start\":72590},{\"end\":73094,\"start\":73036},{\"end\":73315,\"start\":73256},{\"end\":73644,\"start\":73578},{\"end\":73981,\"start\":73904},{\"end\":74364,\"start\":74314},{\"end\":74650,\"start\":74615},{\"end\":75058,\"start\":74997},{\"end\":75481,\"start\":75437},{\"end\":75692,\"start\":75626},{\"end\":76269,\"start\":76183},{\"end\":76750,\"start\":76698},{\"end\":77013,\"start\":76956},{\"end\":77336,\"start\":77267},{\"end\":77857,\"start\":77782},{\"end\":78270,\"start\":78209},{\"end\":78753,\"start\":78709},{\"end\":79217,\"start\":79168},{\"end\":79547,\"start\":79449},{\"end\":80048,\"start\":79963},{\"end\":80353,\"start\":80279},{\"end\":80874,\"start\":80792},{\"end\":81171,\"start\":81123},{\"end\":81511,\"start\":81413},{\"end\":81860,\"start\":81817},{\"end\":82209,\"start\":82148},{\"end\":82464,\"start\":82386},{\"end\":83036,\"start\":82927},{\"end\":83532,\"start\":83484},{\"end\":83950,\"start\":83863},{\"end\":84484,\"start\":84397},{\"end\":84979,\"start\":84889},{\"end\":85416,\"start\":85367},{\"end\":85729,\"start\":85648},{\"end\":86120,\"start\":86026},{\"end\":86631,\"start\":86582},{\"end\":87060,\"start\":86982},{\"end\":87477,\"start\":87450},{\"end\":87825,\"start\":87785},{\"end\":88249,\"start\":88200},{\"end\":55552,\"start\":55494},{\"end\":59532,\"start\":59480},{\"end\":65142,\"start\":65086},{\"end\":68028,\"start\":67982},{\"end\":70134,\"start\":70068},{\"end\":75106,\"start\":75060},{\"end\":76342,\"start\":76271},{\"end\":83132,\"start\":83038},{\"end\":84024,\"start\":83952},{\"end\":84558,\"start\":84486},{\"end\":87852,\"start\":87827}]"}}}, "year": 2023, "month": 12, "day": 17}