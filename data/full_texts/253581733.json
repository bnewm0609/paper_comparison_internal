{"id": 253581733, "updated": "2023-10-05 08:13:29.451", "metadata": {"title": "Task-aware Retrieval with Instructions", "authors": "[{\"first\":\"Akari\",\"last\":\"Asai\",\"middle\":[]},{\"first\":\"Timo\",\"last\":\"Schick\",\"middle\":[]},{\"first\":\"Patrick\",\"last\":\"Lewis\",\"middle\":[]},{\"first\":\"Xilun\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Gautier\",\"last\":\"Izacard\",\"middle\":[]},{\"first\":\"Sebastian\",\"last\":\"Riedel\",\"middle\":[]},{\"first\":\"Hannaneh\",\"last\":\"Hajishirzi\",\"middle\":[]},{\"first\":\"Wen-tau\",\"last\":\"Yih\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We study the problem of retrieval with instructions, where users of a retrieval system explicitly describe their intent along with their queries. We aim to develop a general-purpose task-aware retrieval system using multi-task instruction tuning, which can follow human-written instructions to find the best documents for a given query. We introduce the first large-scale collection of approximately 40 retrieval datasets with instructions, BERRI, and present TART, a multi-task retrieval system trained on BERRI with instructions. TART shows strong capabilities to adapt to a new retrieval task via instructions and advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We further introduce a new evaluation setup, X^2-Retrieval to better reflect real-world scenarios, where diverse domains and tasks are pooled and a system needs to find documents aligning users' intents. In this setup, TART significantly outperforms competitive baselines, further demonstrating the effectiveness of guiding retrieval with instructions.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2211.09260", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/AsaiSL0I0HY23", "doi": "10.18653/v1/2023.findings-acl.225"}}, "content": {"source": {"pdf_hash": "8cf05ed2b7cd3b0f601c454914a678c24d393de3", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2211.09260v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "08866fb925764916ba8c1f304189bbdffd2a22b1", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8cf05ed2b7cd3b0f601c454914a678c24d393de3.txt", "contents": "\nTask-aware Retrieval with Instructions\n\n\nAkari Asai \nUniversity of Washington \u2660 ENS\nPSL University & Inria \u2665 Allen Institute for AI \u2663 University College London\n\n\nTimo Schick \nUniversity of Washington \u2660 ENS\nPSL University & Inria \u2665 Allen Institute for AI \u2663 University College London\n\n\nPatrick Lewis \nUniversity of Washington \u2660 ENS\nPSL University & Inria \u2665 Allen Institute for AI \u2663 University College London\n\n\nXilun Chen \nUniversity of Washington \u2660 ENS\nPSL University & Inria \u2665 Allen Institute for AI \u2663 University College London\n\n\nGautier Izacard \nUniversity of Washington \u2660 ENS\nPSL University & Inria \u2665 Allen Institute for AI \u2663 University College London\n\n\nSebastian Riedel \nUniversity of Washington \u2660 ENS\nPSL University & Inria \u2665 Allen Institute for AI \u2663 University College London\n\n\nHannaneh Hajishirzi \nUniversity of Washington \u2660 ENS\nPSL University & Inria \u2665 Allen Institute for AI \u2663 University College London\n\n\nWen-Tau Yih \nUniversity of Washington \u2660 ENS\nPSL University & Inria \u2665 Allen Institute for AI \u2663 University College London\n\n\nMeta Ai \nUniversity of Washington \u2660 ENS\nPSL University & Inria \u2665 Allen Institute for AI \u2663 University College London\n\n\nTask-aware Retrieval with Instructions\n\nWe study the problem of retrieval with instructions, where users of a retrieval system explicitly describe their intent along with their queries. We aim to develop a general-purpose task-aware retrieval system using multi-task instruction tuning, which can follow humanwritten instructions to find the best documents for a given query. We introduce the first large-scale collection of approximately 40 retrieval datasets with instructions, BERRI, and present TART, a multi-task retrieval system trained on BERRI with instructions. TART shows strong capabilities to adapt to a new retrieval task via instructions and advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We further introduce a new evaluation setup, X 2 -Retrieval to better reflect real-world scenarios, where diverse domains and tasks are pooled and a system needs to find documents aligning users' intents. In this setup, TART significantly outperforms competitive baselines, further demonstrating the effectiveness of guiding retrieval with instructions. 1 . 2016. MS MARCO: A human generated machine reading comprehension dataset. . 2015. Microsoft coco captions: Data collection and evaluation server.\n\nIntroduction\n\nInformation retrieval (IR) is the task of finding relevant documents from a large collection of texts to fulfill a user's information need, typically expressed in the form of a textual query (Singhal et al., 2001). The notion of relevance from the user's perspective (i.e., intent) can be amorphous (Mizzaro, 1998), and a query alone may not fully capture user information needs (Ruthven and Lalmas, 2003;Taylor, 1962). As illustrated in Figure 1 (top), given the same query, \"implementing batch normalization in Python,\" a user may want to retrieve a passage that describes how to do the task or to identify a similar query, or even to directly locate a code snippet.  Figure 1: User intents are not fully captured in query q only (top). Conventional approaches (bottom left) take a query and retrieve documents from a closed corpus using a task-specific retriever. Retrieval with instructions (bottom right) takes a query and explicit intent and retrieves documents aligning with the user's expectations.\n\nMost existing work tries to learn those implicit intents from labeled data (e.g., pairs of queries and relevant documents), yielding separate models for different intents as shown in the bottom left of Figure 1. This approach has several limitations. First, a vast number of annotated examples may be required to train a model to capture the task-specific notion of relevance, while they could benefit from the abundance of data available from related tasks. Second, a model trained on one task may not easily transfer to new tasks that are not closely related.\n\nIn this work we advocate for a new task formulation, retrieval with instructions, to explicitly model a user's search intent by providing a natural language description of the search task (i.e., an instruction). Here, the goal of retrieval systems is to retrieve documents that are both relevant to the query and well-suited to the instructions.\n\nDespite active research in other settings, instruction-following has not been systematically explored in retrieval, partly due to the lack of annotated resources. To facilitate research in retrieval with instructions, we introduce BERRI (Bank of Explicit RetRieval Instructions), a collection of approximately 40 retrieval datasets with diverse instructions in a unified format, covering 10 diverse domains. Each task has on average 3.5 diverse instructions annotated by experts, following our novel instruction schema for retrieval tasks.\n\nWe use BERRI to train TART (Task-aware ReTriever), a single multi-task retrieval system that follows instructions to perform diverse tasks with no parameter updates on each task. We employ two widely explored architectures: TART-dual is a dense dual-encoder architecture, retrieving documents based on the similarity of independently encoded query and document embeddings; TARTfull calculates probabilities of a document being relevant to the query according to the instruction using a cross-encoder. TART is trained with carefully designed negative samples, including our novel instruction-unfollowing negatives samples.\n\nThe TART models, particularly TART-full yields state-of-the-art results on two popular zeroshot retrieval benchmarks, BEIR (Thakur et al., 2021) and LOTTE-pooled (Santhanam et al., 2022), outperforming systems using three times more parameters (Nogueira et al. 2020;Ni et al. 2021; Muennighoff 2022) as well as task-specific retrievers trained on millions of automatically generated examples (Dai et al., 2022;Wang et al., 2022a).\n\nWe further introduce a new evaluation setup, X 2 -Retrieval (Cross-task Cross-domain Retrieval), where a system needs to handle queries with diverse intents to find relevant documents from a large-scale, cross-domain pooled corpus, simulating challenges in real-world retrieval applications. In this under-explored setting, TART outperforms other state-of-the-art methods, demonstrating its ability to find documents in a large-scale opendomain corpus by leveraging explicit textual intents. Our analysis shows that training a model on diverse tasks with instructions, our new negative samples leveraging instructions and giving informative instructions are crucial.\n\nIn summary, our contributions are as follows:\n\n\u2022 Retrieval with instructions, a new formulation to model users' intent explicitly (Section 3).\n\n\u2022 BERRI, a new large-scale collection of approximately 40 retrieval datasets in diverse domains with instructions (Section 4).\n\n\u2022 TART, a task-aware retriever trained on BERRI that advances state of the art on zeroshot and cross-task retrieval (Section 5).\n\n\nBackground and Related Work\n\nZero-shot training of retrievers. Recent neural retrievers (Karpukhin et al., 2020;Lee et al., 2019;Khattab and Zaharia, 2020) show their superiority over term-based retrievers (e.g., BM25; Robertson and Zaragoza 2009) across domains when training data is abundant (Luo et al., 2022;Asai et al., 2021;Petroni et al., 2021). Due to the high cost of annotating retrieval datasets for new target tasks, improving neural retrievers in zeroshot settings is an active area of study. The first line of work uses purely unsupervised approaches, such as pre-training neural retrievers on unlabeled data (e.g., Contriever;Izacard et al. 2022). The second line of work trains a single retrieval system on large-scale supervised datasets such as MS MARCO (Bajaj et al., 2016) and then performs a transfer to new datasets (Izacard et al., 2022;Khattab and Zaharia, 2020;Nogueira et al., 2020;Chen et al., 2022), which often struggle with tasks unlike those used for training (Dai et al., 2022). To address this, the third paradigm trains customized retrievers for each task using unlabeled corpora, leveraging another model to automatically generate training data (Wang et al., 2022a). Concurrent to our work, Dai et al. (2022) use task-specific templates and few-shot samples to automatically generate in-domain training queries given randomly sampled documents from the target corpus using FLAN (Wei et al., 2022a). It often entails running massive LMs and training separate retrievers, resulting in slow and costly adaptation.\n\nInstruction tuning. Training large language models (LLMs) with instructions or demonstrations on many tasks has proven very effective for zeroor few-shot transfer in a variety of settings (Wei et al., 2022a;Sanh et al., 2022;Ouyang et al., 2022;Min et al., 2022;Wang et al., 2022b;Mishra et al., 2022;Chung et al., 2022). However, such instruction tuning has not been systematically explored in retrieval for several reasons. First, large-scale instruction-annotated datasets (Bach et al., 2022;Wang et al., 2022b) do not include retrieval tasks. Second, successful instruction-following LLMs are encoder-decoder or decoder-only models with tens of billions of parameters, which are difficult to be adapted for retrieval tasks, as we typically need to Dataset Instruction NQ Retrieve a Wikipedia paragraph that answers this question. Med Simple Your task is to find a simplified paragraph of this paragraph from a medical paper.\n\n\nQReCC\n\nFind a dialogue response from dialogue history to answer the user's question.\n\n\nArguana\n\nRetrieve a paragraph from an argument website that argues against the following argument.\n\n\nSciFact\n\nFind a sentence from a scientific paper to check if the statement is correct or not. MultiLexSum I want to find the one-sentence summary of this legal case.  et al., 2021), Arguana (Wachsmuth et al., 2018), Sci-Fact (Wadden et al., 2020) and MultiLexSum (Shen et al., 2022). Each instruction defines intent, domain and unit.\n\nencode millions of documents.\n\nRetrieval with descriptions. To incorporate more fine-grained information needs, retrieval with descriptions or narratives (e.g., TREC 2004 Robust Track; Voorhees 2005) has been studied. Descriptions or narratives are more detailed natural language explanations that describe the information needs (i.e., desirable documents) for each query mainly for query disambiguation. However, early work shows that concatenating descriptions or narratives only marginally outperforms the baselines with titles only (Walker et al., 1998;Yates et al., 2021). More recent work (Dai andCallan, 2019, 2020) suggests that powerful encoders such as BERT (Devlin et al., 2019) could better incorporate rich linguistic context and boost performance.\n\n\nTask Formulation\n\nThis work introduces a new task formulation, retrieval with instructions ( Figure 1 bottom right). We are given a large collection of N documents D = {d 1 , . . . , d N }, a search task instruction t and a query q. The problem of retrieval with instructions is to find a document d \u2208 D that is relevant to q according to the instruction t. Compared to the standard retrieval problem setting (e.g., Figure 1 bottom left), the difference is the explicit definition of relevance in the instruction t as additional input to the system, which can be diverse and may not be fully defined by the query only (Ruthven and Lalmas, 2003). Thus, retrieval with instructions lets us build retrieval systems that are very general and task-aware-changing their relevance measure by attending to the instruction.\n\nThis new formulation brings both new research challenges and opportunities. For instance, a retriever is now required to modify its search behavior according to the instructions. On the plus side, different datasets can be naturally grouped to train a single retriever, yielding benefits from cross-task interdependence, which has been observed in instruction tuning of LLMs. Compared to training a retriever on multiple tasks with task identifiers (Maillard et al., 2021), instructions provide greater flexibility and enable zero-shot transfer via natural language instructions. Furthermore, a multi-task instruction-following retriever obviates the need to host multiple task-specific retrievers.\n\nAs noted previously, multi-task training with instructions has not been studied in the area of retrieval due to the lack of resources and dedicated models. To facilitate the research on retrieval with instructions, we introduce the first large-scale retrieval benchmark with expert-written annotations (Section 4) and subsequently the multi-task instruction-following retrievers (Section 5).\n\n\nBERRI: Collections of Instruction-annotated Retrieval Tasks\n\nTo facilitate research in retrieval with natural language instructions, we build a unified largescale retrieval dataset, BERRI (Bank of Explicit RetRieval Instructions). BERRI consists of instructions and instances from diverse domains and tasks, by carefully curating retrieval datasets and casting diverse NLP datasets as retrieval tasks.\n\n\nUnified Task and Instructions Scheme\n\nTask format. Each task T in BERRI consists of a corpus D, queries Q = {q 1 , . . . , q K }, and an instruction t, where K is the number of the queries included in the task. An instance of each task includes a query q, gold (relevant) documents d + , and negative (irrelevant) documents d \u2212 . For each task, an explicit intent t, which can be paraphrased in multiple ways, is given.  Instruction schema for retrieval. Informative and diverse instructions are key for successful instruction tuning (Sanh et al., 2022). We introduce a novel scheme to define an informative instruction for retrieval tasks, which have not been studied in prior instruction-following literature. An instruction that sufficiently describes an arbitrary retrieval task should include: intent, domain and unit. Specifically, intent describes how the retrieved text relates to the query, such as whether the text answers a question in the query or paraphrases it. Domain is the expected source or type of retrieved text, such as Wikipedia or PubMed articles. Unit defines the text block to retrieve, such as a sentence or a paragraph. Table 1 shows examples of instructions, and Appendix A.3 shows the full list.\n\n\nDataset Collection\n\nBERRI includes diverse datasets, including widely-used retrieval (e.g., MS MARCO; Bajaj et al. 2016), retrieval-centric datasets (NQ-open;Kwiatkowski et al. 2019) and non-retrieval datasets that can be repurposed as retrieval tasks. Figure 2 shows the source datasets in BERRI. We manually annotate in multiple instructions per dataset, and conduct multiple post-processing steps to provide a set of the query, gold, and negative documents. Table 5 shows the full dataset list.\n\nSelecting source datasets. We manually collect datasets from (1) KILT (Petroni et al., 2021), (2) the Sentence-Transformers Training Data for Text Embedding Models 2 , and (3) manual searches in ACL anthologies and huggingface datasets 3 to cover a diverse set of tasks and domains. We observe that except for a few domains (e.g., Wikipedia) many domains do not have retrieval datasets while 2 https://huggingface.co/datasets/ sentence-transformers/embedding-training-data 3 https://huggingface.co/docs/datasets/index there are a few datasets for other NLP tasks that can be cast as retrieval (e.g., sentence paraphrase). Re-purposing those non-retrieval tasks as retrieval tasks enables to diversity of the domains as well as the instructions in BERRI.\n\nFrom an initial list of more than 60 datasets, we assess whether it is suitable for repurposing as a retrieval task. Specifically, we sample 20 instances from the candidate dataset and check if the queries are self-contained. 4 If the majority of queries fail this test, we exclude the corresponding dataset. Consequently, we use 37 datasets, including more than 5 million instances in total. For datasets that are orders of magnitude larger than other datasets (e.g., PAQ; Lewis et al. 2021), we randomly sample up to 300k instances, except for MS MARCO. 5 As a result, BERRI covers diverse domains (e.g., Wikipedia, scientific papers) and tasks (e.g., fact verification, dialogue response retrieval, QA). See Appendix A.4 for more details.\n\nUnification and instruction annotations. For retrieval datasets such as MS MARCO, we use the annotated gold documents as positive documents d + to a given query q. Regarding non-retrieval tasks, we use the original input sequence as a query q and the original output or given context as d + . For instance, given a summarization dataset we use a source text and a summary as a query and a gold document, respectively.\n\nFor each dataset, the authors of this paper manually wrote instructions (8 maximum and 3.5 average number of instructions per task). For datasets without preprocessed retrieval targets, 6 we gather all positive and negative documents provided by  Figure 3: Examples of documents that are considered gold documents d + , and two types of negative documents d \u2212 : hard negatives d HD and instruction-unfollowing negatives d UF for two different query and instruction pairs. the original dataset to build a single task-specific retrieval corpus D. More details are described in Appendix Section A.2.\n\nNegative documents selection. Negative samples are crucial for training retrieval systems (Zhan et al., 2021). In addition to the randomly sampled negative samples (random negative documents), we introduce two types of challenging negative samples: denoised hard negative documents d HD and instruction-unfollowing negative documents d UF . Figure 3 shows examples of gold documents, hard negatives, and instruction-unfollowing documents. We mine hard negative documents d HD using an off-the-shelf retriever and then filter out false negative documents using an off-the-shelf reranker, following Qu et al. (2021). In particular, we retrieve top documents from the target corpus using Contriever (Izacard et al., 2022) and then add new documents whose normalized scores predicted by a cross-encoder model, ms-marco-MiniLM-L-12-v2 7 are below 0.1 as hard negative documents.\n\nWe further introduce a new negative sampling strategy, instruction-unfollowing negative samples d UF , to make systems learn to follow instructions and retrieve documents that align with the specified intents. As shown in Figure 3, given an instruction \"find an informative dialogue response\", a system should not retrieve a Wikipedia paragraph about armadillos. To obtain those instruction-unfollowing negative documents, we retrieve documents from a different task's target corpus using the same off-theshelf Contriever and consider all those documents to be negatives since they do not satisfy the instruction. More details about instruction-unfollowing negatives are in Appendix Section C.3. 7 https://huggingface.co/cross-encoder/ ms-marco-MiniLM-L-12-v2 5 TART: Multi-task Instructed Retriever\n\nWe now present our unified multi-task retriever TART (TAsk-aware ReTriever) trained on BERRI via multi-task instruction-tuning.\n\n\nModel Architecture\n\nTART-dual. TART-dual adopts a dual-encoder architecture, where a single encoder encodes queries with instructions and documents independently. We use maximum inner product search (MIPS) over the embeddings to find relevant documents (Karpukhin et al., 2020). Formally, the similarity scores between a query q and a document d, given an instruction t, is calculated as follows:\ns(t, q, d) = E([t; q]) T E(d),(1)\nwhere E(\u00b7) is the embedding function 8 and [t; q] is the concatenation of the instruction and query. For this model, document embeddings can be computed offline, improving inference efficiency at the cost of storage space (Yamada et al., 2021).\n\nTART-full. The bi-encoder architecture is known to be less expressive since it only has limited interactions between queries and documents (Khattab and Zaharia, 2020), especially when the training data is limited (Hofst\u00e4tter et al., 2021). To address this issue, we also explore a cross-encoder architecture (Nogueira and Cho, 2019), which computes the relevance between a query and each document by jointly encoding them with cross-attention. A cross-encoder model is often prohibitively expensive to scale up to millions of documents, so we first run a lightweight off-the-shelf retrieval system (e.g., a bi-encoder retrieval system) to retrieve the top documents. For each of these documents, our instruction-aware cross-encoder, TART-full, computes a similarity score between a query and the document d as:\ns(t, q, d) = FFN(E([t; q; d])),(2)\nwhere FFN represents an additional feed-forward network that predicts whether the document follows the instruction and is related to the query. We explored different encoder-only and encoderdecoder models to initialize our cross-encoder; we found that initializing TART-full with encoders of T5-based instruction-following pretrained models, namely T0-3B (Sanh et al., 2022) and FLAN-T5 (Chung et al., 2022), empirically leads to superior performance as found in prior work (Sachan et al., 2022). We follow the EncT5 approach (Liu et al., 2021) and prepended each sequence with a start-of-sequence token. The token representation is then fed to a newly initialized feed-forward network. Unlike MonoT5 (Nogueira et al., 2020), we use their encoders only to reduce parameters and improve inference-time efficiency.\n\n\nTraining TART\n\nWe train TART-dual and TART-full using the positive documents and three types of negative documents in BERRI with instructions ( Figure 3).\n\nTraining TART-dual. During training, we combine documents annotated with the queries in BERRI as well as in-batch negatives and train the model as follows:\nL = \u2212 log e s(q,d + ,t) d\u2208B e s(q,d,t) ,\nwhere B denotes all documents in the same minibatch (Karpukhin et al., 2020).\n\nTraining TART-full. Following prior work (Nogueira and Cho, 2019), TART-full is trained with the cross entropy loss as:\nL = \u2212 d\u2208d + log s(t, q, d)\u2212 d\u2208d \u2212 log(1\u2212s(t, q, d)).\nKnowledge distillation from TART-full to TART-dual. The default hard negatives in BERRI rely on off-the-shelf models fine-tuned on MS MARCO; for some domains, the hard negative samples mined by those models can be less reliable. Especially for a smaller bi-encoder model, those false positive and negative samples can significantly diminish performance (Qu et al., 2021). We found that leveraging more powerful TART-full to denoise and obtain better negative and positive samples lets us distill knowledge from the powerful model to a smaller TART-dual. We first train TART-full on the annotated gold documents and the negative documents mined in BERRI. We then re-run the denoising process described in Section 4.2 with the newly trained cross-encoder. Importantly, unlike the initial denoising step, we now leverage instructions and update hard negative documents d HD and positive documents d + based on more accurate TART-full predictions.\n\n\nExperiments\n\nWe evaluate TART on zero-shot retrieval (Section 6.1) and in a challenging new evaluation setup, X 2 -Retrieval (Cross-task Cross-domain Retrieval; detailed in Section 6.2), comparing them with stateof-the-art models described in Section 6.3.\n\n\nZero-shot Retrieval Evaluations\n\nTo evaluate the models' ability to perform zeroshot transfer via instructions, we run experiments on two widely used zero-shot transfer evaluation benchmarks: BEIR (Thakur et al., 2021) and LOTTE-pooled (Santhanam et al., 2022). Notably, none of the evaluation datasets and instructions overlap with BERRI. Moreover, many tasks differ significantly from tasks used during training (e.g., argument retrieval).\n\nBEIR is a collection of diverse retrieval tasks in multiple domains (e.g., Wikipedia, biomedical) where the retrieval target is restricted to the target corpus in a single domain. We used publicly available datasets. Following Dai et al. (2022), we exclude Natural Questions, MS MARCO, HotpotQA, FEVER and CQADupStack from our evaluation targets for fair comparison since they are included either in encoders' pretraining or in BERRI.\n\nLOTTE-Search samples GooAQ (Khashabi et al., 2021) questions whose answers come from certain forums in StackExchange. We evaluate our model in the pooled setup, where documents come from forums in diverse domains (e.g., cooking, technical). GooAQ is not included in our training set. In LOTTE, our instructions specify which forum domains our system should retrieve evidence from (e.g., \"Retrieve a cooking StackExchange forum post that answers this question\").\n\nMetrics. Following Thakur et al. (2021), for BEIR, we use NDCG@10 as our primary metric on  BEIR. For LOTTE-pooled, we use Success@5 (= Recall@5) as our primary metric, as in the original paper (Santhanam et al., 2022).\n\n\nX 2 -Retrieval: Cross-task Cross-domain Retrieval Evaluation\n\nNormal retrieval benchmarks often assume that a system must deal with only a single intent and a closed corpus, which may oversimplify real-world scenarios: users' intents can be diverse, requiring searching in a truly open-domain environment that includes diverse documents (Piktus et al., 2021). We introduce a more realistic evaluation setup, X 2 -Retrieval, where several retrieval tasks with different intents are pooled to form a single retrieval target containing diverse documents. This setup requires a system to adapt to a new task in a zeroshot manner and to model users' intents expressed in natural languages, to find documents aligning their expectations from an open-domain corpora.\n\nTasks and queries. Our X 2 -Retrieval evaluation covers six datasets across three domains, namely, Wikipedia, Science, and Technical (Table 2) domains. The key challenge here includes datasets with different search intents that may not always be obvious from the queries alone.\n\nA pooled corpus. For the primary pooled setup, we combine all documents from different tasks and the BEIR NQ Wikipedia corpus to form a single retrieval corpus, consisting of approximately 3.7 million documents. We also report the simplified closed setup performance as an oracle setup, where a system retrieves only from the original task corpus, which is similar to BEIR.\n\nMetrics. We report NDCG@10 on both pooled and closed setups for each task. In addition, we evaluate the performance gap between the closed and pooled setups and refer to it as robustness. A smaller gap means that the model is distracted less by the documents from undesirable corpora.\n\n\nBaselines\n\nWe compare TART with various state-of-the-art methods. The first group we consider are unsupervised models that are not trained or trained only on unlabeled text; these include Contriever The final group of models is specialized retrievers trained for each task on additional taskspecific data that was automatically generated given the target corpus. Promptagator \n\n\nExperimental Settings\n\nWe initialize TART-full from the T0-3B encoder (Sanh et al., 2022) and FLAN-T5 encoder (Chung et al., 2022). We sample positive and negative passages with a 1:4 ratio. We train TARTfull up to 10k steps and take the best checkpoint based on development split performance. We initialize TART-dual from a Contriever-MS MARCO checkpoint (Izacard et al., 2022) and train up 30k steps. Per-GPU batch size is 16, and for each positive document, we sample in total 5 negative passages; 90% of them are randomly sampled from   Table 4: Cross-task retrieval results. \u2206 shows the performance gap between the averaged pooled performance and the averaged closed performance. AMB, WQA, SCF, GAT, LSO, CSP denote AmbigQA, WikiQA, SciFact, GooAQ-Technical, LinkSO-Python, and CodeSearchNet-Python, respectively. D, and 10% are sampled from d HD and d UF in addition to the in-batch negative documents. We use 8 GPUs to train TART-full and 64 GPUs to train TART-dual. To retrieve the initial document candidates for TART-full, we use Contriever-MS MARCO and rerank the top 100 documents. 9 Table 9 shows the full list of instructions for evaluations. More details are in Appendix C.1.\n\n\nResults\n\n\nZero-shot Evaluation Results\n\nAs shown in Table 3, TART-full significantly outperforms larger models and customized models 9 We found that combining TART-full with the original Contreiver performs better than combining TART-full with TART-dual, possibly because TART-full uses the hard negative samples retrieved by Contriever's top-retrieved results. trained on millions of synthetically generated indomain data, advancing the state of the art on BEIR and LOTTE. Unlike prior methods that require additional data generation, TART only requires a single human-written instruction to adapt to a new task. Compared to other methods using crossencoder-based reranking models (e.g., BM25 + MonoT5), TART-full uses a much smaller number of paragraphs to be re-ranked, which significantly reduces latency caused by reranking at test time.\n\nThe large performance gain from Contriever-MS MARCO to TART-dual on six out of the nine BEIR tasks (e.g., SciFact, Arguana) shows the effectiveness of instructions and knowledge distillations from a larger to a smaller model. On the other hand, for the other three datasets (e.g., Touche-2020, Climate-FEVER), TART-dual shows large performance deterioration, degrading average performance. We hypothesize that model capacity (i.e., BERT-base encoder) and limited interactions between the query and document embeddings could be major bottlenecks. Prior work on instruction training in LLMs has shown that smaller models often do not get as much benefit as larger ones from instructions and increasing dataset size, possibly due to their limited model capacities (Chung et al., 2022;Wang et al., 2022b).\n\nFor LOTTE-pooled, TART-full significantly outperforms prior state of the art by a large margin. We found that simply adding instructions at test time does not help, indicating that our instruction models do not simply exploit lexical matching. See more detailed results in Section 8.1. Table 4 shows the models' X 2 -Retrieval performance. Contriever and Contriever+CE show competitive closed performance in the closed setup, as in BEIR, but they struggle in the pooled setup due to their inability to handle human instructions. Especially Contriever+CE shows a large performance drop on AmbigQA-pooled by retrieving documents instead of queries due to the biases from fine-tuning on a QA dataset (i.e., MS MARCO) only.\n\n\nX 2 -Retrieval Evaluation Results\n\nTART-full shows the best closed performance and pooled performance, indicating its strong zeroshot adaptation and cross-task abilities. We found that a model can flexibly change its behavior based on the instructions, as shown in Table 11.\n\nAlthough the closed setting performance of TART-dual under-performs Contriever, TARTdual shows strong performance on the pooled setup, resulting in the best robustness among all. This indicates that even smaller models can be guided by instructions, although they may have limited capabilities of zero-shot transfer to new tasks due to the limited model capacity and interactions.\n\n\nAnalysis\n\nWe conduct a set of analyses to understand the factors that contribute to the models' ability to follow instructions, in particular, the effects of instructions at training and inference (Section 8.1), dataset scale (Section 8.2), model scale (Section 8.3) and carefully-designed negative samples (Section 8.4). Our analysis in this section focuses on the more powerful TART-full initialized with T0-3B.   Figure 4 shows the performance of those baselines. On all benchmarks, ablating instructions during training or test time causes a notable performance drop. We also see that a model trained with instructions but given no instruction at test time still yields a few performance improvements over the model trained completely without instructions, indicating the effectiveness of incorporating instructions during multi-task training.\n\nRobustness toward instructions. We evaluate TART-full's robustness towards diverse instructions. Figure 5 shows the performance variance given multiple different instructions. The blue circles represent the performance with no instruction. Instructions significantly improve model performance without instructions. Although different instructions give small performance variance, TARTfull often outperforms other competitive baselines when informative and correct instructions are given. We also observe larger performance deterioration when inaccurate instructions are given. See Table 15 for individual instructions and performance.\n\n\nDataset Scale\n\nTo assess the effectiveness of diverse datasets, we conduct dataset ablation experiments, where we ablate datasets during training. Following prior work on language models instruction tuning, we conduct these experiments based on the number of datasets (Wang et al., 2022b) and task clusters (Wei et al., 2022a). In addition, we run ablations based on the number of domains, where we ablate datasets based on their source domains. Figures 6a and 7 show these experiments' results. Figure 6a shows the average performance on four BEIR datasets of TARTfull trained on randomly sampled 5, 10 and 20 datasets. We observe that increasing the number of the training datasets helps TART to perform better.\n\n\nNumber of datasets.\n\nTask diversity. As shown in Figure 7, task diversity is a key to improve models' zero-shot transfer performance. QA only struggles on Arguana, where the tasks significantly differ from QA.\n\nDomain diversity. Figure 7 shows that having more diversity in training datasets' domains is also crucial, especially when the target datasets are in non-general domains. For instance, a model trained only on Wikipedia datasets struggles on Touche-2020 or SciFact, where documents come from argument websites and scientific papers, respectively. \n\n\nModel Scale\n\nWe test different TART-full sizes to see how model scale affects final performance. Prior work has shown that scaling up re-ranking models often improves reranking performance (Rosa et al., 2022), and models' instruction-following abilities improve as models get larger (Wang et al., 2022b;Sanh et al., 2022;Wei et al., 2022b). We investigate how model scale affects the ability to generalize to new tasks and follow instructions. For a fair comparison, we train TART-full using different T5 LM-Adapt (base, large, and XL) and evaluate performance using them to rerank the top 100 Contriever results. Figure 6b shows TART-full's average performance across different model scales. We observe clear performance improvements by increasing model size as observed in prior work on LLM.\n\n\nNegative Samples\n\nWe analyze the effectiveness of negative samples by ablating them during training. Figure 8 shows the performance of the models trained without negative samples on BEIR and X 2 -Retrieval. Adding more challenging negative documents (i.e., d HD and d UF ) during training largely improves the model performance on BEIR. Moreover, the model trained without instruction-following samples (w/o d UF ) results in lower X 2 -Retrieval performance, although this model performs on par with the original TART-full on BEIR. This indicates that our new instruction-unfollowing negative documents largely contribute to improving the ability to distinguish instructions and are thus crucial to build a robust task-aware retrieval system.\n\n\nDiscussions and Conclusion\n\nThis paper lays the foundation for building a general-purpose task-aware retriever that can follow natural language instructions. We introduced a new problem, retrieval with instructions, to model users' intents explicitly. We presented BERRI, the first large-scale retrieval dataset with expert-written annotations. Building upon BERRI, we trained the first instruction-following retrieval system by massive multi-task instruction-tuning, TART, adopting two widely used architectures. TART advances the state of the art on the popular zeroshot retrieval benchmarks BEIR and LOTTE as well as on our newly introduced challenging evaluation setup, X 2 -Retrieval. Our analysis shows that key factors to building a successful multi-task instruction-following retrieval system include informative instructions at training and test time, diversity in data and model scale, and carefully designed negative samples. We conclude with two interesting open questions, which future work can explore.\n\nImproving efficiency of instruction-following retrievers. Although our TART-full model shows the effectiveness of instruction-tuning for retrieval, TART-dual shows some performance drop from its non-instruction-following counterpart, caused by large performance deteriorations on a few datasets. We hypothesize that a smaller model size (i.e., 110 million parameters) and limited interactions between query and document embeddings make building an instruction-following retrieval system more challenging. Future work can study the effect of scaling up bi-encoder models as well as explore modeling architectures that enable rich interaction but are still more efficient than the cross-encoder, such as ColBERT-v2 (Santhanam et al., 2022).\n\nFurther scaling up the number of datasets. Retrieval tasks are excluded in prior work on instruction-following of LLMs. This work is the first to explore instruction-tuning in the area of retrieval, and we annotate more than 100 instructions for approximately 40 tasks, and we demonstrate the effectiveness of the dataset scale in retrieval. Recent work (Wang et al., 2022b;Chung et al., 2022) show that scaling up the number of the training datasets improves LLMs' ability to adapt to new task via instructions. We open-source our instruction data and call for community efforts to collect more retrieval tasks and human-written instructions as in instruction-following for LMs (Wang et al., 2022b;Bach et al., 2022), to investigate whether further increasing the number of the datasets (e.g., more than 100 datasets) improves zero-shot and cross-task retrieval.  Table 5 shows all datasets we used in BERRI. Table 6 provides references for these datasets.\n\n\nReferences\n\n\nA.2 Details of Dataset Unification\n\nAs shown in Table 5, some datasets were not originally retrieval datasets (e.g., summarization datasets). We describe how we convert these into the unified retrieval task format.\n\nQA. For QA datasets, where each instance consists of a query, a gold context, and answers, we assume the original gold context as the gold document used as a positive sample during training. For some exceptional datasets, we performed additional preprocessing. We found that ReCoRD instances are occasionally self-containing due to the nature of the cloze-style QA; therefore, for ReCoRD, we replace the original placeholder with the gold answer and use this original question with the answer as the query and the original context as a gold document. For MedMCQA, we use the source exam question as the query and the answer evidence as the positive document.\n\nSummarization. For summarization datasets, we use target summarizations as the gold document and source text as the query.\n\nText simplifications. For text simplification datasets, we use source (often more complex) sentences as the query and simplified sentences as the gold document.\n\nCode search. We use the source comment as the query and the corresponding implication as the gold document. We exclude the python subset from BERRI as we use it for X 2 -Retrieval. Table 7 shows the full list of the instructions in BERRI. Note that we present only one instruction for each dataset. A full list of the instructions will be released in our repository.\n\n\nA.3 Instructions for BERRI\n\n\nA.4 BERRI Statistics\n\nWe conduct analyses on BERRI to understand its domain and intent diversities. Intents. Open-ended intents are diverse and hard to classify into fixed sets of categories. As a proxy for intents, Figure 9 shows the distributions of the source task categories. QA is the most representative category, while summarization and question duplication detection is also common due to their abundance in large-scale datasets. On the other hand, around 50 % of the tasks do not belong to those top three categories, such as code search or caption generations, which contribute to the diversity of BERRI. We also find that traditional nonretrieval tasks, such as sentence simplification or dialogue, can be repurposed as retrieval tasks. In Section 8.2, we analyze the effect of training data task diversity.\n\nDomains. Our dataset covers diverse domains. Figure 10 shows that Wikipedia (e.g., NQ), web (e.g., MS MARCO), Community QA (e.g., Quora), News(e.g., CNN/Daily) dominate, while we also have some expert domains (e.g., medical, legal, technical). We found that although many expert domain datasets are smaller than the ones in general domains like Wikipedia, adding those high-quality expert domain datasets helps the system learn to adapt to those domains or unseen expert domains with a similar writing style (e.g., scientific papers).   Table 6 shows references for them.  Table 6: References for datasets used in BERRI and evaluations. We use the preprocessed versions available on the SentenceTransformers (Reimers and Gurevych, 2019) embedding data page 10 for the datasets with * . We use the preprocessed versions from KILT (Petroni et al., 2021) for the datasets with \u2020 .\n\n\nAltlex\n\nRetrieve a sentence from Wikipedia that simplifies the following 2. SE (title \u2192 title) I want to find a related question asked in StackExchange. Can you find one for me?\n\n\nSE (title \u2192 title)\n\nStackExchange is a community QA forum for diverse topics including technical or science. Help me to find a question body that duplicates my question\n\n\nYahooAnswers\n\nRetrieve the most voted answer for this question from Yahoo Answers.\n\n\nMSMARCO\n\nI want to know the answer to the question. Can you find good evidence on the web?.\n\n\nELI5\n\nYou have to answer a why / how question from users. Retrieve a Wikipedia paragraph that provides a piece of good evidence for the answer.\n\n\nWikiHow\n\nFind a detailed paragraph from WikiHow that explains how-to to achieve 8. SearchQA\n\nPick up the top web search results snippets for the following question.\n\n\nAGNews\n\nFind a news summary sentence corresponding to the following header.\n\n\nNPR\n\nGiven a news article headline published at npr.org, find a corresponding summary of the news 11. CodeSearchNet (Java)\n\nMatch the following natural language instruction to Java codes 12. CodeSearchNet (ruby)\n\nRetrieve ruby codes from GitHub commit history that implement this feature 13. CodeSearchNet (JavaScript) Find a javascript code implementation on GitHub for the following natural language instructions 14. CodeSearchNet (Go)\n\nCan you find a Go implementation of this?\n\n\nPAQ\n\nCan you answer my question by finding an article on the web?\n\n\nSentence Compression\n\nYou have to match this long sentence to a shorter compressed one 17. CNN Daily Mail\n\nThe following sentences are the summaries of a news article. Find the source news article.\n\n\nXSUM\n\nRetrieve a news article that is summarized as following.\n\n\nCoco captions\n\nCan you find an image caption talking about the same image as.\n\n\nQuora Dup. Questions\n\nCheck if a Quora question is duplicated with this question.\n\n\nCC News\n\nI want to know the details of this news. Can you find a detailed news article on this for me?\n\n\nFEVER\n\nRetrieve a Wikipedia paragraph to verify this claim 23. HotpotQA\n\nFind a paragraph that provides useful information to answer this question 24. NQ Retrieve passages from Wikipedia to answer 25. TriviaQA I want to find an answer for this Trivia question. Can you find some paragraphs that provide evidence from Wikipedia?\n\n\nWoW-Knowledge\n\nFind a Wikipedia paragraph related to the following conversation topic.\n\n\nWoW-Response\n\nFind a meaningful dialogue response to answer the user's question 28. Medical Simplification Please retrieve a medical paper summary that is written in a simple language so that my patient can understand 29. SciTLDR\n\nFind a sentence-length summary of this paper.\n\n\nPubMedQA\n\nHelp me to find a highly related PubMed paper to answer this question.\n\n\nMedMCQA\n\nFind the explanation for the correct answer of this medical question.\n\n\nGigaord\n\nRetrieve an extremely short summary of the following Gigaword article.\n\n\nRecord\n\nFind a News article to verify the following sentence 34. MultiLexSum\n\nMap this legal case summary to a sentence-long summary\n\n\nQrecc\n\nYou need to find a good response from a collection of previous responses and help users to know this topic more 36. OQA Find a question that is paraphrased of this 37. SQuAD\n\nFind a Wikipedia paragraph that answer the question  Retrieve an argument that counter argues the following paragraph Touche\n\nYou have to retrieve an argument to this debate question DBPedia\n\nRetrieve a Wikipedia introduction paragraph of the following entity SCIDOCS Find scientific paper titles that are related to the following Climate-Fever I want to know if the following claim is true or not. Retrieve a Wikipedia paragraph on climate change for this.\n\n\nSciFact\n\nRetrieve a scientific paper sentence to verify if the following claim is true WIKIQA\n\nRetrieve an answer sentence from Wikipedia AmbigQA Retrieve a question that is similar to this SciFact\n\nRetrieve scientific evidence to verify this claim GooAQ-technical Find a StackExchange forum that answers this question Codesearchnet-py Retrieve a python code that implements the following feature.\n\n\nLinkSO-Py\n\nYou have to find a python implementation of this huggingface datasets, 11 and we use the question and answer sentence pairs that are labeled as 1 as the queries for evaluations, and use the answer sentences as the gold documents. Regarding the retrieval target, we use all sentences available in the WIKIQA dataset, including the sentences that are labeled as 0. For LinkSO, we use the original datasets' test split for the python domain and sample 1,000 queries. 12 We find questions that 11 https://huggingface.co/datasets/wiki_qa 12 https://sites.google.com/view/linkso are labeled as duplicated and use their corpus as our retrieval target. For GooAQ-technical, we sample 1,000 GooAQ questions whose answers are from stackoverflow.com. As 20% of the sampled GooAQ tech queries share the same answer posts, we remove the duplicated paragraphs. For CodeSearchNet-Python, we use the comments describing the codes as queries and the corresponding python codes as positive documents. We sample 1,000 queries from the test split.\n\nExamples. Examples of X 2 -Retrieval are shown in Table 8. As shown, queries themselves often do not fully indicate the users' intents. By specifying users' intents as explicit textual instructions, our model can effectively perform multi-task retrieval over a single pooled corpus.\n\nHuman evaluations of quality. To access the possibility of having false negative passages, we run an off-the-shelf retrieval system to retrieve the top 10 documents for randomly sampled 20 questions for each task, and we evaluate if any of the negative passages, especially from the non-target corpus, are indeed positive. We found that the false negative ratio is less than 10%.\n\nC Modeling Details C.1 Hyperparameters of TART TART-dual. We set the learning rate to be 1 \u00d7 10 \u22125 and warm-up steps to be 1,000. The softmax temperature is set to 0.05. The batch size is 1024. We use 7 negative samples per instance; 10% of the time we use hard negative or instructionunfollowing negatives, while 90% of the time we use negative documents that are randomly sampled from the same target corpus. The maximum document chunk length is set to 256.\n\nTART-full. To train a cross-encoder using the T0-3B encoder, we set the maximum sequence length to 512 and the batch size to 1, increasing the gradient accumulation steps to 8. We set the dropout rate to 0.1 and the learning rate to 1 \u00d710 \u22125 . Table 9 lists the instructions used for the BEIR and X 2 -Retrieval evaluation.\n\n\nC.2 Instructions for Evaluations\n\n\nC.3 Negative Sampling\n\nMining instruction-unfollowing samples. To sample instruction-unfollowing samples, given a query from a target dataset, we retrieve the top 20 documents from another task's corpus using Contriever-MS MARCO. For instance, given a Pub-MedQA, a system should not retrieve a document from a Wikipedia paragraph. A list of source target task and retrieval corpus combinations is shown in Table 10.\n\nSampling d \u2212 for TART-full training. Challenging negative samples help a system to effectively learn the task. On the other hand, prior work also shows that it can lead to large performance drops in out-of-domain datasets, and having both randomly sampled negative documents and carefully designed negative documents is a key to building a system that is competitive in both in-domain and out-of-domain retrieval (Ni et al., 2021). To effectively combine the negative documents during training, we first combine random samples and hard negative samples, and then we randomly sample 4 negative documents per one positive document. The number of instruction-unfollowing documents, if applicable, is limited to less than 20% of the negative documents, and we set the maximum number of instruction-unfollowing samples from certain combinations listed in Table 10 up to 10k.\n\n\nD Further Results and Analyses\n\nD.1 Qualitative Results on X 2 -Retrieval \n\n\nD.2 Analysis of Instruction Effectiveness\n\nFull results of instruction ablations. Table 13 shows the full BEIR results of ablating instructions in Section 8.1, and Table 14 shows the ones on LOTTE and X 2 -Retrieval. On all of the benchmarks, removing instructions at training or test time largely hurts the performance, indicating the effectiveness of instructions.\n\nExamples of prompts with performance. Table 15 shows the instructions and TART-full performance on three BEIR datasets. We also provide a comparison of the model performance when uninformative instructions are given in Table 16. We see that more informative and related instructions often result in a strong performance, while irrelevant instructions degrade it. Table 17 shows the NDCG@10 across different model scales. We compare the TART-full initialized with different sizes of T5-LM-adapt for a fair comparison. We see in general that larger models perform better.   proves, which is consistent with previous work on instruction-tuning in LLMs (Wang et al., 2022b).\n\n\nD.3 Analysis on Model and Dataset Scale\n\n\nD.4 Analysis on Different Pre-trained Models\n\nOur TART-full is initialized with the T0-3B encoder. We experiment with more recent pretrained instruction-following models: FLAN-T5-XL (Chung et al., 2022) and Tk-Instruct (Wang et al., 2022b), which are trained on the order of magnitude of more datasets. We analyze TARTfull performance when we initialize encoders using different pre-trained encoder models, including the ones that are released recently. Table 19 shows the results of TART-full, when the encoder is initialized with three different recent instructionfollowing pretrained models, T0-3B, FLAN-T5-XL (Chung et al., 2022) and Tk-Instruct-3B (Wang et al., 2022b). FLAN-T5 shows the best average BEIR performance, outperforming TART-full by 0.7 NDCG@10. Tk-Instruct shows a notable performance drop on some datasets (e.g., TREC COVID), resulting in slightly lower performance than the original TART-full (T0-3B).\n\nQuery: 10% of sudden infant death syndrome (SIDS) deaths happen in newborns aged less than 6 months. Instructions: Retrieve a scientific paper abstract to verify this Contriever TART-full By definition, SIDS deaths occur under the age of one year, with the peak incidence occurring when the infant is at 2 to 4 months of age. This is considered a critical period because the infant's ability to rouse from sleep is not yet mature (Wikipedia paragraph) Despite declines in prevalence during the past two decades, sudden infant death syndrome (SIDS) continues to be the leading cause of death for infants aged between 1 month and 1 year in developed countries. Behavioral risk factors identified in epidemiological studies include prone and side positions for infant sleep, smoke exposure, soft bedding, and sleep surfaces, and overheating. (paper) Query: Which city will host the next winter Olympics? Instructions: find an answer from Wikipedia Contriever TART-full Where will the next winter Olympics be held 2018? (Ambig question)\n\nThe host city for the 2022 Winter Olympics, is Beijing in northern China, elected on 31 July 2015, at the 128th IOC Session in Kuala Lumpur. Beijing will be the first city ever to have hosted both the Summer and Winter Olympics. The 2022 Winter Olympics will take place between 4 and 20 February 2022. (Wikipedia paragraph) Query: use batch normalization tensorflow Instructions: Can you find python code implementing this? Contriever TART-full could use batch normalization tensorflow would like use batch normalization TensorFlow since found source code rel noreferrer core ops nn ops cc however find documented different semantics mlp cnn sure exactly bn find method called either c code copied reference (StackOverflow post) Query: how many planets is jupiter away from the sun? Instructions: Can you find an answer sentence to this question for me? Contriever TART-full Jupiter is the only planet whose barycenter with the Sun lies outside the volume of the Sun, though by only 7% of the Sun's radius. [80] The average distance between Jupiter and the Sun is 778 million km (about 5.2 times the average distance between Earth and the Sun, or 5.2 AU) (Wikipedia paragraph)\n\nJupiter is the fifth planet from the Sun and the largest planet in the Solar System. (Wikipedia answer sentence)         \n\nFigure 2 :\n2Examples of datasets included in BERRI.\n\n\n(Izacard  et al., 2022)  and BM25. We also compare TART with UPR (Sachan et al., 2022), which reranks the Contriever results using a pretrained T0-3B.Thesecond group trains retrievers and rerankers on MS MARCO or a few large-scale datasets and directly transfers them to new tasks with no adaptations, including MonoT5 (Nogueira et al., 2020), Contriever-MS MARCO and Contriever-MS MARCO + Cross Encoder (CE), ColBERT v2 (Santhanam et al., 2022), GTR (Ni et al., 2021) and SGPT-BE (Muennighoff, 2022).\n\n\n(Dai et al., 2022) generates large amount of in-domain data using FLAN (Wei et al., 2022a), and GPL (Wang et al., 2022a) generates them using DocT5Query (Nogueira et al., 2019).\n\nFigure 4 :Figure 5 :\n45Ablations of instructions. w/o I (train), w/o I (test), and w/o (train & test) indicate the ablations (a), (b), and (c), respectively. Performance variance across different instructions. C-Fever indicates Climate-Fever. Blue circles indicate performance without instructions at test time, while red circles indicate performance with different instructions. \"PT\", \"Cont\", \"+CE\" denote Promptagator, Contriever and Contriever+CE.\n\nFigure 6 :\n6Analysis of dataset and model scale.\n\nFigure 7 :Figure 8 :\n78Dataset ablation results. Wikipedia-only denotes TART-full performance trained on Wikipediabased datasets only. QA-only denotes the model trained on QA datasets only. Ablations of negative samples. \"w/o d HD and d UF \" denotes a model trained without hard and instruction-unfollowing negative documents while \"w/o d UF \" ablates instruction-unfollowing documents only.\n\nFigure 9 :Figure 10 :\n910The task distributions of the datasets included in BERRI. The domain distributions of the datasets included in BERRI.\n\n\ndatasets used in BERRI Altlex * (Hidey and McKeown, 2016), StackExchange (duplicate questions, question-title, question-question) (Reimers and Gurevych, 2019), Yahoo Answers * (Rakshit, 2019), MSMARCO * (Bajaj et al., 2016), ELI5 * (Fan et al., 2019), WikiHow * (Koupaee and Wang, 2018), SearchQA * (Dunn et al., 2017), AG News * (Gulli, 2004), NPR * (pushshift, 2021), CodeSearchNet * (Husain et al., 2019), PAQ * (Lewis et al., 2021), Sentence Compression * (Filippova and Altun, 2013), CNN Daily Mail * (See et al., 2017), XSUM * (Narayan et al., 2018), COCO captions * (Chen et al., 2015), Quora Duplicated Questions (Shankar Iyer, 2012), CC News * (Hamborg et al., 2017), SQuAD * (Rajpurkar et al., 2016), FEVER \u2020 (Thorne et al., 2018), HotpotQA \u2020 (Yang et al., 2018), Natural Questions \u2020 (Kwiatkowski et al., 2019), TriviaQA \u2020 (Joshi et al., 2017), Wizard of Wikipedia \u2020 (Dinan et al., 2019), Medical Simplification Dataset (Devaraj et al., 2021), SCITLDR (Cachola et al., 2020), PubMedQA (Jin et al., 2019), MedMCQA (Pal et al., 2022), Gigaword (Rush et al., 2015), ReCoRD (Zhang et al., 2018), MultiLexSum (Shen et al., 2022), Qrecc (Anantha et al., 2021), OQA (Fader et al., 2014). datasets used during evaluations TREC-COVID (Voorhees et al., 2021), FIQA (Maia et al., 2018), NF Corpus (Boteva et al., 2016), Arguana (Wachsmuth et al., 2018), Touche-2020 (Bondarenko et al., 2020), DBPedia (Hasibi et al., 2017), SciDocs (Cohan et al., 2020), Climate-Fever (Diggelmann et al., 2020), SciFact (Wadden et al., 2020), GooAQ (Khashabi et al., 2021), LinkSO (Liu et al., 2018), AmbigQA (Min et al., 2020), WIKIQA (Yang et al., 2015).\n\nTable 1 :\n1Example instructions for natural questions (NQ; Kwiatkowski et al. 2019), medical text simplification (Med Simple; Devaraj et al. 2021), QReCC (Anantha\n\nq :\nqHow to compute square root in iOS?q: Are armadillos native to a \nSpanish-speaking part of the world? \n\nHow can we calculate \nthe square root in \nObjective C or Swift? \n\nYes, they are most \ncommonly found in \nNorth, Central, and \nSouth America. \n\nWhich python function can \nI use to compute sq root? \n\nYou can just use the Objective \nC or Swift's sqrt function \n\nI love animals and think \narmadillos are awesome \nwith their leathery shell. \n\nArmadillos are medium-sized \nmammals found in North, \nCentral, and South America \n\nDialogue Response Retrieval \n\nt 1 : Find an informative dialogue \n\nresponse to this user's conversation \n\nDup. Question Retrieval \n\nt 1 : Retrieve a question asked in \n\nStackOverflow similar to this \n\nStackOverflow Question \nStackOverflow Question \n\nStackOverflow Answer \n\nDialogue Response \n\nWikipedia Paragraph \n\nTasks \n\nDialogue Response \n\nGold documents \n\nHard negative documents \nInstruction-unfollowing negatives \n\nNegative documents \n\n\n\nTable 2 :\n2The X 2 -Retrieval evaluation. Example pairs of queries and documents are shown inTable 8.In addition \n\n\nTable 3 :\n3Zero-shot retrieval results on BEIR and LOTTE-Search (pooled). \u2020 indicates the models using crossencoder-based reranking models. The first group of models use no labeled data during training. The second group uses MS MARCO at training time but have no customized task-specific data. The third group trains individual retrieval systems using automatically generated data.TREC, NFC, FQA, ARG, TOU, DBP, SCD, CLI, SCF indicates TREC-COVID (Voorhees et al., 2021), FIQA (Maia et al., 2018), NF Corpus (Boteva et al., 2016), Arguana (Wachsmuth et al., 2018), Touche-2020 (Bondarenko et al., 2020), DBPedia (Hasibi et al., 2017), Sci-Docs (Cohan et al., 2020), Climate-Fever (Diggelmann et al., 2020), and SciFact (Wadden et al., 2020), respectively. \"\u00d79\" of GPL, Promptagator means that those models train customized models for each of the datasets. TART-full (FLAN) 94.0 89.6 86.9 55.9 77.4 66.3 78.3 66.7 18.1 18.4 51.8 50.1 67.7 57.8 9.9AMB \nWQA \nSCF \nGAT \nLSO \nCSP \nAvg. \n\u2206 \ncl \npl \ncl \npl \ncl \npl \ncl \npl \ncl \npl \ncl \npl \ncl \npl cl\u2212pl \n\nContriever \n96.8 93.8 80.9 54.1 67.7 57.4 73.2 59.8 28.0 26.7 36.7 36.1 63.9 54.6 9.3 \nContriever+CE \n96.6 47.4 78.2 58.4 69.1 61.7 75.4 66.0 32.1 31.4 42.0 40.2 65.5 50.9 13.4 \nTART-dual \n96.3 95.3 80.2 63.1 70.1 66.2 75.0 65.0 23.0 23.4 31.3 31.3 60.5 53.6 6.9 \nTART-full (T0) \n91.1 90.5 82.1 52.5 74.7 66.2 80.5 68.6 25.1 24.9 51.4 51.4 67.5 59.1 8.4 \n\n\n\n8.1 Effects of InstructionsAblating instructions. To analyze the effectiveness of instructions, we compare TART-cross with three variants: (a) train without instructions, test with instructions prepends instructions at test time only to test if the models just exploit keyword matching only at test time; (b) train with instructions, test without instructions uses TART-full without instructions at test time; (c) train without instructions, test without instructions does not use instructions at all during training and test time.\n\n\nZhuyun Dai and Jamie Callan. 2019. Deeper text understanding for IR with contextual neural language modeling. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Christopher Hidey and Kathy McKeown. 2016. Identifying causal relations using parallel Wikipedia articles. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Man Luo, Arindam Mitra, Tejas Gokhale, and Chitta Baral. 2022. Improving biomedical information retrieval with neural retrievers. In Proceedings of the AAAI Conference on Artificial Intelligence. intensive tasks. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Stefano Mizzaro. 1998. How many relevances in information retrieval? Interacting with Computers. Amit Singhal et al. 2001. Modern information retrieval: A brief overview. IEEE Data Engineering Bulletin. and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Ellen Voorhees. 2005. Overview of the trec 2004 robust retrieval track. In Proceedings of the Thirteenth Text REtrieval Conference.Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, \nShayne Longpre, Stephen Pulman, and Srinivas \nChappidi. 2021. Open-domain question answering \ngoes conversational via question rewriting. Proceed-\nings of the 2021 Conference of the North American \nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies. \n\nAkari Asai, Xinyan Yu, Jungo Kasai, and Hanna Ha-\njishirzi. 2021. One question answering model for \nmany languages with cross-lingual dense passage re-\ntrieval. Proceedings of Advances in Neural Informa-\ntion Processing Systems. \n\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert \nWebson, Colin Raffel, Nihal V. Nayak, Abheesht \nSharma, Taewoon Kim, M Saiful Bari, Thibault \nFevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, \nZhiqing Sun, Srulik Ben-david, Canwen Xu, Gun-\njan Chhablani, Han Wang, Jason Fries, Maged Al-\nshaibani, Shanya Sharma, Urmish Thakker, Khalid \nAlmubarak, Xiangru Tang, Dragomir Radev, Mike \nTian-jian Jiang, and Alexander Rush. 2022. Prompt-\nSource: An integrated development environment \nand repository for natural language prompts. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics: System Demon-\nstrations. \nArman Cohan, Sergey Feldman, Iz Beltagy, Doug \nDowney, and Daniel Weld. 2020. \nSPECTER: \nDocument-level representation learning using \ncitation-informed transformers. \nIn Proceedings \nof the 58th Annual Meeting of the Association for \nComputational Linguistics. \n\nZhuyun Dai and Jamie Callan. 2020. Context-aware \ndocument term weighting for ad-hoc search. In Pro-\nceedings of The Web Conference 2020. \n\nZhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo \nNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. \n\nHall, and Ming-Wei Chang. 2022. Promptagator: \nFew-shot dense retrieval from 8 examples. \n\nAshwin Devaraj, Iain Marshall, Byron Wallace, and \nJunyi Jessy Li. 2021. Paragraph-level simplification \nof medical texts. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies. \n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and \nKristina Toutanova. 2019. BERT: Pre-training of \ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of \nthe North American Chapter of the Association for \nComputational Linguistics: Human Language Tech-\nnologies. \n\nThomas Diggelmann, Jordan L. Boyd-Graber, Jannis \nBulian, Massimiliano Ciaramita, and Markus Leip-\npold. 2020. CLIMATE-FEVER: A dataset for verifi-\ncation of real-world climate claims. In Proceedings \nof Tackling Climate Change with Machine Learning \nWorkshop at NeurIPS. \n\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela \nFan, Michael Auli, and Jason Weston. 2019. Wizard \nof wikipedia: Knowledge-powered conversational \nagents. In Proceedings of International Conference \non Learning Representations. \n\nMatthew Dunn, Levent Sagun, Mike Higgins, V Ugur \nGuney, Volkan Cirik, and Kyunghyun Cho. 2017. \nSearchQA: A new q&a dataset augmented with con-\ntext from a search engine. \n\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni. \n2014. Open question answering over curated and ex-\ntracted knowledge bases. In Proceedings of the 20th \nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining. \n\nKatja Filippova and Yasemin Altun. 2013. Overcom-\ning the lack of parallel data in sentence compression. \nIn Proceedings of the 2013 Conference on Empirical \nMethods in Natural Language Processing. \n\nAntonio Gulli. 2004. Ag's corpus of news articles. \n\nFelix Hamborg, Norman Meuschke, Corinna Bre-\nitinger, and Bela Gipp. 2017. news-please: A \ngeneric news crawler and extractor. In Proceedings \nof the 15th International Symposium of Information \nScience. \n\nFaegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, \nKrisztian Balog, Svein Erik Bratsberg, Alexander \nKotov, and Jamie Callan. 2017. Dbpedia-entity v2: \nA test collection for entity search. In Proceedings \nof the 40th International ACM SIGIR Conference on \nResearch and Development in Information Retrieval. \n\nSebastian Hofst\u00e4tter, Sheng-Chieh Lin, Jheng-Hong \nYang, Jimmy Lin, and Allan Hanbury. 2021. Ef-\nficiently teaching an effective dense retriever with \nbalanced topic aware sampling. In Proceedings of \nthe 44th International ACM SIGIR Conference on \nResearch and Development in Information Retrieval. \n\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis \nAllamanis, and Marc Brockschmidt. 2019. Code-\nsearchnet challenge: Evaluating the state of seman-\ntic code search. \n\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin, \nand Edouard Grave. 2022. Unsupervised dense in-\nformation retrieval with contrastive learning. Trans-\nactions on Machine Learning Research. \n\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William \nCohen, and Xinghua Lu. 2019. PubMedQA: A \ndataset for biomedical research question answering. \nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and \nthe 9th International Joint Conference on Natural \nLanguage Processing. \n\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke \nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguis-\ntics. \n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick \nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and \nWen-tau Yih. 2020. Dense passage retrieval for \nopen-domain question answering. In Proceedings of \nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing. \n\nDaniel Khashabi, Amos Ng, Tushar Khot, Ashish Sab-\nharwal, Hannaneh Hajishirzi, and Chris Callison-\nBurch. 2021. GooAQ: Open question answering \nwith diverse answer types. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2021. \n\nOmar Khattab and Matei Zaharia. 2020. ColBERT: Ef-\nficient and effective passage search via contextual-\nized late interaction over bert. In Proceedings of the \n43rd International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval. \n\nMahnaz Koupaee and William Yang Wang. 2018. Wik-\nihow: A large scale text summarization dataset. \n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti, \nDanielle Epstein, Illia Polosukhin, Matthew Kelcey, \nJacob Devlin, Kenton Lee, Kristina N. Toutanova, \nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob \n\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral Questions: a benchmark for question answering \nresearch. Transactions of the Association of Compu-\ntational Linguistics. \n\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. \n2019. Latent retrieval for weakly supervised open \ndomain question answering. In Proceedings of the \n57th Annual Meeting of the Association for Compu-\ntational Linguistics. \n\nPatrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale \nMinervini, Heinrich K\u00fcttler, Aleksandra Piktus, Pon-\ntus Stenetorp, and Sebastian Riedel. 2021. PAQ: 65 \nmillion probably-asked questions and what you can \ndo with them. Transactions of the Association for \nComputational Linguistics. \n\nFrederick Liu, Siamak Shakeri, Hongkun Yu, and Jing \nLi. 2021. EncT5: Fine-tuning t5 encoder for non-\nautoregressive tasks. \n\nXueqing Liu, Chi Wang, Yue Leng, and ChengXiang \nZhai. 2018. LinkSO: A dataset for learning to re-\ntrieve similar question answer pairs on software de-\nvelopment forums. In Proceedings of the 4th ACM \nSIGSOFT International Workshop on NLP for Soft-\nware Engineering. \n\nMacedo Maia, Siegfried Handschuh, Andr\u00e9 Freitas, \nBrian Davis, Ross McDermott, Manel Zarrouk, and \nAlexandra Balahur. 2018. WWW'18 open chal-\nlenge: Financial opinion mining and question an-\nswering. In Companion Proceedings of the The Web \nConference 2018. \n\nJean Maillard, Vladimir Karpukhin, Fabio Petroni, \nWen-tau Yih, Barlas Oguz, Veselin Stoyanov, and \nGargi Ghosh. 2021. \nMulti-task retrieval for \nknowledge-Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022. MetaICL: Learning to learn \nin context. In Proceedings of the 2022 Conference of \nthe North American Chapter of the Association for \nComputational Linguistics: Human Language Tech-\nnologies. \n\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and \nLuke Zettlemoyer. 2020. AmbigQA: Answering am-\nbiguous open-domain questions. In Proceedings of \nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing. \n\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and \nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions. \nIn Proceedings of the 60th Annual Meeting of the \n\nAssociation for Computational Linguistics (Volume \n1: Long Papers), pages 3470-3487, Dublin, Ireland. \nAssociation for Computational Linguistics. \n\nNiklas Muennighoff. 2022. SGPT: Gpt sentence em-\nbeddings for semantic search. \n\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. \n2018. Don't give me the details, just the summary! \ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018 \nConference on Empirical Methods in Natural Lan-\nguage Processing. \nBiderman, Leo Gao, Thomas Wolf, and Alexan-\nder M Rush. 2022. Multitask prompted training en-\nables zero-shot task generalization. In Proceedings \nof International Conference on Learning Represen-\ntations. \n\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon, \nChristopher Potts, and Matei Zaharia. 2022. Col-\nBERTv2: Effective and efficient retrieval via \nlightweight late interaction. In Proceedings of the \n2022 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Hu-\nman Language Technologies. \n\nAbigail See, Peter J. Liu, and Christopher D. Manning. \n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational \nLinguistics (Volume 1: Long Papers). \n\nKornrel Csernai Shankar Iyer, Nikhil Dandekar. 2012. \nFirst quora dataset release: Question pairs. \n\nZejiang Shen, Kyle Lo, Lauren Yu, Nathan Dahlberg, \nMargo Schlanger, and Doug Downey. 2022. Multi-\nlexsum: Real-world summaries of civil rights law-\nsuits at multiple granularities. In Proceedings of the \n36th Conference on Neural Information Processing \nSystems Datasets and Benchmarks Track. \n\nRobert S. Taylor. 1962. The process of asking ques-\ntions. American Documentation. \n\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR: \nA heterogeneous benchmark for zero-shot evalua-\ntion of information retrieval models. In Proceedings \nof the 35th Conference on Neural Information Pro-\ncessing Systems Datasets and Benchmarks Track. \n\nJames Thorne, \nAndreas Vlachos, \nChristos \nChristodoulopoulos, Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina \nDemner-Fushman, William R. Hersh, Kyle Lo, Kirk \nRoberts, Ian Soboroff, and Lucy Lu Wang. 2021. \nTREC-COVID: Constructing a pandemic informa-\ntion retrieval test collection. SIGIR Forum. \n\nHenning Wachsmuth, Shahbaz Syed, and Benno Stein. \n2018. Retrieval of the best counterargument with-\nout prior topic knowledge. In Proceedings of the \n56th Annual Meeting of the Association for Compu-\ntational Linguistics. \n\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu \nWang, Madeleine van Zuylen, Arman Cohan, and \nHannaneh Hajishirzi. 2020. Fact or fiction: Verify-\ning scientific claims. In Proceedings of the 2020 \nConference on Empirical Methods in Natural Lan-\nguage Processing. \n\nS. Walker, Stephen Robertson, M. Boughanem, G. J. F. \nJones, and K. Sparck Jones. 1998. Okapi at trec-6: \nAutomatic adhoc, vlc, routing, filtering and qsdr. In \nThe Sixth Text REtrieval Conference. \n\nKexin Wang, Nandan Thakur, Nils Reimers, and Iryna \nGurevych. 2022a. GPL: Generative pseudo label-\ning for unsupervised domain adaptation of dense re-\ntrieval. In Proceedings of the 2022 Conference of \nthe North American Chapter of the Association for \nComputational Linguistics: Human Language Tech-\nnologies. \n\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei, \nAnjana Arunkumar, Arjun Ashok, Arut Selvan \nDhanasekaran, Atharva Naik, David Stap, et al. \n2022b. Super-NaturalInstructions: Generalization \nvia declarative instructions on 1600+ nlp tasks. In \nProceedings of the 2022 Conference on Empirical \nMethods in Natural Language Processing. \n\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, \nAdams Wei Yu, Brian Lester, Nan Du, Andrew M. \nDai, and Quoc V Le. 2022a. Finetuned language \nmodels are zero-shot learners. In Proceedings of \nInternational Conference on Learning Representa-\ntions. \n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, \nPercy Liang, Jeff Dean, and William Fedus. 2022b. \nEmergent abilities of large language models. Trans-\nactions on Machine Learning Research. Survey Cer-\ntification. \n\nIkuya Yamada, Akari Asai, and Hannaneh Hajishirzi. \n2021. Efficient passage retrieval with hashing for \nopen-domain question answering. In Proceedings of \nthe 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International \nJoint Conference on Natural Language Processing. \n\nYi Yang, Wen-tau Yih, and Christopher Meek. 2015. \nWikiQA: A challenge dataset for open-domain ques-\ntion answering. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language \nProcessing. \n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, \nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for \ndiverse, explainable multi-hop question answering. \nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing. \n\n\nTable 5 :\n5The complete list of datasets included in BERRI.\n\nTable 7 :\n7Full list of the instructions for the BERRI datasets. We present one instruction per dataset. All of the instructions are available at our GitHub repository.B Further Detail about the X 2 -Retrieval \nevaluation \n\nQuery and corpus creations. For AmbigQA, \nwe use the official development split, including \n\n1,172 queries, as the official test split annotations \nare not publicly available. We use all paraphrased \nquestions for all train and development sets to form \nthe retrieval corpus. For WIKIQA, we combine \nthe development split and test split available at the \n\n\n\nTable 8 :\n8X 2 -Retrieval examples data.Dataset \nInstruction \n\nTREC-COVID \nRetrieve Scientific paper paragraph to answer this question \nNF Corpus \nRetrieve Scientific paper paragraph to answer this question \nFIQA \nFind financial web article paragraph to answer \nArguana \n\n\nTable 9 :\n9Full list of the instructions used for evaluations.\n\nTable 11\n11shows the qualitative examples given different instructions on X 2 -Retrieval, and Table 12 compares TART-full with Contriever MS MARCO.\n\nTable 18\n18shows the full BEIR results of TARTfull trained on varying numbers of datasets. We see that as we increase the number of datasets used during training, model performance often im-dataset \n\nexpected output \ninstruction-unfollowing corpus \n\nGigaword \narticle summary \nWikipedia paragraph \nMedical Paragraph Simplification \nsimplified text of medical cases Wikipedia paragraph \nMS MARCO \nweb answers \nOQA questions \nOQA \nsimilar questions \nYahoo Answers answer \nPubMedQA \nmedical paper abstract \nWikipedia paragraph \nQrecc \ndialogue responses \nWikipedia paragraph \nQuora \nduplicated questions \nWikipedia paragraph \nsentence compression \nsimplified sentence \nWikipedia paragraph \nStackExchange (question\u2192answer) title StackExchange answer \nStackExchange title \nStackExchange (title \u2192title) title \nStackExchange title \nStackExchange answer \nYahoo Answers \nYahoo Answers answer \nWikipedia paragraphs \n\n\n\nTable 10 :\n10The list of the combinations of the dataset and corresponding instruction-unfollowing corpora to mine instruction-unfollowing negative documents. : how to calculate the distance between two points using longitude and latitude Instruction Top documentRetrieve an answer post from StackOverflow to this question SELECT getDistance(lat1,lng1,lat2,lng2) as distance FROM your_table. Here's a MySQL function that will take two latitude longitude pairs, and give you the distance in degrees between the two points. It uses the Haversine formula to calculate the distance. Find a similar question asked in StackOverflow tried implementing formula good two points testing yet code working distance returns.Query: When did the kim family come to power? Instruction Top document find an answer sentence Kim came to lead the Soviet-backed North's provisional government, becoming the first premier of its new government, the Democratic People's Republic of Korea (commonly known as North Korea), in 1948. He started the Korean War in 1950 with hopes to reunify the region. (Wikipedia) Find a similar question When did the kim family come to power in North Korea? (Ambig QA) Query: 10% of sudden infant death syndrome (SIDS) deaths happen in newborns aged less than 6 months Instruction Top document retrieve a scientific paper paragraph to verify this Despite declines in prevalence during the past two decades, sudden infant death syndrome (SIDS) continues to be the leading cause of death for infants aged between 1 month and 1 year in developed countries. Behavioral risk factors identified in epidemiological studies include prone and side positions for infant sleep, smoke exposure, soft bedding, and sleep surfaces, and overheating. (Scientific paper) Find a Wikipedia paragraph to verify thisBy definition, SIDS deaths occur under the age of one year, with the peak incidence occurring when the infant is at 2 to 4 months of age. (Wikipedia)Query\n\nTable 11 :\n11Examples of the model's predictions given different instructions with the same query. The queries and documents are from X 2 -Retrieval.\n\n\nQuery: Who won the final hoh big brother 20? Instructions: a question similar to this Contriever TART-full \u2022 Who won the Final HoH in the American reality show Big Brother 20? (AmbigQA) \u2022 Who won the final vote in the British reality show Celebrity Big Brother 20? (AmbigQA) \u2022 Caleb Reynolds was a castaway on Survivor: Ka\u00f4h R\u014dng; he was medically evacuated from the game, and placed 15th. Nicole Franzel returned as a HouseGuest on Big Brother 18 where she was crowned the winner and became the first female winner to win against a male in the final 2. (Wikipedia paragraph) \u2022 Who won the final vote in the British reality show Celebrity Big Brother 20? (AmbigQA) \u2022 Who is left in the American big brother house at the end of season 20? (AmbigQA) \u2022 Who won the Final HoH in the American reality show Big Brother 20? (AmbigQA)\n\nTable 12 :\n12We compare TART-full outputs with the Contriever-MS MARCO (Izacard et al., 2022) predictions on X 2 -Retrieval. We show the top one prediction for the first four examples, and show the top three predictions for the bottom examples. mean that the documents follow instructions while mean that the documents do not satisfy the instructions. BEIR at training at test TREC NFC FQA ARG TOU DBP SCD CLI SCF avg. best TART-full 72.8 34.6 42.0 50.0 35.3 46.1 18.4 35.2 73.7 44.4 5Using instructions \n\nAblations \n\n61.1 21.9 38.4 39.8 23.6 36.1 15.0 24.7 65.2 36.2 0 \n67.6 34.9 40.6 39.5 20.5 47.1 17.5 39.8 75.4 42.5 3 \n57.2 37.1 41.3 50.0 18.3 41.3 18.3 32.5 73.2 41.1 2 \n\n\n\nTable 13 :\n13The full results of the instruction ablations on BEIR. TREC, NFC, FQA, ARG, TOU, DBP, SCD, CLI, SCF indicate TREC-COVID, FIQA, NF Corpus, Arguana, Touche-2020, DBPedia, SciDocs, Climate-Fever, and SciFact, respectively. instructions LOTTE X 2 -Retrieval at training at test AMB WQA SCF GAT LSO CSP avg.Using TART-full \n75.7 \n90.5 52.5 66.2 68.6 24.9 51.4 59.1 \n\nAblations \n\n68.5 \n59.3 54.4 61.7 62.0 15.1 46.8 49.9 \n70.5 \n40.1 47.2 64.0 69.5 25.5 43.7 48.3 \n69.9 \n34.5 32.5 60.8 58.2 24.2 49.3 43.3 \n\n\n\nTable 14 :\n14: Instruction ablations on LOTTE (Search pooled) and X 2 -Retrieval (pooled) evaluation. AMB, WQA, SCF, GAT, LSO, CSP denotes AmbigQA, WikiQA, SciFact, GooAQ-Technical, LinkSO-Python, and CodeSearchNet-Python, respectively.SciFact Find a scientific paper sentence to verify this questions 75.4 Retrieve a scientific paper abstract to verify this claim 75.7 can you retrieve reliable scientific evidence to check if the following claim is true or not? 74.3 please retrieve evidence for me to verify the following 73.8 a scientific paper sentence supporting or refuting the following statement retrieve an argument paragraph to answer this question 30.6 retrieve a paragraph to answer this debate question 30.9 Find a opinion to this debate question 29.5 retrieve an argument paragraph that supports this debate question to this debate question Climate-FEVER Retrieve a scientific paper abstract to verify the following claim Retrieve a Wikipedia paragraph to answer this question 30.4 Retrieve a Wikipedia paragraph to verify the following claim about climate change 30.8 I want to know if the following claim is true or not. Can you find Wikipedia evidence? Find a Wikipedia paragraph to verify the following claim 30.8Dataset \nInstruction \n\n\nTable 15 :\n15Performance on SciFact, Climate-FEVER and Touche-2020 with different instructions.SciFactRetrieve a scientific paper abstract to verify this claim 75.7 Retrieve a Wikipedia paragraph to verify the following claimDataset \n\nInstruction \n\n\nTable 16 :\n16Full list of the instructions used for evaluations. [NULL] means that at inference time, no instruction is given to TART-full. means a correct instruction, while means incorrect instructions. pretrained models TREC NFC FQA ARG TOU DBP SCD CLI SCF avg. best T5-LM-large 385M 73.3 34.2 40.2 47.1 32.8 45.3 18.2 35.2 74.9 43.7 3 T5-LM-XL 1.5B 71.6 33.1 41.8 43.1 34.0 46.0 18.5 38.3 75.5 44.7 6model size \nBEIR \n\nT5-LM-base \n110M \n62.9 29.7 33.9 37.8 30.8 38.6 15.1 29.2 70.7 38.7 0 \n\n\nTable 17 :\n17Zero-shot retrieval results for different sizes of TART-full on BEIR. TREC, NFC, FQA, ARG, TOU, SCD, CLI, SCF indicate TREC-COVID, FIQA, NF Corpus, Arguana, Touche-2020, DBPedia, SciDocs, Climate-Fever, and SciFact, respectively. dataset number BEIR pretrained models TREC NFC FQA ARG TOU DBP SCD CLI SCF avg. bestT5-LM-XL \n5 \n63.3 28.3 37.6 47.8 24.3 42.3 17.0 30.8 73.4 40.5 0 \nT5-LM-XL \n10 \n68.8 30.5 39.5 47.5 29.4 46.7 18.2 26.9 76.0 42.6 3 \nT5-LM-XL \n20 \n71.0 33.7 41.7 48.7 33.2 46.1 18.2 29.8 74.7 44.1 6 \n\n\n\nTable 18 :\n18Zero-shot retrieval results for different training dataset scales of TART-full on BEIR. TREC, NFC, FQA, ARG, TOU, SCD, CLI, SCF indicate TREC-COVID, FIQA, NF Corpus, Arguana, Touche-2020, DBPedia, SciDocs, Climate-Fever, and SciFact, respectively. pretrained models TREC NFC FQA ARG TOU DBP SCD CLI SCF avg.BEIR \n\n\n\nTable 19 :\n19Zero-shot retrieval results for TART-full initialized using different pretrained models on BEIR. TREC, NFC, FQA, ARG, TOU, SCD, CLI, SCF indicate TREC-COVID, FIQA, NF Corpus, Arguana, Touche-2020, DBPedia, SciDocs, Climate-Fever, and SciFact, respectively.\nCode, data and pretrained model checkpoints are available at https://github.com/facebookresearch/tart.\nFor examples, finding a corresponding review text for the review title \"I love this!\" is under-specified.5  Prior work has shown that MS MARCO can be beneficial to many downstream retrieval tasks(Izacard et al., 2022).6  For example, KILT datasets such as FEVER or NQ use the unified Wikipedia corpus.\nWe use a shared encoder since having separate encoders gave no additional gains in preliminary experiments.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\nAcknowledgementsWe thank Allen School NLP and Meta AI researchers for their insightful discussions, and Jeff Dalton, Mike Lewis, Sheng-Chieh Lin, Sandy Kaplan and Yizhong Wang for their helpful feedback on this paper and discussions.Appendix A Further BERRI DetailsA.1 Dataset List\n. Andrew Yates, Rodrigo Nogueira, Jimmy Lin, Andrew Yates, Rodrigo Nogueira, and Jimmy Lin.\n\nPretrained transformers for text ranking: Bert and beyond. 10.1145/3404835.3462812Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information RetrievalPretrained transformers for text ranking: Bert and beyond. In Proceedings of the 44th Interna- tional ACM SIGIR Conference on Research and De- velopment in Information Retrieval.\n\nOptimizing dense retrieval model training with hard negatives. Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, Shaoping Ma, 10.1145/3404835.3462880SI-GIR '21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2021. Optimizing dense retrieval model training with hard negatives. In SI- GIR '21: The 44th International ACM SIGIR Con- ference on Research and Development in Informa- tion Retrieval.\n\nReCoRD: Bridging the gap between human and machine commonsense reading comprehension. Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, Benjamin Van Durme, Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. ReCoRD: Bridging the gap between human and ma- chine commonsense reading comprehension.\n", "annotations": {"author": "[{\"end\":162,\"start\":42},{\"end\":284,\"start\":163},{\"end\":408,\"start\":285},{\"end\":529,\"start\":409},{\"end\":655,\"start\":530},{\"end\":782,\"start\":656},{\"end\":912,\"start\":783},{\"end\":1034,\"start\":913},{\"end\":1152,\"start\":1035}]", "publisher": null, "author_last_name": "[{\"end\":52,\"start\":48},{\"end\":174,\"start\":168},{\"end\":298,\"start\":293},{\"end\":419,\"start\":415},{\"end\":545,\"start\":538},{\"end\":672,\"start\":666},{\"end\":802,\"start\":792},{\"end\":924,\"start\":921},{\"end\":1042,\"start\":1040}]", "author_first_name": "[{\"end\":47,\"start\":42},{\"end\":167,\"start\":163},{\"end\":292,\"start\":285},{\"end\":414,\"start\":409},{\"end\":537,\"start\":530},{\"end\":665,\"start\":656},{\"end\":791,\"start\":783},{\"end\":920,\"start\":913},{\"end\":1039,\"start\":1035}]", "author_affiliation": "[{\"end\":161,\"start\":54},{\"end\":283,\"start\":176},{\"end\":407,\"start\":300},{\"end\":528,\"start\":421},{\"end\":654,\"start\":547},{\"end\":781,\"start\":674},{\"end\":911,\"start\":804},{\"end\":1033,\"start\":926},{\"end\":1151,\"start\":1044}]", "title": "[{\"end\":39,\"start\":1},{\"end\":1191,\"start\":1153}]", "venue": null, "abstract": "[{\"end\":2443,\"start\":1193}]", "bib_ref": "[{\"end\":2672,\"start\":2650},{\"end\":2773,\"start\":2758},{\"end\":2864,\"start\":2838},{\"end\":2877,\"start\":2864},{\"end\":5685,\"start\":5659},{\"end\":5727,\"start\":5690},{\"end\":5807,\"start\":5785},{\"end\":5822,\"start\":5807},{\"end\":5951,\"start\":5933},{\"end\":5970,\"start\":5951},{\"end\":7156,\"start\":7132},{\"end\":7173,\"start\":7156},{\"end\":7199,\"start\":7173},{\"end\":7356,\"start\":7338},{\"end\":7374,\"start\":7356},{\"end\":7395,\"start\":7374},{\"end\":7685,\"start\":7674},{\"end\":7705,\"start\":7685},{\"end\":7836,\"start\":7810},{\"end\":7904,\"start\":7882},{\"end\":7930,\"start\":7904},{\"end\":7952,\"start\":7930},{\"end\":7970,\"start\":7952},{\"end\":8053,\"start\":8035},{\"end\":8244,\"start\":8224},{\"end\":8287,\"start\":8270},{\"end\":8476,\"start\":8452},{\"end\":8798,\"start\":8779},{\"end\":8816,\"start\":8798},{\"end\":8836,\"start\":8816},{\"end\":8853,\"start\":8836},{\"end\":8872,\"start\":8853},{\"end\":8892,\"start\":8872},{\"end\":8911,\"start\":8892},{\"end\":9086,\"start\":9067},{\"end\":9105,\"start\":9086},{\"end\":9890,\"start\":9877},{\"end\":9924,\"start\":9892},{\"end\":9956,\"start\":9926},{\"end\":9992,\"start\":9961},{\"end\":10602,\"start\":10581},{\"end\":10621,\"start\":10602},{\"end\":10648,\"start\":10640},{\"end\":10667,\"start\":10648},{\"end\":11453,\"start\":11427},{\"end\":12097,\"start\":12074},{\"end\":13676,\"start\":13657},{\"end\":14508,\"start\":14499},{\"end\":14531,\"start\":14508},{\"end\":14941,\"start\":14914},{\"end\":15831,\"start\":15830},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17473,\"start\":17454},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17977,\"start\":17971},{\"end\":18082,\"start\":18060},{\"end\":19447,\"start\":19423},{\"end\":19844,\"start\":19823},{\"end\":20085,\"start\":20060},{\"end\":21067,\"start\":21048},{\"end\":21100,\"start\":21072},{\"end\":21188,\"start\":21167},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21237,\"start\":21219},{\"end\":22483,\"start\":22466},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23535,\"start\":23530},{\"end\":23577,\"start\":23540},{\"end\":24246,\"start\":24223},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24698,\"start\":24692},{\"end\":25239,\"start\":25218},{\"end\":27051,\"start\":27032},{\"end\":27092,\"start\":27056},{\"end\":27340,\"start\":27318},{\"end\":28289,\"start\":28288},{\"end\":29780,\"start\":29760},{\"end\":29799,\"start\":29780},{\"end\":32995,\"start\":32976},{\"end\":34153,\"start\":34134},{\"end\":34248,\"start\":34228},{\"end\":34266,\"start\":34248},{\"end\":34284,\"start\":34266},{\"end\":37242,\"start\":37207},{\"end\":37619,\"start\":37599},{\"end\":37638,\"start\":37619},{\"end\":37944,\"start\":37924},{\"end\":37962,\"start\":37944},{\"end\":41449,\"start\":41422},{\"end\":46082,\"start\":46080},{\"end\":48979,\"start\":48962},{\"end\":50783,\"start\":50763},{\"end\":50820,\"start\":50788},{\"end\":51214,\"start\":51194},{\"end\":51254,\"start\":51234},{\"end\":53550,\"start\":53546}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":53891,\"start\":53839},{\"attributes\":{\"id\":\"fig_1\"},\"end\":54395,\"start\":53892},{\"attributes\":{\"id\":\"fig_2\"},\"end\":54575,\"start\":54396},{\"attributes\":{\"id\":\"fig_3\"},\"end\":55027,\"start\":54576},{\"attributes\":{\"id\":\"fig_4\"},\"end\":55077,\"start\":55028},{\"attributes\":{\"id\":\"fig_5\"},\"end\":55470,\"start\":55078},{\"attributes\":{\"id\":\"fig_6\"},\"end\":55614,\"start\":55471},{\"attributes\":{\"id\":\"fig_7\"},\"end\":57255,\"start\":55615},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":57419,\"start\":57256},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":58392,\"start\":57420},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":58508,\"start\":58393},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":59913,\"start\":58509},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":60447,\"start\":59914},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":75972,\"start\":60448},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":76033,\"start\":75973},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":76615,\"start\":76034},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":76888,\"start\":76616},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":76952,\"start\":76889},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":77101,\"start\":76953},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":78010,\"start\":77102},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":79967,\"start\":78011},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":80118,\"start\":79968},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":80947,\"start\":80119},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":81627,\"start\":80948},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":82143,\"start\":81628},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":83399,\"start\":82144},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":83649,\"start\":83400},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":84145,\"start\":83650},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":84675,\"start\":84146},{\"attributes\":{\"id\":\"tab_26\",\"type\":\"table\"},\"end\":85004,\"start\":84676},{\"attributes\":{\"id\":\"tab_27\",\"type\":\"table\"},\"end\":85275,\"start\":85005}]", "paragraph": "[{\"end\":3465,\"start\":2459},{\"end\":4028,\"start\":3467},{\"end\":4375,\"start\":4030},{\"end\":4916,\"start\":4377},{\"end\":5539,\"start\":4918},{\"end\":5971,\"start\":5541},{\"end\":6639,\"start\":5973},{\"end\":6686,\"start\":6641},{\"end\":6783,\"start\":6688},{\"end\":6911,\"start\":6785},{\"end\":7041,\"start\":6913},{\"end\":8589,\"start\":7073},{\"end\":9519,\"start\":8591},{\"end\":9606,\"start\":9529},{\"end\":9707,\"start\":9618},{\"end\":10043,\"start\":9719},{\"end\":10074,\"start\":10045},{\"end\":10806,\"start\":10076},{\"end\":11623,\"start\":10827},{\"end\":12323,\"start\":11625},{\"end\":12716,\"start\":12325},{\"end\":13120,\"start\":12780},{\"end\":14347,\"start\":13161},{\"end\":14847,\"start\":14370},{\"end\":15602,\"start\":14849},{\"end\":16345,\"start\":15604},{\"end\":16764,\"start\":16347},{\"end\":17362,\"start\":16766},{\"end\":18237,\"start\":17364},{\"end\":19038,\"start\":18239},{\"end\":19167,\"start\":19040},{\"end\":19566,\"start\":19190},{\"end\":19845,\"start\":19601},{\"end\":20657,\"start\":19847},{\"end\":21505,\"start\":20693},{\"end\":21662,\"start\":21523},{\"end\":21819,\"start\":21664},{\"end\":21938,\"start\":21861},{\"end\":22059,\"start\":21940},{\"end\":23056,\"start\":22113},{\"end\":23314,\"start\":23072},{\"end\":23758,\"start\":23350},{\"end\":24194,\"start\":23760},{\"end\":24657,\"start\":24196},{\"end\":24878,\"start\":24659},{\"end\":25640,\"start\":24943},{\"end\":25919,\"start\":25642},{\"end\":26294,\"start\":25921},{\"end\":26580,\"start\":26296},{\"end\":26959,\"start\":26594},{\"end\":28152,\"start\":26985},{\"end\":28997,\"start\":28195},{\"end\":29800,\"start\":28999},{\"end\":30521,\"start\":29802},{\"end\":30798,\"start\":30559},{\"end\":31180,\"start\":30800},{\"end\":32030,\"start\":31193},{\"end\":32666,\"start\":32032},{\"end\":33382,\"start\":32684},{\"end\":33594,\"start\":33406},{\"end\":33942,\"start\":33596},{\"end\":34738,\"start\":33958},{\"end\":35484,\"start\":34759},{\"end\":36503,\"start\":35515},{\"end\":37243,\"start\":36505},{\"end\":38202,\"start\":37245},{\"end\":38432,\"start\":38254},{\"end\":39092,\"start\":38434},{\"end\":39216,\"start\":39094},{\"end\":39378,\"start\":39218},{\"end\":39746,\"start\":39380},{\"end\":40596,\"start\":39800},{\"end\":41475,\"start\":40598},{\"end\":41655,\"start\":41486},{\"end\":41826,\"start\":41678},{\"end\":41911,\"start\":41843},{\"end\":42005,\"start\":41923},{\"end\":42151,\"start\":42014},{\"end\":42245,\"start\":42163},{\"end\":42318,\"start\":42247},{\"end\":42396,\"start\":42329},{\"end\":42521,\"start\":42404},{\"end\":42610,\"start\":42523},{\"end\":42836,\"start\":42612},{\"end\":42879,\"start\":42838},{\"end\":42947,\"start\":42887},{\"end\":43055,\"start\":42972},{\"end\":43147,\"start\":43057},{\"end\":43212,\"start\":43156},{\"end\":43292,\"start\":43230},{\"end\":43376,\"start\":43317},{\"end\":43481,\"start\":43388},{\"end\":43555,\"start\":43491},{\"end\":43811,\"start\":43557},{\"end\":43900,\"start\":43829},{\"end\":44132,\"start\":43917},{\"end\":44179,\"start\":44134},{\"end\":44262,\"start\":44192},{\"end\":44343,\"start\":44274},{\"end\":44425,\"start\":44355},{\"end\":44504,\"start\":44436},{\"end\":44560,\"start\":44506},{\"end\":44743,\"start\":44570},{\"end\":44869,\"start\":44745},{\"end\":44935,\"start\":44871},{\"end\":45202,\"start\":44937},{\"end\":45298,\"start\":45214},{\"end\":45402,\"start\":45300},{\"end\":45602,\"start\":45404},{\"end\":46643,\"start\":45616},{\"end\":46927,\"start\":46645},{\"end\":47308,\"start\":46929},{\"end\":47769,\"start\":47310},{\"end\":48094,\"start\":47771},{\"end\":48547,\"start\":48155},{\"end\":49418,\"start\":48549},{\"end\":49495,\"start\":49453},{\"end\":49864,\"start\":49541},{\"end\":50536,\"start\":49866},{\"end\":51503,\"start\":50627},{\"end\":52537,\"start\":51505},{\"end\":53715,\"start\":52539},{\"end\":53838,\"start\":53717}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":19600,\"start\":19567},{\"attributes\":{\"id\":\"formula_1\"},\"end\":20692,\"start\":20658},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21860,\"start\":21820},{\"attributes\":{\"id\":\"formula_3\"},\"end\":22112,\"start\":22060}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14277,\"start\":14270},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":14818,\"start\":14811},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25783,\"start\":25775},{\"end\":27510,\"start\":27503},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":28214,\"start\":28207},{\"end\":30095,\"start\":30088},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30797,\"start\":30789},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":32621,\"start\":32613},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":38117,\"start\":38110},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":38273,\"start\":38266},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":39568,\"start\":39561},{\"end\":41142,\"start\":41135},{\"end\":41178,\"start\":41171},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":46702,\"start\":46695},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":48022,\"start\":48015},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":48546,\"start\":48538},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":49407,\"start\":49399},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":49588,\"start\":49580},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":49670,\"start\":49662},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":49912,\"start\":49904},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":50093,\"start\":50085},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":50237,\"start\":50229},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":51043,\"start\":51035}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2457,\"start\":2445},{\"attributes\":{\"n\":\"2\"},\"end\":7071,\"start\":7044},{\"end\":9527,\"start\":9522},{\"end\":9616,\"start\":9609},{\"end\":9717,\"start\":9710},{\"attributes\":{\"n\":\"3\"},\"end\":10825,\"start\":10809},{\"attributes\":{\"n\":\"4\"},\"end\":12778,\"start\":12719},{\"attributes\":{\"n\":\"4.1\"},\"end\":13159,\"start\":13123},{\"attributes\":{\"n\":\"4.2\"},\"end\":14368,\"start\":14350},{\"attributes\":{\"n\":\"5.1\"},\"end\":19188,\"start\":19170},{\"attributes\":{\"n\":\"5.2\"},\"end\":21521,\"start\":21508},{\"attributes\":{\"n\":\"6\"},\"end\":23070,\"start\":23059},{\"attributes\":{\"n\":\"6.1\"},\"end\":23348,\"start\":23317},{\"attributes\":{\"n\":\"6.2\"},\"end\":24941,\"start\":24881},{\"attributes\":{\"n\":\"6.3\"},\"end\":26592,\"start\":26583},{\"attributes\":{\"n\":\"6.4\"},\"end\":26983,\"start\":26962},{\"attributes\":{\"n\":\"7\"},\"end\":28162,\"start\":28155},{\"attributes\":{\"n\":\"7.1\"},\"end\":28193,\"start\":28165},{\"attributes\":{\"n\":\"7.2\"},\"end\":30557,\"start\":30524},{\"attributes\":{\"n\":\"8\"},\"end\":31191,\"start\":31183},{\"attributes\":{\"n\":\"8.2\"},\"end\":32682,\"start\":32669},{\"end\":33404,\"start\":33385},{\"attributes\":{\"n\":\"8.3\"},\"end\":33956,\"start\":33945},{\"attributes\":{\"n\":\"8.4\"},\"end\":34757,\"start\":34741},{\"attributes\":{\"n\":\"9\"},\"end\":35513,\"start\":35487},{\"end\":38215,\"start\":38205},{\"end\":38252,\"start\":38218},{\"end\":39775,\"start\":39749},{\"end\":39798,\"start\":39778},{\"attributes\":{\"n\":\"1.\"},\"end\":41484,\"start\":41478},{\"attributes\":{\"n\":\"3.\"},\"end\":41676,\"start\":41658},{\"attributes\":{\"n\":\"4.\"},\"end\":41841,\"start\":41829},{\"attributes\":{\"n\":\"5.\"},\"end\":41921,\"start\":41914},{\"attributes\":{\"n\":\"6.\"},\"end\":42012,\"start\":42008},{\"attributes\":{\"n\":\"7.\"},\"end\":42161,\"start\":42154},{\"attributes\":{\"n\":\"9.\"},\"end\":42327,\"start\":42321},{\"attributes\":{\"n\":\"10.\"},\"end\":42402,\"start\":42399},{\"attributes\":{\"n\":\"15.\"},\"end\":42885,\"start\":42882},{\"attributes\":{\"n\":\"16.\"},\"end\":42970,\"start\":42950},{\"attributes\":{\"n\":\"18.\"},\"end\":43154,\"start\":43150},{\"attributes\":{\"n\":\"19.\"},\"end\":43228,\"start\":43215},{\"attributes\":{\"n\":\"20.\"},\"end\":43315,\"start\":43295},{\"attributes\":{\"n\":\"21.\"},\"end\":43386,\"start\":43379},{\"attributes\":{\"n\":\"22.\"},\"end\":43489,\"start\":43484},{\"attributes\":{\"n\":\"26.\"},\"end\":43827,\"start\":43814},{\"attributes\":{\"n\":\"27.\"},\"end\":43915,\"start\":43903},{\"attributes\":{\"n\":\"30.\"},\"end\":44190,\"start\":44182},{\"attributes\":{\"n\":\"31.\"},\"end\":44272,\"start\":44265},{\"attributes\":{\"n\":\"32.\"},\"end\":44353,\"start\":44346},{\"attributes\":{\"n\":\"33.\"},\"end\":44434,\"start\":44428},{\"attributes\":{\"n\":\"35.\"},\"end\":44568,\"start\":44563},{\"end\":45212,\"start\":45205},{\"end\":45614,\"start\":45605},{\"end\":48129,\"start\":48097},{\"end\":48153,\"start\":48132},{\"end\":49451,\"start\":49421},{\"end\":49539,\"start\":49498},{\"end\":50578,\"start\":50539},{\"end\":50625,\"start\":50581},{\"end\":53850,\"start\":53840},{\"end\":54597,\"start\":54577},{\"end\":55039,\"start\":55029},{\"end\":55099,\"start\":55079},{\"end\":55493,\"start\":55472},{\"end\":57266,\"start\":57257},{\"end\":57424,\"start\":57421},{\"end\":58403,\"start\":58394},{\"end\":58519,\"start\":58510},{\"end\":75983,\"start\":75974},{\"end\":76044,\"start\":76035},{\"end\":76626,\"start\":76617},{\"end\":76899,\"start\":76890},{\"end\":76962,\"start\":76954},{\"end\":77111,\"start\":77103},{\"end\":78022,\"start\":78012},{\"end\":79979,\"start\":79969},{\"end\":80959,\"start\":80949},{\"end\":81639,\"start\":81629},{\"end\":82155,\"start\":82145},{\"end\":83411,\"start\":83401},{\"end\":83661,\"start\":83651},{\"end\":84157,\"start\":84147},{\"end\":84687,\"start\":84677},{\"end\":85016,\"start\":85006}]", "table": "[{\"end\":58392,\"start\":57460},{\"end\":58508,\"start\":58495},{\"end\":59913,\"start\":59456},{\"end\":75972,\"start\":62050},{\"end\":76615,\"start\":76203},{\"end\":76888,\"start\":76657},{\"end\":78010,\"start\":77293},{\"end\":79967,\"start\":79962},{\"end\":81627,\"start\":81434},{\"end\":82143,\"start\":81944},{\"end\":83399,\"start\":83377},{\"end\":83649,\"start\":83626},{\"end\":84145,\"start\":84055},{\"end\":84675,\"start\":84474},{\"end\":85004,\"start\":84997}]", "figure_caption": "[{\"end\":53891,\"start\":53852},{\"end\":54395,\"start\":53894},{\"end\":54575,\"start\":54398},{\"end\":55027,\"start\":54600},{\"end\":55077,\"start\":55041},{\"end\":55470,\"start\":55102},{\"end\":55614,\"start\":55497},{\"end\":57255,\"start\":55617},{\"end\":57419,\"start\":57268},{\"end\":57460,\"start\":57426},{\"end\":58495,\"start\":58405},{\"end\":59456,\"start\":58521},{\"end\":60447,\"start\":59916},{\"end\":62050,\"start\":60450},{\"end\":76033,\"start\":75985},{\"end\":76203,\"start\":76046},{\"end\":76657,\"start\":76628},{\"end\":76952,\"start\":76901},{\"end\":77101,\"start\":76965},{\"end\":77293,\"start\":77114},{\"end\":79962,\"start\":78025},{\"end\":80118,\"start\":79982},{\"end\":80947,\"start\":80121},{\"end\":81434,\"start\":80962},{\"end\":81944,\"start\":81642},{\"end\":83377,\"start\":82158},{\"end\":83626,\"start\":83414},{\"end\":84055,\"start\":83664},{\"end\":84474,\"start\":84160},{\"end\":84997,\"start\":84690},{\"end\":85275,\"start\":85019}]", "figure_ref": "[{\"end\":2905,\"start\":2897},{\"end\":3137,\"start\":3129},{\"end\":3677,\"start\":3669},{\"end\":10910,\"start\":10902},{\"end\":11233,\"start\":11225},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14611,\"start\":14603},{\"end\":17021,\"start\":17013},{\"end\":17713,\"start\":17705},{\"end\":18469,\"start\":18461},{\"end\":21660,\"start\":21652},{\"end\":31607,\"start\":31599},{\"end\":32137,\"start\":32129},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":33131,\"start\":33115},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":33174,\"start\":33165},{\"end\":33442,\"start\":33434},{\"end\":33622,\"start\":33614},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":34568,\"start\":34559},{\"end\":34850,\"start\":34842},{\"end\":40002,\"start\":39994},{\"end\":40652,\"start\":40643}]", "bib_author_first_name": "[{\"end\":86125,\"start\":86119},{\"end\":86140,\"start\":86133},{\"end\":86156,\"start\":86151},{\"end\":86751,\"start\":86744},{\"end\":86764,\"start\":86758},{\"end\":86775,\"start\":86770},{\"end\":86788,\"start\":86781},{\"end\":86797,\"start\":86794},{\"end\":86813,\"start\":86805},{\"end\":87308,\"start\":87303},{\"end\":87324,\"start\":87316},{\"end\":87338,\"start\":87330},{\"end\":87352,\"start\":87344},{\"end\":87363,\"start\":87358},{\"end\":87377,\"start\":87369}]", "bib_author_last_name": "[{\"end\":86131,\"start\":86126},{\"end\":86149,\"start\":86141},{\"end\":86160,\"start\":86157},{\"end\":86756,\"start\":86752},{\"end\":86768,\"start\":86765},{\"end\":86779,\"start\":86776},{\"end\":86792,\"start\":86789},{\"end\":86803,\"start\":86798},{\"end\":86816,\"start\":86814},{\"end\":87314,\"start\":87309},{\"end\":87328,\"start\":87325},{\"end\":87342,\"start\":87339},{\"end\":87356,\"start\":87353},{\"end\":87367,\"start\":87364},{\"end\":87387,\"start\":87378}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":86208,\"start\":86117},{\"attributes\":{\"doi\":\"10.1145/3404835.3462812\",\"id\":\"b1\",\"matched_paper_id\":222310837},\"end\":86679,\"start\":86210},{\"attributes\":{\"doi\":\"10.1145/3404835.3462880\",\"id\":\"b2\",\"matched_paper_id\":233289894},\"end\":87215,\"start\":86681},{\"attributes\":{\"id\":\"b3\"},\"end\":87572,\"start\":87217}]", "bib_title": "[{\"end\":86267,\"start\":86210},{\"end\":86742,\"start\":86681}]", "bib_author": "[{\"end\":86133,\"start\":86119},{\"end\":86151,\"start\":86133},{\"end\":86162,\"start\":86151},{\"end\":86758,\"start\":86744},{\"end\":86770,\"start\":86758},{\"end\":86781,\"start\":86770},{\"end\":86794,\"start\":86781},{\"end\":86805,\"start\":86794},{\"end\":86818,\"start\":86805},{\"end\":87316,\"start\":87303},{\"end\":87330,\"start\":87316},{\"end\":87344,\"start\":87330},{\"end\":87358,\"start\":87344},{\"end\":87369,\"start\":87358},{\"end\":87389,\"start\":87369}]", "bib_venue": "[{\"end\":86403,\"start\":86292},{\"end\":86949,\"start\":86841},{\"end\":87301,\"start\":87217},{\"end\":86501,\"start\":86405}]"}}}, "year": 2023, "month": 12, "day": 17}