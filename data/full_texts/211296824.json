{"id": 211296824, "updated": "2023-10-06 18:24:26.164", "metadata": {"title": "Fully automated body composition analysis in routine CT imaging using 3D semantic segmentation convolutional neural networks", "authors": "[{\"first\":\"Sven\",\"last\":\"Koitka\",\"middle\":[]},{\"first\":\"Lennard\",\"last\":\"Kroll\",\"middle\":[]},{\"first\":\"Eugen\",\"last\":\"Malamutmann\",\"middle\":[]},{\"first\":\"Arzu\",\"last\":\"Oezcelik\",\"middle\":[]},{\"first\":\"Felix\",\"last\":\"Nensa\",\"middle\":[]}]", "venue": "European Radiology", "journal": "European Radiology", "publication_date": {"year": 2020, "month": 9, "day": 18}, "abstract": "Objectives Body tissue composition is a long-known biomarker with high diagnostic and prognostic value not only in cardiovascular, oncological, and orthopedic diseases but also in rehabilitation medicine or drug dosage. In this study, the aim was to develop a fully automated, reproducible, and quantitative 3D volumetry of body tissue composition from standard CT examinations of the abdomen in order to be able to offer such valuable biomarkers as part of routine clinical imaging. Methods Therefore, an in-house dataset of 40 CTs for training and 10 CTs for testing were fully annotated on every fifth axial slice with five different semantic body regions: abdominal cavity, bones, muscle, subcutaneous tissue, and thoracic cavity. Multi-resolution U-Net 3D neural networks were employed for segmenting these body regions, followed by subclassifying adipose tissue and muscle using known Hounsfield unit limits. Results The S\u00f8rensen Dice scores averaged over all semantic regions was 0.9553 and the intra-class correlation coefficients for subclassified tissues were above 0.99. Conclusions Our results show that fully automated body composition analysis on routine CT imaging can provide stable biomarkers across the whole abdomen and not just on L3 slices, which is historically the reference location for analyzing body composition in the clinical routine. Key Points \u2022 Our study enables fully automated body composition analysis on routine abdomen CT scans. \u2022 The best segmentation models for semantic body region segmentation achieved an averaged S\u00f8rensen Dice score of 0.9553. \u2022 Subclassified tissue volumes achieved intra-class correlation coefficients over 0.99. Electronic supplementary material The online version of this article (10.1007/s00330-020-07147-3) contains supplementary material, which is available to authorized users.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2002.10776", "mag": "3087234158", "acl": null, "pubmed": "32945971", "pubmedcentral": "7979624", "dblp": "journals/corr/abs-2002-10776", "doi": "10.1007/s00330-020-07147-3"}}, "content": {"source": {"pdf_hash": "20c230a9b5e8349e8cabd6fd8d9c2147e6f5111c", "pdf_src": "PubMedCentral", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://link.springer.com/content/pdf/10.1007/s00330-020-07147-3.pdf", "status": "HYBRID"}}, "grobid": {"id": "940b0f03b043937b98619e6d592d72ee7a498b47", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/20c230a9b5e8349e8cabd6fd8d9c2147e6f5111c.txt", "contents": "\nFully automated body composition analysis in routine CT imaging using 3D semantic segmentation convolutional neural networks\n\n\nSven Koitka \nIMAGING INFORMATICS AND ARTIFICIAL INTELLIGENCE\n\n\nLennard Kroll \nIMAGING INFORMATICS AND ARTIFICIAL INTELLIGENCE\n\n\nEugen Malamutmann \nIMAGING INFORMATICS AND ARTIFICIAL INTELLIGENCE\n\n\nArzu Oezcelik \nIMAGING INFORMATICS AND ARTIFICIAL INTELLIGENCE\n\n\nFelix Nensa \nIMAGING INFORMATICS AND ARTIFICIAL INTELLIGENCE\n\n\nFully automated body composition analysis in routine CT imaging using 3D semantic segmentation convolutional neural networks\nReceived: 26 February 2020 / Revised: 18 June 2020 / Accepted: 4 August 2020Abdomen Body composition Computer-assisted image analysis Deep learning\nObjectives Body tissue composition is a long-known biomarker with high diagnostic and prognostic value not only in cardiovascular, oncological, and orthopedic diseases but also in rehabilitation medicine or drug dosage. In this study, the aim was to develop a fully automated, reproducible, and quantitative 3D volumetry of body tissue composition from standard CT examinations of the abdomen in order to be able to offer such valuable biomarkers as part of routine clinical imaging. Methods Therefore, an in-house dataset of 40 CTs for training and 10 CTs for testing were fully annotated on every fifth axial slice with five different semantic body regions: abdominal cavity, bones, muscle, subcutaneous tissue, and thoracic cavity. Multiresolution U-Net 3D neural networks were employed for segmenting these body regions, followed by subclassifying adipose tissue and muscle using known Hounsfield unit limits. Results The S\u00f8rensen Dice scores averaged over all semantic regions was 0.9553 and the intra-class correlation coefficients for subclassified tissues were above 0.99. Conclusions Our results show that fully automated body composition analysis on routine CT imaging can provide stable biomarkers across the whole abdomen and not just on L3 slices, which is historically the reference location for analyzing body composition in the clinical routine. Key Points \u2022 Our study enables fully automated body composition analysis on routine abdomen CT scans.\u2022 The best segmentation models for semantic body region segmentation achieved an averaged S\u00f8rensen Dice score of 0.9553. \u2022 Subclassified tissue volumes achieved intra-class correlation coefficients over 0.99.\n\nIntroduction\n\nThanks to advances in computer-aided image analysis, radiological image data are now increasingly considered a valuable source of quantitative biomarkers [1][2][3][4][5][6]. Body tissue composition is a long-known biomarker with high diagnostic and prognostic value not only in cardiovascular, oncological, and orthopedic diseases but also in rehabilitation medicine or drug dosage. As obvious and simple as a quantitative determination of tissue composition based on modern radiological sectional Electronic supplementary material The online version of this article (https://doi.org/10.1007/s00330-020-07147-3) contains supplementary material, which is available to authorized users. imaging may seem, the actual extraction of this information in clinical routine is not feasible, since a manual assessment requires an extraordinary amount of human labor. A recent study has shown that some anthropometric measures can be estimated from simple and reproducible 2D measurements in CT using linear regression models [7]. Another study showed that a fully automated 2D segmentation of CT sectional images at the level of L3 vertebra into subcutaneous adipose tissue, muscle, viscera, and bone was possible using a 2D U-Net architecture [8]. The determination of the tissue composition at the level of L3 is often used as a reference in clinical routine to limit the amount of work required for the assessment. However, even here, this is only a rough approximation, since the inter-individual variability between patients is large and the section at the level of L3 does not necessarily have to be representative of the entire human anatomy. Other dedicated techniques for analyzing body composition using dual-energy X-ray absorptiometry or magnetic resonance imaging exist [9] but require additional potentially time-consuming or expensive procedures to be performed.\n\nThe aim of our study was therefore to develop a fully automated, reproducible, and quantitative 3D volumetry of body tissue composition from standard CT examinations of the abdomen in order to be able to offer such valuable biomarkers as part of routine clinical imaging.\n\n\nMaterials and methods\n\nDataset A retrospective dataset was collected, consisting of 40 abdominal CTs for training and 10 abdominal CTs for testing ( Table 1). The included scans were randomly selected from abdominal CT studies performed between 2015 and 2019 at the University Hospital Essen. The indication of the studies was not considered. According to the distribution of clinical studies in our department, more than 50% should have been examined for oncological indications. Each CT volume has a slice thickness of 5 mm and was reconstructed using a soft tissue convolutional reconstruction kernel. The data was annotated with five different labels: background (= outside the human body), muscle, bones, subcutaneous tissue, abdominal cavity, and thoracic cavity. For annotation, the ITK Snap [10] software (version 3.8.0) was used. Region segmentation was performed manually with a polygon tool. In order to reduce the annotation effort, every fifth slice was fully annotated. Remaining slices were marked with an ignore label, as visualized in Fig. 1. The final dataset contains 751 fully annotated slices for training and 186 for testing.\n\n\nNetwork architectures\n\nMany different architectural designs exist implementing semantic segmentation, some utilizing pre-trained classification networks trained on ImageNet; others are designed to be trained from scratch. For this study, two different network architectures were chosen for training, namely the commonly used U-Net 3D [11] and a more recent variant multi-resolution U-Net 3D [12]. The latter is shown in Fig. 2; however, U-Net 3D is very similar to residual path blocks replaced by identity operations and multi-resolution blocks replaced by two successive convolutions. In this case, volumetric data limits the batch size to a single example per batch due to a large memory footprint. Therefore, instance normalization [13] layers were utilized in favor of batch normalization layers [14]. In the original architectures, transposed convolutions were employed to upsample feature maps back to the original image size. However, transposed convolutions tend to generate checkerboard artifacts [15]. This is why trilinear upsampling followed by a 3 \u00d7 3 \u00d7 3 convolution was used instead, which is computationally more expensive, but more stable during optimization. Additionally, different choices for the initial number of feature maps n f are evaluated: 16, 32, and 64. After each pooling step, the number gets doubled, resulting in 256, 512, and 1024 feature maps in the lowest resolution, respectively.\n\n\nTraining details\n\nThe implementation of network architectures and training was done in Python using Tensorflow 2.0 [16] and the Keras API. Nvidia Titan RTX GPUs with 24-GB VRAM were used, which enable the training of more complex network architectures when using large volumetric data.\n\nAdam [17] with decoupled weight decay regularization [18] was utilized, configured with beta_1 = 0.9, beta_2 = 0.999, eps = 1e-7, and weight decay of 1e-4. An exponentially decaying learning rate with an initial value of 1e-4, multiplied by 0.95 every 50 epochs, helped to stabilize the optimization process at the end of the training. For selecting the best model weights during training, fivefold cross-validation was used on the training set and the average dice score was monitored on the respective validation splits. Since the training dataset consists of 40 abdominal CTs, each training run was performed using 32 CTs for training and 8 CTs for validation.\n\nDuring training, several data augmentations were applied in order to virtually increase the unique sample size for training a generalizable network. For example, in [11,12,19], it has been shown that aggressive data augmentation strategies can prevent overfitting on small sample sizes by capturing expectable variations in the data. First, random scale augmentation was applied with a scaling factor sampled uniformly between 0.8 and 1.2. Since this factor was sampled independently for both x-and yaxis, it also acts as an aspect ratio augmentation. Second, random flipping was utilized to mirror volumes on the x-axis. Third, subvolumes of size 32 \u00d7 256 \u00d7 256 were randomly cropped from the full volume with size n \u00d7 512 \u00d7 512. During inference, the same number of slices was used, but with x-and y-dimension kept unchanged, and the whole volume was processed using a sliding window approach with a 75% overlap. To improve segmentation accuracy, predictions for overlapping subvolumes were aggregated in a weighted fashion, giving the central slices more weight than the outermost.\n\nBesides random data augmentations, additional preprocessing steps were performed before feeding the image data into the neural networks. Volumes were downscaled by factor 2 to 128 \u00d7 128 on the x-/y-axes, retaining a slice thickness of 5 mm on the z-axis. CT images are captured as Hounsfield units (HU), which capture fine details and allow for different interpretations depending on which transfer function is used to map HUs to a color (e.g., black/white). Normally, when using floating-point values, the typical scanner quantization of 12 bits can be stored lossless and a network should be able to process all information without any problems.  data in order to map to [0, 1] with clipping outliers to the respective minimum and maximum values and stacked as channels. Lastly, the network inputs were centered around zero with a minimum value at \u2212 1 and maximum value at + 1. For supervision, a combination of softmax cross-entropy loss and generalized S\u00f8rensen Dice loss [20] was chosen, similar to [19]. Voxels marked with an ignore label do not contribute to the loss computation. Both losses are defined as below:\nL XCE \u00bc \u2212 1 N \u00c1 \u2211 N n\u22121 \u2211 C c\u00bc1 y c;n \u00c1 log b y c;n L Dice \u00bc 1:0\u2212 1 C\u22121 \u00c1 \u2211 c\u00bc2 \u2211 N n\u00bc1 2 \u00c1 b y c;n \u00c1 y c;n \u00fe \u03f5 \u2211 N n\u00bc1\nb y c;n \u00fe y c;n \u00fe \u03f5 C stands for the total number of classes, which equals six for the problem at hand. b y c;n and y c,n represent the prediction respectively groundtruth label for class c at voxel location n.\n\nThe background class is in this work explicitly not covered by the dice loss in order to give the foreground classes more weight in the optimization process. This choice is well known for class imbalanced problems where the foreground class only covers little areas compared with the background class. The final loss is an equally weighted combination of both losses:\nL SV \u00bc 0:5 \u00c1 L XCE \u00fe 0:5 \u00c1 L Dice\n\nTissue quantification\n\nVarious materials can be extracted from a CT by thresholding the HU to a specific intensity range. For quantifying tissues, the reporting system uses a mixture of classical thresholding and modern semantic segmentation neural networks for building semantic relationships. During training, fivefold cross- validation [21] was employed to measure the generalization performance of the selected model configuration, which in the end produced five trained model weights per configuration. For inference, those five models were used to build an ensemble system [21] by averaging the probabilities of all individual predictions, which a common method for increasing the stability and accuracy of a machine learning model. The final output of the quantification system is a report about subcutaneous adipose tissue (SAT), visceral adipose tissue (VAT), and muscle volume. Muscular tissue is identified by thresholding the HU between \u2212 29 and 150 [22]. Adipose tissue is identified by thresholding the HU between \u2212 190 and \u2212 30 [22]. If an adipose voxel is within the abdominal cavity region, it is counted as VAT. If it is within the subcutaneous tissue region, it is counted as SAT. Automatically subclassified tissue volumes were validated against the tissue volumes derived from groundtruth annotations using the intraclass correlation method on a slice by slice basis.\n\n\nResults\n\n\nModel evaluation\n\nAs described in the \"Network architecture\" and \"Training details\" sections, two different network architectures with the varying initial number of feature maps were systematically evaluated using a fivefold cross-validation scheme on the training dataset. The results are stated in  Fig. 3. Most slices show almost perfect segmentation boundaries; however, especially the ribs are problematic due to the partial volume effect. In 5-mm CTs, it is even sometimes hard for human readers to correctly assign one or the other region.\n\n\nAblation study\n\nDuring model development, it was observed that the choice of HU window has an impact on optimization stability and final achieved scores. Therefore, a small ablation study was conducted in order to systematically evaluate the influence of different HU limits. Additional models were trained using the same training parameters, but only with changed input pre-processing. The results are stated in Table 3.\n\nIncreasing the HU intensity range consistently improves dice scores. By combining multiple HU windows as separate input channels, the dice scores can be even more improved to over 0.95 dice score on average on both cross-validation and test set. The lowest scores of 0.829 dice on average for crossvalidation and 0.875 for the test set were achieved by an abdominal HU window ranging from \u2212 150 to 250.\n\n\nTissue quantification report\n\nAs described in the \"Tissue quantification\" section, the segmentation models are intended to be used for assigning  Fig. 4. In order to visually inspect the quality of the tissue segmentation, a PDF report with sagittal and coronal slices is generated, in conjunction with a stacked bar plot showing the volumes of segmented muscle, SAT, and VAT per axial slice (see Fig. 5). This is only intended to give the human reader a first visual impression on the system output. For analysis, an additional table with all numeric values per slice is generated. The PDF file is encapsulated into DICOM and automatically sent back to the PACS, in order to make use of existing DICOM infrastructure.\n\n\nDiscussion\n\nOur study aimed to develop a fully automated, reproducible, and quantitative 3D volumetry of body tissue composition from standard abdominal CT examinations in order to provide valuable biomarkers as part of routine clinical imaging. Our best approach using a multi-resolution U-Net 3D with an initial feature map count of 64 was able to fully automatically segment abdominal cavity, bones, muscle, subcutaneous tissue, and thoracic cavity with a mean S\u00f8rensen Dice coefficient of 0.9553 and thus yielded excellent results. The derived tissue volumetry had intra-class correlation coefficients of over 0.99. Further experiments showed a high performance with heavily reduced parameter counts which enables considering speed/accuracy trade-offs depending on the type of application. Choosing the transfer function to map from HU to a normalized value range for feeding images into neural networks was found to have a huge impact on segmentation performance.\n\nIn a recent study, manual single-slice CT measurements were used to build linear regression models for predicting stable anthropometric measures [7]. As the authors suggest, these measures may be important as biomarkers for several diseases like e.g. sarcopenia, but could also be used where the real measurements are not available. However, manual single-Image Groundtruth Prediction Fig. 3 Comparison of different slices, their respective groundtruth annotation, and predictions of the ensemble formed from five trained models on cross-validation splits slice CT measurements are still prone to intra-patient variability and inter-and intra-rater variability. By using a fully automated approach, derived anthropometric measures from more than a single CT slice should in theory be more stable.\n\nFully automated analysis of body composition has been attempted many times in the past. Older methods utilize classical image processing and binary morphological operations [23][24][25] in order to isolate the SAT and VAT from total adipose tissue (TAT). Other studies use prior knowledge about contours and shapes and actively fit a contour or template to a given CT image [26][27][28][29][30]. Those methods are prone to variations in intensity values and assume certain body structures for algorithmic separation between SAT and VAT. Apart from purely CT imaging-based studies, there have been efforts to apply similar techniques to magnetic resonance imaging (MRI) [31][32][33]. However, MRI procedures are more cost and time expensive than CT imaging in the clinical routine. Specific MRI procedures exist for body fat assessment, but have to be performed explicitly. Our approach can be used on routine CT imaging and may be used as supplementary material for diagnosis or screening purposes.\n\nRecently, deep learning-based methods have been proposed [8,34]. In both studies, models were trained solely on single L3 CT slices. However, Weston et al [8] visually showed that their model was able to generalize for other abdominal slices well without being trained on such data. Nonetheless, they mentioned that extending the training and evaluation data to the whole abdomen would be beneficial for stability but also analysis capabilities. Our study uses annotated data for training and evaluation across the whole abdomen and thus is a true volumetric approach to body composition analysis. In addition, they segmented SAT and VAT directly, whereas in our study, the semantic body region was segmented and adipose tissue was subclassified using known HU thresholds.\n\nOne major disadvantage of the collected dataset is the slice thickness of 5 mm. Several tissues, materials, and potentially air can be contained within a distance of 5 mm; the resulting HU at a specific location is an average of all components. This is also known as partial volume effect and can be counteracted by using a smaller slice thickness, ideally with isometric voxel sizes. However, a reconstructed slice thickness of 5 mm is Final visual report of the tissue quantification system output. SAT is shown in red, VAT is shown in green, and muscle tissue is shown yellow Fig. 4 Bland-Altman plot of SAT, VAT, and muscle volumetry with data points for every annotated slice in the test set common in clinical routine CT and it is questionable whether the increased precision of calculating the tissue composition on 1-mm slices would have clinical relevance. Nevertheless, we plan to investigate the influence of thinner slices in further studies, as the reading on thin slices is becoming routine in more and more institutions. Another limitation is the differentiation between visceral fat and fat contained within organs. Currently, every voxel with HU in the fat intensity value range, which is contained within the abdominal cavity region, is counted as VAT. However, per definition, fat cells within organs do not count as VAT and thus should be excluded from the final statistics. Public datasets like [35,36] already exist for multi-organ semantic segmentation and could be utilized to postprocess the segmentation results from this study by masking organs in the abdominal cavity.\n\nIt is quite common to find metal foreign objects like implants in abdominal CTs and thus to encounter beam hardening artifacts. Those artifacts, depending on how strong they are, may affect the segmentation quality, as shown in Fig. 6. Even if the segmentation model is able to predict the precise boundary of the individual semantic regions, streaking and cupping artifacts make it impossible to threshold fatty or muscular tissue based on HU intensities potentially invalidating quantification reports. In a future version of our tool, we are therefore planning functionality for automatic detection and handling of image artifacts.\n\nIn future works, we plan to extend the body composition analysis system to incorporate other regions of the body as well. For example, [24] already showed an analysis of adipose tissue and muscle for thighs. Ideally, the system should be capable of analyzing the whole body in order to derive stable biomarkers. Furthermore, an external validation is required in order to prove the stability and generalizability of the developed system. This includes data from different scanners as well as a large variety of body composition cases.\n\n\nConclusion\n\nIn the present study, we presented a deep learning-based, fully automated volumetric tissue classification system for the extraction of robust biomarkers from clinical CT examinations of the abdomen. In the future, we plan to extend the system to thoracic examinations and to add important tissue classes such as pericardial adipose tissue and myocardium.\n\nFunding Open Access funding provided by Projekt DEAL.\n\n\nCompliance with ethical standards\n\nGuarantor The scientific guarantor of this publication is Felix Nensa.\n\n\nConflict of interest\n\nThe authors of this manuscript declare no relationships with any companies whose products or services may be related to the subject matter of the article.\n\nStatistics and biometry One of the authors has significant statistical expertise.\n\nInformed consent Written informed consent was waived by the Institutional Review Board.\n\nEthical approval Institutional Review Board approval was obtained. Fig. 6 Beam hardening artifacts may not only harm segmentation quality (top) but also prevent accurate identification of tissues (bottom). Strong beam hardening artifacts with faults in the segmentation output (left). Beam hardening artifacts with mostly accurate segmentation, but streaking artifacts prevent accurate muscle and SAT identification (middle). No beam hardening artifacts at all, but metal foreign object detected (right) Methodology \u2022 retrospective \u2022 diagnostic or prognostic study \u2022 performed at one institution Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\n\nMetal Clip\n\nFig. 5\n5Fig. 5 Final visual report of the tissue quantification system output. SAT is shown in red, VAT is shown in green, and muscle tissue is shown yellow\n\n\nIn this work, multiple HU windows [\u2212 1024, 3071], [\u2212 150, 250], and [\u2212 95, 155] were applied to the 16-bit integer\n\nTable 1\n1Patient characteristics and acquisition parameters of the collected cohortTraining \nTest \n\n\n\nConv 1x1x1 =\n1x1x1Residual Path BlockFig. 2Schematic overview of the multi-resolution U-Net 3D architecture (red box: multi-resolution block; orange box: residual path block; green box: upsampling block; blue arrow: max-pooling layer; black arrow: identity data flow)ReLU \nInstanceNorm \n\nConv 1x1x1 \n\nConv 3x3x3 \nInstanceNorm \n\nReLU \n\nAdd \n\n= \n\nUpsample Block \n\nTrilinear Upsample \nReLU \nInstanceNorm \n\nConv 3x3x3 \n\nSoftmax \n\nMaxPool3D \n\n32 x 256 x 256 x 3 \n\nargmax \n\nIdentity \n\n= \n\nMulti Resolution Block \n\nConv 3x3x3 \nConv 3x3x3 \nConv 3x3x3 \nInstanceNorm \n\nReLU \n\nAdd \n\nConv 1x1x1 \n\n\n\nTable 2 (\n2additional \ncomplementary evaluation metrics are available for the inter-\nested reader in Table A.1, A.2, and A.3). First of all, all net-\nworks delivered promising results with average dice scores \nover 0.93. Second, multi-resolution U-Net variants achieved \nconstantly higher scores compared with their respective U-Net \n\n\n\nTable 2\n2Evaluation for the fivefold cross-validation runs (stated as mean overall runs) and ensemble predictions on the test set. AC, abdominal cavity; B, bones; M, muscle; ST, subcutaneous tissue; TC, thoracic cavityDice score \n\n\n\nTable 3 Evaluation\n3of multi-\nresolution U-Nets with n f = 32 \ntrained on different mappings \nfrom Hounsfield units to the \ntarget intensity value range of [\u2212 \n1, 1]. Multi-window stands for a \ncombination of theoretical value \nrange of 12-bit CT scans, \nabdomen window, and liver \nwindow. AC, abdominal cavity; \nB, bones; M, muscle; ST, \nsubcutaneous tissue; TC, thoracic \ncavity \n\nDice score \n\nHU window \nAC \nB \nM \nST \nTC \nAverage \n\nFivefold CV \nMulti-window \n0.9680 \n0.9554 \n0.9399 \n0.9596 \n0.9414 \n0.9529 \n[\u2212 1024, 3071] \n0.9561 \n0.9403 \n0.9217 \n0.9494 \n0.9254 \n0.9386 \n[\u2212 1024, 2047] \n0.9533 \n0.9410 \n0.9144 \n0.9412 \n0.9303 \n0.9360 \n[\u2212 1024, 1023] \n0.8731 \n0. 8778 \n0.7875 \n0.6959 \n0.8696 \n0.8208 \n[\u2212 150, 250] \n0.8598 \n0.8687 \n0.7632 \n0.7772 \n0.8759 \n0.8289 \nTest set \nMulti-window \n0.9736 \n0.9409 \n0.9328 \n0.9627 \n0.9629 \n0.9546 \n[\u2212 1024, 3071] \n0.9682 \n0.9392 \n0.9261 \n0.9606 \n0.9532 \n0.9495 \n[\u2212 1024, 2047] \n0.9644 \n0.9331 \n0.9174 \n0.9560 \n0.9569 \n0.9455 \n[\u22121024, 1023] \n0.9329 \n0.9002 \n0.8412 \n0.8879 \n0.9066 \n0.8938 \n[\u2212 150, 250] \n0.8950 \n0.8997 \n0.8004 \n0.8482 \n0.9311 \n0.8749 \n\n\nDifferential effect of subcutaneous abdominal and visceral adipose tissue on cardiometabolic risk. S Sam, 10.1515/hmbci-2018-0014Horm Mol Biol Clin Invest. 33Sam S (2018) Differential effect of subcutaneous abdominal and visceral adipose tissue on cardiometabolic risk. Horm Mol Biol Clin Invest 33. https://doi.org/10.1515/hmbci-2018-0014\n\nPrevalence of sarcopenia and associated & outcomes in the clinical setting. S J Peterson, C A Braunschweig, Nutr Clin Pract. 31Peterson SJ, Braunschweig CA (2016) Prevalence of sarcopenia and associated & outcomes in the clinical setting. Nutr Clin Pract 31:40-48\n\nThe role of adipose tissue immune cells in obesity and low-grade inflammation. M Mraz, M Haluzik, J Endocrinol. 222Mraz M, Haluzik M (2014) The role of adipose tissue immune cells in obesity and low-grade inflammation. J Endocrinol 222:R113- R127\n\nCorrelation between birth weight and maternal body composition. E Kent, V O&apos;dwyer, C Fattah, N Farah, C O&apos;connor, M J Turner, Obstet Gynecol. 121Kent E, O'Dwyer V, Fattah C, Farah N, O'Connor C, Turner MJ (2013) Correlation between birth weight and maternal body com- position. Obstet Gynecol 121:46-50\n\nExcessive adipose tissue infiltration in skeletal muscle in individuals with obesity, diabetes mellitus, and peripheral neuropathy: association with performance and function. T N Hilton, L J Tuttle, K L Bohnert, M J Mueller, D R Sinacore, Phys Ther. 88Hilton TN, Tuttle LJ, Bohnert KL, Mueller MJ, Sinacore DR (2008) Excessive adipose tissue infiltration in skeletal muscle in individuals with obesity, diabetes mellitus, and peripheral neurop- athy: association with performance and function. Phys Ther 88: 1336-1344\n\nInterrelations between fat distribution, muscle lipid content, adipocytokines, and insulin resistance: effect of moderate weight loss in older women. G Mazzali, Di Francesco, V Zoico, E , Am J Clin Nutr. 84Mazzali G, Di Francesco V, Zoico E et al (2006) Interrelations between fat distribution, muscle lipid content, adipocytokines, and insulin resistance: effect of moderate weight loss in older women. Am J Clin Nutr 84:1193-1199\n\nSingle-slice CT measurements allow for accurate assessment of sarcopenia and body composition. D Zopfs, S Theurich, Gro\u00dfe Hokamp, N , Eur Radiol. 30Zopfs D, Theurich S, Gro\u00dfe Hokamp N et al (2020) Single-slice CT measurements allow for accurate assessment of sarcopenia and body composition. Eur Radiol 30:1701-1708\n\nAutomated abdominal segmentation of CT scans for body composition analysis using deep learning. A D Weston, P Korfiatis, T L Kline, Radiology. 290Weston AD, Korfiatis P, Kline TL et al (2019) Automated abdom- inal segmentation of CT scans for body composition analysis using deep learning. Radiology 290:669-679\n\nImaging methods for analyzing body composition in human obesity and cardiometabolic disease. L A Seabolt, E B Welch, H J Silver, Ann N Y Acad Sci. 1353Seabolt LA, Welch EB, Silver HJ (2015) Imaging methods for analyzing body composition in human obesity and cardiometabolic disease. Ann N Y Acad Sci 1353:41-59\n\nUser-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability. P A Yushkevich, J Piven, H C Hazlett, Neuroimage. 31Yushkevich PA, Piven J, Hazlett HC et al (2006) User-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability. Neuroimage 31:1116-1128\n\n(eds) Medical image computing and computerassisted intervention -MICCAI 2016. \u00d6 \u00c7i\u00e7ek, A Abdulkadir, S S Lienkamp, T Brox, O Ronneberger, 10.1007/978-3-319-46723-8_49Ourselin S, Joskowicz L, Sabuncu MR, Unal G, Wells WSpringer International PublishingCham3D U-net: learning dense volumetric segmentation from sparse annotation\u00c7i\u00e7ek \u00d6, Abdulkadir A, Lienkamp SS, Brox T, Ronneberger O (2016) 3D U-net: learning dense volumetric segmentation from sparse annotation. In: Ourselin S, Joskowicz L, Sabuncu MR, Unal G, Wells W (eds) Medical image computing and computer- assisted intervention -MICCAI 2016. Springer International Publishing, Cham, pp 424-432. https://doi.org/10.1007/978-3- 319-46723-8_49\n\nMultiResUNet: rethinking the U-Net architecture for multimodal biomedical image segmentation. N Ibtehaz, M S Rahman, Neural Netw. 121Ibtehaz N, Rahman MS (2020) MultiResUNet: rethinking the U- Net architecture for multimodal biomedical image segmentation. Neural Netw 121:74-87\n\nImproved texture networks: maximizing quality and diversity in feed-forward stylization and texture synthesis. D Ulyanov, A Vedaldi, V Lempitsky, 10.1109/CVPR.2017.437The IEEE Conference on Computer Vision and Pattern Recognition. Ulyanov D, Vedaldi A, Lempitsky V (2017) Improved texture net- works: maximizing quality and diversity in feed-forward stylization and texture synthesis. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) https://doi.org/10.1109/ CVPR.2017.437\n\nBatch normalization: accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, Proceedings of the 32nd international conference on machine learning. Bach F, Blei Dthe 32nd international conference on machine learningLillePMLRIoffe S, Szegedy C (2015) Batch normalization: accelerating deep network training by reducing internal covariate shift. In: Bach F, Blei D (eds) Proceedings of the 32nd international conference on machine learning. PMLR, Lille, pp 448-456\n\nDeconvolution and checkerboard artifacts. A Odena, V Dumoulin, C Olah, 10.23915/distill.00003Odena A, Dumoulin V, Olah C (2016) Deconvolution and check- erboard artifacts. Distill. https://doi.org/10.23915/distill.00003\n\nTensorFlow: a system for large-scale machine learning. M Abadi, P Barham, J Chen, 12th USENIX symposium on operating systems design and implementation (OSDI 16). USENIX Association. Savannah, GAAbadi M, Barham P, Chen J, et al (2016) TensorFlow: a system for large-scale machine learning. 12th USENIX symposium on operat- ing systems design and implementation (OSDI 16). USENIX Association, Savannah, GA, pp 265-283\n\nAdam: a method for stochastic optimization. D P Kingma, J Ba, 3rd international conference on learning representations (ICLR). San Diego, CA, USAKingma DP, Ba J (2015) Adam: a method for stochastic optimiza- tion. In: 3rd international conference on learning representations (ICLR). San Diego, CA, USA\n\nDecoupled weight decay regularization. I Loshchilov, F Hutter, seventh international conference on learning representations (ICLR). Ernest N. Morial Convention Center. New Orleans, USALoshchilov I, Hutter F (2019) Decoupled weight decay regulariza- tion. In: seventh international conference on learning representa- tions (ICLR). Ernest N. Morial Convention Center, New Orleans, USA\n\nnnU-Net: self-adapting framework for U-net-based medical image segmentation. F Isensee, J Petersen, A Klein, Bildverarbeitung f\u00fcr die Medizin. Handels H, Deserno TM, Maier A, Maier-Hein KH, Palm C, Tolxdorff TIsensee F, Petersen J, Klein A et al (2019) nnU-Net: self-adapting framework for U-net-based medical image segmentation. In: Handels H, Deserno TM, Maier A, Maier-Hein KH, Palm C, Tolxdorff T (eds) Bildverarbeitung f\u00fcr die Medizin 2019.\n\n. Springer Fachmedien Wiesbaden, 10.1007/978-3-658-25326-4_7WiesbadenSpringer Fachmedien Wiesbaden, Wiesbaden, pp 22-22. https:// doi.org/10.1007/978-3-658-25326-4_7\n\nGeneralised dice overlap as a deep learning loss function for highly unbalanced segmentations. C H Sudre, W Li, T Vercauteren, S Ourselin, Jorge Cardoso ; Cardoso, M J Arbel, T Carneiro, G Syeda-Mahmood, T , Jmrs T Moradi, M Bradley, A Greenspan, H Papa, J P Madabhushi, A Nascimento, J C Cardoso, J S Belagiannis, V Lu, Z , 10.1007/978-3-319-67558-9_28Springer International PublishingChamDeep learning in medical image analysis and multimodal learning for clinical decision supportSudre CH, Li W, Vercauteren T, Ourselin S, Jorge Cardoso M (2017) Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations. In: Cardoso MJ, Arbel T, Carneiro G, Syeda-Mahmood T, JMRS T, Moradi M, Bradley A, Greenspan H, Papa JP, Madabhushi A, Nascimento JC, Cardoso JS, Belagiannis V, Lu Z (eds) Deep learning in medical image analysis and multimodal learning for clinical decision support. Springer International Publishing, Cham, pp 240-248. https://doi.org/10. 1007/978-3-319-67558-9_28\n\nDeep learning. I Goodfellow, Y Bengio, A Courville, MIT PressGoodfellow I, Bengio Y, Courville A (2016) Deep learning. MIT Press https://www.deeplearningbook.org\n\nMeasurement of skeletal muscle radiation attenuation and basis of its biological variation. J Aubrey, N Esfandiari, V E Baracos, Acta Physiol (Oxf). 210Aubrey J, Esfandiari N, Baracos VE et al (2014) Measurement of skeletal muscle radiation attenuation and basis of its biological var- iation. Acta Physiol (Oxf) 210:489-497\n\nBody fat assessment method using CT images with separation mask algorithm. Y J Kim, S H Lee, T Y Kim, J Y Park, S H Choi, K G Kim, J Digit Imaging. 26Kim YJ, Lee SH, Kim TY, Park JY, Choi SH, Kim KG (2013) Body fat assessment method using CT images with separation mask algorithm. J Digit Imaging 26:155-162\n\nAutomated analysis of liver fat, muscle and adipose tissue distribution from CT suitable for large-scale studies. J Kullberg, A Hedstr\u00f6m, J Brandberg, Sci Rep. 710425Kullberg J, Hedstr\u00f6m A, Brandberg J et al (2017) Automated anal- ysis of liver fat, muscle and adipose tissue distribution from CT suitable for large-scale studies. Sci Rep 7:10425\n\nDevelopment of automated quantification of visceral and subcutaneous adipose tissue volumes from abdominal CT scans. S D Mensink, J W Spliethoff, R Belder, J M Klaase, R Bezooijen, C H Slump, 10.1117/12.878017Medical imaging 2011: computer-aided diagnosis. SPIE. M.D RMS, Ginneken B vanMensink SD, Spliethoff JW, Belder R, Klaase JM, Bezooijen R, Slump CH (2011) Development of automated quantification of vis- ceral and subcutaneous adipose tissue volumes from abdominal CT scans. In: M.D RMS, Ginneken B van (eds) Medical imaging 2011: computer-aided diagnosis. SPIE, pp 799-810. https://doi.org/10. 1117/12.878017\n\nUnsupervised quantification of abdominal fat from CT images using Greedy Snakes. C Agarwal, A H Dallal, M R Arbabshirani, A Patel, G Moore, 10.1117/12.2254139Medical Imaging 2017: Image processing. SPIE. Styner MA, Angelini EDAgarwal C, Dallal AH, Arbabshirani MR, Patel A, Moore G (2017) Unsupervised quantification of abdominal fat from CT images using Greedy Snakes. In: Styner MA, Angelini ED (eds) Medical Imaging 2017: Image processing. SPIE, pp 785-792. https://doi. org/10.1117/12.2254139\n\nDevelopment of an automated 3D segmentation program for volume quantification of body fat distribution using CT. S Ohshima, S Yamamoto, T Yamaji, Nihon Hoshasen Gijutsu Gakkai Zasshi. 64Ohshima S, Yamamoto S, Yamaji T et al (2008) Development of an automated 3D segmentation program for volume quantification of body fat distribution using CT. Nihon Hoshasen Gijutsu Gakkai Zasshi 64:1177-1181\n\nDevelopment and validation of a rapid and robust method to determine visceral adipose tissue volume using computed tomography images. A M Parikh, A M Coletta, Z H Yu, PLoS One. 12Parikh AM, Coletta AM, Yu ZH et al (2017) Development and validation of a rapid and robust method to determine visceral adipose tissue volume using computed tomography images. PLoS One 12:1-11\n\nAutomatic segmentation of abdominal fat from CT data. A Pednekar, A N Bandekar, I A Kakadiaris, M Naghavi, 10.1109/ACVMOT.2005.312005 seventh IEEE workshops on applications of computer vision (WACV/MOTION'05). Pednekar A, Bandekar AN, Kakadiaris IA, Naghavi M (2005) Automatic segmentation of abdominal fat from CT data. In: 2005 seventh IEEE workshops on applications of computer vision (WACV/MOTION'05), pp 308-315. https://doi.org/10.1109/ ACVMOT.2005.31\n\nBody composition assessment in axial CT images using FEMbased automatic segmentation of skeletal muscle. K Popuri, D Cobzas, N Esfandiari, V Baracos, M J\u00e4gersand, IEEE Trans Med Imaging. 35Popuri K, Cobzas D, Esfandiari N, Baracos V, J\u00e4gersand M (2016) Body composition assessment in axial CT images using FEM- based automatic segmentation of skeletal muscle. IEEE Trans Med Imaging 35:512-520\n\nAutomatic intra-subject registration-based segmentation of abdominal fat from water-fat MRI. A A Joshi, H H Hu, R M Leahy, M I Goran, K S Nayak, J Magn Reson Imaging. 37Joshi AA, Hu HH, Leahy RM, Goran MI, Nayak KS (2013) Automatic intra-subject registration-based segmentation of abdom- inal fat from water-fat MRI. J Magn Reson Imaging 37:423-430\n\nAn accurate and robust method for unsupervised assessment of abdominal fat by MRI. V Positano, A Gastaldelli, A M Sironi, M F Santarelli, M Lombardi, L Landini, J Magn Reson Imaging. 20Positano V, Gastaldelli A, Sironi AM, Santarelli MF, Lombardi M, Landini L (2004) An accurate and robust method for unsupervised assessment of abdominal fat by MRI. J Magn Reson Imaging 20: 684-689\n\nNovel segmentation method for abdominal fat quantification by MRI. A Zhou, H Murillo, Q Peng, J Magn Reson Imaging. 34Zhou A, Murillo H, Peng Q (2011) Novel segmentation method for abdominal fat quantification by MRI. J Magn Reson Imaging 34: 852-860\n\nContext-aware operating theaters, computer assisted robotic endoscopy, clinical imagebased procedures, and skin image analysis. C P Bridge, M Rosenthal, B Wright, 10.1007/978-3-030-01201-4_222.0Springer International PublishingChamFully-automated analysis of body composition from CT in cancer patients using convolutional neural networksBridge CP, Rosenthal M, Wright B et al (2018) Fully-automated analysis of body composition from CT in cancer patients using convolutional neural networks. In: Stoyanov D, Taylor Z, Sarikaya D, McLeod J, Gonz\u00e1lez Ballester MA, NCF C, Martel A, Maier-Hein L, Malpani A, Zenati MA, De Ribaupierre S, Xiongbiao L, Collins T, Reichl T, Drechsler K, Erdt M, Linguraru MG, Oyarzun Laura C, Shekhar R, Wesarg S, Celebi ME, Dana K, Halpern A (eds) OR 2.0 Context-aware operating theaters, computer assisted robotic endoscopy, clinical image- based procedures, and skin image analysis. Springer International Publishing, Cham, pp 204-213. https://doi.org/10.1007/978-3-030- 01201-4_22\n\nAutomatic multi-organ segmentation on abdominal CT with dense V-networks. E Gibson, F Giganti, Y Hu, IEEE Trans Med Imaging. 37Gibson E, Giganti F, Hu Y et al (2018) Automatic multi-organ segmentation on abdominal CT with dense V-networks. IEEE Trans Med Imaging 37:1822-1834\n\nMulti-organ abdominal CT reference standard segmentations. E Gibson, F Giganti, Y Hu, 10.5281/zenodo.1169361Gibson E, Giganti F, Hu Y et al (2018) Multi-organ abdominal CT reference standard segmentations. Zenodo. https://doi.org/10.5281/ zenodo.1169361\n\nPublisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Publisher's note Springer Nature remains neutral with regard to jurisdic- tional claims in published maps and institutional affiliations.\n", "annotations": {"author": "[{\"end\":190,\"start\":128},{\"end\":255,\"start\":191},{\"end\":324,\"start\":256},{\"end\":389,\"start\":325},{\"end\":452,\"start\":390}]", "publisher": null, "author_last_name": "[{\"end\":139,\"start\":133},{\"end\":204,\"start\":199},{\"end\":273,\"start\":262},{\"end\":338,\"start\":330},{\"end\":401,\"start\":396}]", "author_first_name": "[{\"end\":132,\"start\":128},{\"end\":198,\"start\":191},{\"end\":261,\"start\":256},{\"end\":329,\"start\":325},{\"end\":395,\"start\":390}]", "author_affiliation": "[{\"end\":189,\"start\":141},{\"end\":254,\"start\":206},{\"end\":323,\"start\":275},{\"end\":388,\"start\":340},{\"end\":451,\"start\":403}]", "title": "[{\"end\":125,\"start\":1},{\"end\":577,\"start\":453}]", "venue": null, "abstract": "[{\"end\":2397,\"start\":726}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2570,\"start\":2567},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2573,\"start\":2570},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2576,\"start\":2573},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2579,\"start\":2576},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2582,\"start\":2579},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2585,\"start\":2582},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3431,\"start\":3428},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3650,\"start\":3647},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4189,\"start\":4186},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5359,\"start\":5355},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6044,\"start\":6040},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6101,\"start\":6097},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6446,\"start\":6442},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6511,\"start\":6507},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6717,\"start\":6713},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7246,\"start\":7242},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7423,\"start\":7419},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7471,\"start\":7467},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8248,\"start\":8244},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8251,\"start\":8248},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8254,\"start\":8251},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10145,\"start\":10141},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10173,\"start\":10169},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11365,\"start\":11361},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11605,\"start\":11601},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11988,\"start\":11984},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12069,\"start\":12065},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15639,\"start\":15636},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16466,\"start\":16462},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16470,\"start\":16466},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16474,\"start\":16470},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16667,\"start\":16663},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16671,\"start\":16667},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16675,\"start\":16671},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16679,\"start\":16675},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16683,\"start\":16679},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":16962,\"start\":16958},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16966,\"start\":16962},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16970,\"start\":16966},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17349,\"start\":17346},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17352,\"start\":17349},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17447,\"start\":17444},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19483,\"start\":19479},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19486,\"start\":19483},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20436,\"start\":20432}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":23334,\"start\":23177},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":23451,\"start\":23335},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":23553,\"start\":23452},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":24140,\"start\":23554},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":24477,\"start\":24141},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":24710,\"start\":24478},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":25802,\"start\":24711}]", "paragraph": "[{\"end\":4280,\"start\":2413},{\"end\":4553,\"start\":4282},{\"end\":5703,\"start\":4579},{\"end\":7124,\"start\":5729},{\"end\":7412,\"start\":7145},{\"end\":8077,\"start\":7414},{\"end\":9163,\"start\":8079},{\"end\":10286,\"start\":9165},{\"end\":10617,\"start\":10407},{\"end\":10986,\"start\":10619},{\"end\":12410,\"start\":11045},{\"end\":12969,\"start\":12441},{\"end\":13393,\"start\":12988},{\"end\":13797,\"start\":13395},{\"end\":14518,\"start\":13830},{\"end\":15489,\"start\":14533},{\"end\":16287,\"start\":15491},{\"end\":17287,\"start\":16289},{\"end\":18061,\"start\":17289},{\"end\":19659,\"start\":18063},{\"end\":20295,\"start\":19661},{\"end\":20831,\"start\":20297},{\"end\":21201,\"start\":20846},{\"end\":21256,\"start\":21203},{\"end\":21364,\"start\":21294},{\"end\":21543,\"start\":21389},{\"end\":21626,\"start\":21545},{\"end\":21715,\"start\":21628},{\"end\":23163,\"start\":21717}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10406,\"start\":10287},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11020,\"start\":10987}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":4712,\"start\":4705},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":13392,\"start\":13385}]", "section_header": "[{\"end\":2411,\"start\":2399},{\"end\":4577,\"start\":4556},{\"end\":5727,\"start\":5706},{\"end\":7143,\"start\":7127},{\"end\":11043,\"start\":11022},{\"end\":12420,\"start\":12413},{\"end\":12439,\"start\":12423},{\"end\":12986,\"start\":12972},{\"end\":13828,\"start\":13800},{\"end\":14531,\"start\":14521},{\"end\":20844,\"start\":20834},{\"end\":21292,\"start\":21259},{\"end\":21387,\"start\":21367},{\"end\":23176,\"start\":23166},{\"end\":23184,\"start\":23178},{\"end\":23460,\"start\":23453},{\"end\":23567,\"start\":23555},{\"end\":24151,\"start\":24142},{\"end\":24486,\"start\":24479},{\"end\":24730,\"start\":24712}]", "table": "[{\"end\":23553,\"start\":23536},{\"end\":24140,\"start\":23822},{\"end\":24477,\"start\":24153},{\"end\":24710,\"start\":24697},{\"end\":25802,\"start\":24732}]", "figure_caption": "[{\"end\":23334,\"start\":23186},{\"end\":23451,\"start\":23337},{\"end\":23536,\"start\":23462},{\"end\":23822,\"start\":23573},{\"end\":24697,\"start\":24488}]", "figure_ref": "[{\"end\":5614,\"start\":5608},{\"end\":6132,\"start\":6126},{\"end\":12730,\"start\":12724},{\"end\":13952,\"start\":13946},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14203,\"start\":14197},{\"end\":15882,\"start\":15876},{\"end\":18648,\"start\":18642},{\"end\":19895,\"start\":19889},{\"end\":21790,\"start\":21784}]", "bib_author_first_name": "[{\"end\":25904,\"start\":25903},{\"end\":26222,\"start\":26221},{\"end\":26224,\"start\":26223},{\"end\":26236,\"start\":26235},{\"end\":26238,\"start\":26237},{\"end\":26490,\"start\":26489},{\"end\":26498,\"start\":26497},{\"end\":26723,\"start\":26722},{\"end\":26731,\"start\":26730},{\"end\":26747,\"start\":26746},{\"end\":26757,\"start\":26756},{\"end\":26766,\"start\":26765},{\"end\":26783,\"start\":26782},{\"end\":26785,\"start\":26784},{\"end\":27148,\"start\":27147},{\"end\":27150,\"start\":27149},{\"end\":27160,\"start\":27159},{\"end\":27162,\"start\":27161},{\"end\":27172,\"start\":27171},{\"end\":27174,\"start\":27173},{\"end\":27185,\"start\":27184},{\"end\":27187,\"start\":27186},{\"end\":27198,\"start\":27197},{\"end\":27200,\"start\":27199},{\"end\":27642,\"start\":27641},{\"end\":27654,\"start\":27652},{\"end\":27667,\"start\":27666},{\"end\":27676,\"start\":27675},{\"end\":28020,\"start\":28019},{\"end\":28029,\"start\":28028},{\"end\":28045,\"start\":28040},{\"end\":28055,\"start\":28054},{\"end\":28338,\"start\":28337},{\"end\":28340,\"start\":28339},{\"end\":28350,\"start\":28349},{\"end\":28363,\"start\":28362},{\"end\":28365,\"start\":28364},{\"end\":28648,\"start\":28647},{\"end\":28650,\"start\":28649},{\"end\":28661,\"start\":28660},{\"end\":28663,\"start\":28662},{\"end\":28672,\"start\":28671},{\"end\":28674,\"start\":28673},{\"end\":28987,\"start\":28986},{\"end\":28989,\"start\":28988},{\"end\":29003,\"start\":29002},{\"end\":29012,\"start\":29011},{\"end\":29014,\"start\":29013},{\"end\":29310,\"start\":29309},{\"end\":29319,\"start\":29318},{\"end\":29333,\"start\":29332},{\"end\":29335,\"start\":29334},{\"end\":29347,\"start\":29346},{\"end\":29355,\"start\":29354},{\"end\":30027,\"start\":30026},{\"end\":30038,\"start\":30037},{\"end\":30040,\"start\":30039},{\"end\":30323,\"start\":30322},{\"end\":30334,\"start\":30333},{\"end\":30345,\"start\":30344},{\"end\":30805,\"start\":30804},{\"end\":30814,\"start\":30813},{\"end\":31253,\"start\":31252},{\"end\":31262,\"start\":31261},{\"end\":31274,\"start\":31273},{\"end\":31487,\"start\":31486},{\"end\":31496,\"start\":31495},{\"end\":31506,\"start\":31505},{\"end\":31893,\"start\":31892},{\"end\":31895,\"start\":31894},{\"end\":31905,\"start\":31904},{\"end\":32191,\"start\":32190},{\"end\":32205,\"start\":32204},{\"end\":32613,\"start\":32612},{\"end\":32624,\"start\":32623},{\"end\":32636,\"start\":32635},{\"end\":33245,\"start\":33244},{\"end\":33247,\"start\":33246},{\"end\":33256,\"start\":33255},{\"end\":33262,\"start\":33261},{\"end\":33277,\"start\":33276},{\"end\":33293,\"start\":33288},{\"end\":33314,\"start\":33313},{\"end\":33316,\"start\":33315},{\"end\":33325,\"start\":33324},{\"end\":33337,\"start\":33336},{\"end\":33354,\"start\":33353},{\"end\":33363,\"start\":33357},{\"end\":33373,\"start\":33372},{\"end\":33384,\"start\":33383},{\"end\":33397,\"start\":33396},{\"end\":33405,\"start\":33404},{\"end\":33407,\"start\":33406},{\"end\":33421,\"start\":33420},{\"end\":33435,\"start\":33434},{\"end\":33437,\"start\":33436},{\"end\":33448,\"start\":33447},{\"end\":33450,\"start\":33449},{\"end\":33465,\"start\":33464},{\"end\":33471,\"start\":33470},{\"end\":34176,\"start\":34175},{\"end\":34190,\"start\":34189},{\"end\":34200,\"start\":34199},{\"end\":34416,\"start\":34415},{\"end\":34426,\"start\":34425},{\"end\":34440,\"start\":34439},{\"end\":34442,\"start\":34441},{\"end\":34725,\"start\":34724},{\"end\":34727,\"start\":34726},{\"end\":34734,\"start\":34733},{\"end\":34736,\"start\":34735},{\"end\":34743,\"start\":34742},{\"end\":34745,\"start\":34744},{\"end\":34752,\"start\":34751},{\"end\":34754,\"start\":34753},{\"end\":34762,\"start\":34761},{\"end\":34764,\"start\":34763},{\"end\":34772,\"start\":34771},{\"end\":34774,\"start\":34773},{\"end\":35073,\"start\":35072},{\"end\":35085,\"start\":35084},{\"end\":35097,\"start\":35096},{\"end\":35424,\"start\":35423},{\"end\":35426,\"start\":35425},{\"end\":35437,\"start\":35436},{\"end\":35439,\"start\":35438},{\"end\":35453,\"start\":35452},{\"end\":35463,\"start\":35462},{\"end\":35465,\"start\":35464},{\"end\":35475,\"start\":35474},{\"end\":35488,\"start\":35487},{\"end\":35490,\"start\":35489},{\"end\":36006,\"start\":36005},{\"end\":36017,\"start\":36016},{\"end\":36019,\"start\":36018},{\"end\":36029,\"start\":36028},{\"end\":36031,\"start\":36030},{\"end\":36047,\"start\":36046},{\"end\":36056,\"start\":36055},{\"end\":36536,\"start\":36535},{\"end\":36547,\"start\":36546},{\"end\":36559,\"start\":36558},{\"end\":36952,\"start\":36951},{\"end\":36954,\"start\":36953},{\"end\":36964,\"start\":36963},{\"end\":36966,\"start\":36965},{\"end\":36977,\"start\":36976},{\"end\":36979,\"start\":36978},{\"end\":37245,\"start\":37244},{\"end\":37257,\"start\":37256},{\"end\":37259,\"start\":37258},{\"end\":37271,\"start\":37270},{\"end\":37273,\"start\":37272},{\"end\":37287,\"start\":37286},{\"end\":37755,\"start\":37754},{\"end\":37765,\"start\":37764},{\"end\":37775,\"start\":37774},{\"end\":37789,\"start\":37788},{\"end\":37800,\"start\":37799},{\"end\":38138,\"start\":38137},{\"end\":38140,\"start\":38139},{\"end\":38149,\"start\":38148},{\"end\":38151,\"start\":38150},{\"end\":38157,\"start\":38156},{\"end\":38159,\"start\":38158},{\"end\":38168,\"start\":38167},{\"end\":38170,\"start\":38169},{\"end\":38179,\"start\":38178},{\"end\":38181,\"start\":38180},{\"end\":38478,\"start\":38477},{\"end\":38490,\"start\":38489},{\"end\":38505,\"start\":38504},{\"end\":38507,\"start\":38506},{\"end\":38517,\"start\":38516},{\"end\":38519,\"start\":38518},{\"end\":38533,\"start\":38532},{\"end\":38545,\"start\":38544},{\"end\":38846,\"start\":38845},{\"end\":38854,\"start\":38853},{\"end\":38865,\"start\":38864},{\"end\":39159,\"start\":39158},{\"end\":39161,\"start\":39160},{\"end\":39171,\"start\":39170},{\"end\":39184,\"start\":39183},{\"end\":40119,\"start\":40118},{\"end\":40129,\"start\":40128},{\"end\":40140,\"start\":40139},{\"end\":40381,\"start\":40380},{\"end\":40391,\"start\":40390},{\"end\":40402,\"start\":40401}]", "bib_author_last_name": "[{\"end\":25908,\"start\":25905},{\"end\":26233,\"start\":26225},{\"end\":26251,\"start\":26239},{\"end\":26495,\"start\":26491},{\"end\":26506,\"start\":26499},{\"end\":26728,\"start\":26724},{\"end\":26744,\"start\":26732},{\"end\":26754,\"start\":26748},{\"end\":26763,\"start\":26758},{\"end\":26780,\"start\":26767},{\"end\":26792,\"start\":26786},{\"end\":27157,\"start\":27151},{\"end\":27169,\"start\":27163},{\"end\":27182,\"start\":27175},{\"end\":27195,\"start\":27188},{\"end\":27209,\"start\":27201},{\"end\":27650,\"start\":27643},{\"end\":27664,\"start\":27655},{\"end\":27673,\"start\":27668},{\"end\":28026,\"start\":28021},{\"end\":28038,\"start\":28030},{\"end\":28052,\"start\":28046},{\"end\":28347,\"start\":28341},{\"end\":28360,\"start\":28351},{\"end\":28371,\"start\":28366},{\"end\":28658,\"start\":28651},{\"end\":28669,\"start\":28664},{\"end\":28681,\"start\":28675},{\"end\":29000,\"start\":28990},{\"end\":29009,\"start\":29004},{\"end\":29022,\"start\":29015},{\"end\":29316,\"start\":29311},{\"end\":29330,\"start\":29320},{\"end\":29344,\"start\":29336},{\"end\":29352,\"start\":29348},{\"end\":29367,\"start\":29356},{\"end\":30035,\"start\":30028},{\"end\":30047,\"start\":30041},{\"end\":30331,\"start\":30324},{\"end\":30342,\"start\":30335},{\"end\":30355,\"start\":30346},{\"end\":30811,\"start\":30806},{\"end\":30822,\"start\":30815},{\"end\":31259,\"start\":31254},{\"end\":31271,\"start\":31263},{\"end\":31279,\"start\":31275},{\"end\":31493,\"start\":31488},{\"end\":31503,\"start\":31497},{\"end\":31511,\"start\":31507},{\"end\":31902,\"start\":31896},{\"end\":31908,\"start\":31906},{\"end\":32202,\"start\":32192},{\"end\":32212,\"start\":32206},{\"end\":32621,\"start\":32614},{\"end\":32633,\"start\":32625},{\"end\":32642,\"start\":32637},{\"end\":33013,\"start\":32984},{\"end\":33253,\"start\":33248},{\"end\":33259,\"start\":33257},{\"end\":33274,\"start\":33263},{\"end\":33286,\"start\":33278},{\"end\":33311,\"start\":33294},{\"end\":33322,\"start\":33317},{\"end\":33334,\"start\":33326},{\"end\":33351,\"start\":33338},{\"end\":33370,\"start\":33364},{\"end\":33381,\"start\":33374},{\"end\":33394,\"start\":33385},{\"end\":33402,\"start\":33398},{\"end\":33418,\"start\":33408},{\"end\":33432,\"start\":33422},{\"end\":33445,\"start\":33438},{\"end\":33462,\"start\":33451},{\"end\":33468,\"start\":33466},{\"end\":34187,\"start\":34177},{\"end\":34197,\"start\":34191},{\"end\":34210,\"start\":34201},{\"end\":34423,\"start\":34417},{\"end\":34437,\"start\":34427},{\"end\":34450,\"start\":34443},{\"end\":34731,\"start\":34728},{\"end\":34740,\"start\":34737},{\"end\":34749,\"start\":34746},{\"end\":34759,\"start\":34755},{\"end\":34769,\"start\":34765},{\"end\":34778,\"start\":34775},{\"end\":35082,\"start\":35074},{\"end\":35094,\"start\":35086},{\"end\":35107,\"start\":35098},{\"end\":35434,\"start\":35427},{\"end\":35450,\"start\":35440},{\"end\":35460,\"start\":35454},{\"end\":35472,\"start\":35466},{\"end\":35485,\"start\":35476},{\"end\":35496,\"start\":35491},{\"end\":36014,\"start\":36007},{\"end\":36026,\"start\":36020},{\"end\":36044,\"start\":36032},{\"end\":36053,\"start\":36048},{\"end\":36062,\"start\":36057},{\"end\":36544,\"start\":36537},{\"end\":36556,\"start\":36548},{\"end\":36566,\"start\":36560},{\"end\":36961,\"start\":36955},{\"end\":36974,\"start\":36967},{\"end\":36982,\"start\":36980},{\"end\":37254,\"start\":37246},{\"end\":37268,\"start\":37260},{\"end\":37284,\"start\":37274},{\"end\":37295,\"start\":37288},{\"end\":37762,\"start\":37756},{\"end\":37772,\"start\":37766},{\"end\":37786,\"start\":37776},{\"end\":37797,\"start\":37790},{\"end\":37810,\"start\":37801},{\"end\":38146,\"start\":38141},{\"end\":38154,\"start\":38152},{\"end\":38165,\"start\":38160},{\"end\":38176,\"start\":38171},{\"end\":38187,\"start\":38182},{\"end\":38487,\"start\":38479},{\"end\":38502,\"start\":38491},{\"end\":38514,\"start\":38508},{\"end\":38530,\"start\":38520},{\"end\":38542,\"start\":38534},{\"end\":38553,\"start\":38546},{\"end\":38851,\"start\":38847},{\"end\":38862,\"start\":38855},{\"end\":38870,\"start\":38866},{\"end\":39168,\"start\":39162},{\"end\":39181,\"start\":39172},{\"end\":39191,\"start\":39185},{\"end\":40126,\"start\":40120},{\"end\":40137,\"start\":40130},{\"end\":40143,\"start\":40141},{\"end\":40388,\"start\":40382},{\"end\":40399,\"start\":40392},{\"end\":40405,\"start\":40403}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1515/hmbci-2018-0014\",\"id\":\"b0\",\"matched_paper_id\":3817908},\"end\":26143,\"start\":25804},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":41983431},\"end\":26408,\"start\":26145},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1457733},\"end\":26656,\"start\":26410},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":13231970},\"end\":26970,\"start\":26658},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":25322107},\"end\":27489,\"start\":26972},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4442846},\"end\":27922,\"start\":27491},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":208329571},\"end\":28239,\"start\":27924},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":54481309},\"end\":28552,\"start\":28241},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":45908755},\"end\":28864,\"start\":28554},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1660596},\"end\":29229,\"start\":28866},{\"attributes\":{\"doi\":\"10.1007/978-3-319-46723-8_49\",\"id\":\"b10\"},\"end\":29930,\"start\":29231},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":60440399},\"end\":30209,\"start\":29932},{\"attributes\":{\"doi\":\"10.1109/CVPR.2017.437\",\"id\":\"b12\",\"matched_paper_id\":5917270},\"end\":30708,\"start\":30211},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":5808102},\"end\":31208,\"start\":30710},{\"attributes\":{\"doi\":\"10.23915/distill.00003\",\"id\":\"b14\"},\"end\":31429,\"start\":31210},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6287870},\"end\":31846,\"start\":31431},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6628106},\"end\":32149,\"start\":31848},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":53592270},\"end\":32533,\"start\":32151},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":52875989},\"end\":32980,\"start\":32535},{\"attributes\":{\"doi\":\"10.1007/978-3-658-25326-4_7\",\"id\":\"b19\"},\"end\":33147,\"start\":32982},{\"attributes\":{\"doi\":\"10.1007/978-3-319-67558-9_28\",\"id\":\"b20\"},\"end\":34158,\"start\":33149},{\"attributes\":{\"id\":\"b21\"},\"end\":34321,\"start\":34160},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2568716},\"end\":34647,\"start\":34323},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":12230071},\"end\":34956,\"start\":34649},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":5251788},\"end\":35304,\"start\":34958},{\"attributes\":{\"doi\":\"10.1117/12.878017\",\"id\":\"b25\",\"matched_paper_id\":26072951},\"end\":35922,\"start\":35306},{\"attributes\":{\"doi\":\"10.1117/12.2254139\",\"id\":\"b26\",\"matched_paper_id\":41103888},\"end\":36420,\"start\":35924},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":23146952},\"end\":36815,\"start\":36422},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":10345109},\"end\":37188,\"start\":36817},{\"attributes\":{\"doi\":\"10.1109/ACVMOT.2005.31\",\"id\":\"b29\",\"matched_paper_id\":1150221},\"end\":37647,\"start\":37190},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14993688},\"end\":38042,\"start\":37649},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":12346943},\"end\":38392,\"start\":38044},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":45039430},\"end\":38776,\"start\":38394},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":29899817},\"end\":39028,\"start\":38778},{\"attributes\":{\"doi\":\"10.1007/978-3-030-01201-4_22\",\"id\":\"b34\"},\"end\":40042,\"start\":39030},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":3549557},\"end\":40319,\"start\":40044},{\"attributes\":{\"id\":\"b36\"},\"end\":40574,\"start\":40321},{\"attributes\":{\"id\":\"b37\"},\"end\":40849,\"start\":40576}]", "bib_title": "[{\"end\":25901,\"start\":25804},{\"end\":26219,\"start\":26145},{\"end\":26487,\"start\":26410},{\"end\":26720,\"start\":26658},{\"end\":27145,\"start\":26972},{\"end\":27639,\"start\":27491},{\"end\":28017,\"start\":27924},{\"end\":28335,\"start\":28241},{\"end\":28645,\"start\":28554},{\"end\":28984,\"start\":28866},{\"end\":30024,\"start\":29932},{\"end\":30320,\"start\":30211},{\"end\":30802,\"start\":30710},{\"end\":31484,\"start\":31431},{\"end\":31890,\"start\":31848},{\"end\":32188,\"start\":32151},{\"end\":32610,\"start\":32535},{\"end\":34413,\"start\":34323},{\"end\":34722,\"start\":34649},{\"end\":35070,\"start\":34958},{\"end\":35421,\"start\":35306},{\"end\":36003,\"start\":35924},{\"end\":36533,\"start\":36422},{\"end\":36949,\"start\":36817},{\"end\":37242,\"start\":37190},{\"end\":37752,\"start\":37649},{\"end\":38135,\"start\":38044},{\"end\":38475,\"start\":38394},{\"end\":38843,\"start\":38778},{\"end\":40116,\"start\":40044}]", "bib_author": "[{\"end\":25910,\"start\":25903},{\"end\":26235,\"start\":26221},{\"end\":26253,\"start\":26235},{\"end\":26497,\"start\":26489},{\"end\":26508,\"start\":26497},{\"end\":26730,\"start\":26722},{\"end\":26746,\"start\":26730},{\"end\":26756,\"start\":26746},{\"end\":26765,\"start\":26756},{\"end\":26782,\"start\":26765},{\"end\":26794,\"start\":26782},{\"end\":27159,\"start\":27147},{\"end\":27171,\"start\":27159},{\"end\":27184,\"start\":27171},{\"end\":27197,\"start\":27184},{\"end\":27211,\"start\":27197},{\"end\":27652,\"start\":27641},{\"end\":27666,\"start\":27652},{\"end\":27675,\"start\":27666},{\"end\":27679,\"start\":27675},{\"end\":28028,\"start\":28019},{\"end\":28040,\"start\":28028},{\"end\":28054,\"start\":28040},{\"end\":28058,\"start\":28054},{\"end\":28349,\"start\":28337},{\"end\":28362,\"start\":28349},{\"end\":28373,\"start\":28362},{\"end\":28660,\"start\":28647},{\"end\":28671,\"start\":28660},{\"end\":28683,\"start\":28671},{\"end\":29002,\"start\":28986},{\"end\":29011,\"start\":29002},{\"end\":29024,\"start\":29011},{\"end\":29318,\"start\":29309},{\"end\":29332,\"start\":29318},{\"end\":29346,\"start\":29332},{\"end\":29354,\"start\":29346},{\"end\":29369,\"start\":29354},{\"end\":30037,\"start\":30026},{\"end\":30049,\"start\":30037},{\"end\":30333,\"start\":30322},{\"end\":30344,\"start\":30333},{\"end\":30357,\"start\":30344},{\"end\":30813,\"start\":30804},{\"end\":30824,\"start\":30813},{\"end\":31261,\"start\":31252},{\"end\":31273,\"start\":31261},{\"end\":31281,\"start\":31273},{\"end\":31495,\"start\":31486},{\"end\":31505,\"start\":31495},{\"end\":31513,\"start\":31505},{\"end\":31904,\"start\":31892},{\"end\":31910,\"start\":31904},{\"end\":32204,\"start\":32190},{\"end\":32214,\"start\":32204},{\"end\":32623,\"start\":32612},{\"end\":32635,\"start\":32623},{\"end\":32644,\"start\":32635},{\"end\":33015,\"start\":32984},{\"end\":33255,\"start\":33244},{\"end\":33261,\"start\":33255},{\"end\":33276,\"start\":33261},{\"end\":33288,\"start\":33276},{\"end\":33313,\"start\":33288},{\"end\":33324,\"start\":33313},{\"end\":33336,\"start\":33324},{\"end\":33353,\"start\":33336},{\"end\":33357,\"start\":33353},{\"end\":33372,\"start\":33357},{\"end\":33383,\"start\":33372},{\"end\":33396,\"start\":33383},{\"end\":33404,\"start\":33396},{\"end\":33420,\"start\":33404},{\"end\":33434,\"start\":33420},{\"end\":33447,\"start\":33434},{\"end\":33464,\"start\":33447},{\"end\":33470,\"start\":33464},{\"end\":33474,\"start\":33470},{\"end\":34189,\"start\":34175},{\"end\":34199,\"start\":34189},{\"end\":34212,\"start\":34199},{\"end\":34425,\"start\":34415},{\"end\":34439,\"start\":34425},{\"end\":34452,\"start\":34439},{\"end\":34733,\"start\":34724},{\"end\":34742,\"start\":34733},{\"end\":34751,\"start\":34742},{\"end\":34761,\"start\":34751},{\"end\":34771,\"start\":34761},{\"end\":34780,\"start\":34771},{\"end\":35084,\"start\":35072},{\"end\":35096,\"start\":35084},{\"end\":35109,\"start\":35096},{\"end\":35436,\"start\":35423},{\"end\":35452,\"start\":35436},{\"end\":35462,\"start\":35452},{\"end\":35474,\"start\":35462},{\"end\":35487,\"start\":35474},{\"end\":35498,\"start\":35487},{\"end\":36016,\"start\":36005},{\"end\":36028,\"start\":36016},{\"end\":36046,\"start\":36028},{\"end\":36055,\"start\":36046},{\"end\":36064,\"start\":36055},{\"end\":36546,\"start\":36535},{\"end\":36558,\"start\":36546},{\"end\":36568,\"start\":36558},{\"end\":36963,\"start\":36951},{\"end\":36976,\"start\":36963},{\"end\":36984,\"start\":36976},{\"end\":37256,\"start\":37244},{\"end\":37270,\"start\":37256},{\"end\":37286,\"start\":37270},{\"end\":37297,\"start\":37286},{\"end\":37764,\"start\":37754},{\"end\":37774,\"start\":37764},{\"end\":37788,\"start\":37774},{\"end\":37799,\"start\":37788},{\"end\":37812,\"start\":37799},{\"end\":38148,\"start\":38137},{\"end\":38156,\"start\":38148},{\"end\":38167,\"start\":38156},{\"end\":38178,\"start\":38167},{\"end\":38189,\"start\":38178},{\"end\":38489,\"start\":38477},{\"end\":38504,\"start\":38489},{\"end\":38516,\"start\":38504},{\"end\":38532,\"start\":38516},{\"end\":38544,\"start\":38532},{\"end\":38555,\"start\":38544},{\"end\":38853,\"start\":38845},{\"end\":38864,\"start\":38853},{\"end\":38872,\"start\":38864},{\"end\":39170,\"start\":39158},{\"end\":39183,\"start\":39170},{\"end\":39193,\"start\":39183},{\"end\":40128,\"start\":40118},{\"end\":40139,\"start\":40128},{\"end\":40145,\"start\":40139},{\"end\":40390,\"start\":40380},{\"end\":40401,\"start\":40390},{\"end\":40407,\"start\":40401}]", "bib_venue": "[{\"end\":30966,\"start\":30908},{\"end\":31625,\"start\":31613},{\"end\":31993,\"start\":31975},{\"end\":32335,\"start\":32319},{\"end\":25958,\"start\":25933},{\"end\":26268,\"start\":26253},{\"end\":26520,\"start\":26508},{\"end\":26808,\"start\":26794},{\"end\":27220,\"start\":27211},{\"end\":27693,\"start\":27679},{\"end\":28068,\"start\":28058},{\"end\":28382,\"start\":28373},{\"end\":28699,\"start\":28683},{\"end\":29034,\"start\":29024},{\"end\":29307,\"start\":29231},{\"end\":30060,\"start\":30049},{\"end\":30440,\"start\":30378},{\"end\":30892,\"start\":30824},{\"end\":31250,\"start\":31210},{\"end\":31611,\"start\":31513},{\"end\":31973,\"start\":31910},{\"end\":32317,\"start\":32214},{\"end\":32676,\"start\":32644},{\"end\":33242,\"start\":33149},{\"end\":34173,\"start\":34160},{\"end\":34470,\"start\":34452},{\"end\":34795,\"start\":34780},{\"end\":35116,\"start\":35109},{\"end\":35567,\"start\":35515},{\"end\":36126,\"start\":36082},{\"end\":36604,\"start\":36568},{\"end\":36992,\"start\":36984},{\"end\":37398,\"start\":37319},{\"end\":37834,\"start\":37812},{\"end\":38209,\"start\":38189},{\"end\":38575,\"start\":38555},{\"end\":38892,\"start\":38872},{\"end\":39156,\"start\":39030},{\"end\":40167,\"start\":40145},{\"end\":40378,\"start\":40321},{\"end\":40710,\"start\":40576}]"}}}, "year": 2023, "month": 12, "day": 17}