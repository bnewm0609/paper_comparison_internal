{"id": 102491580, "updated": "2023-10-08 12:35:54.583", "metadata": {"title": "FatSegNet: A fully automated deep learning pipeline for adipose tissue segmentation on abdominal dixon MRI", "authors": "[{\"first\":\"Santiago\",\"last\":\"Estrada\",\"middle\":[]},{\"first\":\"Ran\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Sailesh\",\"last\":\"Conjeti\",\"middle\":[]},{\"first\":\"Ximena\",\"last\":\"Orozco\u2010Ruiz\",\"middle\":[]},{\"first\":\"Joana\",\"last\":\"Panos\u2010Willuhn\",\"middle\":[]},{\"first\":\"Monique M. B.\",\"last\":\"Breteler\",\"middle\":[]},{\"first\":\"Martin\",\"last\":\"Reuter\",\"middle\":[]}]", "venue": "Magnetic resonance in medicine", "journal": "Magnetic resonance in medicine", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Introduce and validate a novel, fast, and fully automated deep learning pipeline (FatSegNet) to accurately identify, segment, and quantify visceral and subcutaneous adipose tissue (VAT and SAT) within a consistent, anatomically defined abdominal region on Dixon MRI scans.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": "1904.02082", "mag": "3098509689", "acl": null, "pubmed": "31631409", "pubmedcentral": null, "dblp": "journals/corr/abs-1904-02082", "doi": "10.1002/mrm.28022"}}, "content": {"source": {"pdf_hash": "a526c5668e34c185e788f3ee5939cff99ada0ee9", "pdf_src": "Wiley", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/mrm.28022", "status": "HYBRID"}}, "grobid": {"id": "490a6ad6517ac1bb9963a0bdac790d06180c9222", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a526c5668e34c185e788f3ee5939cff99ada0ee9.txt", "contents": "\nFatSegNet: A fully automated deep learning pipeline for adipose tissue segmentation on abdominal dixon MRI of International Society for Magnetic Resonance in Medicine\n2020\n\nSantiago Estrada santiago.estrada@dzne.de \nImage Analysis\nGerman Center for Neurodegenerative Diseases (DZNE)\nBonnGermany\n\nPopulation Health Sciences\nGerman Center for Neurodegenerative Diseases (DZNE)\nBonnGermany\n\nRan Lu \nPopulation Health Sciences\nGerman Center for Neurodegenerative Diseases (DZNE)\nBonnGermany\n\n| Sailesh Conjeti \nImage Analysis\nGerman Center for Neurodegenerative Diseases (DZNE)\nBonnGermany\n\nXimena Orozco-Ruiz \nPopulation Health Sciences\nGerman Center for Neurodegenerative Diseases (DZNE)\nBonnGermany\n\nJoana Panos-Willuhn \nPopulation Health Sciences\nGerman Center for Neurodegenerative Diseases (DZNE)\nBonnGermany\n\nMonique M B Breteler \nPopulation Health Sciences\nGerman Center for Neurodegenerative Diseases (DZNE)\nBonnGermany\n\nFaculty of Medicine\nInstitute for Medical Biometry\nInformatics and Epidemiology (IMBIE)\nUniversity of Bonn\nBonnGermany\n\nMartin Reuter \nImage Analysis\nGerman Center for Neurodegenerative Diseases (DZNE)\nBonnGermany\n\nA.A. Martinos Center for Biomedical Imaging\nMassachusetts General Hospital\nBostonMassachusetts\n\nDepartment of Radiology\nHarvard Medical School\nCampus 1 BLDG 9953127Boston, BonnMassachusettsUSA, Germany\n\nSantiago Correspondence \nDZNE, Venusberg-Estrada \nFatSegNet: A fully automated deep learning pipeline for adipose tissue segmentation on abdominal dixon MRI of International Society for Magnetic Resonance in Medicine\n\nMagn Reson Med\n83202010.1002/mrm.28022Received: 27 March 2019 | Revised: 17 August 2019 | Accepted: 6 September 2019\nPurpose: Introduce and validate a novel, fast, and fully automated deep learning pipeline (FatSegNet) to accurately identify, segment, and quantify visceral and subcutaneous adipose tissue (VAT and SAT) within a consistent, anatomically defined abdominal region on Dixon MRI scans. Methods: FatSegNet is composed of three stages: (a) Consistent localization of the abdominal region using two 2D-Competitive Dense Fully Convolutional Networks (CDFNet), (b) Segmentation of adipose tissue on three views by independent CDFNets, and (c) View aggregation. FatSegNet is validated by: (1) comparison of segmentation accuracy (sixfold cross-validation), (2) test-retest reliability, (3) generalizability to randomly selected manually re-edited cases, and (4) replication of age and sex effects in the Rhineland Study-a large prospective population cohort. Results: The CDFNet demonstrates increased accuracy and robustness compared to traditional deep learning networks. FatSegNet Dice score outperforms manual raters on VAT (0.850 vs. 0.788) and produces comparable results on SAT (0.975 vs. 0.982). The pipeline has excellent agreement for both test-retest (ICC VAT 0.998 and SAT 0.996) and manual re-editing (ICC VAT 0.999 and SAT 0.999). Conclusions: FatSegNet generalizes well to different body shapes, sensitively replicates known VAT and SAT volume effects in a large cohort study and permits localized analysis of fat compartments. Furthermore, it can reliably analyze a 3D Dixon MRI in \u223c1 minute, providing an efficient and validated pipeline for abdominal adipose tissue analysis in the Rhineland Study.K E Y W O R D S deep learning, dixon MRI, neural networks, semantic segmentation, subcutaneous adipose tissue, visceral adipose tissue 1472 | ESTRADA ET Al. 2 | METHODS 2.1 | Data 2.1.1 | MR imaging acquisition MR image acquisition was performed at two different sites both with identical 3T Siemens MAGNETOM Prisma MR scanners (Siemens Healthcare, Erlangen, Germany).\n\n| INTRODUCTION\n\nThe excess of body fat depots is an increasing major public health issue worldwide and an important risk factor for the development of metabolic disorders and reduced quality of life. 1,2 While the body mass index (BMI) is a widely used indicator of adipose tissue accumulation in the body, it does not provide information on fat distribution 3 neither with respect to different fat tissue types nor with respect to deposit location. Different compartments of adipose tissue are associated with different physiopathological effects. 4,5 Abdominal adipose tissue (AAT), composed of subcutaneous and visceral adipose tissue (SAT and VAT), has long been associated with an increased risk of chronic cardiovascular diseases, glucose impairment, and dyslipidemia. 6,7 Recently, several studies have indicated a stronger relation between the accumulation of VAT with an adverse metabolic and inflammatory profile compared to SAT. 8,9 Therefore, an accurate and independent measurement of VAT and SAT volumes (VAT-V and SAT-V) is of significant clinical and research interest.\n\nCurrently, the gold standard for measuring VAT-V and SAT-V is the manual segmentation of abdominal fat images from Dixon magnetic resonance (MR) scans-a very expensive and time-consuming process. Thus, especially for large studies, automatic segmentation methods are required. However, achieving good accuracy is challenging due to complex AAT structures, a wide variety of VAT shapes, large anatomical differences across subjects, and the inherent properties of the Dixon images: low intensity contrast between adipose tissue classes, inhomogeneous signals, and potential organ motion. So far, those limitations impeded the widespread implementation of automatic and semi-automatic techniques based on intensity and shape features, such as fuzzy-clustering, 10 k-means clustering, 11 graph cut 12,13 active contour methods, 14 and statistical shape models. 15 Recently, fully convolutional neural networks (F-CNNs) 16,17 have been widely adopted in the computer vision community for pixel/voxel-wise image segmentation in an end-toend fashion to overcome above-mentioned challenges. With these methods there is no need to extract manual features, divide images into patches, or implement sliding window techniques. F-CNNs can automatically extract intrinsic features and integrate global context to resolve local ambiguities thereby improving the results of the predicted models. 17 Langer et al 18 proposed a three-channel UNet for AAT segmentation, which is a conventional architecture for 2D medical image segmentation. 19 While this method showed promising results, we demonstrate that our network architecture outperforms the traditional UNet for segmenting AAT on our images with a wide range of anatomical variation. More recent architectures such as the SD-Net 20 and Dense-UNet, a densely connected network, 21 have the potential to improve generalizability and robustness by encouraging feature re-usability and strengthening information propagation across the network. 21 In prior work, we introduced a competitive dense fully convolutional network (CDFNet) 22 as a new 2D F-CNN architecture that promotes feature selectivity within a network by introducing maximum attention through a maxout activation unit. 23 The maxout boosts performance by allowing the creation of specialized sub-networks that target a specific structure during training. 24 Therefore, this approach facilitates the learning of more complex structures 22,24 with the added benefit of reducing the number of training parameters relative to the aforementioned networks.\n\nIn this paper, we propose FatSegNet, a novel fully automated deep learning pipeline based on our CDFNet architecture to localize and segment VAT and SAT on abdominal Dixon MR images from the Rhineland Study, an ongoing large population-based cohort study. 25,26 To constrain AAT segmentations to a consistent anatomically defined region, the proposed pipeline consists of three stages:\n\n1. Localization of the abdominal region using a semantic segmentation approach by implementing CDFNet models on sagittal and coronal planes; we use the lumbar vertebrae positions as reference points for selecting the region of interest. 2. Segmentation of VAT and SAT within the abdominal region through 2D CDFNet models on three different planes (axial, sagittal, and coronal). 3. A view aggregation stage where the previous generated label maps are combined to generate a final 3D segmentation.\n\nWe initially evaluate and compare the individual stages of the pipeline with other deep learning approaches in a sixfold crossvalidation. We show that the proposed network architecture (CDFNet) improves segmentation performance and simultaneously reduces the number of required training parameters in step 1 and 2. After asserting segmentation accuracy, we evaluate the whole pipeline (FatSegNet) with respect to robustness and reliability against two independent test sets: a manually edited and a test-retest set. Finally, we present a case study on unseen data comparing the VAT-V and SAT-V calculated from the FatSegNet segmentations against BMI to replicate age and sex effects on these volumes in a large cohort.\n\n\n| 1473\n\nESTRADA ET Al. \n\n\n| Datasets\n\nThe Rhineland Study is an ongoing population-based prospective cohort (https ://www.rhein land-studie.de/) which enrolls participants aged 30 years and above at baseline from Bonn, Germany. The study is carried out in accordance with the recommendations of the International Council for Harmonisation (ICH) Good Clinical Practice (GCP) standards (ICH-GCP). Written informed consent was obtained from all participants in accordance with the Declaration of Helsinki.\n\nThe first 641 subjects from the Rhineland Study with BMI and abdominal MR Dixon scans are included. The sample presents a mean age of 54.2 years (range 30 to 95) and 55.2% of the subjects are women. The BMI of the participants ranges from 17.2 to 47.7 kg/m 2 with a mean of 25.2 kg/m 2 . Subjects were stratified into two subsets: 38 scans were manually annotated for training and testing; the remaining 603 subjects were segmented using the proposed pipeline. After visual inspection, 16 subjects were excluded due to poor image quality or extreme motion artifacts (e.g. potentially caused by breathing). Thus, 587 participants were used for the case study analysis and a subset of 50 subjects were randomly selected for manual corrections of the predicted label maps. This manually edited set and an independent test-retest set of 17 healthy young volunteers were used to assess reliability of the automated segmentation and volume estimates.\n\nGround truth data 38 subjects were randomly selected from sex and BMI strata to ensure a balanced population distribution. These scans were manually annotated by two trained raters without any semi-automated support such as thresholding, which can reduce accuracy in the ground truth and lead to overestimation of the performance of the proposed automated method.\n\nSpecific label schemes were created for each individual task of the pipeline. For localizing the abdominal region, raters divided the scans into three different blocks defined by the location of the vertebrae as follows: the abdominal region (from lower bound of twelfth thoracic vertebra (Th12) to the lower bound of L5), the thoracic region (all above the lower bound of Th12), and the pelvic region (everything below the lower bound of L5), as illustrated in Figure 1E). For AAT segmentation, 60 slices per subject were manually labeled into three classes: SAT, VAT, and bone with neighbouring tissues. The bone was labeled to prevent bone marrow from being misclassified as adipose tissue. In order to improve spatial context and prevent misclassification of the arms, the dataset was complemented by a synthetic class defined as \"other tissue\" that was composed of any soft tissue inside the abdomen cavity that is not VAT or SAT. The manual annotations are illustrated in Figure 1B,C. Furthermore, four subjects were labeled by both raters to evaluate the inter-rater variability.\n\n\nTest-retest data\n\n17 additional subjects were recruited with the exclusive purpose of measuring the acquisition protocol reliability. The group presents a mean age of 25.5 years (range: 20 to 31) and 65.0% of the participants are women; all of them have a normal BMI (BMI <25 kg/m 2 ). Subjects were scanned in two consecutive sessions. Before starting the second session, subjects were removed from the scanner and re-positioned.\n\n\n| FatSegNet pipeline\n\nThe FatSegNet is to be deployed as a post-processing adipose analysis pipeline for the abdominal Dixon MR images acquired in the Rhineland Study. Therefore, it should meet the following requirements: (1) be fully automated, (2) segment the different adipose tissue types within the anatomically defined abdominal region, and (3) be robust to body type variations and generalizable in presence of high population heterogeneity. Following the prior conditions, we designed FatSegNet as a fully automated deep learning pipeline for adipose segmentation ( Figure 2). The proposed pipeline consists of three stages: (1) the abdominal region is localized by averaging bounding boxes from two abdominal segmentation maps generated by CDFNets on the sagittal and coronal view. For each view a bounding box is set to the full image width. The height is extracted by localizing the highest and lowest slice with at least 85% of none background voxels classified as abdominal region. Highest and lowest slice position are averaged across the views. (2) Afterward, adipose tissue is segmented within the abdominal region by three CDFNets on different views (axial, coronal, and sagittal) with standardized input sizes (zero padding).\n\n(3) Finally, a view aggregation network merges the predicted label maps from the previous stage into a final segmentation; the implemented multi-view scheme is designed to improve segmentation of structures that are not clearly visible due to poor lateral resolution. This 2.5D strategy produces a fully automated pipeline to accurately segment adipose tissue inside a consistent anatomically defined abdominal region.\n\n\n| Pipeline components\n\n\nCompetitive dense fully convolutional network (CDFNet)\n\nFor the segmentation task, we introduce the CDFNet architecture due to its robustness and generalizability properties. The proposed network improves feature selectivity and, thus, boosts the learning of fine-grained anatomies without increasing the number of learned parameters. 22 We implemented the CDFNet by suitably adopting the Dense-UNet architecture proposed by Roy et al 27 and extending it toward competitive learning via maxout activations. 24 The Dense-UNet proposed in 27 follows the usual dumbbell like architecture with four dense-block encoders, four dense-block decoders and one bottleneck layer. Each denseblock is based on short-range skip connections between convolutional layers as introduced for densely connected neural networks 28 ; the dense connection approach stacks multiple convolutional layers in sequence and the input of a layer is iteratively concatenated with the outputs of the previous layers. This type of connectivity improves feature reusability, increases information propagation, and alleviates vanishing gradients. 28 The architecture additionally incorporates the traditional long-range skip connections between all encoder and decoder blocks of the same spatial resolution as introduced by Ronnenberger et al 19 which improves gradient flow and spatial information recovery.\n\nWithin the Dense-UNet, the information aggregation through these connections is performed by concatenation layers. Such a design increases the size of the output feature map along the feature channels, which in turn results in the need to F I G U R E 2 Proposed FatSegNet Pipeline for segmenting AAT. The pipeline is divided into three stages: First, localization of abdominal region. Then, tissue segmentation on the abdominal region and finally, view aggregation. Both local and global volume estimates of individual structures are calculated on the final prediction learn filters with a higher number of parameters. Goodfellow et al introduced the idea of competitive learning through maxout activations, 23 which was adapted by Liao and Carneiro 24 for competitive pooling of multi-scale filter outputs. Both 23 and 24 proved that the use of maxout competitive units boosts performance by creating a large number of dedicated subnetworks within a network that learns to target specific sub-tasks and reduces the number of required parameters significantly, which in turn can prevent over-fitting.\n\nThe maxout is a simple feed-forward activation function that chooses the maximum value from its inputs. 23 Within a CNN, a maxout feature map is constructed by taking the maximum across multiple input feature maps for a particular spatial location. The proposed CDFNet uses competitive layers (maxout activation) instead of concatenation layers. Our preliminary results 22 demonstrate that these competitive units promote the formation of dedicated local sub-networks in each of the densely connected blocks within the encoder and the decoder paths. This encourages sub-modularity through a network-in-network design that can learn more efficiently. Toward this, we propose two novel architectural elements targeted at introducing competition within the short-and longrange connections, as follows:\n\n\nLocal Competition-Competitive Dense Block (CDB):\n\nBy introducing maxout activations within the short-range skip connections of each of the densely connected convolutional layers (at the same resolution), we encourage local competition during learning of filters. The multiple convolution layers in each block prevent filter co-adaptation.\n\n\nGlobal Competition-Competitive Un-pooling Block (CUB):\n\nWe introduce a maxout activation between a longrange skip connection from the encoder and the features up-sampled from the prior lower resolution decoder block. This promotes competition between finer feature maps with smaller receptive fields (skip connections) and coarser feature maps from the decoder path that spans much wider receptive fields encompassing higher contextual information.\n\nIn brief, the proposed CDFNet comprises a sequence of four CDBs, constituting the encoder path (down-sampling block), and four CDBs constituting the decoder path (up-sampling block), which is joined via a bottleneck layer. The bottleneck consists of a 2D convolutional layer followed by a Batch Normalization. The skip-connections from each of the encoder blocks feed into the CUB that subsequently forward features into the corresponding decoder block of the same resolution as illustrated in Figure 3.\n\n\nView aggregation network\n\nThe proposed view aggregation network is designed to regularize the prediction for a given voxel by considering spatial information from the coronal, axial, and sagittal view. The network, therefore, merges the probability maps of the three different CDFNets from the previous stage by applying a (3 \u00d7 3 \u00d7 3) 3D-convolution (30 filters) followed by a Batch Normalization. Then a (1 \u00d7 1 \u00d7 1) 3D-convolution is employed to reduce the feature maps to the desired number of classes (n = 5). The final prediction probabilities are obtained via a concluding softmax layer (as illustrated in Supporting Information Figure S1). Our approach learns to weigh each view differently on a voxel level, compared to standard hard-coded global view aggregation schemes. Such hard-coded weighting schemes can be suboptimal when working with anisotropic voxels sizes (e.g., here 2 mm \u00d7 2 mm \u00d7 5 mm) as resolution differences impose a challenge when combining the spatial information from the finer (within-plane) and coarser (across slice) resolutions. Additionally, in the presence of high variance, abdominal body shapes across subjects segmentation benefit from data-driven approaches that can flexibly adopt weights to individual situations and even spatial locations, which are not possible if hardcoded global weights are being used.\n\n\n| Experimental setup\n\nFor training and testing the pipeline, we perform a sixfold cross-validation subject-space split on the ground truth dataset. \n\n\n| Baselines and comparative methods\n\nWe validate the FatSegNet by comparing the performance of each stage of the pipeline against the cross-validation test sets using Dice score index (DSC) to measure similarity between the prediction and the ground truth. Let M (ground truth) and P (prediction) denote the labels binary segmentation, the Dice score index is defined as where |M| and |P| represents the number of elements in each segmentation and |M \u2229 P| the number of common elements. Therefore, the DSC ranges from 0 to 1 and a higher DSC represents a better agreement between segmentations. Additionally, we benchmark the proposed CDFNet models for abdominal region localization and AAT delineation with state-of-the-art segmentation F-CNNs such as UNet, 19 SD-Net, 20 and Dense-UNet. 27 We use the probability maps generated from the aforementioned networks to train the view aggregation model and measure performance with and without view aggregation. The proposed view aggregation performance for each FCNNs is compared against two non-data-driven (hard-coded) methods: equally balanced weights for all views and axial focus weights (accounting for higher in-plane resolution, axial = 0.5, coronal = 0.25, sagittal = 0.25). Finally, to permit a fair comparison, all benchmark networks follow the same architecture of four encoder blocks, four decoders blocks, and one bottleneck layer as illustrated in Figure 3 with an input image size of 224 \u00d7 256. Note, significant differences between our proposed methods and comparative baselines are evaluated by a Wilcoxon signed-rank test 29 after multiple comparisons correction using a one-sided adaptive FDR. 30 The aforementioned models are implemented in Keras 31 with a TensorFlow back-end using an NVIDIA Titan Xp GPU with 12 GB RAM and the following parameters: batch size of 8, momentum set to 0.9, constant weight decay of 10 \u221206 , and an initial learning rate of 0.01 decreased by a order of 10 every 20 epochs. The models are trained for 60 epochs with an earlystopping criterion (no relevant changes on the validation loss after the last 8 epochs-convergence was observed around 50 epochs). A composite loss function of median frequency balanced logistic loss and Dice loss 20 is used. This loss function emphasizes the boundaries between classes and supports learning of unbalanced classes such as VAT. Finally, online data augmentation (translation, rotation and global scaling) is performed to increase training set size and improve the networks generalizability. Note, the FatSegNet implementation is available at https ://github.com/reuter-lab/FatSe gNet.\n\n\n| Pipeline reliability\n\nWe assess the FatSegNet reliability by comparing the difference of VAT-V and SAT-V across sessions for each subject of the test-retest and manually edited set. Given a predicted label map and N i (l) the number of voxels classified as l (VAT or SAT) in session i (test-retest, or manual-automated), the absolute percent difference (APD(l)) of a label volume measures variability across sessions. It is defined as Additionally, we calculate the agreement of total VAT-V and SAT-V between sessions by an intra-class correlation (ICC) using a two-way fixed, absolute agreement and single measures ICC(A,1). 32\n\n\n| Case study analysis on the\n\n\nRhineland study\n\nWe compare the volumes of abdominal adipose tissue (AAT-V, SAT-V, and VAT-V) generated from FatSegNet with BMI on the unseen dataset. A fast quality control is\n(1) DSC = 2 \u22c5 |M \u2229 P| |M| + |P| (2) APD(l) = 2 \u22c5 | | N 1 (l) \u2212 N 2 (l) | | N 1 (l) + N 2 (l) \u22c5 100 | 1477 ESTRADA ET Al.\nperformed to identify drastic failure cases. The differences among BMI groups are evaluated with a one-way analysis of variance (ANOVA) with subsequent Tukey's honest significant difference (HSD) post hoc comparisons. The associations of volumes of abdominal adipose tissue and BMI are assessed using partial correlation and linear regression after accounting for age, sex, and height of the abdominal region. Separate linear regression analyses are performed to explore the effect of age on SAT-V and VAT-V in men and women. All the statistical analyses are performed in R. 33 \n\n\n| RESULTS\n\n\n| Method validation\n\n\n| Localization of abdominal region\n\nFor assessing the performance of abdominal region detection after creation of an average bounding box from the coronal and sagittal views the average Dice overlap (sixfold crossvalidation) was calculated, as illustrated on the Supporting Information Figure S2. We observe that all models perform extremely well on the relatively easy task of localizing the desired abdominal region (DSC >0.96). There is no significant difference between the models; however, we use our CDFNet because it requires substantially less parameters (see Table 1) compared to the UNet and Dense-UNet.\n\n\n| Segmentation of AAT\n\nIn Table 1, we present the average Dice score (sixfold crossvalidation) for VAT and SAT for each individual view as well as for the view aggregation model. Here, we observe that all methods work extremely well for SAT segmentation. Nevertheless, our proposed CDFNet outperforms the UNet and SD-Net on all single-view models and, when compared with the Dense-UNet, there is significant improvement in the sagittal and coronal views. For the more challenging task of VAT recognition, which is a more fine-grained compartment with large shape variation, the proposed CDFNet outperforms the SD-Net on all single planes; when compared with Dense-UNet and U-Net, there is only significant improvement in the axial and coronal plane. Nonetheless, CDFNet achieves this performance with \u223c30% (Dense-UNet) and \u223c80% (UNet) less parameters, demonstrating that the proposed architecture improves feature selectivity and simplifies network learning. Furthermore, fewer parameters can help decrease overfitting error, especially when training with limited annotated data, and thus improve generalizability. Note, that Dice scores increase and difference of pairwise comparisons is slightly reduced after the view aggregation ( Table 1), showing that this steps helps all individual networks to reach a better performance by introducing spatial information from multiple views and regularizing the prediction maps. The proposed data-driven aggregation scheme outperforms (DSC) the hard-coded models for SAT and with statistically significance for VAT as shown in Table 2. Furthermore, learned weights are spatially varying and can adjust to subject-specific anatomy, which in turn can improve generalizability. We empirically observe that the aggregation model smoothes the label maps slightly, resulting in visually more appealing boundaries. It also significantly reduces the arms from being misclassified as adipose tissue which can otherwise be observed in different views, especially on overweight and obese subjects, where arms are located closer to the abdominal cavity, as seen Supporting Information Figure S3.\n\nFinally it should be highlighted, that all single-view and the view aggregation models achieve similarly excellent results on the SAT segmentation compared to inter-rater variability and outperform the manual raters for the more challenging VAT segmentation by a margin. Table 3 presents the reliability metrics evaluated on the testretest and the manually edited test set. The proposed pipeline presents only a small absolute percent volume difference (APD) for VAT and SAT, and excellent agreement between the predicted and corrected segmentation maps. It must be noted, that APD is larger for both tissue types in the testretest setting as it also includes variance from acquisition noise (e.g. motion artefacts, non-linearities based on different positioning) in addition to potential variances of the processing pipelines. Nevertheless, we observe excellent agreement (ICC) between sessions for the test-retest dataset for both adipose tissue types.\n\n\n| FatSegNet reliability\n\n\n| Case study: Analysis of Rhineland study data\n\n\n| The characteristics of the study population\n\nAfter visual quality inspection, 16 scans were flagged due to image artefacts, such as motion or low contrast (see Figure 4C,D for two examples). The characteristics of the remaining 587 participants with valid data on BMI and volumes of abdominal adipose tissue are presented in Supporting Information Table S1. The mean (SD) age of the subjects is 54.2 (13.3) years, and 54.7% are women. 311 (53.0%) subjects are normal weight, 209 (35.6%) overweight, and 67 (11.4%) obese. We observed a BMI increase with age (\u03b2 = 0.03, P = .007) and a borderline significance of age difference among BMI groups (P = .052, ANOVA). Obvious differences are observed in AAT-V, VAT-V, and SAT-V across BMI groups (P < .001, ANOVA). VAT-V to SAT-V ratio is higher in overweight and obese participants compared to those with normal weight (P < .001), but there is no difference between overweight and obese (P = .505).\n\n\n| The association between abdominal adipose tissue volumes and BMI\n\nBMI shows a strong positive correlation with AAT-V and SAT-V (AAT-V: r = .88, P < .001; SAT-V: r = .85, P < .001), T A B L E 2 Mean (and standard deviation) Dice scores (cross-validation) of hard-coded balanced weights, hard-coded axial focus weights, and the proposed view aggregation for abdominal adipose tissue segmentation but only a moderate correlation with VAT-V (r = 0.65, P < .001) after adjusting for age, sex, and abdominal region height. As illustrated in Figure 5, both SAT-V and VAT-V are positively associated with BMI after accounting for age, sex, and abdominal region height (P < .001). The accumulation of SAT-V is higher than VAT-V as BMI increases.\n\n\nSubcutaneous (SAT) Visceral (VAT)\n\n\n| Influence of age and sex on\n\n\nVAT-V and SAT-V\n\nThe influence of age and sex on VAT-V and SAT-V follows different patterns (as illustrated in Figure 6). Men tend to have lower SAT and higher VAT compared to women (P < .001).\n\nVAT-V significantly increase with age in both men and women. Conversely, SAT-V is weakly associated with age in women (\u03b2 = 0.02, P = .012), but not in men (\u03b2 = \u22120.01, P = .337).\n\n\n| DISCUSSION\n\nIn our study, we established, validated, and implemented a novel deep learning pipeline to segment and quantify the components of abdominal adipose tissue, namely, VAT-V, SAT-V, and AAT-V on a fast acquisition abdominal Dixon MR protocol for subjects from the Rhineland Study, a large population-based cohort. The proposed pipeline is fully automated and requires approximately 1 minute for analyzing a subject's whole volume. Moreover, since the pipeline is based on deep learning models, it can be easily updated and retrained as the study progresses and new manual data are generated-which can further improve overall pipeline robustness and generalizability, providing a pragmatic solution for a population-based study. The proposed pipeline, termed FatSegNet implements a three-stage design with the CDFNet architecture at the core for localizing the abdominal region and segmenting the AAT. The introduction of our CDFNet inside the pipeline boosts the competition among filters to improve feature selectivity within the networks. CDFNet introduces competition at a local scale by substituting concatenation layers with maxout activations that prevent filter coadaptation and reduce the overall network complexity. It also induces competition at a global scale through competitive unpooling. This network design, in turn, can learn more efficiently.\n\nFor the first stage of the pipeline, i.e. localization of the abdominal region, all FCNNs can successfully determine the upper and lower limit of the abdominal region from a segmentation prediction map. However, our CDFNet requires significantly fewer parameters compared to the traditional UNet and Dense-UNet. Furthermore, the localization block is able to identify the abdominal region correctly even in cases with scoliosis (curved spine) as illustrated in Figure 7F. For the more challenging task of segmenting AAT, we demonstrate that CDFNet recovers VAT significantly better than traditional deep learning variants that rely on The selection of an inhomogeneous BMI testing set ensures that our method is evaluated for different body types and avoids biases, as better segmentation performance can be achieved on subjects with high content of AAT compared to lean subjects. 34,35 Moreover, images from individuals with high AAT could be accompanied by other types of issues, such as fat shadowing ( Figure 7D), or arms located in close proximity to the abdominal cavity ( Figure 7A,D,E). These issues are mitigated by our view aggregation model that regularizes the predicted segmentation by combining the spatial context from different views ultimately improving segmentation of tissue boundaries. Moreover, this approach automatically prevents misclassification of arms whereas previous deep learning AAT segmentation methods required manual removal of the upper extremities in a pre-processing step. 18 Note, that we prefer the 2D over a full 3D approach in this work. A full 3D network architecture has more parameters, requiring significantly more expert annotated training data (full 3D cases) and/or artificial data augmentation, which could increase the chance of overfitting-in addition to increased GPU memory requirements.\n\nAs demonstrated on the Rhineland Study data, the proposed pipeline exhibits high robustness and generalizability across a wide range of age, BMI, and a variety of body shapes as seen in Figures 7 and 4A,B. FatSegNet successfully identifies the AAT in different abdomen morphologies, spine curvatures, adipose shadowing, arms positioning, or intensity inhomogeneities. Furthermore, the pipeline has a high test-retest reliability between the calculated volumes of VAT and SAT without the need of any image pre-processing (biascorrection, image registration, etc.) or manual selection of a slice or region. Furthermore, the manually edited test set demonstrates a high similarity of automated and manual labels and excellent agreement of volume estimates. However, as is usual with any automated method, segmentation reliability decreases when input images have low quality as illustrated in Figure 4C,D where the scans present severe motion/breathing artifacts or very low-image contrast. In order to detect these problematic images in large studies, an automated or manual quality control protocol should be implemented before passing images to automated pipelines.\n\nIn accordance with previous studies on smaller data sets, 13,36 our data showed a lower correlation of BMI with VAT-V than with AAT-V and SAT-V. We also observed a sex difference of the SAT-V and VAT-V accumulation as previously reported 37,38 : men were more likely to have higher VAT-V and lower SAT-V compared to women. Moreover, we further explored the association between age with SAT-V and VAT-V and found an obvious age effect on the accumulation of VAT-V in both men and women, and a weak age effect on SAT-V in women but not in men. This discrepancy was previously observed by Machann et al, 37 who assessed the body composition using MRI in 150 healthy volunteers aged 19 to 69 years. They reported a strong correlation between VAT-V and age both in men and women, whereas SAT-V only slightly increased with age in women. The fact that our results replicate these previous findings on a large unseen dataset corroborates stability and sensitivity of our pipeline.\n\nIn conclusion, we have developed a fully automated postprocessing pipeline for adipose tissue segmentation on abdominal Dixon MRI based on deep learning methods. While reducing the number of required parameters, the pipeline outperforms other deep learning architectures and demonstrates high reliability. Furthermore, the proposed method was successfully deployed in a large population-based cohort, where it replicated well known SAT-V and VAT-V age and sex associations and demonstrated generalizability across a large range of anatomical differences, both with respect to body shape and fat distribution.\n\nF\nI G U R E 1 MR Dixon images and ground truth from two subjects with different BMI (obese (upper), normal (lower). A, Fat images: axial plane. B, Initial manual segmentation (blue: SAT, green: VAT, orange: bone and surrounding structures). C, Ground truth with additional synthetic class (red: other-tissue) and filled-in bone structures (orange). D, Fat images: coronal plane. E, Ground truth for localization of region of interest (red: thoracic region, white: abdominal region (region of interest), blue: pelvic region)\n\nF I G U R E 3\n3Proposed network architecture: Competitive Dense Fully Convolutional Network (CDFNet), with 4 competitive dense blocks (CDB) on each encoder and decoder path and 4 competitive unpool blocks (CUB) between them. CDB and CUB induce local and global competition within the network. Note-the output filters for all convolutional layers in CUB, CDB, and Bottleneck were standardized to 64 channels\n\nT\nA B L E 1 Mean (and standard deviation) Dice scores (cross-validation) of the FCNN models for abdominal adipose tissue segmentation We show FDR corrected significance indicators of Wilcoxon signed-rank test 29 comparing the proposed CDFNet vs. benchmark FCNNs. a The approximately number of learn parameters reported is for the models without the View Aggregation Network. b Statistical difference using a one-sided adaptive FDR multiple comparison correction 30 at a level of 0.05.\n\nFE 5\n5I G U R E 4 Examples of FatSegNet predictions and excluded cases on the Rhineland Study. (A, B) Subjects with different body shapes and accurate segmentations. (C, D) Excluded subjects from the case study due to extreme motion noise (C), or low image contrast quality (D). Association of BMI with SAT-Volume and VAT-Volume concatenation layers. Additionally, each individual CDFNet view model outperforms manual raters for segmenting the complex VAT and accomplishes equivalent results on SAT.\nACKNOWLEDGEMENTSWe would like to thank the Rhineland Study group for supporting the data acquisition and management, as well as Mohammad Shahid for his support on the deployment of the FatSegNet method into the Rhineland Study processing pipeline. Furthermore we acknowledge Tony St\u00f6cker and his team for developing and implementing the Dixon MRI sequence used in this work. This work was supported by the Diet-Body-Brain Competence Cluster in Nutrition Research funded by the Federal Ministry of Education and Research (BMBF), Germany (grant numbers 01EA1410C and FKZ: 01EA1809C), by the JPI HDHL on Biomarkers for Nutrition and Health (HEALTHMARK), BMBF (grant number 01EA1705B), the NIH R01NS083534, R01LM012719, and an NVIDIA Hardware Award.ORCIDSantiago Estradahttp://orcid.org/0000-0003-0339-8870Monique M. B. Breteler https://orcid.org/0000-0002-0626-9305Martin Reuter https://orcid.org/0000-0002-2665-9693SUPPORTING INFORMATIONAdditional supporting information may be found online in the Supporting Information section at the end of the article.FIGURE S1View aggregation Network. The proposed network is composed of a initial 3D convolution layer with 30 channels, followed by a batch normalization and a 3D convolutional layer for reducing the feature map dimensionality into the number of classes(n = 5) FIGURE S2Step 1: Abdominal region localization. Dice scores box-plot: Average Dice score (cross-validation) of the abdominal | 1483region detection comparing the Proposed CDFNet vs. other FCNN architectures. The Dice scores are calculated on the average abdominal region generated from the average bounding boxes of the sagittal and coronal model. There is no significant difference between models, nonetheless, the proposed method achieves the same performance with \u223c30% and \u223c80% less parameters compared to Dense-UNet and UNet, respectively FIGURE S3 Comparison of single view model (left) vs. view aggregation (right): AAT predictions of two unseen subjects: A, normal subject, B, obese subject. View aggregation avoids armmisclassification (red boxes) and improves SAT (purple box)\nRelationship among body fat percentage, body mass index, and all-cause mortality: a cohort study. R Padwal, W D Leslie, L M Lix, S R Majumdar, Ann Int Med. 164Padwal R, Leslie WD, Lix LM, Majumdar SR. Relationship among body fat percentage, body mass index, and all-cause mortality: a cohort study. Ann Int Med. 2016;164:532-541.\n\nGlobal, regional, and national prevalence of overweight and obesity in children and adults during 1980-2013: a systematic analysis for the Global Burden of Disease Study. M Ng, T Fleming, M Robinson, The Lancet. 384Ng M, Fleming T, Robinson M, et al. Global, regional, and national prevalence of overweight and obesity in children and adults during 1980-2013: a systematic analysis for the Global Burden of Disease Study 2013. The Lancet. 2014;384:766-781.\n\nMisclassification of cardiometabolic health when using body mass index categories in NHANES 2005-2012. A J Tomiyama, J M Hunger, J Nguyen-Cuu, C Wells, Int J Obesity. 40Tomiyama AJ, Hunger JM, Nguyen-Cuu J, Wells C. Misclassification of cardiometabolic health when using body mass index categories in NHANES 2005-2012. Int J Obesity. 2016;40:883-886.\n\nBody composition profiling in the UK biobank imaging study. J Linge, M Borga, J West, Obesity. 26Linge J, Borga M, West J, et al. Body composition profiling in the UK biobank imaging study. Obesity. 2018;26:1785-1795.\n\nBody fat distribution and risk of cardiovascular disease: an update. J P Despr\u00e9s, Circulation. 126Despr\u00e9s JP. Body fat distribution and risk of cardiovascular dis- ease: an update. Circulation. 2012;126:1301-1313.\n\nRelation of body fat distribution to metabolic complications of obesity. A H Kissebah, N Vydelingum, R Murray, D J Evans, R K Kalkhoff, P W Adams, J Clinical Endocrinol Metabolism. 54Kissebah AH, Vydelingum N, Murray R, Evans DJ, Kalkhoff RK, Adams PW. Relation of body fat distribution to metabolic complications of obesity. J Clinical Endocrinol Metabolism. 1982;54:254-260.\n\nRegional distribution of body fat, plasma lipoproteins, and cardiovascular disease. J P Despres, S Moorjani, P J Lupien, A Tremblay, A Nadeau, C Bouchard, Arteriosclerosis Thrombosis Vascular Biol. 10Despres JP, Moorjani S, Lupien PJ, Tremblay A, Nadeau A, Bouchard C. Regional distribution of body fat, plasma lipoproteins, and cardiovascular disease. Arteriosclerosis Thrombosis Vascular Biol. 1990;10:497-511.\n\nAbdominal obesity and metabolic syndrome. J P Despr\u00e9s, I Lemieux, Nature. 444Despr\u00e9s JP, Lemieux I. Abdominal obesity and metabolic syn- drome. Nature. 2006;444:881-887.\n\nVisceral/epicardial adiposity in nonobese and apparently healthy young adults: association with the cardiometabolic profile. E De Larochelli\u00e8re, J C\u00f4t\u00e9, G Gilbert, Atherosclerosis. 234De Larochelli\u00e8re E, C\u00f4t\u00e9 J, Gilbert G, et al. Visceral/epicar- dial adiposity in nonobese and apparently healthy young adults: association with the cardiometabolic profile. Atherosclerosis. 2014;234:23-29.\n\nNovel segmentation method for abdominal fat quantification by MRI. A Zhou, H Murillo, Q Peng, J Magn Reson Imaging. 34Zhou A, Murillo H, Peng Q. Novel segmentation method for abdominal fat quantification by MRI. J Magn Reson Imaging. 2011;34:852-860.\n\nSoftware for automated MRI-based quantification of abdominal fat and preliminary evaluation in morbidly obese patients. G Th\u00f6rmer, H H Bertram, N Garnov, J Magn Reson Imaging. 37Th\u00f6rmer G, Bertram HH, Garnov N, et al. Software for auto- mated MRI-based quantification of abdominal fat and preliminary evaluation in morbidly obese patients. J Magn Reson Imaging. 2013;37:1144-1150.\n\nAutomatic segmentation of abdominal fat in MRI-Scans, using graph-cuts and image derived energies. A N Christensen, C T Larsen, C M Mandrup, Scandinavian Conference on Image Analysis. Tromso. NorwaySpringer2017Christensen AN, Larsen CT, Mandrup CM, et al. Automatic seg- mentation of abdominal fat in MRI-Scans, using graph-cuts and image derived energies. In: Scandinavian Conference on Image Analysis. Tromso, Norway: Springer; 2017:109-120.\n\nAutomated segmentation of visceral and subcutaneous (deep and superficial) adipose tissues in normal and overweight men. S A Sadananthan, B Prakash, Mks Leow, J Magn Reson Imaging. 41Sadananthan SA, Prakash B, Leow MKS, et al. Automated seg- mentation of visceral and subcutaneous (deep and superficial) adipose tissues in normal and overweight men. J Magn Reson Imaging. 2015;41:924-934.\n\nAutomatic segmentation of abdominal adipose tissue in MRI. T H Mosbech, K Pilgaard, A Vaag, R Larsen, Scandinavian Conference on Image Analysis. Ystad, SwedenSpringerMosbech TH, Pilgaard K, Vaag A, Larsen R. Automatic seg- mentation of abdominal adipose tissue in MRI. In: Scandinavian Conference on Image Analysis. Ystad, Sweden: Springer; 2011: 501-511.\n\nAutomatic quantification of subcutaneous and visceral adipose tissue from whole-body magnetic resonance images suitable for large cohort studies. D Wald, B Teucher, J Dinkel, J Magn Reson Imaging. 36Wald D, Teucher B, Dinkel J, et al. Automatic quantification of subcutaneous and visceral adipose tissue from whole-body mag- netic resonance images suitable for large cohort studies. J Magn Reson Imaging. 2012;36:1421-1434.\n\nSegnet: a deep convolutional encoder-decoder architecture for image segmentation. V Badrinarayanan, A Kendall, R Cipolla, IEEE Trans Pattern Anal Mach Intelligence. 39Badrinarayanan V, Kendall A, Cipolla R. Segnet: a deep con- volutional encoder-decoder architecture for image segmen- tation. IEEE Trans Pattern Anal Mach Intelligence. 2017;39: 2481-2495.\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionBoston, MALong J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, Boston, MA, 2015. pp. 3431-3440.\n\nFully convolutional networks for automated segmentation of abdominal adipose tissue depots in multicenter water-fat MRI. T Langner, A Hedstr\u00f6m, K M\u00f6rwald, Magn Reson Med. 81Langner T, Hedstr\u00f6m A, M\u00f6rwald K, et al. Fully convolutional networks for automated segmentation of abdominal adipose tis- sue depots in multicenter water-fat MRI. Magn Reson Med. 2019;81:2736-2745.\n\nU-net: convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical image computing and computer-assisted intervention. Munich, GermanySpringer2015Ronneberger O, Fischer P, Brox T. U-net: convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention. Munich, Germany: Springer; 2015:234-241.\n\nError corrective boosting for learning fully convolutional networks with limited data. A G Roy, S Conjeti, D Sheet, A Katouzian, N Navab, C Wachinger, International Conference on Medical Image Computing and Computer-Assisted Intervention. Quebec City, CanadaSpringer2017Roy AG, Conjeti S, Sheet D, Katouzian A, Navab N, Wachinger C. Error corrective boosting for learning fully convolutional networks with limited data. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. Quebec City, Canada: Springer; 2017:231-239.\n\nThe one hundred layers tiramisu: fully convolutional densenets for semantic segmentation. S J\u00e9gou, M Drozdzal, D Vazquez, A Romero, Y Bengio, 2017 IEEE Conference on IEEE Computer Vision and Pattern Recognition Workshops (CVPRW). Honolulu, HIJ\u00e9gou S, Drozdzal M, Vazquez D, Romero A, Bengio Y. The one hundred layers tiramisu: fully convolutional densenets for seman- tic segmentation. In: 2017 IEEE Conference on IEEE Computer Vision and Pattern Recognition Workshops (CVPRW). Honolulu, HI: 2017:1175-1183.\n\nCompetition vs. concatenation in skip connections of fully convolutional networks. S Estrada, S Conjeti, M Ahmad, N Navab, M Reuter, International Workshop on Machine Learning in Medical Imaging. Granada, SpainSpringerEstrada S, Conjeti S, Ahmad M, Navab N, Reuter M. Competition vs. concatenation in skip connections of fully con- volutional networks. In: International Workshop on Machine Learning in Medical Imaging. Granada, Spain: Springer; 2018: 214-222.\n\nMaxout networks. I J Goodfellow, D Warde-Farley, M Mirza, A Courville, Y Bengio, Proceedings of the 30th International Conference on International Conference on Machine Learning. the 30th International Conference on International Conference on Machine Learningorg, Atlanta, GA28III-1319Goodfellow IJ, Warde-Farley D, Mirza M, Courville A, Bengio Y. Maxout networks. In Proceedings of the 30th International Conference on International Conference on Machine Learning- Volume 28 JMLR. org, Atlanta, GA, 2013. pp. III-1319.\n\nA deep convolutional neural network module that promotes competition of multiple-size filters. Z Liao, G Carneiro, Pattern Recognition. 71Liao Z, Carneiro G. A deep convolutional neural network mod- ule that promotes competition of multiple-size filters. Pattern Recognition. 2017;71:94-105.\n\nMRI in the Rhineland study: a novel protocol for population neuroimaging. M M Breteler, T St\u00f6cker, E Pracht, D Brenner, R Stirnberg, Alzheimer's Dementia. 1092Breteler MM, St\u00f6cker T, Pracht E, Brenner D, Stirnberg R. MRI in the Rhineland study: a novel protocol for population neuroimaging. Alzheimer's Dementia. 2014;10:P92.\n\nBig data: the Rhineland study. T St\u00f6cker, Proceedings of the 24th Scientific Meeting of the International Society for Magnetic Resonance in Medicine. the 24th Scientific Meeting of the International Society for Magnetic Resonance in MedicineSingaporeSt\u00f6cker T. Big data: the Rhineland study. In Proceedings of the 24th Scientific Meeting of the International Society for Magnetic Resonance in Medicine (Singapore); 2016. https ://index.miras mart. com/ISMRM 2016/PDFfi les/6865.html.\n\nQuickNAT: a fully convolutional network for quick and accurate segmentation of neuroanatomy. A G Roy, S Conjeti, N Navab, NeuroImage. 186Roy AG, Conjeti S, Navab N, et al. QuickNAT: a fully convolu- tional network for quick and accurate segmentation of neuroanat- omy. NeuroImage. 2019;186:713-727.\n\nDensely connected convolutional networks. G Huang, Z Liu, L Van Der Maaten, K Q Weinberger, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Honolulu, HIIEEE2017Huang G, Liu Z, Van Der Maaten L, Weinberger KQ. Densely connected convolutional networks. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Honolulu, HI: IEEE; 2017:2261-2269.\n\nIndividual comparisons by ranking methods. F Wilcoxon, Biometrics Bull. 1Wilcoxon F. Individual comparisons by ranking methods. Biometrics Bull. 1945;1:80-83.\n\nAdaptive linear step-up procedures that control the false discovery rate. Y Benjamini, A M Krieger, D Yekutieli, Biometrika. 93Benjamini Y, Krieger AM, Yekutieli D. Adaptive linear step-up procedures that control the false discovery rate. Biometrika. 2006;93:491-507.\n\n. F Chollet, Chollet F, et al. Keras; 2015. https://keras.io/getting-started/faq/ #how-should-i-cite-keras.\n\nForming inferences about some intraclass correlation coefficients. K O Mcgraw, S P Wong, Psychol Methods. 1McGraw KO, Wong SP. Forming inferences about some intraclass correlation coefficients. Psychol Methods. 1996;1:30-46.\n\nR: a language and environment for statistical computing. R Foundation for Statistical Computing. R Core Team, Vienna, AustriaR Core Team. R: a language and environment for statistical com- puting. R Foundation for Statistical Computing, Vienna, Austria; 2013, http://www.R-proje ct.org/\n\nAutomated and reproducible segmentation of visceral and subcutaneous adipose tissue from abdominal MRI. J Kullberg, H Ahlstr\u00f6m, L Johansson, H Frimmel, Int J Obesity. 31Kullberg J, Ahlstr\u00f6m H, Johansson L, Frimmel H. Automated and reproducible segmentation of visceral and subcutaneous adipose tissue from abdominal MRI. Int J Obesity. 2007;31:1806-1817.\n\nValidation of volumetric and single-slice MRI adipose analysis using a novel fully automated segmentation method. B T Addeman, S Kutty, T G Perkins, J Magn Reson Imaging. 41Addeman BT, Kutty S, Perkins TG, et al. Validation of volumetric and single-slice MRI adipose analysis using a novel fully automated segmentation method. J Magn Reson Imaging. 2015;41:233-241.\n\nAutomated quantification of abdominal adiposity by magnetic resonance imaging. J Sun, B Xu, J Freeland-Graves, Am J Human Biol. 28Sun J, Xu B, Freeland-Graves J. Automated quantification of ab- dominal adiposity by magnetic resonance imaging. Am J Human Biol. 2016;28:757-766.\n\nAge and gender related effects on adipose tissue compartments of subjects with increased risk for type 2 diabetes: a whole body MRI/MRS study. J Machann, C Thamer, B Schnoedt, Magn Reson Mater Phys Biol Med. 18Machann J, Thamer C, Schnoedt B, et al. Age and gender related effects on adipose tissue compartments of subjects with increased risk for type 2 diabetes: a whole body MRI/MRS study. Magn Reson Mater Phys Biol Med. 2005;18:128-137.\n\nWaist circumference and abdominal adipose tissue distribution: influence of age and sex. J L Kuk, S Lee, S B Heymsfield, R Ross, Am J Clinical Nutrition. 81Kuk JL, Lee S, Heymsfield SB, Ross R. Waist circumference and abdominal adipose tissue distribution: influence of age and sex-. Am J Clinical Nutrition. 2005;81:1330-1334.\n", "annotations": {"author": "[{\"end\":388,\"start\":174},{\"end\":488,\"start\":389},{\"end\":587,\"start\":489},{\"end\":699,\"start\":588},{\"end\":812,\"start\":700},{\"end\":1046,\"start\":813},{\"end\":1344,\"start\":1047},{\"end\":1369,\"start\":1345},{\"end\":1394,\"start\":1370}]", "publisher": null, "author_last_name": "[{\"end\":190,\"start\":183},{\"end\":395,\"start\":393},{\"end\":506,\"start\":491},{\"end\":606,\"start\":595},{\"end\":719,\"start\":706},{\"end\":833,\"start\":825},{\"end\":1060,\"start\":1054},{\"end\":1368,\"start\":1354},{\"end\":1393,\"start\":1386}]", "author_first_name": "[{\"end\":182,\"start\":174},{\"end\":392,\"start\":389},{\"end\":490,\"start\":489},{\"end\":594,\"start\":588},{\"end\":705,\"start\":700},{\"end\":820,\"start\":813},{\"end\":824,\"start\":821},{\"end\":1053,\"start\":1047},{\"end\":1353,\"start\":1345}]", "author_affiliation": "[{\"end\":295,\"start\":217},{\"end\":387,\"start\":297},{\"end\":487,\"start\":397},{\"end\":586,\"start\":508},{\"end\":698,\"start\":608},{\"end\":811,\"start\":721},{\"end\":925,\"start\":835},{\"end\":1045,\"start\":927},{\"end\":1140,\"start\":1062},{\"end\":1236,\"start\":1142},{\"end\":1343,\"start\":1238}]", "title": "[{\"end\":167,\"start\":1},{\"end\":1561,\"start\":1395}]", "venue": "[{\"end\":1577,\"start\":1563}]", "abstract": "[{\"end\":3654,\"start\":1680}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3858,\"start\":3856},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3859,\"start\":3858},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4207,\"start\":4205},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4208,\"start\":4207},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4433,\"start\":4431},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4434,\"start\":4433},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4598,\"start\":4596},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4599,\"start\":4598},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5504,\"start\":5502},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5527,\"start\":5525},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5541,\"start\":5538},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5543,\"start\":5541},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5570,\"start\":5568},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5603,\"start\":5601},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5661,\"start\":5659},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6126,\"start\":6124},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6142,\"start\":6140},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6269,\"start\":6267},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6563,\"start\":6561},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6726,\"start\":6724},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6815,\"start\":6813},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6967,\"start\":6965},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7103,\"start\":7101},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7184,\"start\":7181},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7186,\"start\":7184},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7557,\"start\":7554},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7559,\"start\":7557},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14268,\"start\":14266},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14368,\"start\":14366},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14440,\"start\":14438},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14470,\"start\":14468},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14740,\"start\":14738},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15045,\"start\":15043},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15241,\"start\":15239},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16016,\"start\":16014},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16058,\"start\":16056},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16121,\"start\":16119},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16514,\"start\":16512},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20768,\"start\":20766},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20779,\"start\":20777},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20798,\"start\":20796},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21597,\"start\":21595},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21670,\"start\":21668},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24171,\"start\":24169},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":32372,\"start\":32369},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":32374,\"start\":32372},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34558,\"start\":34555},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":34560,\"start\":34558},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34738,\"start\":34735},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":34740,\"start\":34738},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":35100,\"start\":35098}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36605,\"start\":36081},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37013,\"start\":36606},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37499,\"start\":37014},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38000,\"start\":37500}]", "paragraph": "[{\"end\":4741,\"start\":3672},{\"end\":7296,\"start\":4743},{\"end\":7683,\"start\":7298},{\"end\":8181,\"start\":7685},{\"end\":8901,\"start\":8183},{\"end\":8927,\"start\":8912},{\"end\":9406,\"start\":8942},{\"end\":10352,\"start\":9408},{\"end\":10717,\"start\":10354},{\"end\":11805,\"start\":10719},{\"end\":12238,\"start\":11826},{\"end\":13484,\"start\":12263},{\"end\":13904,\"start\":13486},{\"end\":15304,\"start\":13987},{\"end\":16406,\"start\":15306},{\"end\":17206,\"start\":16408},{\"end\":17547,\"start\":17259},{\"end\":17998,\"start\":17606},{\"end\":18503,\"start\":18000},{\"end\":19853,\"start\":18532},{\"end\":20004,\"start\":19878},{\"end\":22629,\"start\":20044},{\"end\":23262,\"start\":22656},{\"end\":23472,\"start\":23313},{\"end\":24172,\"start\":23594},{\"end\":24822,\"start\":24245},{\"end\":26951,\"start\":24848},{\"end\":27907,\"start\":26953},{\"end\":28930,\"start\":28032},{\"end\":29671,\"start\":29001},{\"end\":29935,\"start\":29759},{\"end\":30114,\"start\":29937},{\"end\":31486,\"start\":30131},{\"end\":33328,\"start\":31488},{\"end\":34495,\"start\":33330},{\"end\":35470,\"start\":34497},{\"end\":36080,\"start\":35472}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":23593,\"start\":23473}]", "table_ref": "[{\"end\":24784,\"start\":24777},{\"end\":24858,\"start\":24851},{\"end\":26068,\"start\":26060},{\"end\":26402,\"start\":26395},{\"end\":27231,\"start\":27224}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3670,\"start\":3656},{\"end\":8910,\"start\":8904},{\"attributes\":{\"n\":\"2.1.2\"},\"end\":8940,\"start\":8930},{\"end\":11824,\"start\":11808},{\"attributes\":{\"n\":\"2.2\"},\"end\":12261,\"start\":12241},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":13928,\"start\":13907},{\"end\":13985,\"start\":13931},{\"attributes\":{\"n\":\"1.\"},\"end\":17257,\"start\":17209},{\"attributes\":{\"n\":\"2.\"},\"end\":17604,\"start\":17550},{\"end\":18530,\"start\":18506},{\"attributes\":{\"n\":\"2.3\"},\"end\":19876,\"start\":19856},{\"attributes\":{\"n\":\"2.3.1\"},\"end\":20042,\"start\":20007},{\"attributes\":{\"n\":\"2.3.2\"},\"end\":22654,\"start\":22632},{\"attributes\":{\"n\":\"2.3.3\"},\"end\":23293,\"start\":23265},{\"end\":23311,\"start\":23296},{\"attributes\":{\"n\":\"3\"},\"end\":24184,\"start\":24175},{\"attributes\":{\"n\":\"3.1\"},\"end\":24206,\"start\":24187},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":24243,\"start\":24209},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":24846,\"start\":24825},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":27933,\"start\":27910},{\"attributes\":{\"n\":\"3.2\"},\"end\":27982,\"start\":27936},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":28030,\"start\":27985},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":28999,\"start\":28933},{\"end\":29707,\"start\":29674},{\"attributes\":{\"n\":\"3.2.3\"},\"end\":29739,\"start\":29710},{\"end\":29757,\"start\":29742},{\"attributes\":{\"n\":\"4\"},\"end\":30129,\"start\":30117},{\"end\":36083,\"start\":36082},{\"end\":36620,\"start\":36607},{\"end\":37016,\"start\":37015},{\"end\":37505,\"start\":37501}]", "table": null, "figure_caption": "[{\"end\":36605,\"start\":36084},{\"end\":37013,\"start\":36622},{\"end\":37499,\"start\":37017},{\"end\":38000,\"start\":37507}]", "figure_ref": "[{\"end\":11190,\"start\":11181},{\"end\":11706,\"start\":11697},{\"end\":12823,\"start\":12815},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18502,\"start\":18494},{\"end\":19149,\"start\":19140},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21425,\"start\":21417},{\"end\":24504,\"start\":24495},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26950,\"start\":26941},{\"end\":28156,\"start\":28147},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29478,\"start\":29470},{\"end\":29861,\"start\":29853},{\"end\":31958,\"start\":31949},{\"end\":32503,\"start\":32494},{\"end\":32581,\"start\":32567},{\"end\":33532,\"start\":33516},{\"end\":34229,\"start\":34220}]", "bib_author_first_name": "[{\"end\":40200,\"start\":40199},{\"end\":40210,\"start\":40209},{\"end\":40212,\"start\":40211},{\"end\":40222,\"start\":40221},{\"end\":40224,\"start\":40223},{\"end\":40231,\"start\":40230},{\"end\":40233,\"start\":40232},{\"end\":40604,\"start\":40603},{\"end\":40610,\"start\":40609},{\"end\":40621,\"start\":40620},{\"end\":40994,\"start\":40993},{\"end\":40996,\"start\":40995},{\"end\":41008,\"start\":41007},{\"end\":41010,\"start\":41009},{\"end\":41020,\"start\":41019},{\"end\":41034,\"start\":41033},{\"end\":41303,\"start\":41302},{\"end\":41312,\"start\":41311},{\"end\":41321,\"start\":41320},{\"end\":41531,\"start\":41530},{\"end\":41533,\"start\":41532},{\"end\":41750,\"start\":41749},{\"end\":41752,\"start\":41751},{\"end\":41764,\"start\":41763},{\"end\":41778,\"start\":41777},{\"end\":41788,\"start\":41787},{\"end\":41790,\"start\":41789},{\"end\":41799,\"start\":41798},{\"end\":41801,\"start\":41800},{\"end\":41813,\"start\":41812},{\"end\":41815,\"start\":41814},{\"end\":42139,\"start\":42138},{\"end\":42141,\"start\":42140},{\"end\":42152,\"start\":42151},{\"end\":42164,\"start\":42163},{\"end\":42166,\"start\":42165},{\"end\":42176,\"start\":42175},{\"end\":42188,\"start\":42187},{\"end\":42198,\"start\":42197},{\"end\":42511,\"start\":42510},{\"end\":42513,\"start\":42512},{\"end\":42524,\"start\":42523},{\"end\":42765,\"start\":42764},{\"end\":42785,\"start\":42784},{\"end\":42793,\"start\":42792},{\"end\":43098,\"start\":43097},{\"end\":43106,\"start\":43105},{\"end\":43117,\"start\":43116},{\"end\":43403,\"start\":43402},{\"end\":43414,\"start\":43413},{\"end\":43416,\"start\":43415},{\"end\":43427,\"start\":43426},{\"end\":43764,\"start\":43763},{\"end\":43766,\"start\":43765},{\"end\":43781,\"start\":43780},{\"end\":43783,\"start\":43782},{\"end\":43793,\"start\":43792},{\"end\":43795,\"start\":43794},{\"end\":44231,\"start\":44230},{\"end\":44233,\"start\":44232},{\"end\":44248,\"start\":44247},{\"end\":44261,\"start\":44258},{\"end\":44559,\"start\":44558},{\"end\":44561,\"start\":44560},{\"end\":44572,\"start\":44571},{\"end\":44584,\"start\":44583},{\"end\":44592,\"start\":44591},{\"end\":45003,\"start\":45002},{\"end\":45011,\"start\":45010},{\"end\":45022,\"start\":45021},{\"end\":45364,\"start\":45363},{\"end\":45382,\"start\":45381},{\"end\":45393,\"start\":45392},{\"end\":45695,\"start\":45694},{\"end\":45703,\"start\":45702},{\"end\":45716,\"start\":45715},{\"end\":46203,\"start\":46202},{\"end\":46214,\"start\":46213},{\"end\":46226,\"start\":46225},{\"end\":46520,\"start\":46519},{\"end\":46535,\"start\":46534},{\"end\":46546,\"start\":46545},{\"end\":46988,\"start\":46987},{\"end\":46990,\"start\":46989},{\"end\":46997,\"start\":46996},{\"end\":47008,\"start\":47007},{\"end\":47017,\"start\":47016},{\"end\":47030,\"start\":47029},{\"end\":47039,\"start\":47038},{\"end\":47549,\"start\":47548},{\"end\":47558,\"start\":47557},{\"end\":47570,\"start\":47569},{\"end\":47581,\"start\":47580},{\"end\":47591,\"start\":47590},{\"end\":48051,\"start\":48050},{\"end\":48062,\"start\":48061},{\"end\":48073,\"start\":48072},{\"end\":48082,\"start\":48081},{\"end\":48091,\"start\":48090},{\"end\":48447,\"start\":48446},{\"end\":48449,\"start\":48448},{\"end\":48463,\"start\":48462},{\"end\":48479,\"start\":48478},{\"end\":48488,\"start\":48487},{\"end\":48501,\"start\":48500},{\"end\":49047,\"start\":49046},{\"end\":49055,\"start\":49054},{\"end\":49319,\"start\":49318},{\"end\":49321,\"start\":49320},{\"end\":49333,\"start\":49332},{\"end\":49344,\"start\":49343},{\"end\":49354,\"start\":49353},{\"end\":49365,\"start\":49364},{\"end\":49603,\"start\":49602},{\"end\":50150,\"start\":50149},{\"end\":50152,\"start\":50151},{\"end\":50159,\"start\":50158},{\"end\":50170,\"start\":50169},{\"end\":50399,\"start\":50398},{\"end\":50408,\"start\":50407},{\"end\":50415,\"start\":50414},{\"end\":50433,\"start\":50432},{\"end\":50435,\"start\":50434},{\"end\":50788,\"start\":50787},{\"end\":50979,\"start\":50978},{\"end\":50992,\"start\":50991},{\"end\":50994,\"start\":50993},{\"end\":51005,\"start\":51004},{\"end\":51176,\"start\":51175},{\"end\":51350,\"start\":51349},{\"end\":51352,\"start\":51351},{\"end\":51362,\"start\":51361},{\"end\":51364,\"start\":51363},{\"end\":51901,\"start\":51900},{\"end\":51913,\"start\":51912},{\"end\":51925,\"start\":51924},{\"end\":51938,\"start\":51937},{\"end\":52267,\"start\":52266},{\"end\":52269,\"start\":52268},{\"end\":52280,\"start\":52279},{\"end\":52289,\"start\":52288},{\"end\":52291,\"start\":52290},{\"end\":52599,\"start\":52598},{\"end\":52606,\"start\":52605},{\"end\":52612,\"start\":52611},{\"end\":52941,\"start\":52940},{\"end\":52952,\"start\":52951},{\"end\":52962,\"start\":52961},{\"end\":53330,\"start\":53329},{\"end\":53332,\"start\":53331},{\"end\":53339,\"start\":53338},{\"end\":53346,\"start\":53345},{\"end\":53348,\"start\":53347},{\"end\":53362,\"start\":53361}]", "bib_author_last_name": "[{\"end\":40207,\"start\":40201},{\"end\":40219,\"start\":40213},{\"end\":40228,\"start\":40225},{\"end\":40242,\"start\":40234},{\"end\":40607,\"start\":40605},{\"end\":40618,\"start\":40611},{\"end\":40630,\"start\":40622},{\"end\":41005,\"start\":40997},{\"end\":41017,\"start\":41011},{\"end\":41031,\"start\":41021},{\"end\":41040,\"start\":41035},{\"end\":41309,\"start\":41304},{\"end\":41318,\"start\":41313},{\"end\":41326,\"start\":41322},{\"end\":41541,\"start\":41534},{\"end\":41761,\"start\":41753},{\"end\":41775,\"start\":41765},{\"end\":41785,\"start\":41779},{\"end\":41796,\"start\":41791},{\"end\":41810,\"start\":41802},{\"end\":41821,\"start\":41816},{\"end\":42149,\"start\":42142},{\"end\":42161,\"start\":42153},{\"end\":42173,\"start\":42167},{\"end\":42185,\"start\":42177},{\"end\":42195,\"start\":42189},{\"end\":42207,\"start\":42199},{\"end\":42521,\"start\":42514},{\"end\":42532,\"start\":42525},{\"end\":42782,\"start\":42766},{\"end\":42790,\"start\":42786},{\"end\":42801,\"start\":42794},{\"end\":43103,\"start\":43099},{\"end\":43114,\"start\":43107},{\"end\":43122,\"start\":43118},{\"end\":43411,\"start\":43404},{\"end\":43424,\"start\":43417},{\"end\":43434,\"start\":43428},{\"end\":43778,\"start\":43767},{\"end\":43790,\"start\":43784},{\"end\":43803,\"start\":43796},{\"end\":44245,\"start\":44234},{\"end\":44256,\"start\":44249},{\"end\":44266,\"start\":44262},{\"end\":44569,\"start\":44562},{\"end\":44581,\"start\":44573},{\"end\":44589,\"start\":44585},{\"end\":44599,\"start\":44593},{\"end\":45008,\"start\":45004},{\"end\":45019,\"start\":45012},{\"end\":45029,\"start\":45023},{\"end\":45379,\"start\":45365},{\"end\":45390,\"start\":45383},{\"end\":45401,\"start\":45394},{\"end\":45700,\"start\":45696},{\"end\":45713,\"start\":45704},{\"end\":45724,\"start\":45717},{\"end\":46211,\"start\":46204},{\"end\":46223,\"start\":46215},{\"end\":46234,\"start\":46227},{\"end\":46532,\"start\":46521},{\"end\":46543,\"start\":46536},{\"end\":46551,\"start\":46547},{\"end\":46994,\"start\":46991},{\"end\":47005,\"start\":46998},{\"end\":47014,\"start\":47009},{\"end\":47027,\"start\":47018},{\"end\":47036,\"start\":47031},{\"end\":47049,\"start\":47040},{\"end\":47555,\"start\":47550},{\"end\":47567,\"start\":47559},{\"end\":47578,\"start\":47571},{\"end\":47588,\"start\":47582},{\"end\":47598,\"start\":47592},{\"end\":48059,\"start\":48052},{\"end\":48070,\"start\":48063},{\"end\":48079,\"start\":48074},{\"end\":48088,\"start\":48083},{\"end\":48098,\"start\":48092},{\"end\":48460,\"start\":48450},{\"end\":48476,\"start\":48464},{\"end\":48485,\"start\":48480},{\"end\":48498,\"start\":48489},{\"end\":48508,\"start\":48502},{\"end\":49052,\"start\":49048},{\"end\":49064,\"start\":49056},{\"end\":49330,\"start\":49322},{\"end\":49341,\"start\":49334},{\"end\":49351,\"start\":49345},{\"end\":49362,\"start\":49355},{\"end\":49375,\"start\":49366},{\"end\":49611,\"start\":49604},{\"end\":50156,\"start\":50153},{\"end\":50167,\"start\":50160},{\"end\":50176,\"start\":50171},{\"end\":50405,\"start\":50400},{\"end\":50412,\"start\":50409},{\"end\":50430,\"start\":50416},{\"end\":50446,\"start\":50436},{\"end\":50797,\"start\":50789},{\"end\":50989,\"start\":50980},{\"end\":51002,\"start\":50995},{\"end\":51015,\"start\":51006},{\"end\":51184,\"start\":51177},{\"end\":51359,\"start\":51353},{\"end\":51369,\"start\":51365},{\"end\":51616,\"start\":51605},{\"end\":51910,\"start\":51902},{\"end\":51922,\"start\":51914},{\"end\":51935,\"start\":51926},{\"end\":51946,\"start\":51939},{\"end\":52277,\"start\":52270},{\"end\":52286,\"start\":52281},{\"end\":52299,\"start\":52292},{\"end\":52603,\"start\":52600},{\"end\":52609,\"start\":52607},{\"end\":52628,\"start\":52613},{\"end\":52949,\"start\":52942},{\"end\":52959,\"start\":52953},{\"end\":52971,\"start\":52963},{\"end\":53336,\"start\":53333},{\"end\":53343,\"start\":53340},{\"end\":53359,\"start\":53349},{\"end\":53367,\"start\":53363}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":44727563},\"end\":40430,\"start\":40101},{\"attributes\":{\"id\":\"b1\"},\"end\":40888,\"start\":40432},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6897416},\"end\":41240,\"start\":40890},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":29163301},\"end\":41459,\"start\":41242},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":444770},\"end\":41674,\"start\":41461},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":22492269},\"end\":42052,\"start\":41676},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2562582},\"end\":42466,\"start\":42054},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":11944065},\"end\":42637,\"start\":42468},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":24220101},\"end\":43028,\"start\":42639},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":29899817},\"end\":43280,\"start\":43030},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":28780899},\"end\":43662,\"start\":43282},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":215763797},\"end\":44107,\"start\":43664},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7307221},\"end\":44497,\"start\":44109},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10809818},\"end\":44854,\"start\":44499},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":33679550},\"end\":45279,\"start\":44856},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":60814714},\"end\":45636,\"start\":45281},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1629541},\"end\":46079,\"start\":45638},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":49655339},\"end\":46452,\"start\":46081},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3719281},\"end\":46898,\"start\":46454},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":8783493},\"end\":47456,\"start\":46900},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":206597918},\"end\":47965,\"start\":47458},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":49904475},\"end\":48427,\"start\":47967},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10600578},\"end\":48949,\"start\":48429},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":44605320},\"end\":49242,\"start\":48951},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":54368333},\"end\":49569,\"start\":49244},{\"attributes\":{\"id\":\"b25\"},\"end\":50054,\"start\":49571},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":53768212},\"end\":50354,\"start\":50056},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9433631},\"end\":50742,\"start\":50356},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":53662922},\"end\":50902,\"start\":50744},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5639435},\"end\":51171,\"start\":50904},{\"attributes\":{\"id\":\"b30\"},\"end\":51280,\"start\":51173},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":144653780},\"end\":51506,\"start\":51282},{\"attributes\":{\"id\":\"b32\"},\"end\":51794,\"start\":51508},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":23626497},\"end\":52150,\"start\":51796},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":22879092},\"end\":52517,\"start\":52152},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":25181483},\"end\":52795,\"start\":52519},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":24732245},\"end\":53238,\"start\":52797},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":4468521},\"end\":53567,\"start\":53240}]", "bib_title": "[{\"end\":40197,\"start\":40101},{\"end\":40601,\"start\":40432},{\"end\":40991,\"start\":40890},{\"end\":41300,\"start\":41242},{\"end\":41528,\"start\":41461},{\"end\":41747,\"start\":41676},{\"end\":42136,\"start\":42054},{\"end\":42508,\"start\":42468},{\"end\":42762,\"start\":42639},{\"end\":43095,\"start\":43030},{\"end\":43400,\"start\":43282},{\"end\":43761,\"start\":43664},{\"end\":44228,\"start\":44109},{\"end\":44556,\"start\":44499},{\"end\":45000,\"start\":44856},{\"end\":45361,\"start\":45281},{\"end\":45692,\"start\":45638},{\"end\":46200,\"start\":46081},{\"end\":46517,\"start\":46454},{\"end\":46985,\"start\":46900},{\"end\":47546,\"start\":47458},{\"end\":48048,\"start\":47967},{\"end\":48444,\"start\":48429},{\"end\":49044,\"start\":48951},{\"end\":49316,\"start\":49244},{\"end\":49600,\"start\":49571},{\"end\":50147,\"start\":50056},{\"end\":50396,\"start\":50356},{\"end\":50785,\"start\":50744},{\"end\":50976,\"start\":50904},{\"end\":51347,\"start\":51282},{\"end\":51898,\"start\":51796},{\"end\":52264,\"start\":52152},{\"end\":52596,\"start\":52519},{\"end\":52938,\"start\":52797},{\"end\":53327,\"start\":53240}]", "bib_author": "[{\"end\":40209,\"start\":40199},{\"end\":40221,\"start\":40209},{\"end\":40230,\"start\":40221},{\"end\":40244,\"start\":40230},{\"end\":40609,\"start\":40603},{\"end\":40620,\"start\":40609},{\"end\":40632,\"start\":40620},{\"end\":41007,\"start\":40993},{\"end\":41019,\"start\":41007},{\"end\":41033,\"start\":41019},{\"end\":41042,\"start\":41033},{\"end\":41311,\"start\":41302},{\"end\":41320,\"start\":41311},{\"end\":41328,\"start\":41320},{\"end\":41543,\"start\":41530},{\"end\":41763,\"start\":41749},{\"end\":41777,\"start\":41763},{\"end\":41787,\"start\":41777},{\"end\":41798,\"start\":41787},{\"end\":41812,\"start\":41798},{\"end\":41823,\"start\":41812},{\"end\":42151,\"start\":42138},{\"end\":42163,\"start\":42151},{\"end\":42175,\"start\":42163},{\"end\":42187,\"start\":42175},{\"end\":42197,\"start\":42187},{\"end\":42209,\"start\":42197},{\"end\":42523,\"start\":42510},{\"end\":42534,\"start\":42523},{\"end\":42784,\"start\":42764},{\"end\":42792,\"start\":42784},{\"end\":42803,\"start\":42792},{\"end\":43105,\"start\":43097},{\"end\":43116,\"start\":43105},{\"end\":43124,\"start\":43116},{\"end\":43413,\"start\":43402},{\"end\":43426,\"start\":43413},{\"end\":43436,\"start\":43426},{\"end\":43780,\"start\":43763},{\"end\":43792,\"start\":43780},{\"end\":43805,\"start\":43792},{\"end\":44247,\"start\":44230},{\"end\":44258,\"start\":44247},{\"end\":44268,\"start\":44258},{\"end\":44571,\"start\":44558},{\"end\":44583,\"start\":44571},{\"end\":44591,\"start\":44583},{\"end\":44601,\"start\":44591},{\"end\":45010,\"start\":45002},{\"end\":45021,\"start\":45010},{\"end\":45031,\"start\":45021},{\"end\":45381,\"start\":45363},{\"end\":45392,\"start\":45381},{\"end\":45403,\"start\":45392},{\"end\":45702,\"start\":45694},{\"end\":45715,\"start\":45702},{\"end\":45726,\"start\":45715},{\"end\":46213,\"start\":46202},{\"end\":46225,\"start\":46213},{\"end\":46236,\"start\":46225},{\"end\":46534,\"start\":46519},{\"end\":46545,\"start\":46534},{\"end\":46553,\"start\":46545},{\"end\":46996,\"start\":46987},{\"end\":47007,\"start\":46996},{\"end\":47016,\"start\":47007},{\"end\":47029,\"start\":47016},{\"end\":47038,\"start\":47029},{\"end\":47051,\"start\":47038},{\"end\":47557,\"start\":47548},{\"end\":47569,\"start\":47557},{\"end\":47580,\"start\":47569},{\"end\":47590,\"start\":47580},{\"end\":47600,\"start\":47590},{\"end\":48061,\"start\":48050},{\"end\":48072,\"start\":48061},{\"end\":48081,\"start\":48072},{\"end\":48090,\"start\":48081},{\"end\":48100,\"start\":48090},{\"end\":48462,\"start\":48446},{\"end\":48478,\"start\":48462},{\"end\":48487,\"start\":48478},{\"end\":48500,\"start\":48487},{\"end\":48510,\"start\":48500},{\"end\":49054,\"start\":49046},{\"end\":49066,\"start\":49054},{\"end\":49332,\"start\":49318},{\"end\":49343,\"start\":49332},{\"end\":49353,\"start\":49343},{\"end\":49364,\"start\":49353},{\"end\":49377,\"start\":49364},{\"end\":49613,\"start\":49602},{\"end\":50158,\"start\":50149},{\"end\":50169,\"start\":50158},{\"end\":50178,\"start\":50169},{\"end\":50407,\"start\":50398},{\"end\":50414,\"start\":50407},{\"end\":50432,\"start\":50414},{\"end\":50448,\"start\":50432},{\"end\":50799,\"start\":50787},{\"end\":50991,\"start\":50978},{\"end\":51004,\"start\":50991},{\"end\":51017,\"start\":51004},{\"end\":51186,\"start\":51175},{\"end\":51361,\"start\":51349},{\"end\":51371,\"start\":51361},{\"end\":51618,\"start\":51605},{\"end\":51912,\"start\":51900},{\"end\":51924,\"start\":51912},{\"end\":51937,\"start\":51924},{\"end\":51948,\"start\":51937},{\"end\":52279,\"start\":52266},{\"end\":52288,\"start\":52279},{\"end\":52301,\"start\":52288},{\"end\":52605,\"start\":52598},{\"end\":52611,\"start\":52605},{\"end\":52630,\"start\":52611},{\"end\":52951,\"start\":52940},{\"end\":52961,\"start\":52951},{\"end\":52973,\"start\":52961},{\"end\":53338,\"start\":53329},{\"end\":53345,\"start\":53338},{\"end\":53361,\"start\":53345},{\"end\":53369,\"start\":53361}]", "bib_venue": "[{\"end\":43862,\"start\":43856},{\"end\":44657,\"start\":44644},{\"end\":45877,\"start\":45805},{\"end\":46656,\"start\":46641},{\"end\":47158,\"start\":47139},{\"end\":47700,\"start\":47688},{\"end\":48177,\"start\":48163},{\"end\":48705,\"start\":48608},{\"end\":49821,\"start\":49721},{\"end\":50532,\"start\":50520},{\"end\":40255,\"start\":40244},{\"end\":40642,\"start\":40632},{\"end\":41055,\"start\":41042},{\"end\":41335,\"start\":41328},{\"end\":41554,\"start\":41543},{\"end\":41855,\"start\":41823},{\"end\":42250,\"start\":42209},{\"end\":42540,\"start\":42534},{\"end\":42818,\"start\":42803},{\"end\":43144,\"start\":43124},{\"end\":43456,\"start\":43436},{\"end\":43854,\"start\":43805},{\"end\":44288,\"start\":44268},{\"end\":44642,\"start\":44601},{\"end\":45051,\"start\":45031},{\"end\":45444,\"start\":45403},{\"end\":45803,\"start\":45726},{\"end\":46250,\"start\":46236},{\"end\":46639,\"start\":46553},{\"end\":47137,\"start\":47051},{\"end\":47686,\"start\":47600},{\"end\":48161,\"start\":48100},{\"end\":48606,\"start\":48510},{\"end\":49085,\"start\":49066},{\"end\":49397,\"start\":49377},{\"end\":49719,\"start\":49613},{\"end\":50188,\"start\":50178},{\"end\":50518,\"start\":50448},{\"end\":50814,\"start\":50799},{\"end\":51027,\"start\":51017},{\"end\":51386,\"start\":51371},{\"end\":51603,\"start\":51508},{\"end\":51961,\"start\":51948},{\"end\":52321,\"start\":52301},{\"end\":52645,\"start\":52630},{\"end\":53003,\"start\":52973},{\"end\":53392,\"start\":53369}]"}}}, "year": 2023, "month": 12, "day": 17}