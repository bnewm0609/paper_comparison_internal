{"id": 237353356, "updated": "2023-10-05 23:43:50.499", "metadata": {"title": "N24News: A New Dataset for Multimodal News Classification", "authors": "[{\"first\":\"Zhen\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Xu\",\"last\":\"Shan\",\"middle\":[]},{\"first\":\"Xiangxie\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "LREC", "journal": "Proceedings of the Thirteenth Language Resources and Evaluation Conference", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Current news datasets merely focus on text features on the news and rarely leverage the feature of images, excluding numerous essential features for news classification. In this paper, we propose a new dataset, N24News, which is generated from New York Times with 24 categories and contains both text and image information in each news. We use a multitask multimodal method and the experimental results show multimodal news classification performs better than text-only news classification. Depending on the length of the text, the classification accuracy can be increased by up to 8.11%. Our research reveals the relationship between the performance of a multimodal classifier and its sub-classifiers, and also the possible improvements when applying multimodal in news classification. N24News is shown to have great potential to prompt the multimodal news studies.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2108.13327", "mag": null, "acl": "2022.lrec-1.729", "pubmed": null, "pubmedcentral": null, "dblp": "conf/lrec/WangSZY22", "doi": null}}, "content": {"source": {"pdf_hash": "e2d5a60c3932d351ac1d504ef16c0fe705eacec1", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2022.lrec-1.729.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d475e0883a17f260ea5c8d808545c6482718465e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e2d5a60c3932d351ac1d504ef16c0fe705eacec1.txt", "contents": "\nN24News: A New Dataset for Multimodal News Classification\nJune 2022\n\nZhen Wang \nDelft University of Technology\nNetherlands\n\nXu Shan \nDelft University of Technology\nNetherlands\n\nXiangxie Zhang \nDelft University of Technology\nNetherlands\n\nJie Yang \nDelft University of Technology\nNetherlands\n\nN24News: A New Dataset for Multimodal News Classification\n\nProceedings of the 13th Conference on Language Resources and Evaluation (LREC 2022)\nthe 13th Conference on Language Resources and Evaluation (LREC 2022)MarseilleJune 2022Language Resources Association (ELRA), licensed under CC-BY-NC-4.0 6768Multimodal DatasetNews ArticleText Classification\nCurrent news datasets merely focus on text features on the news and rarely leverage the feature of images, excluding numerous essential features for news classification. In this paper, we propose a new dataset, N24News, which is generated from New York Times with 24 categories and contains both text and image information in each news. We use a multitask multimodal method and the experimental results show multimodal news classification performs better than text-only news classification. Depending on the length of the text, the classification accuracy can be increased by up to 8.11%. Our research reveals the relationship between the performance of a multimodal classifier and its sub-classifiers, and also the possible improvements when applying multimodal in news classification. N24News is shown to have great potential to prompt the multimodal news studies.\n\nIntroduction\n\nPeople have tried to use different carriers to record news. Ancient people first drew images by hands-on walls to record important things. After language was invented, words became the main tools for recording. Thanks to parchment preserved to this day, we can study people who lived a long time ago. Later, with the invention of the camera, images are widely used in news. Compared with text, images can bring us more intuitive information, even if we cannot understand the language used in the news. It is safe to say that images and text play an equally important role in news. News classification is one of the essential tasks in news research (Katari and Myneni, 2020). We use the information provided by the news to group them into different categories.There is already some research about news classification, for example, news datasets, such as 20NEWS (Lang, 1995) and AG News (Zhang et al., 2015). However, they choose to ignore the images and merely pay attention to the text. This is not in line with the actual situation, especially when almost all the news today has images. In this work, we aim to use both images and text to achieve better news classification.\n\nIn order to combine heterogeneous information extracted from images and texts, multimodal methods are needed. Multimodal approaches can process various types of information simultaneously and has been used in news studies before. For example, in the fake news dataset Fakeddit (Nakamura et al., 2019), the authors propose a hybrid text+image model to classifier fake news. However, to best of our knowledge, currently there is no valid public news dataset containing enough real news with both images and texts that can be used to do multimodal news classification. Thus, in this work, we use the New York Times to build a new dataset called N24News. N24News is a large-scale multimodal news dataset comprising 60K image-text * Equal Contribution pairs and 24 categories, which makes it possible to do multimodal real news classification tasks. Further, we use a multitask multimodal network to conduct a preliminary experiment in multimodal news classification, and the experiment shows the multimodal method can achieve higher accuracy than text-only news classification. Our error analysis reveals the relationship between the performance of a multimodal classifier and its sub-classifiers, and also the possible improvements when applying multimodal approaches in news classification.\n\n\nRelated Work\n\nIn news studies, the most commonly used datasets are 20NEWS (Lang, 1995) and AG NEWS (Zhang et al., 2015). 20NEWS is a collection of approximately 20,000 newsgroup documents across 20 different newsgroups, and AG News contains 1 million news articles gathered from more than 2000 news sources and grouped into four categories. These two datasets are now used as benchmarks for testing text classification models, such as BERT (Devlin et al., 2018) and XLNet . Multimodal deep learning (Ngiam et al., 2011) is able to leverage different types of features, such as voice, image, and text, to achieve better performance. Nowadays, multimodal methods have been used in lots of tasks, for example, multimodal sentiment analysis (Soleymani et al., 2017), multimodal translation (Sanabria et al., 2018), multimodal emotion recognition (Tzirakis et al., 2017), and multimodal question answering (Yagcioglu et al., 2018). One common multimodal architecture is to use different types of models to process the corresponding input data, such as first using an image classifier to obtain image features, a text classifier to obtain text features, and then combining these features before subsequent processing. In multimodal deep learning, the most critical part is feature fusion. Recent researches have proposed various feature fusion methods (Zhang   et al., 2020a). Concatenation (Nojavanasghari et al., 2016;Anastasopoulos et al., 2019) is the most commonly used method. It splices different features directly along a certain dimension. Further, other fusion methods, for example, weighted-sum and pooling, are also able to achieve good results. Weighted-sum (Vielzeuf et al., 2018) assigns different weights to each feature and sum them up. Pooling (Chao et al., 2015) methods, including max-pooling and average-pooling, are also used in many fusion scenarios, which can find the most important pieces of information in each feature and finally integrate them. Additionally, attentionbased fusion methods (Zhang et al., 2020b;Shih et al., 2016), which using the attention mechanism to let the model learn to automatically find the most crucial part of the feature through training, are playing an increasingly important role in multimodal deep learning tasks. Multimodal methods are also commonly used in news studies. Previous multimodal news researches mainly focus on fake news detection. Nakamura et al. (2019) propose a multimodal fake news dataset from Reddit with six categories according to the degree and type of fake news in the news. Giachanou et al. (2020) use word2vec to extract the news text features and five different image models to extract news image features. Wang et al. (2018) use an adversarial neural network to identify fake news on newly emerged events in online social platforms. Fake news detection is a variant of news classification, which mostly has binary categories (true or false), making the task is not so difficult. Furthermore, there are few studies on the application of multimodal classification focus on real news.\n\nIn that case, we collect and apply multimodal methods on our dataset N24News, which containing massive news images and texts, as well as many different categories, to facilitate the research of multimodal news classification applied in real news study. The code and dataset will be on Github 1 .\n\n\nThe N24News Dataset\n\n\nDataset Collection\n\nThe N24News is extracted from the New York Times. New York Times is an American daily newspaper that was founded in 1851. It publishes worldwide news on various topics every day. Starting from the 2000s, the New York Times fully turned to digitization (P\u00e9rez-Pe\u00f1a, 2008), and previous news was transferred to the Internet to facilitate people's reading and provide internet API for scientific research purposes.\n\nTo build the N24News dataset, we use the API provided by New York Times to obtain all the links published from 2010 to 2020. Then we use these links to retrieve all the actual web pages in the past decade. After analyzing those web pages, we exclude video news, and  only the news articles in text form are retained. While most news has only one image, to better balance the number of images and news, we only choose one image for each news and drop out the news which does not contain any images. All news belongs to 24 different categories. We do not merge similar categories, such as science and technology, arts and theater. To make the dataset more balance, we collect up to 3000 samples for each category. Finally, 60K news articles are collected in total. The amount of each category is shown in Table 1. Each article sample contains one category tag, one headline, one abstract, one article body, one image, and one corresponding image caption. An example is shown in Table 2. We randomly split datasets into training/validation/testing sets in the ratio of 8:1:1. Compared with other multimodal research such as fake news detection, our dataset comes from a professional news website, which ensures the correctness of the dataset, thus no additional manual annotation work is needed.\n\n\nDataset Statistics\n\nIn \n\n\nMultimodal Analysis\n\nText feature in classification task has been well studied before, so we will focus on what the news images in N24News are able to provide to improve the classification results. In Figure 1, we list some image examples of each category. It is obvious that news images are usually closely related to the category they belong to.\n\nTo better understand what can be learned from news images by current image classification models, we use a Faster-RCNN (Ren et al., 2015) model trained on Visual Genome (Krishna et al., 2016), a dataset aiming at providing semantic information from images. We also use a Resnet (He et al., 2016) trained on N24News to reveal the critical part of news images. Two examples are shown in Figure 2. Faster-RCNN extracts the important semantic information in the images, such as ball and books, and Resnet focuses on salient objects: player and book cover. In Figure 2, it is hard to recognize the topic only given the two headlines. The one on the left-hand side may be related to many topics, while the right-hand side one is closer to the topic of opinion. However, with the information obtained by images, we can easily guess that the left one is about a Dataset Size Classes Type Source Topic 20NEWS (Lang, 1995) 20,000 20 text Newsgroup real news AG NEWS (Zhang et al., 2015) 1,000,000 4 text AG News real news Guardian News (Hayat, 2018) 52,900 4 text Guardian News real news Yahoo News  160,515 31 text Yahoo real news BBC News (Kaggle, 2018) 2   sports-related topic, while the right one is about a book. Figure 2 shows that the information in the image can be used to strengthen news classification. Moreover, The information provided by images can also help to distinguish similar categories. There are limited similar categories in previous news datasets. However, in N24News, there are some similar categories, for example, theater and movies. Only with text, it is difficult to tell the story is happening in a theater or on a screen, but images make things much easier. Theater-related images always happened on a stage, but movies not, as shown in Figure 1. This makes a huge difference, and if we can make good use of image information, the classification accuracy will be much higher.\n\n\nChallenges\n\nWhile multimodal data can introduce lots of new information to facilitate the news classification, N24News also releases some new challenges. The biggest challenge is how to better understand news images. Current image classification models are able to extract the features of objects and the relationships between them in images. However, directly using those models to classify news images cannot achieve a strong result because they are mainly designed to classify specific objects, such as cats or dogs, while a news image is more likely to reflect an event. An identical object may have different meanings in different scenarios. For example, the two people in Figure 3 are Facebook and Apple's CEO, thus this image comes from a news related to technology. However, existing image models only recognize there are two people but cannot obtain more meaningful information. Therefore, the features obtained through those models cannot fully reflect the hidden contextual information in news images. We hope N24News can also prompt the research in event image classification, which is a new and challenging field.\n\n\nModel\n\nTo figure out how images can enhance the news classification and the potential challenges when applying the multimodal methods, we use a simple multitask  multimodal network and conduct some experiments on N24News. As illustrated in Figure 4, our model consists of two kinds of feature extraction models. On the bottom is Vision Transformer (ViT) (Dosovitskiy et al., 2020), one of the current state-of-the-art image classification models. Above it is RoBERTa (Liu et al., 2019), one of the current state-of-the-art text classification models. The ViT we use is pre-trained on imagenet2012 (Krizhevsky et al., 2012), consists of a Resnet-50 and a base version of vision transformer with 12 layers transformer encoder. The pre-trained RoBERTa is also a base version and consists 12 layers transformer encoder. We firstly use ViT and RoBERTa to obtain the image feature and text feature separately, where embedding is the embeddings extracted from the original text and image, CLS is a 1D embedding containing the information of its corresponding image or text. Then we concatenate those two kinds of features together. After obtaining the fused feature, we then use three multilayer perceptrons (MLPs) to predict the label for image feature, text feature, fusion feature separately. Finally, the cross-entropy is used to calculate the Loss for each prediction. The final T otalLoss will be the sum of all three types of Loss. When testing on the test set, we only use the output of the fusion feature to calculate the final prediction result.\n\n\nExperiments\n\n\nExperimental Settings\n\nWe trained all the models in the N24News training set and the accuracy is tested on the testing set. Batch size is set to 32 and the learning rate is 1e-5 with an Adam (Kingma and Ba, 2014) optimizer. Each input image is resized to 224 \u00d7 224 and the maximum length of each input text is set to 512. Training device is an NVIDIA Tesla V100 with 16 GB RAM. For each training process, we train the model with the training set, retain the model that performs best on the validation set, and apply it on the testing set.\n\n\nResults\n\nAll the experiment results are shown in Table 4. We firstly classify the images and texts using ViT and RoBERTa respectively. In image classification task the accuracy is only 52.80 in F1, while RoBERTa behaves much better at the news text classification task. The lowest F1 is 70.31 using Headline and the highest F1 is 87.65 with Body. There is a direct correlation between RoBERTa classification accuracy and text length. From Headline, Caption, Abstract to Body, the longer the text length, the higher classification accuracy can be achieved. This is because RoBERTa can better understand the text with more meaningful words. It is found that the multimodal classifier is better than either the image classifier or the text classifier. Even for the Body, the improvement reaches 2.79 in F1 (87.65 vs. 90.44). This is powerful proof that multimodal learning combining image features and text features benefits news classification. And the result also shows that the shorter the text (from body to headline), the more obvious the gain effect of adding image features. In other words, when text contains insufficient information, image is a perfect supplement.\n\n\nError Analysis\n\nTo explore why the multimodal method surpasses the text-only method, we separate the trained baseline model into three types: original multimodal network, image classification network with only the ViT, and text classification network with only the RoBERTa. We then test them in the testing dataset using imageheadline pairs. The experimental results are shown in Table 5. It is evident that when image and text are both correctly classified, the multimodal network can nearly always classify news correctly. The correct-to-incorrect ratio is 42.46:0.03. Additionally, when image and text are both wrongly classified, the multimodal network also tends to be incorrect, but the correct-to-incorrect ratio is 14.22:2.56, much lower than the previous Three True situation. This shows that multimodal network can learn something useful after the features fusion of image and text, which may not be discovered if we process image and text separately. Things are much more complex when only one of the image and the text classifiers is correct. The correct-to-incorrect ratio of multimodal classifier is (27.69+7.01=34.7):(2.40+3.63=6.03) in this situation. This shows that after proper training, the multimodal network will be more affected by the sub-network which can correctly perform the classification task. And this explains why our multimodal method is useful and able to outperform image-only and text-only networks.\n\nThe experiment results can be better understood by the examples in Table 5. It can be observed that images and texts can provide some complementary information. The multimodal method can thus classify news  Table 5: The experiment results with three types of network. True means the classification is correct and False means the classification is incorrect. For each type of prediction, an example of corresponding image-headline pair and its ground truth label is provided. And the Percent represents each result ratio in testing set. more accurately. In the third row, the topic of news may be easily considered about Automobiles if only considered the keyword Shift Gears in text. But when considering the image, the scene described in this image obviously talks about the food, not the car. On the contrary, in the fourth row, a group of people are performing on the stage. It is hard to categorize whether this news article belongs to Dance or Theater without texts. Luckily, the headline directly tells us they are dancing, and this article must belong to Dance. Based on the above analysis, there are two main methods to further improve the performance of multimodal classification networks. The first one is to improve the behavior of each sub-network. If the accuracy (error) of sub-models is higher (lower), the multimodal prediction will also be improved. Our experimental results show that current state-of-the-art image classification models still have a long way to classify all news images correctly. The second method is to let the multimodal classifier be able to determine which sub-classifier extracts the more valuable feature. To do this, a more effective fusion network is needed to better combine image and text features.\n\n\nConclusion\n\nIn this paper, we introduce a multimodal news dataset N24News, which is collected from the New York Times containing both images and texts, enables the multimodal research in real news classification. Compared to previous datasets, it covers almost all the essential news categories in our daily life, making the research on it more applicable to the real world. Based on N24News, we propose a multitask multimodal network, which leverages the current state-of-the-art image classification model and text classification model. Experimental results show that combining image features and text features can achieve better classification accuracy comparing to the previous text-only methods. Our error analysis explains multimodal approach is helpful because the information of images and texts can complement each other. Accordingly, future work on improving the multimodal classification accuracy could include two main aspects: 1) improving image and text classification accuracy separately, especially the news image classification; 2) designing a more effective fusion network to better combine image and text features.\n\nFigure 1 :\n1Image examples of 24 categories.\n\nFigure 3 :\n3Breaking Point: How Mark Zuckerberg and Tim Cook Became Foes\n\nFigure 4 :\n4Overview of our multitask multimodal network.\n\n\nIII, unarmed and physically restrained, was shot in the back by a BART transit officer...Category: Movies \nHeadline: A Man's Death, a Career's Birth \nImage: \n\nCaption: Ryan Coogler on the BART platform at \nFruitvale, where Oscar Grant III was killed. His \nfilm of that story, \"Fruitvale Station,\" opens next \nmonth. \nAbstract:A killing at a Bay Area rapid-transit sta-\ntion has inspired Ryan Coogler's feature-film de-\nbut, a movie already honored at the Sundance and \nCannes film festivals. \nBody: OAKLAND -It had been nearly a year \nsince Ryan Coogler last stood on the arrival plat-\nform on the upper-level of the Fruitvale Bay Area \nRapid Transit Station, where 22-year-old Oscar \nGrant \n\nTable 2 :\n2A sample from N24News.\n\nTable 3 ,\n3we show some information about N24News and other news-related datasets in previous researches. Compare to other datasets, N24News has some unique advantages. Firstly, N24News has 24 categories, which exceeds most of the previous news datasets, especially compare with multimodal datasets. Moreover, there is no valid multimodal news dataset that can be used to do real news classification before N24News. Previous multimodal researches in news classification mainly fo-Figure 2: Visualization of critical parts in images. The news headlines lacks keywords that can be used for classification, but there are in the images. cus on fake news detection with limited categories. The lengths of Headline, Caption, Abstract and Body are 52.33, 115.27, 129.42 and 4701.08 respectively. From   Headline to Body, average lengths are progressively increasing. This allows us to study the gain effect of images on different lengths and different types of text classification tasks with N24News.\n\nTable 3 :\n3Comparison of various news datasets.\n\nTable 4 :\n4The evaluation results on the N24News testing set.\nhttps://github.com/billywzh717/ N24News\n\nA Anastasopoulos, S Kumar, H Liao, arXiv:1903.02930Neural language modeling with visual features. arXiv preprintAnastasopoulos, A., Kumar, S., and Liao, H. (2019). Neural language modeling with visual features. arXiv preprint arXiv:1903.02930.\n\nLong short term memory recurrent neural network based multimodal dimensional emotion recognition. L Chao, J Tao, M Yang, Y Li, Wen , Z , Proceedings of the 5th international workshop on audio/visual emotion challenge. the 5th international workshop on audio/visual emotion challengeChao, L., Tao, J., Yang, M., Li, Y., and Wen, Z. (2015). Long short term memory recurrent neural network based multimodal dimensional emotion recognition. In Proceedings of the 5th international workshop on audio/visual emotion challenge, pages 65-72.\n\nJ Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n\nA Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, arXiv:2010.11929An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprintDosovitskiy, A., Beyer, L., Kolesnikov, A., Weis- senborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. (2020). An image is worth 16x16 words: Transform- ers for image recognition at scale. arXiv preprint arXiv:2010.11929.\n\n. A Giachanou, G Zhang, P Rosso, Giachanou, A., Zhang, G., and Rosso, P. (2020).\n\nMultimodal fake news detection with textual, visual and semantic information. International Conference on Text, Speech, and Dialogue. SpringerMultimodal fake news detection with textual, visual and semantic information. In International Confer- ence on Text, Speech, and Dialogue, pages 30-38. Springer.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHe, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778.\n\nA survey on news classification techniques. R Katari, M B Myneni, 2020 International Conference on Computer Science, Engineering and Applications (ICCSEA). IEEEKatari, R. and Myneni, M. B. (2020). A survey on news classification techniques. In 2020 Interna- tional Conference on Computer Science, Engineer- ing and Applications (ICCSEA), pages 1-5. IEEE.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintKingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\n\nR Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz, S Chen, Y Kalantidis, L.-J Li, D A Shamma, M Bernstein, L Fei-Fei, Visual genome: Connecting language and vision using crowdsourced dense image annotations. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.- J., Shamma, D. A., Bernstein, M., and Fei-Fei, L. (2016). Visual genome: Connecting language and vision using crowdsourced dense image annotations.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. 25Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classification with deep convolutional neu- ral networks. Advances in neural information pro- cessing systems, 25:1097-1105.\n\nY Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. arXiv preprintLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019). Roberta: A robustly op- timized bert pretraining approach. arXiv preprint arXiv:1907.11692.\n\nMultimodal deep learning. J Ngiam, A Khosla, M Kim, J Nam, H Lee, A Y Ng, ICML. Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., and Ng, A. Y. (2011). Multimodal deep learning. In ICML.\n\nDeep multimodal fusion for persuasiveness prediction. B Nojavanasghari, D Gopinath, J Koushik, T Baltru\u0161aitis, L.-P Morency, Proceedings of the 18th ACM International Conference on Multimodal Interaction. the 18th ACM International Conference on Multimodal InteractionNojavanasghari, B., Gopinath, D., Koushik, J., Bal- tru\u0161aitis, T., and Morency, L.-P. (2016). Deep multi- modal fusion for persuasiveness prediction. In Pro- ceedings of the 18th ACM International Conference on Multimodal Interaction, pages 284-288.\n\nTimes plans to combine sections of the paper. The New York Times. R P\u00e9rez-Pe\u00f1a, NewP\u00e9rez-Pe\u00f1a, R. (2008). Times plans to combine sec- tions of the paper. The New York Times. New.\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, Advances in neural information processing systems. Ren, S., He, K., Girshick, R., and Sun, J. (2015). Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural infor- mation processing systems, 201.\n\nWhere to look: Focus regions for visual question answering. K J Shih, S Singh, D Hoiem, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionShih, K. J., Singh, S., and Hoiem, D. (2016). Where to look: Focus regions for visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4613-4621.\n\nA survey of multimodal sentiment analysis. M Soleymani, D Garcia, B Jou, B Schuller, S.-F Chang, M Pantic, Image and Vision Computing. 65Soleymani, M., Garcia, D., Jou, B., Schuller, B., Chang, S.-F., and Pantic, M. (2017). A survey of multimodal sentiment analysis. Image and Vision Computing, 65:3-14.\n\nEnd-to-end multimodal emotion recognition using deep neural networks. P Tzirakis, G Trigeorgis, M A Nicolaou, B W Schuller, S Zafeiriou, IEEE Journal of Selected Topics in Signal Processing. 118Tzirakis, P., Trigeorgis, G., Nicolaou, M. A., Schuller, B. W., and Zafeiriou, S. (2017). End-to-end mul- timodal emotion recognition using deep neural net- works. IEEE Journal of Selected Topics in Signal Processing, 11(8):1301-1309.\n\nCentralnet: a multilayer approach for multimodal fusion. V Vielzeuf, A Lechervy, S Pateux, Jurie , F , Proceedings of the European Conference on Computer Vision (ECCV) Workshops. the European Conference on Computer Vision (ECCV) WorkshopsVielzeuf, V., Lechervy, A., Pateux, S., and Jurie, F. (2018). Centralnet: a multilayer approach for mul- timodal fusion. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, pages 0-0.\n\nEann: Event adversarial neural networks for multi-modal fake news detection. Y Wang, F Ma, Z Jin, Y Yuan, G Xun, K Jha, L Su, J Gao, Proceedings of the 24th acm sigkdd international conference on knowledge discovery & data mining. the 24th acm sigkdd international conference on knowledge discovery & data miningWang, Y., Ma, F., Jin, Z., Yuan, Y., Xun, G., Jha, K., Su, L., and Gao, J. (2018). Eann: Event adversar- ial neural networks for multi-modal fake news de- tection. In Proceedings of the 24th acm sigkdd inter- national conference on knowledge discovery & data mining, pages 849-857.\n\nZ Yang, Z Dai, Y Yang, J Carbonell, R Salakhutdinov, Q V Le, arXiv:1906.08237Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprintYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdi- nov, R., and Le, Q. V. (2019). Xlnet: Generalized autoregressive pretraining for language understand- ing. arXiv preprint arXiv:1906.08237.\n\nMultimodal intelligence: Representation learning, information fusion, and applications. C Zhang, Z Yang, X He, L Deng, IEEE Journal of Selected Topics in Signal Processing. 143Zhang, C., Yang, Z., He, X., and Deng, L. (2020a). Multimodal intelligence: Representation learning, information fusion, and applications. IEEE Journal of Selected Topics in Signal Processing, 14(3):478- 493.\n\nTell and guess: cooperative learning for natural image caption generation with hierarchical refined attention. W Zhang, S Tang, J Su, J Xiao, Y Zhuang, Multimedia Tools and Applications. Zhang, W., Tang, S., Su, J., Xiao, J., and Zhuang, Y. (2020b). Tell and guess: cooperative learning for natural image caption generation with hierarchi- cal refined attention. Multimedia Tools and Applica- tions, pages 1-16.\n\nLanguage Resource References. Language Resource References\n\n. Bondarenko Alexander, M V\u00f6lske, A Panchenko, C Biemann, B Stein, M , Hagen , Alexander, Bondarenko, M., V\u00f6lske, A., Panchenko, C., Biemann, B., Stein, M., and Hagen. (2018).\n\nWebis at trec 2018: Common core track. Webis at trec 2018: Common core track. https: //github.com/irgroup/datasets/ blob/master/WAPost/README.md.\n\nDetection and visualization of misleading content on twitter. C Boididou, S Papadopoulos, M Zampoglou, L Apostolidis, O Papadopoulou, Y Kompatsiaris, International Journal of Multimedia Information Retrieval. 71Boididou, C., Papadopoulos, S., Zampoglou, M., Apostolidis, L., Papadopoulou, O., and Kompat- siaris, Y. (2018). Detection and visualization of mis- leading content on twitter. International Journal of Multimedia Information Retrieval, 7(1):71-86.\n\nGuardian news dataset. S Hayat, Hayat, S. (2018). Guardian news dataset. https: //www.kaggle.com/sameedhayat/ guardian-news-dataset.\n\nBbc news dataset. Kaggle, Kaggle. (2018). Bbc news dataset. https://www. kaggle.com/c/learn-ai-bbc.\n\nNewsweeder: Learning to filter netnews. Ken Lang, ElsevierLang, Ken. (1995). Newsweeder: Learning to filter netnews. Elsevier.\n\nr/fakeddit: A new multimodal benchmark dataset for fine-grained fake news detection. Kai Nakamura, Sharon Levy, William Wang, Yang, Nakamura, Kai and Levy, Sharon and Wang, William Yang. (2019). r/fakeddit: A new multimodal bench- mark dataset for fine-grained fake news detection.\n\nMultimodal news article analysis. A Ramisa Ayats, Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence. the Twenty-Sixth International Joint Conference on Artificial IntelligenceRamisa Ayats, A. (2017). Multimodal news article analysis. In Proceedings of the Twenty-Sixth Inter- national Joint Conference on Artificial Intelligence, pages 5136-5140.\n\nHow2: a large-scale dataset for multimodal language understanding. Ramon Sanabria, Ozan Caglayan, Palaskar, Shruti, Desmond Elliott, Lo\u00efc Barrault, Lucia Specia, Florian Metze, Sanabria, Ramon and Caglayan, Ozan and Palaskar, Shruti and Elliott, Desmond and Barrault, Lo\u00efc and Specia, Lucia and Metze, Florian. (2018). How2: a large-scale dataset for multimodal language under- standing.\n\nRecipeqa: A challenge dataset for multimodal comprehension of cooking recipes. Semih Yagcioglu, Aykut Erdem, Erkut Erdem, Ikizler-Cinbis, Nazli. Yagcioglu, Semih and Erdem, Aykut and Erdem, Erkut and Ikizler-Cinbis, Nazli. (2018). Recipeqa: A challenge dataset for multimodal comprehension of cooking recipes.\n\nRead, attend and comment: A deep architecture for automatic news comment generation. Z Yang, C Xu, W Wu, Li , Z , Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsYang, Z., Xu, C., Wu, W., and Li, Z. (2019). Read, attend and comment: A deep architecture for au- tomatic news comment generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 5076-5088, Hong Kong, China, November. Association for Computa- tional Linguistics.\n\nCharacter-level convolutional networks for text classification. Xiang Zhang, Junbo Zhao, Yann Lecun, Zhang, Xiang and Zhao, Junbo and LeCun, Yann. (2015). Character-level convolutional networks for text classification.\n\nD Zlatkova, P Nakov, I Koychev, arXiv:1908.11722Fact-checking meets fauxtography: Verifying claims about images. arXiv preprintZlatkova, D., Nakov, P., and Koychev, I. (2019). Fact-checking meets fauxtography: Verifying claims about images. arXiv preprint arXiv:1908.11722.\n", "annotations": {"author": "[{\"end\":124,\"start\":70},{\"end\":177,\"start\":125},{\"end\":237,\"start\":178},{\"end\":291,\"start\":238}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":75},{\"end\":132,\"start\":128},{\"end\":192,\"start\":187},{\"end\":246,\"start\":242}]", "author_first_name": "[{\"end\":74,\"start\":70},{\"end\":127,\"start\":125},{\"end\":186,\"start\":178},{\"end\":241,\"start\":238}]", "author_affiliation": "[{\"end\":123,\"start\":81},{\"end\":176,\"start\":134},{\"end\":236,\"start\":194},{\"end\":290,\"start\":248}]", "title": "[{\"end\":58,\"start\":1},{\"end\":349,\"start\":292}]", "venue": "[{\"end\":434,\"start\":351}]", "abstract": "[{\"end\":1508,\"start\":642}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2197,\"start\":2172},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2396,\"start\":2384},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2429,\"start\":2409},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3001,\"start\":2978},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4078,\"start\":4066},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4111,\"start\":4091},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4453,\"start\":4432},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4511,\"start\":4491},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4753,\"start\":4729},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4801,\"start\":4778},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4857,\"start\":4834},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4917,\"start\":4893},{\"end\":5361,\"start\":5347},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5406,\"start\":5377},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5434,\"start\":5406},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5680,\"start\":5657},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5767,\"start\":5748},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6025,\"start\":6004},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6043,\"start\":6025},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6413,\"start\":6391},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6567,\"start\":6544},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6697,\"start\":6679},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9616,\"start\":9598},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9670,\"start\":9648},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9774,\"start\":9757},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10391,\"start\":10379},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10455,\"start\":10435},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10518,\"start\":10505},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10624,\"start\":10610},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12888,\"start\":12862},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12993,\"start\":12975},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13130,\"start\":13105},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14285,\"start\":14264},{\"end\":15434,\"start\":15414}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":20150,\"start\":20105},{\"attributes\":{\"id\":\"fig_1\"},\"end\":20224,\"start\":20151},{\"attributes\":{\"id\":\"fig_2\"},\"end\":20283,\"start\":20225},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":20977,\"start\":20284},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":21012,\"start\":20978},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":22007,\"start\":21013},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":22056,\"start\":22008},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":22119,\"start\":22057}]", "paragraph": "[{\"end\":2699,\"start\":1524},{\"end\":3989,\"start\":2701},{\"end\":7054,\"start\":4006},{\"end\":7351,\"start\":7056},{\"end\":7807,\"start\":7396},{\"end\":9101,\"start\":7809},{\"end\":9127,\"start\":9124},{\"end\":9477,\"start\":9151},{\"end\":11376,\"start\":9479},{\"end\":12505,\"start\":11391},{\"end\":14056,\"start\":12515},{\"end\":14611,\"start\":14096},{\"end\":15784,\"start\":14623},{\"end\":17222,\"start\":15803},{\"end\":18968,\"start\":17224},{\"end\":20104,\"start\":18983}]", "formula": null, "table_ref": "[{\"end\":8619,\"start\":8612},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":8792,\"start\":8785},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":14670,\"start\":14663},{\"end\":16174,\"start\":16167},{\"end\":17298,\"start\":17291},{\"end\":17438,\"start\":17431}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1522,\"start\":1510},{\"attributes\":{\"n\":\"2.\"},\"end\":4004,\"start\":3992},{\"attributes\":{\"n\":\"3.\"},\"end\":7373,\"start\":7354},{\"attributes\":{\"n\":\"3.1.\"},\"end\":7394,\"start\":7376},{\"attributes\":{\"n\":\"3.2.\"},\"end\":9122,\"start\":9104},{\"attributes\":{\"n\":\"3.3.\"},\"end\":9149,\"start\":9130},{\"attributes\":{\"n\":\"3.4.\"},\"end\":11389,\"start\":11379},{\"attributes\":{\"n\":\"4.\"},\"end\":12513,\"start\":12508},{\"attributes\":{\"n\":\"5.\"},\"end\":14070,\"start\":14059},{\"attributes\":{\"n\":\"5.1.\"},\"end\":14094,\"start\":14073},{\"attributes\":{\"n\":\"5.2.\"},\"end\":14621,\"start\":14614},{\"attributes\":{\"n\":\"5.3.\"},\"end\":15801,\"start\":15787},{\"attributes\":{\"n\":\"6.\"},\"end\":18981,\"start\":18971},{\"end\":20116,\"start\":20106},{\"end\":20162,\"start\":20152},{\"end\":20236,\"start\":20226},{\"end\":20988,\"start\":20979},{\"end\":21023,\"start\":21014},{\"end\":22018,\"start\":22009},{\"end\":22067,\"start\":22058}]", "table": "[{\"end\":20977,\"start\":20375}]", "figure_caption": "[{\"end\":20150,\"start\":20118},{\"end\":20224,\"start\":20164},{\"end\":20283,\"start\":20238},{\"end\":20375,\"start\":20286},{\"end\":21012,\"start\":20990},{\"end\":22007,\"start\":21025},{\"end\":22056,\"start\":22020},{\"end\":22119,\"start\":22069}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9339,\"start\":9331},{\"end\":9872,\"start\":9864},{\"end\":10042,\"start\":10034},{\"end\":10696,\"start\":10688},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11246,\"start\":11238},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12065,\"start\":12057},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12756,\"start\":12748}]", "bib_author_first_name": "[{\"end\":22162,\"start\":22161},{\"end\":22180,\"start\":22179},{\"end\":22189,\"start\":22188},{\"end\":22505,\"start\":22504},{\"end\":22513,\"start\":22512},{\"end\":22520,\"start\":22519},{\"end\":22528,\"start\":22527},{\"end\":22536,\"start\":22533},{\"end\":22540,\"start\":22539},{\"end\":22942,\"start\":22941},{\"end\":22955,\"start\":22951},{\"end\":22964,\"start\":22963},{\"end\":22971,\"start\":22970},{\"end\":23273,\"start\":23272},{\"end\":23288,\"start\":23287},{\"end\":23297,\"start\":23296},{\"end\":23311,\"start\":23310},{\"end\":23326,\"start\":23325},{\"end\":23334,\"start\":23333},{\"end\":23349,\"start\":23348},{\"end\":23361,\"start\":23360},{\"end\":23373,\"start\":23372},{\"end\":23384,\"start\":23383},{\"end\":23770,\"start\":23769},{\"end\":23783,\"start\":23782},{\"end\":23792,\"start\":23791},{\"end\":24201,\"start\":24200},{\"end\":24207,\"start\":24206},{\"end\":24216,\"start\":24215},{\"end\":24223,\"start\":24222},{\"end\":24609,\"start\":24608},{\"end\":24619,\"start\":24618},{\"end\":24621,\"start\":24620},{\"end\":24965,\"start\":24964},{\"end\":24967,\"start\":24966},{\"end\":24977,\"start\":24976},{\"end\":25122,\"start\":25121},{\"end\":25133,\"start\":25132},{\"end\":25140,\"start\":25139},{\"end\":25149,\"start\":25148},{\"end\":25160,\"start\":25159},{\"end\":25168,\"start\":25167},{\"end\":25179,\"start\":25178},{\"end\":25187,\"start\":25186},{\"end\":25204,\"start\":25200},{\"end\":25210,\"start\":25209},{\"end\":25212,\"start\":25211},{\"end\":25222,\"start\":25221},{\"end\":25235,\"start\":25234},{\"end\":25653,\"start\":25652},{\"end\":25667,\"start\":25666},{\"end\":25680,\"start\":25679},{\"end\":25682,\"start\":25681},{\"end\":25937,\"start\":25936},{\"end\":25944,\"start\":25943},{\"end\":25951,\"start\":25950},{\"end\":25960,\"start\":25959},{\"end\":25966,\"start\":25965},{\"end\":25975,\"start\":25974},{\"end\":25983,\"start\":25982},{\"end\":25991,\"start\":25990},{\"end\":26000,\"start\":25999},{\"end\":26015,\"start\":26014},{\"end\":26354,\"start\":26353},{\"end\":26363,\"start\":26362},{\"end\":26373,\"start\":26372},{\"end\":26380,\"start\":26379},{\"end\":26387,\"start\":26386},{\"end\":26394,\"start\":26393},{\"end\":26396,\"start\":26395},{\"end\":26570,\"start\":26569},{\"end\":26588,\"start\":26587},{\"end\":26600,\"start\":26599},{\"end\":26611,\"start\":26610},{\"end\":26630,\"start\":26626},{\"end\":27101,\"start\":27100},{\"end\":27295,\"start\":27294},{\"end\":27302,\"start\":27301},{\"end\":27308,\"start\":27307},{\"end\":27320,\"start\":27319},{\"end\":27628,\"start\":27627},{\"end\":27630,\"start\":27629},{\"end\":27638,\"start\":27637},{\"end\":27647,\"start\":27646},{\"end\":28046,\"start\":28045},{\"end\":28059,\"start\":28058},{\"end\":28069,\"start\":28068},{\"end\":28076,\"start\":28075},{\"end\":28091,\"start\":28087},{\"end\":28100,\"start\":28099},{\"end\":28378,\"start\":28377},{\"end\":28390,\"start\":28389},{\"end\":28404,\"start\":28403},{\"end\":28406,\"start\":28405},{\"end\":28418,\"start\":28417},{\"end\":28420,\"start\":28419},{\"end\":28432,\"start\":28431},{\"end\":28795,\"start\":28794},{\"end\":28807,\"start\":28806},{\"end\":28819,\"start\":28818},{\"end\":28833,\"start\":28828},{\"end\":28837,\"start\":28836},{\"end\":29265,\"start\":29264},{\"end\":29273,\"start\":29272},{\"end\":29279,\"start\":29278},{\"end\":29286,\"start\":29285},{\"end\":29294,\"start\":29293},{\"end\":29301,\"start\":29300},{\"end\":29308,\"start\":29307},{\"end\":29314,\"start\":29313},{\"end\":29783,\"start\":29782},{\"end\":29791,\"start\":29790},{\"end\":29798,\"start\":29797},{\"end\":29806,\"start\":29805},{\"end\":29819,\"start\":29818},{\"end\":29836,\"start\":29835},{\"end\":29838,\"start\":29837},{\"end\":30233,\"start\":30232},{\"end\":30242,\"start\":30241},{\"end\":30250,\"start\":30249},{\"end\":30256,\"start\":30255},{\"end\":30642,\"start\":30641},{\"end\":30651,\"start\":30650},{\"end\":30659,\"start\":30658},{\"end\":30665,\"start\":30664},{\"end\":30673,\"start\":30672},{\"end\":31015,\"start\":31005},{\"end\":31028,\"start\":31027},{\"end\":31038,\"start\":31037},{\"end\":31051,\"start\":31050},{\"end\":31062,\"start\":31061},{\"end\":31071,\"start\":31070},{\"end\":31079,\"start\":31074},{\"end\":31390,\"start\":31389},{\"end\":31402,\"start\":31401},{\"end\":31418,\"start\":31417},{\"end\":31431,\"start\":31430},{\"end\":31446,\"start\":31445},{\"end\":31462,\"start\":31461},{\"end\":31811,\"start\":31810},{\"end\":32065,\"start\":32062},{\"end\":32238,\"start\":32235},{\"end\":32255,\"start\":32249},{\"end\":32269,\"start\":32262},{\"end\":32468,\"start\":32467},{\"end\":32893,\"start\":32888},{\"end\":32908,\"start\":32904},{\"end\":32944,\"start\":32937},{\"end\":32958,\"start\":32954},{\"end\":32974,\"start\":32969},{\"end\":32990,\"start\":32983},{\"end\":33294,\"start\":33289},{\"end\":33311,\"start\":33306},{\"end\":33324,\"start\":33319},{\"end\":33607,\"start\":33606},{\"end\":33615,\"start\":33614},{\"end\":33621,\"start\":33620},{\"end\":33628,\"start\":33626},{\"end\":33632,\"start\":33631},{\"end\":34505,\"start\":34500},{\"end\":34518,\"start\":34513},{\"end\":34529,\"start\":34525},{\"end\":34657,\"start\":34656},{\"end\":34669,\"start\":34668},{\"end\":34678,\"start\":34677}]", "bib_author_last_name": "[{\"end\":22177,\"start\":22163},{\"end\":22186,\"start\":22181},{\"end\":22194,\"start\":22190},{\"end\":22510,\"start\":22506},{\"end\":22517,\"start\":22514},{\"end\":22525,\"start\":22521},{\"end\":22531,\"start\":22529},{\"end\":22949,\"start\":22943},{\"end\":22961,\"start\":22956},{\"end\":22968,\"start\":22965},{\"end\":22981,\"start\":22972},{\"end\":23285,\"start\":23274},{\"end\":23294,\"start\":23289},{\"end\":23308,\"start\":23298},{\"end\":23323,\"start\":23312},{\"end\":23331,\"start\":23327},{\"end\":23346,\"start\":23335},{\"end\":23358,\"start\":23350},{\"end\":23370,\"start\":23362},{\"end\":23381,\"start\":23374},{\"end\":23390,\"start\":23385},{\"end\":23780,\"start\":23771},{\"end\":23789,\"start\":23784},{\"end\":23798,\"start\":23793},{\"end\":24204,\"start\":24202},{\"end\":24213,\"start\":24208},{\"end\":24220,\"start\":24217},{\"end\":24227,\"start\":24224},{\"end\":24616,\"start\":24610},{\"end\":24628,\"start\":24622},{\"end\":24974,\"start\":24968},{\"end\":24980,\"start\":24978},{\"end\":25130,\"start\":25123},{\"end\":25137,\"start\":25134},{\"end\":25146,\"start\":25141},{\"end\":25157,\"start\":25150},{\"end\":25165,\"start\":25161},{\"end\":25176,\"start\":25169},{\"end\":25184,\"start\":25180},{\"end\":25198,\"start\":25188},{\"end\":25207,\"start\":25205},{\"end\":25219,\"start\":25213},{\"end\":25232,\"start\":25223},{\"end\":25243,\"start\":25236},{\"end\":25664,\"start\":25654},{\"end\":25677,\"start\":25668},{\"end\":25689,\"start\":25683},{\"end\":25941,\"start\":25938},{\"end\":25948,\"start\":25945},{\"end\":25957,\"start\":25952},{\"end\":25963,\"start\":25961},{\"end\":25972,\"start\":25967},{\"end\":25980,\"start\":25976},{\"end\":25988,\"start\":25984},{\"end\":25997,\"start\":25992},{\"end\":26012,\"start\":26001},{\"end\":26024,\"start\":26016},{\"end\":26360,\"start\":26355},{\"end\":26370,\"start\":26364},{\"end\":26377,\"start\":26374},{\"end\":26384,\"start\":26381},{\"end\":26391,\"start\":26388},{\"end\":26399,\"start\":26397},{\"end\":26585,\"start\":26571},{\"end\":26597,\"start\":26589},{\"end\":26608,\"start\":26601},{\"end\":26624,\"start\":26612},{\"end\":26638,\"start\":26631},{\"end\":27112,\"start\":27102},{\"end\":27299,\"start\":27296},{\"end\":27305,\"start\":27303},{\"end\":27317,\"start\":27309},{\"end\":27324,\"start\":27321},{\"end\":27635,\"start\":27631},{\"end\":27644,\"start\":27639},{\"end\":27653,\"start\":27648},{\"end\":28056,\"start\":28047},{\"end\":28066,\"start\":28060},{\"end\":28073,\"start\":28070},{\"end\":28085,\"start\":28077},{\"end\":28097,\"start\":28092},{\"end\":28107,\"start\":28101},{\"end\":28387,\"start\":28379},{\"end\":28401,\"start\":28391},{\"end\":28415,\"start\":28407},{\"end\":28429,\"start\":28421},{\"end\":28442,\"start\":28433},{\"end\":28804,\"start\":28796},{\"end\":28816,\"start\":28808},{\"end\":28826,\"start\":28820},{\"end\":29270,\"start\":29266},{\"end\":29276,\"start\":29274},{\"end\":29283,\"start\":29280},{\"end\":29291,\"start\":29287},{\"end\":29298,\"start\":29295},{\"end\":29305,\"start\":29302},{\"end\":29311,\"start\":29309},{\"end\":29318,\"start\":29315},{\"end\":29788,\"start\":29784},{\"end\":29795,\"start\":29792},{\"end\":29803,\"start\":29799},{\"end\":29816,\"start\":29807},{\"end\":29833,\"start\":29820},{\"end\":29841,\"start\":29839},{\"end\":30239,\"start\":30234},{\"end\":30247,\"start\":30243},{\"end\":30253,\"start\":30251},{\"end\":30261,\"start\":30257},{\"end\":30648,\"start\":30643},{\"end\":30656,\"start\":30652},{\"end\":30662,\"start\":30660},{\"end\":30670,\"start\":30666},{\"end\":30680,\"start\":30674},{\"end\":31025,\"start\":31016},{\"end\":31035,\"start\":31029},{\"end\":31048,\"start\":31039},{\"end\":31059,\"start\":31052},{\"end\":31068,\"start\":31063},{\"end\":31399,\"start\":31391},{\"end\":31415,\"start\":31403},{\"end\":31428,\"start\":31419},{\"end\":31443,\"start\":31432},{\"end\":31459,\"start\":31447},{\"end\":31475,\"start\":31463},{\"end\":31817,\"start\":31812},{\"end\":31945,\"start\":31939},{\"end\":32070,\"start\":32066},{\"end\":32247,\"start\":32239},{\"end\":32260,\"start\":32256},{\"end\":32274,\"start\":32270},{\"end\":32280,\"start\":32276},{\"end\":32481,\"start\":32469},{\"end\":32902,\"start\":32894},{\"end\":32917,\"start\":32909},{\"end\":32927,\"start\":32919},{\"end\":32935,\"start\":32929},{\"end\":32952,\"start\":32945},{\"end\":32967,\"start\":32959},{\"end\":32981,\"start\":32975},{\"end\":32996,\"start\":32991},{\"end\":33304,\"start\":33295},{\"end\":33317,\"start\":33312},{\"end\":33330,\"start\":33325},{\"end\":33346,\"start\":33332},{\"end\":33612,\"start\":33608},{\"end\":33618,\"start\":33616},{\"end\":33624,\"start\":33622},{\"end\":34511,\"start\":34506},{\"end\":34523,\"start\":34519},{\"end\":34535,\"start\":34530},{\"end\":34666,\"start\":34658},{\"end\":34675,\"start\":34670},{\"end\":34686,\"start\":34679}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1903.02930\",\"id\":\"b0\"},\"end\":22404,\"start\":22161},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":17206350},\"end\":22939,\"start\":22406},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b2\"},\"end\":23270,\"start\":22941},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b3\"},\"end\":23765,\"start\":23272},{\"attributes\":{\"id\":\"b4\"},\"end\":23847,\"start\":23767},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":221400004},\"end\":24152,\"start\":23849},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206594692},\"end\":24562,\"start\":24154},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":220367010},\"end\":24918,\"start\":24564},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b8\"},\"end\":25119,\"start\":24920},{\"attributes\":{\"id\":\"b9\"},\"end\":25585,\"start\":25121},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":195908774},\"end\":25934,\"start\":25587},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b11\"},\"end\":26325,\"start\":25936},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":352650},\"end\":26513,\"start\":26327},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":765502},\"end\":27032,\"start\":26515},{\"attributes\":{\"id\":\"b14\"},\"end\":27212,\"start\":27034},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":10328909},\"end\":27565,\"start\":27214},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11923637},\"end\":28000,\"start\":27567},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":19491070},\"end\":28305,\"start\":28002},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3302908},\"end\":28735,\"start\":28307},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":52069847},\"end\":29185,\"start\":28737},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":46990556},\"end\":29780,\"start\":29187},{\"attributes\":{\"doi\":\"arXiv:1906.08237\",\"id\":\"b21\"},\"end\":30142,\"start\":29782},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":207852771},\"end\":30528,\"start\":30144},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":214647826},\"end\":30941,\"start\":30530},{\"attributes\":{\"id\":\"b24\"},\"end\":31001,\"start\":30943},{\"attributes\":{\"id\":\"b25\"},\"end\":31178,\"start\":31003},{\"attributes\":{\"id\":\"b26\"},\"end\":31325,\"start\":31180},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3381583},\"end\":31785,\"start\":31327},{\"attributes\":{\"id\":\"b28\"},\"end\":31919,\"start\":31787},{\"attributes\":{\"id\":\"b29\"},\"end\":32020,\"start\":31921},{\"attributes\":{\"id\":\"b30\"},\"end\":32148,\"start\":32022},{\"attributes\":{\"id\":\"b31\"},\"end\":32431,\"start\":32150},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":31100729},\"end\":32819,\"start\":32433},{\"attributes\":{\"id\":\"b33\"},\"end\":33208,\"start\":32821},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":52156147},\"end\":33519,\"start\":33210},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":202783907},\"end\":34434,\"start\":33521},{\"attributes\":{\"id\":\"b36\"},\"end\":34654,\"start\":34436},{\"attributes\":{\"doi\":\"arXiv:1908.11722\",\"id\":\"b37\"},\"end\":34929,\"start\":34656}]", "bib_title": "[{\"end\":22502,\"start\":22406},{\"end\":23925,\"start\":23849},{\"end\":24198,\"start\":24154},{\"end\":24606,\"start\":24564},{\"end\":25650,\"start\":25587},{\"end\":26351,\"start\":26327},{\"end\":26567,\"start\":26515},{\"end\":27292,\"start\":27214},{\"end\":27625,\"start\":27567},{\"end\":28043,\"start\":28002},{\"end\":28375,\"start\":28307},{\"end\":28792,\"start\":28737},{\"end\":29262,\"start\":29187},{\"end\":30230,\"start\":30144},{\"end\":30639,\"start\":30530},{\"end\":31387,\"start\":31327},{\"end\":32465,\"start\":32433},{\"end\":33287,\"start\":33210},{\"end\":33604,\"start\":33521}]", "bib_author": "[{\"end\":22179,\"start\":22161},{\"end\":22188,\"start\":22179},{\"end\":22196,\"start\":22188},{\"end\":22512,\"start\":22504},{\"end\":22519,\"start\":22512},{\"end\":22527,\"start\":22519},{\"end\":22533,\"start\":22527},{\"end\":22539,\"start\":22533},{\"end\":22543,\"start\":22539},{\"end\":22951,\"start\":22941},{\"end\":22963,\"start\":22951},{\"end\":22970,\"start\":22963},{\"end\":22983,\"start\":22970},{\"end\":23287,\"start\":23272},{\"end\":23296,\"start\":23287},{\"end\":23310,\"start\":23296},{\"end\":23325,\"start\":23310},{\"end\":23333,\"start\":23325},{\"end\":23348,\"start\":23333},{\"end\":23360,\"start\":23348},{\"end\":23372,\"start\":23360},{\"end\":23383,\"start\":23372},{\"end\":23392,\"start\":23383},{\"end\":23782,\"start\":23769},{\"end\":23791,\"start\":23782},{\"end\":23800,\"start\":23791},{\"end\":24206,\"start\":24200},{\"end\":24215,\"start\":24206},{\"end\":24222,\"start\":24215},{\"end\":24229,\"start\":24222},{\"end\":24618,\"start\":24608},{\"end\":24630,\"start\":24618},{\"end\":24976,\"start\":24964},{\"end\":24982,\"start\":24976},{\"end\":25132,\"start\":25121},{\"end\":25139,\"start\":25132},{\"end\":25148,\"start\":25139},{\"end\":25159,\"start\":25148},{\"end\":25167,\"start\":25159},{\"end\":25178,\"start\":25167},{\"end\":25186,\"start\":25178},{\"end\":25200,\"start\":25186},{\"end\":25209,\"start\":25200},{\"end\":25221,\"start\":25209},{\"end\":25234,\"start\":25221},{\"end\":25245,\"start\":25234},{\"end\":25666,\"start\":25652},{\"end\":25679,\"start\":25666},{\"end\":25691,\"start\":25679},{\"end\":25943,\"start\":25936},{\"end\":25950,\"start\":25943},{\"end\":25959,\"start\":25950},{\"end\":25965,\"start\":25959},{\"end\":25974,\"start\":25965},{\"end\":25982,\"start\":25974},{\"end\":25990,\"start\":25982},{\"end\":25999,\"start\":25990},{\"end\":26014,\"start\":25999},{\"end\":26026,\"start\":26014},{\"end\":26362,\"start\":26353},{\"end\":26372,\"start\":26362},{\"end\":26379,\"start\":26372},{\"end\":26386,\"start\":26379},{\"end\":26393,\"start\":26386},{\"end\":26401,\"start\":26393},{\"end\":26587,\"start\":26569},{\"end\":26599,\"start\":26587},{\"end\":26610,\"start\":26599},{\"end\":26626,\"start\":26610},{\"end\":26640,\"start\":26626},{\"end\":27114,\"start\":27100},{\"end\":27301,\"start\":27294},{\"end\":27307,\"start\":27301},{\"end\":27319,\"start\":27307},{\"end\":27326,\"start\":27319},{\"end\":27637,\"start\":27627},{\"end\":27646,\"start\":27637},{\"end\":27655,\"start\":27646},{\"end\":28058,\"start\":28045},{\"end\":28068,\"start\":28058},{\"end\":28075,\"start\":28068},{\"end\":28087,\"start\":28075},{\"end\":28099,\"start\":28087},{\"end\":28109,\"start\":28099},{\"end\":28389,\"start\":28377},{\"end\":28403,\"start\":28389},{\"end\":28417,\"start\":28403},{\"end\":28431,\"start\":28417},{\"end\":28444,\"start\":28431},{\"end\":28806,\"start\":28794},{\"end\":28818,\"start\":28806},{\"end\":28828,\"start\":28818},{\"end\":28836,\"start\":28828},{\"end\":28840,\"start\":28836},{\"end\":29272,\"start\":29264},{\"end\":29278,\"start\":29272},{\"end\":29285,\"start\":29278},{\"end\":29293,\"start\":29285},{\"end\":29300,\"start\":29293},{\"end\":29307,\"start\":29300},{\"end\":29313,\"start\":29307},{\"end\":29320,\"start\":29313},{\"end\":29790,\"start\":29782},{\"end\":29797,\"start\":29790},{\"end\":29805,\"start\":29797},{\"end\":29818,\"start\":29805},{\"end\":29835,\"start\":29818},{\"end\":29843,\"start\":29835},{\"end\":30241,\"start\":30232},{\"end\":30249,\"start\":30241},{\"end\":30255,\"start\":30249},{\"end\":30263,\"start\":30255},{\"end\":30650,\"start\":30641},{\"end\":30658,\"start\":30650},{\"end\":30664,\"start\":30658},{\"end\":30672,\"start\":30664},{\"end\":30682,\"start\":30672},{\"end\":31027,\"start\":31005},{\"end\":31037,\"start\":31027},{\"end\":31050,\"start\":31037},{\"end\":31061,\"start\":31050},{\"end\":31070,\"start\":31061},{\"end\":31074,\"start\":31070},{\"end\":31082,\"start\":31074},{\"end\":31401,\"start\":31389},{\"end\":31417,\"start\":31401},{\"end\":31430,\"start\":31417},{\"end\":31445,\"start\":31430},{\"end\":31461,\"start\":31445},{\"end\":31477,\"start\":31461},{\"end\":31819,\"start\":31810},{\"end\":31947,\"start\":31939},{\"end\":32072,\"start\":32062},{\"end\":32249,\"start\":32235},{\"end\":32262,\"start\":32249},{\"end\":32276,\"start\":32262},{\"end\":32282,\"start\":32276},{\"end\":32483,\"start\":32467},{\"end\":32904,\"start\":32888},{\"end\":32919,\"start\":32904},{\"end\":32929,\"start\":32919},{\"end\":32937,\"start\":32929},{\"end\":32954,\"start\":32937},{\"end\":32969,\"start\":32954},{\"end\":32983,\"start\":32969},{\"end\":32998,\"start\":32983},{\"end\":33306,\"start\":33289},{\"end\":33319,\"start\":33306},{\"end\":33332,\"start\":33319},{\"end\":33348,\"start\":33332},{\"end\":33614,\"start\":33606},{\"end\":33620,\"start\":33614},{\"end\":33626,\"start\":33620},{\"end\":33631,\"start\":33626},{\"end\":33635,\"start\":33631},{\"end\":34513,\"start\":34500},{\"end\":34525,\"start\":34513},{\"end\":34537,\"start\":34525},{\"end\":34668,\"start\":34656},{\"end\":34677,\"start\":34668},{\"end\":34688,\"start\":34677}]", "bib_venue": "[{\"end\":22688,\"start\":22624},{\"end\":24370,\"start\":24308},{\"end\":26783,\"start\":26720},{\"end\":27796,\"start\":27734},{\"end\":28975,\"start\":28916},{\"end\":29499,\"start\":29418},{\"end\":32648,\"start\":32574},{\"end\":33988,\"start\":33812},{\"end\":22257,\"start\":22212},{\"end\":22622,\"start\":22543},{\"end\":23079,\"start\":22999},{\"end\":23482,\"start\":23408},{\"end\":23981,\"start\":23927},{\"end\":24306,\"start\":24229},{\"end\":24718,\"start\":24630},{\"end\":24962,\"start\":24920},{\"end\":25333,\"start\":25245},{\"end\":25740,\"start\":25691},{\"end\":26097,\"start\":26042},{\"end\":26405,\"start\":26401},{\"end\":26718,\"start\":26640},{\"end\":27098,\"start\":27034},{\"end\":27375,\"start\":27326},{\"end\":27732,\"start\":27655},{\"end\":28135,\"start\":28109},{\"end\":28496,\"start\":28444},{\"end\":28914,\"start\":28840},{\"end\":29416,\"start\":29320},{\"end\":29931,\"start\":29859},{\"end\":30315,\"start\":30263},{\"end\":30715,\"start\":30682},{\"end\":30971,\"start\":30943},{\"end\":31217,\"start\":31180},{\"end\":31534,\"start\":31477},{\"end\":31808,\"start\":31787},{\"end\":31937,\"start\":31921},{\"end\":32060,\"start\":32022},{\"end\":32233,\"start\":32150},{\"end\":32572,\"start\":32483},{\"end\":32886,\"start\":32821},{\"end\":33353,\"start\":33348},{\"end\":33810,\"start\":33635},{\"end\":34498,\"start\":34436},{\"end\":34767,\"start\":34704}]"}}}, "year": 2023, "month": 12, "day": 17}