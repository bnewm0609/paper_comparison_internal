{"id": 261339179, "updated": "2023-10-04 21:02:07.087", "metadata": {"title": "FPTQ: Fine-grained Post-Training Quantization for Large Language Models", "authors": "[{\"first\":\"Qingyuan\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yifan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Liang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Peng\",\"last\":\"Yao\",\"middle\":[]},{\"first\":\"Bo\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Xiangxiang\",\"last\":\"Chu\",\"middle\":[]},{\"first\":\"Yerui\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Du\",\"middle\":[]},{\"first\":\"Yuchen\",\"last\":\"Xie\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "In the era of large-scale language models, the substantial parameter size poses significant challenges for deployment. Being a prevalent compression technique, quantization has emerged as the mainstream practice to tackle this issue, which is mainly centered on two recipes W8A8 and W4A16 (i.e. weights and activations in such bit widths). In this study, we propose a novel W4A8 post-training quantization method for the available open-sourced LLMs, which combines the advantages of both two recipes. Therefore, we can leverage the benefit in the I/O utilization of 4-bit weight quantization and the acceleration due to 8-bit matrix computation. Nevertheless, the W4A8 faces notorious performance degradation. As a remedy, we involve layerwise activation quantization strategies which feature a novel logarithmic equalization for most intractable layers, and we combine them with fine-grained weight quantization. Without whistles and bells, we eliminate the necessity for further fine-tuning and obtain the state-of-the-art W4A8 quantized performance on BLOOM, LLaMA, and LLaMA-2 on standard benchmarks. We confirm that the W4A8 quantization is achievable for the deployment of large language models, fostering their wide-spreading real-world applications.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2308.15987", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2308-15987", "doi": "10.48550/arxiv.2308.15987"}}, "content": {"source": {"pdf_hash": "9b200baa28a8c30b320c61b167fce0bdd829e8ad", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2308.15987v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "bb55a7df80b0e50dd1e33bd8da68c5aeb072bd64", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9b200baa28a8c30b320c61b167fce0bdd829e8ad.txt", "contents": "\nFPTQ: FINE-GRAINED POST-TRAINING QUANTIZA- TION FOR LARGE LANGUAGE MODELS\n\n\nQingyuan Li \nNanjing University\n\n\nYifan Zhang \nNanjing University\n\n\nLiang Li \nNanjing University\n\n\nPeng Yao \nNanjing University\n\n\nBo Zhang \nNanjing University\n\n\nXiangxiang Chu \nNanjing University\n\n\nYerui Sun \nNanjing University\n\n\nLi Du \nNanjing University\n\n\nYuchen Xie \nNanjing University\n\n\nMeituan \nNanjing University\n\n\nFPTQ: FINE-GRAINED POST-TRAINING QUANTIZA- TION FOR LARGE LANGUAGE MODELS\n\nIn the era of large-scale language models, the substantial parameter size poses significant challenges for deployment. Being a prevalent compression technique, quantization has emerged as the mainstream practice to tackle this issue, which is mainly centered on two recipes W8A8 and W4A16 (i.e. weights and activations in such bit widths). In this study, we propose a novel W4A8 post-training quantization method for the available open-sourced LLMs, which combines the advantages of both two recipes. Therefore, we can leverage the benefit in the I/O utilization of 4-bit weight quantization and the acceleration due to 8-bit matrix computation. Nevertheless, the W4A8 faces notorious performance degradation. As a remedy, we involve layerwise activation quantization strategies which feature a novel logarithmic equalization for most intractable layers, and we combine them with fine-grained weight quantization. Without whistles and bells, we eliminate the necessity for further fine-tuning and obtain the state-of-the-art W4A8 quantized performance on BLOOM, LLaMA, and LLaMA-2 on standard benchmarks. We confirm that the W4A8 quantization is achievable for the deployment of large language models, fostering their wide-spreading real-world applications.\n\nINTRODUCTION\n\nLarge Language Models (LLMs) are distinguished for their exceptional emergent knowledge capacity (Wei et al., 2022), enabling them to perform admirably across a wide variety of language tasks. However, their massive scale poses a significant hurdle to deployment due to the substantial storage and the huge amount of computation required. This challenge is particularly pronounced in environments with limited resources such as edge computing devices and personal devices, where the constraints can inhibit the widespread adoption of these cutting-edge language models.\n\nTo address this issue, several model compression strategies have been proposed, including pruning (Ma et al., 2023;Frantar & Alistarh, 2023;, distillation (Zhang et al., 2023), quantization (Frantar et al., 2022;Xiao et al., 2023), and low-rank decomposition . Each of these approaches has its own limitations. For instance, pruning can achieve reasonable compression rates but it may require significant fine-tuning or are closely tied to specific hardware architectures. In contrast, quantization techniques, despite their universal applicability, are often confronted with the problem of significant quantization errors, particularly with the increasing parameter sizes (Dettmers et al., 2022).\n\nLately, research attention has been shifted towards a more balanced approach to quantization, specifically the usage of lower-bit widths for weights and higher-bit widths for activation, like W4A16 in GPTQ (Frantar et al., 2022). This introduces a novel perspective to tackle the computational and memory-intensive aspects of LLMs, which are typically composed of Transformer Decoder structures (Vaswani et al., 2017). During inference, it can be divided into compute-intensive context decoding stage and memory-intensive self-decoding stage, each presenting unique challenges and opportunities for further optimization.\n\nHowever, there is a conspicuous dearth of research that explores the synergistic combination of two quantization recipes W8A8 and W4A16. This paper aims to bridge the gap by proposing an innovative Fine-grained Post-Training Quantization (called FPTQ) method that combines the benefits of both, thereby providing an effective and efficient W4A8 solution for the deployment of a variety of available large language models that are tested across a myriad of natural language tasks. We first investigate the quantization difficulty by illustrating the activation distributions in different layers, discovering that their ranges differ dramatically which motivates us for a layerwise strategy. Subsequently, we provide a unique activation equalization technique to handle the intractable outliers (Figure 1), and improve the overall performance with fine-grained weight quantization.\n\nIn a nutshell, we make several key contributions to the field of LLM compression and deployment:\n\n1. High performance and low-cost W4A8 compression: We are the first to achieve highperformance W4A8 (INT4 weights and INT8 activation) PTQ compression for large language models, maintaining the accuracy of the original model. Being a post-training quantization technique, it tremendously simplifies the production flow of LLMs.\n\n2. Novel quantization scheme: Based on our comprehensive analysis of the activation distribution of LLMs, we employ a layerwise strategy to cope with different levels of quantization difficulty. Particularly, we devise an offline logarithmic activation equalization to render a quantization-friendly distribution for previously intractable layers.\n\n3. Inference-friendly: Our approach harmonizes the memory and computation efficiency which enables the storage of weights in a 4-bit format while executing INT8 inference, thereby catalyzing both the memory access and computation.\n\n\nRELATED WORK\n\n\nLARGE LANGUAGE MODELS\n\nThe past few years have witnessed the booming of pre-trained language models. BERT (Devlin et al., 2019) is designed to understand the context of words in a sentence and has been used for tasks such as sentiment analysis and question answering. RoBERTa (Liu et al., 2019) is an improved version of BERT with better pre-training techniques and larger training data. T5 (Raffel et al., 2020) is designed to perform a wide range of natural language processing tasks, including language translation and summarization. XLNet (Yang et al., 2019) is designed to handle long sequences of text and has achieved state-of-the-art results on several natural language processing tasks. GPT-3 (Brown et al., 2020) is one of the most advanced LLMs with 175 billion parameters, capable of performing a wide range of natural language processing tasks. Along with the open-sourced ones like GLM (Du et al., 2021), BLOOM (Lauren\u00e7on et al., 2022), OPT  and LLaMa series (Touvron et al., 2023), LLMs have remarkably revolutionized the field of natural language processing and are being used in a wide range of applications.\n\nNevertheless, LLMs have billions of parameters and are often pre-trained on large amounts of text data, which require significant computational resources to train and deploy. There is a call for faster inference time and lower memory requirements to make LLMs more practical.\n\n\nQUANTIZATION ON LLMS\n\nApplying quantization to large language models presents unique challenges. Traditional PTQ schemes have achieved great success in Convolutional Neural Networks (CNN) (Nagel et al., 2019;Wu et al., 2020;Nagel et al., 2021;Yao et al., 2021), but direct application to large language models often results in severe accuracy loss, this is typically due to the presence of many outliers in the activation values of large models (Dettmers et al., 2022).\n\nSeveral approaches have been proposed to address these issues. For example, LLM.int8() (Dettmers et al., 2022) splits the input activation values into two parts: non-outlier dimensions computed with INT8, and outliers computed with FP16. GPTQ (Frantar et al., 2022) and AWQ  circumvent this difficulty by adopting FP16 activation and INT4 weight-only quantization. However, these methods also have their limitations, such as computational overhead and the inability to truly leverage hardware acceleration.\n\nOther approaches like SmoothQuant (Xiao et al., 2023), RPTQ (Yuan et al., 2023), and ZeroQuant-V2  propose different strategies to achieve quantization while mitigating the accuracy loss and computational overhead. However, SmoothQuant is merely a W8A8 solution and it suffers from poor performance on W4A8. The rest tackles the W4A8 challenge but they come with their own set of challenges like weight reordering, asymmetric quantization, and group-wise activation, which can perplex the engineering work and may not well facilitate hardware. In light of these problems, we are driven to achieve W4A8 quantization without relying on QAT or distillation methods, paving the way for the efficient deployment of LLMs.\n\n\nMETHOD\n\n3.1 WHY W4A8?\n\nThe generative inference of LLMs can be divided into two stages: context decoding that generates an output token given an input prompt (embedded as a sequence of tokens), and self-decoding that iteratively predicts the next token in a sequence, see Figure 2 (a). The former is compute-bound due to the first-round computation of lengthy input sequences and the latter is memory-bound as a result of sequential processing, thus two different implementations are required.  decoding enjoys the speed-up using 8-bit matrix multiplication, while self-decoding is also accelerated via reduced memory access using 4-bit weight.\n\nThere are a few existing W4A8 studies. ZeroQuant (Yao et al., 2022) utilizes mixed precision for self-attention weights (W8) and is not tested on larger models, ZeroQuantV2  uses fine-grained activation quantization which is not feasible in practice. ZeroQuant-FP  alleviates the degradation by using higher-precision FP8 computation but it depends on specific hardware (e.g. NVIDIA H100). LLM-QAT (Liu et al., 2023a) adopts QAT to improve W4A8 performance but it requires costly training and is prone to tedious hyper-parameter tuning. Therefore, it is necessary to improve the accuracy of the W4A8 model while not harming its inference speed. The method shall also be made low-cost and generalizable for most up-to-date LLMs.\n\n\nANALYSIS OF ACTIVATION DISTRIBUTION ON LLMS\n\nWith our goal in mind, we are driven to design a robust PTQ method. To begin with, we study why vanilla W4A8 quantization is difficult for current LLMs. We first draw the activation distribution of LLaMA-7B in Figure 3 to find the distinct behaviors of different layers. For instance, o proj has compact distribution while down proj spans extensively. This phenomenon reoccurs in many other LLMs, see Appendix A.3.  As we can see from the above analysis, the maximum fluctuation range of input activation values for certain layers ranges from tens to thousands. Using per-tensor static quantization will result in significant quantization errors, but using per-token dynamic quantization for all layers will not bring adequate hardware acceleration. Therefore, it naturally calls for a layer-specific policy to determine the granularity of quantization.\n\n\nFPTQ: FINE-GRAINED POST-TRAINING QUANTIZATION\n\nMotivated by the above analysis, we propose our post-training quantization method which employs a layerwise quantization strategy regarding disparate activation distributions. Our complete procedure is given in Algorithm 1. The key components are discussed in detail.\n\n\nLAYER-WISE ACTIVATION QUANTIZATION STRATEGY\n\nThe key to resolving the activation quantization difficulty lies in the outlier treatment. Empirically, we can use different activation quantization strategies for different layers, as shown in Table 2. For activation value ranges within tens (denoted as v 0 ), per-tensor static quantization can be safely used. However, to avoid quantization loss for activation ranges over hundreds (denoted as v 1 ), pertoken dynamic quantization shall be put in place although slightly sacrificing hardware acceleration benefits. For most layers that range within hundreds, i.e. (v 0 , v 1 ), it demands a particular strategy that simultaneously reduces the quantization error while not harming the inference speed. Xiao et al. (2023) discover that when larger outliers dominate the distribution, the effective quantization bits of inliers are substantially narrowed. For per-tensor 8-bit quantization, it becomes 2 8 \u00b7 m i /m where m i is the maximum amplitude of channel i and m is the maximum value of the whole tensor. They also observe that outliers stay in fixed channels. Based on these two findings, we are allowed to perform per-channel outlier suppression on activations. SmoothQuant (Xiao et al., 2023) attempts to 'smooth' per-channel distribution by dividing the activation with a scale Set each layer's weight quantization policy as fine-grained 13: end for 14: Update the LLM's weights and activations w.r.t. the chosen quantization policy 15: Get the high-performance quantized LLM \nv \u2264 v 0 per-tensor, static High Dense v 0 < v < v 1 LAE + per-tensor, static High QKV, FC1 v \u2265 v1\nper-token, dynamic Medium FC2\ns i = max(|x i |)/ max(|w i |)\n, where x i and w i are activation and weight of channel i respectively. AWQ  introduces grid-searched hyper-parameters \u03b1 and \u03b2 to lay importance to activation and weight separately, where they find the contribution of weights is marginal and suggest activation-awareness is most important. In this paper, we argue that it is unnecessary to consider weights for computing the activation \"smoothing\" scale. Besides, it is crucial to retain all the activation values with a non-linear lossless mapping, yet it has to satisfy two criteria (1) touching gently with the inliers (2) harshly suppressing the outliers. In this regard, we verify that the logarithmic function rightly fits this purpose.\n\nLogarithmic Activation Equalization. To render a quantization-friendly activation distribution, we propose a new offline logarithmic activation equalization (LAE) method that moderates activation distributions in a non-linear fashion. Specifically, we compute the i-th channel scale s i as the maximum activation value max(|X i |) divided by its logarithmic mapping with a shift of 2 (to have a minimum of scale 1), shown in Equation 1. The formula retains the original information while it squashes various distributions comparably. Figure 1 exhibits its outcome distribution.\ns i = max(|x i |)/ log 2 (2 + max(|x i |)); x i = x i /s i(1)\nOnce the scale s is obtained, we can update the corresponding weight and activation as follows,\nW \u2032 = diag(s)W; X \u2032 = Xdiag(s) \u22121 s.t. X \u2032 W \u2032 = XW(2)\nHence, this update is made in-place as it is mathematically equivalent. Notably, s can be easily fused into the weight of the previous layer. In our case, there are only two types of operations (QKV and FC1) whose activation ranges are in (v 0 , v 1 ). To apply the offline LAE, their activation updates are fused into their preceding operation LayerNorm (Ba et al., 2016).\n\n\nWEIGHT QUANTIZATION\n\nDue to the intricacies of LLMs, it is not tractable to use the vanilla per-channel strategy only, as shown in Figure 4 (a). ZeroQuant (Yao et al., 2022) adopts a fine-grained groupwise weight quantization (Shen et al., 2020) that addresses the quantization difficulty of smaller LLMs like GPT-3 (Brown et al., 2020). As the two strategies are identically costly from the engineering perspective, we adopt fine-grained weight quantization where the scale is computed groupwise for all layers to obtain better performance, depicted in Figure 4 (b). As LLaMA series (Touvron et al., 2023) rises to the mainstream focus, we illustrate our specific quantization scheme for its architecture in Figure 4 (c) and (d). Interestingly, we discover that the trend of LLaMA activation distributions holds for all model series, such that our quantization scheme can be directly reused. Logarithmic activation equalization is performed offline (the scale for activation is then fused into LayerNorm) for QKV and Up/Gate. It's also worth noting that the quantized KV cache is applied to save I/O costs.\n(a) (b) N Quantized Weights (K \u00d7 N) In dim Out dim Scale K/Group Group Quantized Weights (K \u00d7 N) In dim Out dim Scale 1 N LAE (c) Q K V K cache V cache Dense LayerNorm LAE(\n\nEXPERIMENT\n\n\nDATASETS\n\nWe validated our quantization scheme on several datasets, including LAMBADA (Paperno et al., 2016), MMLU (Hendrycks et al., 2020), and a set of Common Sense QA (Talmor et al., 2019) tasks like WinoGrande (Sakaguchi et al., 2021), PIQA (Tata & Patel, 2003), HellaSwag (Zellers et al., 2019), ARC e . For CommonSense QA tasks, we used the Language Model Evaluation Harness (Gao et al., 2021) tool to evaluate our models. For the calibration set, we randomly sampled 512 samples from the Pile dataset (Gao et al., 2020).   (Xiao et al., 2023) and GPTQ (Frantar et al., 2022) as our baselines, given their status as the most prevalent W8A8 and W4A16 quantization schemes, respectively. These methods have been widely adopted in various applications and their performance has been extensively validated, establishing them as reliable benchmarks in the field of LLMs quantization. Simultaneously, to further demonstrate the potential of FPTQ, we compare it with the QAT method, particularly with LLM-QAT (Liu et al., 2023b). It's worth mentioning that QAT introduces a significant computational resource overhead; in contrast, our approach incurs a negligible cost compared to it. Implementation. We find that for the investigated LLMs in our paper, the activation bound v 0 can be typically set as 15 and v 1 150.\n\n\nIMPLEMENTATION\n\n\nModel\n\n\nEXPERIMENTAL RESULTS ON LAMBADA\n\nWe initially conducted our experiments on the LAMBADA dataset (Paperno et al., 2016). Despite the fact that LAMBADA may not effectively reflect the comprehensive capabilities of the model, it serves as a valuable tool for rapidly validating model precision and quantifying the impact on model performance. Our method, Fine-grained Post-training Quantization (FPTQ), achieved W4A8 quantized models that demonstrated precision strikingly similar to their floating-point counterparts on both the BLOOM-7B1 (Scao et al., 2022) and all models in the LLaMA series (Touvron et al., 2023). This is a highly encouraging observation, suggesting the efficacy of our approach.   Given the paucity of other Posttraining Quantization (PTQ) works employing W4A8 quantization, we conducted a comparative study with the Quantization-Aware Training (QAT) method, LLM-QAT (Liu et al., 2023b), on the Common Sense QA dataset. Our approach achieved a precision that was notably closer to the FP16 model compared to LLM-QAT. However, due to the limited data publicly available from LLM-QAT, we present here the experimental results for only LLaMA-7B and LLaMA-13B. It can be observed that our approach yields slightly superior results on every subset of the dataset compared to LLM-QAT, highlighting the effectiveness of our methodology.\n\n\nABLATION STUDY\n\n\nCOMPARISON WITH DATA-FREE QUANTIZATION\n\nWe acknowledge that the calibration dataset may be one of the factors affecting the performance of the quantized model. Therefore, to maintain fairness, we utilized the Pile dataset (Gao et al., 2020) as a calibration dataset in our previous experiments. However, to demonstrate the robustness of our method, we applied randomly generated tokens for model calibration. We conducted ablation studies on BLOOM-7B1, LLaMA-7B and LLaMA-2-7B under W8A8 and W4A8 bit-width settings in Table 6. It's exhilarating to note that, it was found that using a random dataset often resulted in superior results in most cases. This attests that our method is applicable in data-free situations.  Table 6: Ablation study on calibration datasets on MMLU and Common Sense QA.\n\n\nWEIGHT QUANTIZATION WITH GPTQ\n\nWe observe that the GPTQ method, which compensates weights based on the Hessian matrix, is orthogonal to our existing approach. Therefore, we attempted to fine-tune the weights using the GPTQ method after conducting logarithmic activation equalization (LAE) on the model, to investigate the potential for increased precision. However, our experiments in Table 7 demonstrated that the addition of GPTQ operations generally resulted in a negative impact on precision in most cases. We encourage future researchers to conduct more intriguing explorations in this area.  \n\n\nDISCUSSION AND FUTURE DIRECTIONS\n\nAnalysis on computation efficiency. Modern GPUs, such as the NVIDIA A100, support parallel block-wise matrix computation and pipeline processing. Fine-grained weight quantization enjoys such block-wise computation and introduces little overhead. Currently, the W4A16 acceleration is based on the GPU FP16INT4 GEMM kernel (Kim et al., 2022), which implements mixed-type matrix computation. The INT4 weights are first converted to FP16, and matrix computation is then performed with FP16. The underlying computation still uses the GPU's floating-point computation unit, so in the case of long inputs and large batches, the FP16INT4 kernel even has a negative effect compared to direct FP16 computation because of the additional conversion. The W8A8 computation acceleration is based on the GPU INT8 GEMM kernel, which uses INT8 Tensor Cores for underlying computation. There is a noticeable acceleration in the context decoding stage, but in the self-decoding stage, the bottleneck mainly lies in memory access.\n\nTo simultaneously address the acceleration issues in both the context decoding and self-decoding stages, we can design an INT8INT4 kernel, which profits INT8 Tensor Cores for acceleration in the context decoding stage, while keeping the weights loaded as INT4 to reduce memory access time in the self-decoding stage.\n\nData-free quantization. We discover that it is promising to randomly draw samples from the token vocabulary as in Table 6. We believe that there is still room for improvement in this regard.\n\nScale computation requires activation only. For activation quantization, our method completely removes weights for the computation of s i which echoes the findings in . To make our strategy more generalizable, we introduce a hyper-parameter \u03b1 to control the level of suppression, see A.2. It is however possible to devise other non-linear mapping functions that are hyperparameter free and in the meanwhile lead to better performance.\n\n\nCONCLUSION\n\nIn conclusion, our work presents a significant stride in the domain of Large Language Model (LLM) compression. Upon an overview of the existing quantization schemes, we introduce a novel posttraining quantization approach that can make the inference of LLMs more efficient, without compromising their performance. We successfully achieved high performance and efficiency for W4A8, which has the optimal utilization of computational resources which enhances the speed of both content-decoding and self-decoding stages. Furthermore, the removal of the need for fine-tuning during the training process simplifies the deployment pipeline significantly. This attests that our method provides an effective deployable solution for LLMs without sacrificing their accuracy. While our progress is encouraging, we acknowledge the potential for further exploration and refinement in this area. We anticipate that our work will inspire future research endeavors aimed at making LLMs even more efficient and practical.\n\n\nA APPENDIX\n\n\nA.1 PRELIMINARY KNOWLEDGE ON QUANTIZATION\n\nQuantization is a process of mapping continuous values to discrete ones by scaling. The scaling factor is also called quantization step size. In practice, a higher-precision floating point is used for training and the quantized version is used for inference. Consider b-bit integer quantization, for a real tensor x ranges in (min, max), it can be converted to an integer tensor x \u2032 within (\u22122 b\u22121 , 2 b\u22121 \u2212 1) by symmetric uniform quantization as,\nscale = max(|x|)/(2 b\u22121 \u2212 1) (3) x \u2032 = \u230a(x/scale)\u2309(4)\nWeight quantization and activation quantization. Typically, the weight is quantized as integer values. Activation quantization refers to the quantization of intermediate activation feature maps.\n\nStatic quantization vs. dynamic quantization. For static quantization, offline activation statistics are collected to compute the scale and it is kept static during inference. For dynamic quantization, such statistics are computed at runtime.\n\nPer-tensor vs. per-token. In the per-tensor scheme, the tensor matrix is considered as a whole to compute the quantization scale. In the per-token scheme, each input token corresponds to a scale computed upon all activation channels of the specific token. In essence, the per-token scheme is more fine-grained.\n\nPer-channel vs. group-wise. In the per-channel scheme, the quantization scale is computed channel-wise. In the group-wise scheme, each channel is divided into several groups and so are its scales.\n\n\nA.2 GENERALIZED FORM OF LAE\n\nWe give a generalized form of logarithmic activation equalization function. For the majority of LLMs, we use \u03b1 = 1. scale = (log 2 (2 + scale)) \u03b1 (5)\n\n\nA.3 MORE ACTIVATION DISTRIBUTION OF LLMS\n\nWe visualize the activation distributions of the LLaMA series in Figure 5 , 6, 7, 8, 9, and 10. It is rather exciting to find that LLaMA models at different scales share similar distributions in the same operations, which leads to a universal quantization scheme.   \n\nFigure 1 :\n1Activation distribution before and after logarithmic equalization on BLOOM-7B1.\n\nFigure 2 :\n2(a) Two stages of LLM inference where context decoding is compute-bound and selfdecoding is memory-bound. (b) W4A8 speeds up both stages and is faster than the other two.\n\nFigure 3 :\n3Visualization of activation distribution of o proj and down proj on LLaMA-7B.\n\n\nfor each layer-l in the Transformer structure (L layers in total)\n\nFigure 4 :\n4(a) Per-channel weight quantization. (b) Fine-grained per-channel quantization. (c, d) Self-attention and FFN in most LLMs. Light blue: per-tensor static activation quantization. Purple: per-token dynamic activation quantization. All weights are quantized in a fine-grained manner.\n\nFigure 5 :Figure 6 :\n56Visualization of activation distribution of o proj and down proj on LLaMA-2-7B. Visualization of activation distribution of o proj and down proj on LLaMA-2-13B.\n\nFigure 7 : 2 Figure 8 :Figure 9 :\n7289Visualization of activation distribution of o proj and down proj on LLaMA-Visualization of activation distribution of o proj and down proj on LLaMA-7B. Visualization of activation distribution of o proj and down proj on LLaMA-13B.\n\nFigure 10 :\n10Visualization of activation distribution of o proj and down proj on LLaMA-65B.\n\nTable 1 :\n1Comparison of decoding stage efficiency for dif-\nferent quantization methods. CD: Context Decoding, SD: \nSelf-Decoding \n\nMethod \nEfficient CD Efficient SD \n\nZeroQuant \nNo \nNo \nSmoothQuant \nYes \nNo \nGPTQ \nNo \nYes \nAWQ \nNo \nYes \nOurs \nYes \nYes \n\nPrevious quantization methods like \nSmoothquant (Xiao et al., 2023) fea-\ntures W8A8, while AWQ (Lin et al., \n2023) and GPTQ (Frantar et al., 2022) \nuse W4A16. Both recipes compromise \none stage for another, leading to infe-\nrior overall performance, whereas only \nW4A8 can boost both stages, see Fig-\nure 2 (b) and Table 1. That is, context \n\n\nTable 2 :\n2Activation quantization strategies for different ranges of activation values.Activation Value Range Quantization Strategy Hardware Efficiency Typical Operation\n\nTable 3 :\n3Comparison on the LAMBADA Dataset. Baselines. In our experiments, we selected SmoothQuant\n\nTable 4 :\n4Comparison on MMLU and Common Sense QA. BW: BitWidth On Common Sense QA, our approach demonstrates a mere 1% precision gap with the FP16 model across nearly all models, including the previously identified underperforming models LLaMA-7B and LLaMA-13B on MMLU. This observation underscores the robustness of our approach.4.4 RESULTS ON MMLU AND COMMON SENSE QA \n\nMMLU (Hendrycks et al., 2020) and Common Sense QA (Talmor et al., 2019) are currently \nrenowned datasets that comprehensively reflect the performance of LLMs. We conducted extensive \nexperiments on these datasets, including comparative assessments with two state-of-the-art solu-\ntions: SmoothQuant for W8A8, and GPTQ for W4A16. \n\nOn the MMLU dataset, our approach exhibits a performance gap within 1% for most models com-\npared to SmoothQuant. Notable outliers include LLaMA-7B and LLaMA-13B, which show a more \npronounced drop. However, it's important to note that the MMLU dataset, with its predominant \ncomposition of multiple-choice questions, may exhibit bias in precision estimation when the inher-\nent capabilities of the model are limited. \n\n\nTable 5 :\n5Comparison with LLM-QAT on LLaMA-7B.\n\nTable 7 :\n7Ablation on MMLU and Common Sense QA. FPTQ GPTQ : weights updated by GPTQ first.\n\n. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.06450Layer normalization. arXiv preprintJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Conference on Neural Information Processing Systems (NeurIPS). 2020Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Conference on Neural Information Processing Systems (NeurIPS), 2020.\n\nTim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.073398-bit matrix multiplication for transformers at scale. arXiv preprintint8(Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, North American Chapter of the Association for Computational Linguistics (NAACL). Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Associ- ation for Computational Linguistics (NAACL), 2019.\n\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Glm, arXiv:2103.10360General language model pretraining with autoregressive blank infilling. arXiv preprintZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021.\n\nSparsegpt: Massive language models can be accurately pruned in one-shot. Elias Frantar, Dan Alistarh, Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023.\n\nGptq: Accurate post-training quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, arXiv:2210.17323arXiv preprintElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\nThe pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.00027arXiv preprintLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n\nA framework for few-shot language model evaluation. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony Dipofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle Mcdonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, Andy Zou, 10.5281/zenodo.5371628Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot lan- guage model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, arXiv:2009.03300Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprintDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\n\nWho says elephants can't run: Bringing large scale moe models into cloud scale production. Young Jin Kim, Rawn Henry, Raffy Fahim, Hany Hassan Awadalla, arXiv:2211.10017arXiv preprintYoung Jin Kim, Rawn Henry, Raffy Fahim, and Hany Hassan Awadalla. Who says elephants can't run: Bringing large scale moe models into cloud scale production. arXiv preprint arXiv:2211.10017, 2022.\n\nThe BigScience corpus: A 1.6 TB composite multilingual dataset. Hugo Lauren\u00e7on, Lucile Saulnier, Thomas Wang, Christopher Akiki ; Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz\u00e1lez Ponferrada, Huu Nguyen, Albert Villanova del MoralHugo Lauren\u00e7on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz\u00e1lez Ponferrada, Huu Nguyen, et al. The BigScience corpus: A 1.6 TB composite multilingual dataset. 2022.\n\nAwq: Activationaware weight quantization for llm compression and acceleration. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Song Han, Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation- aware weight quantization for llm compression and acceleration, 2023.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nLlm-qat: Data-free quantization aware training for large language models. Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, Vikas Chandra, arXiv:2305.17888arXiv preprintZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023a.\n\nLlm-qat: Data-free quantization aware training for large language models. Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, Vikas Chandra, Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models, 2023b.\n\nLlm-pruner: On the structural pruning of large language models. Xinyin Ma, Gongfan Fang, Xinchao Wang, Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models, 2023.\n\nData-free quantization through weight equalization and bias correction. Markus Nagel, Mart Van Baalen, Tijmen Blankevoort, Max Welling, International Conference on Computer Vision (ICCV). Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In International Conference on Computer Vision (ICCV), 2019.\n\nMarkus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, Tijmen Blankevoort, arXiv:2106.08295A white paper on neural network quantization. arXiv preprintMarkus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, and Tijmen Blankevoort. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295, 2021.\n\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan, Raffaella Pham, Sandro Bernardi, Marco Pezzelle, Gemma Baroni, Raquel Boleda, Fern\u00e1ndez, arXiv:1606.06031The LAMBADA dataset: Word prediction requiring a broad discourse context. arXiv preprintDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The LAMBADA dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 211Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.\n\nWinogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Communications of the ACM. 649Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver- sarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.\n\nAngela Teven Le Scao, Christopher Fan, Ellie Akiki, Suzana Pavlick, Daniel Ili\u0107, Roman Hesslow, Alexandra Sasha Castagn\u00e9, Fran\u00e7ois Luccioni, Matthias Yvon, Gall\u00e9, arXiv:2211.05100A 176b-parameter open-access multilingual language model. arXiv preprintTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b- parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n\nQ-bert: Hessian based ultra low precision quantization of bert. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, W Michael, Kurt Mahoney, Keutzer, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 8815-8821, 2020.\n\nA simple and effective pruning approach for large language models. Mingjie Sun, Zhuang Liu, Anna Bair, J Zico Kolter, arXiv:2306.11695arXiv preprintMingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023.\n\nCommonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge, 2019.\n\nPiQA: An algebra for querying protein data sets. Sandeep Tata, M Jignesh, Patel, International Conference on Scientific and Statistical Database Management. Sandeep Tata and Jignesh M Patel. PiQA: An algebra for querying protein data sets. In International Conference on Scientific and Statistical Database Management, 2003.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Conference on Neural Information Processing Systems (NeurIPS). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Conference on Neural In- formation Processing Systems (NeurIPS), 2017.\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. arXiv preprintJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo- gatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.\n\nInteger quantization for deep learning inference: Principles and empirical evaluation. Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, Paulius Micikevicius, Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius. Integer quantization for deep learning inference: Principles and empirical evaluation, 2020.\n\nZeroquant-fp: A leap forward in llms post-training w4a8 quantization using floating-point formats. Xiaoxia Wu, Zhewei Yao, Yuxiong He, Xiaoxia Wu, Zhewei Yao, and Yuxiong He. Zeroquant-fp: A leap forward in llms post-training w4a8 quantization using floating-point formats, 2023.\n\nSmoothquant: Accurate and efficient post-training quantization for large language models. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song Han, International Conference on Machine Learning. PMLRGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pp. 38087-38099. PMLR, 2023.\n\nXlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, R Russ, Quoc V Salakhutdinov, Le, Advances in neural information processing systems. 32Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019.\n\nHAWQ-v3: Dyadic neural network quantization. Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael Mahoney, International Conference on Machine Learning (ICML). 2021Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael Mahoney, et al. HAWQ-v3: Dyadic neural network quanti- zation. In International Conference on Machine Learning (ICML), 2021.\n\nZeroQuant: Efficient and affordable post-training quantization for large-scale transformers. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He, arXiv:2206.01861arXiv preprintZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. ZeroQuant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.\n\nZeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation. Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong He, Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation, 2023.\n\nRptq: Reorder-based post-training quantization for large language models. Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, Bingzhe Wu, Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language models, 2023.\n\nHellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, arXiv:1905.07830arXiv preprintRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma- chine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\n\nLifting the curse of capacity gap in distilling language models. Chen Zhang, Yang Yang, Jiahao Liu, Jingang Wang, Yunsen Xian, Benyou Wang, Dawei Song, Chen Zhang, Yang Yang, Jiahao Liu, Jingang Wang, Yunsen Xian, Benyou Wang, and Dawei Song. Lifting the curse of capacity gap in distilling language models, 2023.\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.01068Open pre-trained transformer language models. arXiv preprintSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo- pher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n", "annotations": {"author": "[{\"end\":110,\"start\":77},{\"end\":144,\"start\":111},{\"end\":175,\"start\":145},{\"end\":206,\"start\":176},{\"end\":237,\"start\":207},{\"end\":274,\"start\":238},{\"end\":306,\"start\":275},{\"end\":334,\"start\":307},{\"end\":367,\"start\":335},{\"end\":397,\"start\":368}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":86},{\"end\":122,\"start\":117},{\"end\":153,\"start\":151},{\"end\":184,\"start\":181},{\"end\":215,\"start\":210},{\"end\":252,\"start\":249},{\"end\":284,\"start\":281},{\"end\":312,\"start\":310},{\"end\":345,\"start\":342}]", "author_first_name": "[{\"end\":85,\"start\":77},{\"end\":116,\"start\":111},{\"end\":150,\"start\":145},{\"end\":180,\"start\":176},{\"end\":209,\"start\":207},{\"end\":248,\"start\":238},{\"end\":280,\"start\":275},{\"end\":309,\"start\":307},{\"end\":341,\"start\":335},{\"end\":375,\"start\":368}]", "author_affiliation": "[{\"end\":109,\"start\":90},{\"end\":143,\"start\":124},{\"end\":174,\"start\":155},{\"end\":205,\"start\":186},{\"end\":236,\"start\":217},{\"end\":273,\"start\":254},{\"end\":305,\"start\":286},{\"end\":333,\"start\":314},{\"end\":366,\"start\":347},{\"end\":396,\"start\":377}]", "title": "[{\"end\":74,\"start\":1},{\"end\":471,\"start\":398}]", "venue": null, "abstract": "[{\"end\":1730,\"start\":473}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1861,\"start\":1843},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2432,\"start\":2415},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2457,\"start\":2432},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2492,\"start\":2472},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2529,\"start\":2507},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2547,\"start\":2529},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3013,\"start\":2990},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3244,\"start\":3222},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3433,\"start\":3411},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5670,\"start\":5649},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5837,\"start\":5819},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5955,\"start\":5934},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6105,\"start\":6086},{\"end\":6265,\"start\":6239},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6460,\"start\":6443},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6492,\"start\":6468},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6538,\"start\":6516},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7156,\"start\":7136},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7172,\"start\":7156},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7191,\"start\":7172},{\"end\":7208,\"start\":7191},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7416,\"start\":7393},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7529,\"start\":7506},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7684,\"start\":7662},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7980,\"start\":7961},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8006,\"start\":7987},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9358,\"start\":9340},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9708,\"start\":9689},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12006,\"start\":11988},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12485,\"start\":12466},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14788,\"start\":14771},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14965,\"start\":14947},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15037,\"start\":15018},{\"end\":15128,\"start\":15108},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15398,\"start\":15376},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16195,\"start\":16173},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16226,\"start\":16202},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16278,\"start\":16257},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16325,\"start\":16301},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16352,\"start\":16332},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16386,\"start\":16364},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16486,\"start\":16468},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16613,\"start\":16595},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16636,\"start\":16617},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16668,\"start\":16646},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17114,\"start\":17095},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17550,\"start\":17528},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18046,\"start\":18024},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18338,\"start\":18319},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19041,\"start\":19023},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20574,\"start\":20556},{\"end\":25039,\"start\":25019}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25304,\"start\":25212},{\"attributes\":{\"id\":\"fig_1\"},\"end\":25488,\"start\":25305},{\"attributes\":{\"id\":\"fig_3\"},\"end\":25579,\"start\":25489},{\"attributes\":{\"id\":\"fig_4\"},\"end\":25647,\"start\":25580},{\"attributes\":{\"id\":\"fig_5\"},\"end\":25942,\"start\":25648},{\"attributes\":{\"id\":\"fig_6\"},\"end\":26127,\"start\":25943},{\"attributes\":{\"id\":\"fig_7\"},\"end\":26397,\"start\":26128},{\"attributes\":{\"id\":\"fig_8\"},\"end\":26491,\"start\":26398},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":27090,\"start\":26492},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":27262,\"start\":27091},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":27364,\"start\":27263},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":28489,\"start\":27365},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":28538,\"start\":28490},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":28631,\"start\":28539}]", "paragraph": "[{\"end\":2315,\"start\":1746},{\"end\":3014,\"start\":2317},{\"end\":3636,\"start\":3016},{\"end\":4517,\"start\":3638},{\"end\":4615,\"start\":4519},{\"end\":4944,\"start\":4617},{\"end\":5293,\"start\":4946},{\"end\":5525,\"start\":5295},{\"end\":6668,\"start\":5566},{\"end\":6945,\"start\":6670},{\"end\":7417,\"start\":6970},{\"end\":7925,\"start\":7419},{\"end\":8642,\"start\":7927},{\"end\":8666,\"start\":8653},{\"end\":9289,\"start\":8668},{\"end\":10018,\"start\":9291},{\"end\":10919,\"start\":10066},{\"end\":11236,\"start\":10969},{\"end\":12770,\"start\":11284},{\"end\":12898,\"start\":12869},{\"end\":13623,\"start\":12930},{\"end\":14202,\"start\":13625},{\"end\":14360,\"start\":14265},{\"end\":14789,\"start\":14416},{\"end\":15899,\"start\":14813},{\"end\":17405,\"start\":16097},{\"end\":18781,\"start\":17466},{\"end\":19597,\"start\":18841},{\"end\":20198,\"start\":19631},{\"end\":21244,\"start\":20235},{\"end\":21562,\"start\":21246},{\"end\":21754,\"start\":21564},{\"end\":22190,\"start\":21756},{\"end\":23209,\"start\":22205},{\"end\":23716,\"start\":23268},{\"end\":23965,\"start\":23771},{\"end\":24209,\"start\":23967},{\"end\":24521,\"start\":24211},{\"end\":24719,\"start\":24523},{\"end\":24900,\"start\":24751},{\"end\":25211,\"start\":24945}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12868,\"start\":12771},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12929,\"start\":12899},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14264,\"start\":14203},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14415,\"start\":14361},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16072,\"start\":15900},{\"attributes\":{\"id\":\"formula_5\"},\"end\":23770,\"start\":23717}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":11485,\"start\":11478},{\"end\":19327,\"start\":19320},{\"end\":19528,\"start\":19521},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":19992,\"start\":19985},{\"end\":21685,\"start\":21678}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1744,\"start\":1732},{\"attributes\":{\"n\":\"2\"},\"end\":5540,\"start\":5528},{\"attributes\":{\"n\":\"2.1\"},\"end\":5564,\"start\":5543},{\"attributes\":{\"n\":\"2.2\"},\"end\":6968,\"start\":6948},{\"attributes\":{\"n\":\"3\"},\"end\":8651,\"start\":8645},{\"attributes\":{\"n\":\"3.2\"},\"end\":10064,\"start\":10021},{\"attributes\":{\"n\":\"3.3\"},\"end\":10967,\"start\":10922},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":11282,\"start\":11239},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":14811,\"start\":14792},{\"attributes\":{\"n\":\"4\"},\"end\":16084,\"start\":16074},{\"attributes\":{\"n\":\"4.1\"},\"end\":16095,\"start\":16087},{\"attributes\":{\"n\":\"4.2\"},\"end\":17422,\"start\":17408},{\"end\":17430,\"start\":17425},{\"attributes\":{\"n\":\"4.3\"},\"end\":17464,\"start\":17433},{\"attributes\":{\"n\":\"5\"},\"end\":18798,\"start\":18784},{\"attributes\":{\"n\":\"5.1\"},\"end\":18839,\"start\":18801},{\"attributes\":{\"n\":\"5.2\"},\"end\":19629,\"start\":19600},{\"attributes\":{\"n\":\"6\"},\"end\":20233,\"start\":20201},{\"attributes\":{\"n\":\"7\"},\"end\":22203,\"start\":22193},{\"end\":23222,\"start\":23212},{\"end\":23266,\"start\":23225},{\"end\":24749,\"start\":24722},{\"end\":24943,\"start\":24903},{\"end\":25223,\"start\":25213},{\"end\":25316,\"start\":25306},{\"end\":25500,\"start\":25490},{\"end\":25659,\"start\":25649},{\"end\":25964,\"start\":25944},{\"end\":26162,\"start\":26129},{\"end\":26410,\"start\":26399},{\"end\":26502,\"start\":26493},{\"end\":27101,\"start\":27092},{\"end\":27273,\"start\":27264},{\"end\":27375,\"start\":27366},{\"end\":28500,\"start\":28491},{\"end\":28549,\"start\":28540}]", "table": "[{\"end\":27090,\"start\":26504},{\"end\":28489,\"start\":27697}]", "figure_caption": "[{\"end\":25304,\"start\":25225},{\"end\":25488,\"start\":25318},{\"end\":25579,\"start\":25502},{\"end\":25647,\"start\":25582},{\"end\":25942,\"start\":25661},{\"end\":26127,\"start\":25967},{\"end\":26397,\"start\":26167},{\"end\":26491,\"start\":26413},{\"end\":27262,\"start\":27103},{\"end\":27364,\"start\":27275},{\"end\":27697,\"start\":27377},{\"end\":28538,\"start\":28502},{\"end\":28631,\"start\":28551}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4441,\"start\":4431},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8925,\"start\":8917},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10284,\"start\":10276},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14167,\"start\":14159},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":14931,\"start\":14923},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":15358,\"start\":15346},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":15509,\"start\":15501},{\"end\":25018,\"start\":25010}]", "bib_author_first_name": "[{\"end\":28640,\"start\":28635},{\"end\":28644,\"start\":28641},{\"end\":28654,\"start\":28649},{\"end\":28659,\"start\":28655},{\"end\":28675,\"start\":28667},{\"end\":28677,\"start\":28676},{\"end\":28895,\"start\":28892},{\"end\":28911,\"start\":28903},{\"end\":28922,\"start\":28918},{\"end\":28937,\"start\":28930},{\"end\":28952,\"start\":28947},{\"end\":28954,\"start\":28953},{\"end\":28971,\"start\":28963},{\"end\":28988,\"start\":28982},{\"end\":29008,\"start\":29002},{\"end\":29022,\"start\":29016},{\"end\":29037,\"start\":29031},{\"end\":29389,\"start\":29386},{\"end\":29404,\"start\":29400},{\"end\":29418,\"start\":29412},{\"end\":29432,\"start\":29428},{\"end\":29794,\"start\":29789},{\"end\":29811,\"start\":29803},{\"end\":29825,\"start\":29819},{\"end\":29839,\"start\":29831},{\"end\":30182,\"start\":30173},{\"end\":30192,\"start\":30187},{\"end\":30203,\"start\":30199},{\"end\":30213,\"start\":30209},{\"end\":30228,\"start\":30220},{\"end\":30240,\"start\":30234},{\"end\":30250,\"start\":30247},{\"end\":30647,\"start\":30642},{\"end\":30660,\"start\":30657},{\"end\":30871,\"start\":30866},{\"end\":30888,\"start\":30881},{\"end\":30908,\"start\":30905},{\"end\":31216,\"start\":31213},{\"end\":31228,\"start\":31222},{\"end\":31242,\"start\":31239},{\"end\":31258,\"start\":31250},{\"end\":31274,\"start\":31268},{\"end\":31289,\"start\":31282},{\"end\":31303,\"start\":31298},{\"end\":31317,\"start\":31311},{\"end\":31327,\"start\":31322},{\"end\":31338,\"start\":31335},{\"end\":31685,\"start\":31682},{\"end\":31699,\"start\":31691},{\"end\":31711,\"start\":31705},{\"end\":31725,\"start\":31722},{\"end\":31740,\"start\":31733},{\"end\":31756,\"start\":31749},{\"end\":31773,\"start\":31765},{\"end\":31790,\"start\":31783},{\"end\":31800,\"start\":31796},{\"end\":31817,\"start\":31811},{\"end\":31836,\"start\":31831},{\"end\":31849,\"start\":31844},{\"end\":31864,\"start\":31860},{\"end\":31876,\"start\":31871},{\"end\":31887,\"start\":31884},{\"end\":31899,\"start\":31894},{\"end\":31910,\"start\":31906},{\"end\":32286,\"start\":32283},{\"end\":32304,\"start\":32298},{\"end\":32318,\"start\":32312},{\"end\":32331,\"start\":32327},{\"end\":32759,\"start\":32754},{\"end\":32763,\"start\":32760},{\"end\":32773,\"start\":32769},{\"end\":32786,\"start\":32781},{\"end\":32805,\"start\":32794},{\"end\":33111,\"start\":33107},{\"end\":33129,\"start\":33123},{\"end\":33146,\"start\":33140},{\"end\":33164,\"start\":33153},{\"end\":33195,\"start\":33188},{\"end\":33199,\"start\":33196},{\"end\":33215,\"start\":33207},{\"end\":33228,\"start\":33221},{\"end\":33237,\"start\":33229},{\"end\":33253,\"start\":33250},{\"end\":33629,\"start\":33627},{\"end\":33642,\"start\":33635},{\"end\":33656,\"start\":33649},{\"end\":33668,\"start\":33663},{\"end\":33681,\"start\":33675},{\"end\":33692,\"start\":33688},{\"end\":33867,\"start\":33861},{\"end\":33877,\"start\":33873},{\"end\":33888,\"start\":33883},{\"end\":33903,\"start\":33896},{\"end\":33914,\"start\":33908},{\"end\":33927,\"start\":33922},{\"end\":33938,\"start\":33934},{\"end\":33949,\"start\":33945},{\"end\":33961,\"start\":33957},{\"end\":33982,\"start\":33975},{\"end\":34393,\"start\":34387},{\"end\":34405,\"start\":34399},{\"end\":34422,\"start\":34412},{\"end\":34434,\"start\":34429},{\"end\":34448,\"start\":34442},{\"end\":34462,\"start\":34456},{\"end\":34479,\"start\":34471},{\"end\":34495,\"start\":34485},{\"end\":34517,\"start\":34512},{\"end\":34896,\"start\":34890},{\"end\":34908,\"start\":34902},{\"end\":34925,\"start\":34915},{\"end\":34937,\"start\":34932},{\"end\":34951,\"start\":34945},{\"end\":34965,\"start\":34959},{\"end\":34982,\"start\":34974},{\"end\":34998,\"start\":34988},{\"end\":35020,\"start\":35015},{\"end\":35326,\"start\":35320},{\"end\":35338,\"start\":35331},{\"end\":35352,\"start\":35345},{\"end\":35551,\"start\":35545},{\"end\":35563,\"start\":35559},{\"end\":35582,\"start\":35576},{\"end\":35599,\"start\":35596},{\"end\":35869,\"start\":35863},{\"end\":35883,\"start\":35877},{\"end\":35901,\"start\":35897},{\"end\":35905,\"start\":35902},{\"end\":35920,\"start\":35913},{\"end\":35937,\"start\":35933},{\"end\":35956,\"start\":35950},{\"end\":36248,\"start\":36243},{\"end\":36264,\"start\":36258},{\"end\":36285,\"start\":36277},{\"end\":36301,\"start\":36297},{\"end\":36317,\"start\":36308},{\"end\":36330,\"start\":36324},{\"end\":36346,\"start\":36341},{\"end\":36362,\"start\":36357},{\"end\":36377,\"start\":36371},{\"end\":36860,\"start\":36855},{\"end\":36873,\"start\":36869},{\"end\":36887,\"start\":36883},{\"end\":36906,\"start\":36897},{\"end\":36918,\"start\":36912},{\"end\":36934,\"start\":36927},{\"end\":36948,\"start\":36943},{\"end\":36958,\"start\":36955},{\"end\":36970,\"start\":36963},{\"end\":37365,\"start\":37358},{\"end\":37379,\"start\":37377},{\"end\":37394,\"start\":37387},{\"end\":37406,\"start\":37401},{\"end\":37646,\"start\":37640},{\"end\":37673,\"start\":37662},{\"end\":37684,\"start\":37679},{\"end\":37698,\"start\":37692},{\"end\":37714,\"start\":37708},{\"end\":37726,\"start\":37721},{\"end\":37745,\"start\":37736},{\"end\":37751,\"start\":37746},{\"end\":37770,\"start\":37762},{\"end\":37789,\"start\":37781},{\"end\":38236,\"start\":38231},{\"end\":38247,\"start\":38243},{\"end\":38259,\"start\":38254},{\"end\":38271,\"start\":38264},{\"end\":38282,\"start\":38276},{\"end\":38292,\"start\":38288},{\"end\":38303,\"start\":38302},{\"end\":38317,\"start\":38313},{\"end\":38792,\"start\":38785},{\"end\":38804,\"start\":38798},{\"end\":38814,\"start\":38810},{\"end\":38827,\"start\":38821},{\"end\":39111,\"start\":39107},{\"end\":39128,\"start\":39120},{\"end\":39145,\"start\":39137},{\"end\":39162,\"start\":39154},{\"end\":39381,\"start\":39374},{\"end\":39389,\"start\":39388},{\"end\":39655,\"start\":39651},{\"end\":39672,\"start\":39665},{\"end\":39688,\"start\":39681},{\"end\":39704,\"start\":39698},{\"end\":39725,\"start\":39715},{\"end\":39743,\"start\":39735},{\"end\":39758,\"start\":39753},{\"end\":39781,\"start\":39777},{\"end\":39795,\"start\":39789},{\"end\":40182,\"start\":40176},{\"end\":40196,\"start\":40192},{\"end\":40210,\"start\":40206},{\"end\":40224,\"start\":40219},{\"end\":40241,\"start\":40236},{\"end\":40254,\"start\":40249},{\"end\":40256,\"start\":40255},{\"end\":40270,\"start\":40264},{\"end\":40284,\"start\":40279},{\"end\":40592,\"start\":40587},{\"end\":40600,\"start\":40598},{\"end\":40611,\"start\":40606},{\"end\":40628,\"start\":40623},{\"end\":40643,\"start\":40637},{\"end\":40659,\"start\":40650},{\"end\":40674,\"start\":40670},{\"end\":40692,\"start\":40685},{\"end\":40705,\"start\":40700},{\"end\":40718,\"start\":40712},{\"end\":41128,\"start\":41125},{\"end\":41140,\"start\":41133},{\"end\":41154,\"start\":41147},{\"end\":41169,\"start\":41162},{\"end\":41184,\"start\":41177},{\"end\":41477,\"start\":41470},{\"end\":41488,\"start\":41482},{\"end\":41501,\"start\":41494},{\"end\":41751,\"start\":41742},{\"end\":41760,\"start\":41758},{\"end\":41773,\"start\":41766},{\"end\":41785,\"start\":41782},{\"end\":41796,\"start\":41790},{\"end\":41810,\"start\":41806},{\"end\":42193,\"start\":42187},{\"end\":42206,\"start\":42200},{\"end\":42218,\"start\":42212},{\"end\":42230,\"start\":42225},{\"end\":42243,\"start\":42242},{\"end\":42256,\"start\":42250},{\"end\":42608,\"start\":42602},{\"end\":42618,\"start\":42614},{\"end\":42635,\"start\":42625},{\"end\":42647,\"start\":42643},{\"end\":42662,\"start\":42657},{\"end\":42671,\"start\":42667},{\"end\":42683,\"start\":42677},{\"end\":42696,\"start\":42690},{\"end\":42708,\"start\":42704},{\"end\":42722,\"start\":42715},{\"end\":43135,\"start\":43129},{\"end\":43145,\"start\":43141},{\"end\":43153,\"start\":43146},{\"end\":43171,\"start\":43165},{\"end\":43186,\"start\":43179},{\"end\":43199,\"start\":43191},{\"end\":43211,\"start\":43204},{\"end\":43586,\"start\":43580},{\"end\":43599,\"start\":43592},{\"end\":43609,\"start\":43604},{\"end\":43621,\"start\":43614},{\"end\":43635,\"start\":43628},{\"end\":43902,\"start\":43895},{\"end\":43912,\"start\":43909},{\"end\":43924,\"start\":43918},{\"end\":43935,\"start\":43930},{\"end\":43949,\"start\":43941},{\"end\":43963,\"start\":43956},{\"end\":43978,\"start\":43971},{\"end\":43989,\"start\":43984},{\"end\":44002,\"start\":43994},{\"end\":44014,\"start\":44007},{\"end\":44288,\"start\":44283},{\"end\":44301,\"start\":44298},{\"end\":44319,\"start\":44312},{\"end\":44329,\"start\":44326},{\"end\":44344,\"start\":44339},{\"end\":44618,\"start\":44614},{\"end\":44630,\"start\":44626},{\"end\":44643,\"start\":44637},{\"end\":44656,\"start\":44649},{\"end\":44669,\"start\":44663},{\"end\":44682,\"start\":44676},{\"end\":44694,\"start\":44689},{\"end\":44869,\"start\":44864},{\"end\":44884,\"start\":44877},{\"end\":44898,\"start\":44893},{\"end\":44911,\"start\":44906},{\"end\":44925,\"start\":44921},{\"end\":44939,\"start\":44932},{\"end\":44957,\"start\":44946},{\"end\":44969,\"start\":44965},{\"end\":44980,\"start\":44976},{\"end\":44987,\"start\":44985}]", "bib_author_last_name": "[{\"end\":28647,\"start\":28645},{\"end\":28665,\"start\":28660},{\"end\":28684,\"start\":28678},{\"end\":28901,\"start\":28896},{\"end\":28916,\"start\":28912},{\"end\":28928,\"start\":28923},{\"end\":28945,\"start\":28938},{\"end\":28961,\"start\":28955},{\"end\":28980,\"start\":28972},{\"end\":29000,\"start\":28989},{\"end\":29014,\"start\":29009},{\"end\":29029,\"start\":29023},{\"end\":29044,\"start\":29038},{\"end\":29398,\"start\":29390},{\"end\":29410,\"start\":29405},{\"end\":29426,\"start\":29419},{\"end\":29444,\"start\":29433},{\"end\":29801,\"start\":29795},{\"end\":29817,\"start\":29812},{\"end\":29829,\"start\":29826},{\"end\":29849,\"start\":29840},{\"end\":30185,\"start\":30183},{\"end\":30197,\"start\":30193},{\"end\":30207,\"start\":30204},{\"end\":30218,\"start\":30214},{\"end\":30232,\"start\":30229},{\"end\":30245,\"start\":30241},{\"end\":30255,\"start\":30251},{\"end\":30260,\"start\":30257},{\"end\":30655,\"start\":30648},{\"end\":30669,\"start\":30661},{\"end\":30879,\"start\":30872},{\"end\":30903,\"start\":30889},{\"end\":30916,\"start\":30909},{\"end\":30926,\"start\":30918},{\"end\":31220,\"start\":31217},{\"end\":31237,\"start\":31229},{\"end\":31248,\"start\":31243},{\"end\":31266,\"start\":31259},{\"end\":31280,\"start\":31275},{\"end\":31296,\"start\":31290},{\"end\":31309,\"start\":31304},{\"end\":31320,\"start\":31318},{\"end\":31333,\"start\":31328},{\"end\":31348,\"start\":31339},{\"end\":31689,\"start\":31686},{\"end\":31703,\"start\":31700},{\"end\":31720,\"start\":31712},{\"end\":31731,\"start\":31726},{\"end\":31747,\"start\":31741},{\"end\":31763,\"start\":31757},{\"end\":31781,\"start\":31774},{\"end\":31794,\"start\":31791},{\"end\":31809,\"start\":31801},{\"end\":31829,\"start\":31818},{\"end\":31842,\"start\":31837},{\"end\":31858,\"start\":31850},{\"end\":31869,\"start\":31865},{\"end\":31882,\"start\":31877},{\"end\":31892,\"start\":31888},{\"end\":31904,\"start\":31900},{\"end\":31914,\"start\":31911},{\"end\":32296,\"start\":32287},{\"end\":32310,\"start\":32305},{\"end\":32325,\"start\":32319},{\"end\":32335,\"start\":32332},{\"end\":32767,\"start\":32764},{\"end\":32779,\"start\":32774},{\"end\":32792,\"start\":32787},{\"end\":32814,\"start\":32806},{\"end\":33121,\"start\":33112},{\"end\":33138,\"start\":33130},{\"end\":33151,\"start\":33147},{\"end\":33186,\"start\":33165},{\"end\":33205,\"start\":33200},{\"end\":33219,\"start\":33216},{\"end\":33248,\"start\":33238},{\"end\":33260,\"start\":33254},{\"end\":33633,\"start\":33630},{\"end\":33647,\"start\":33643},{\"end\":33661,\"start\":33657},{\"end\":33673,\"start\":33669},{\"end\":33686,\"start\":33682},{\"end\":33696,\"start\":33693},{\"end\":33871,\"start\":33868},{\"end\":33881,\"start\":33878},{\"end\":33894,\"start\":33889},{\"end\":33906,\"start\":33904},{\"end\":33920,\"start\":33915},{\"end\":33932,\"start\":33928},{\"end\":33943,\"start\":33939},{\"end\":33955,\"start\":33950},{\"end\":33973,\"start\":33962},{\"end\":33991,\"start\":33983},{\"end\":34000,\"start\":33993},{\"end\":34397,\"start\":34394},{\"end\":34410,\"start\":34406},{\"end\":34427,\"start\":34423},{\"end\":34440,\"start\":34435},{\"end\":34454,\"start\":34449},{\"end\":34469,\"start\":34463},{\"end\":34483,\"start\":34480},{\"end\":34510,\"start\":34496},{\"end\":34525,\"start\":34518},{\"end\":34900,\"start\":34897},{\"end\":34913,\"start\":34909},{\"end\":34930,\"start\":34926},{\"end\":34943,\"start\":34938},{\"end\":34957,\"start\":34952},{\"end\":34972,\"start\":34966},{\"end\":34986,\"start\":34983},{\"end\":35013,\"start\":34999},{\"end\":35028,\"start\":35021},{\"end\":35329,\"start\":35327},{\"end\":35343,\"start\":35339},{\"end\":35357,\"start\":35353},{\"end\":35557,\"start\":35552},{\"end\":35574,\"start\":35564},{\"end\":35594,\"start\":35583},{\"end\":35607,\"start\":35600},{\"end\":35875,\"start\":35870},{\"end\":35895,\"start\":35884},{\"end\":35911,\"start\":35906},{\"end\":35931,\"start\":35921},{\"end\":35948,\"start\":35938},{\"end\":35968,\"start\":35957},{\"end\":36256,\"start\":36249},{\"end\":36275,\"start\":36265},{\"end\":36295,\"start\":36286},{\"end\":36306,\"start\":36302},{\"end\":36322,\"start\":36318},{\"end\":36339,\"start\":36331},{\"end\":36355,\"start\":36347},{\"end\":36369,\"start\":36363},{\"end\":36384,\"start\":36378},{\"end\":36395,\"start\":36386},{\"end\":36867,\"start\":36861},{\"end\":36881,\"start\":36874},{\"end\":36895,\"start\":36888},{\"end\":36910,\"start\":36907},{\"end\":36925,\"start\":36919},{\"end\":36941,\"start\":36935},{\"end\":36953,\"start\":36949},{\"end\":36961,\"start\":36959},{\"end\":36974,\"start\":36971},{\"end\":37375,\"start\":37366},{\"end\":37385,\"start\":37380},{\"end\":37399,\"start\":37395},{\"end\":37418,\"start\":37407},{\"end\":37424,\"start\":37420},{\"end\":37660,\"start\":37647},{\"end\":37677,\"start\":37674},{\"end\":37690,\"start\":37685},{\"end\":37706,\"start\":37699},{\"end\":37719,\"start\":37715},{\"end\":37734,\"start\":37727},{\"end\":37760,\"start\":37752},{\"end\":37779,\"start\":37771},{\"end\":37794,\"start\":37790},{\"end\":37801,\"start\":37796},{\"end\":38241,\"start\":38237},{\"end\":38252,\"start\":38248},{\"end\":38262,\"start\":38260},{\"end\":38274,\"start\":38272},{\"end\":38286,\"start\":38283},{\"end\":38300,\"start\":38293},{\"end\":38311,\"start\":38304},{\"end\":38325,\"start\":38318},{\"end\":38334,\"start\":38327},{\"end\":38796,\"start\":38793},{\"end\":38808,\"start\":38805},{\"end\":38819,\"start\":38815},{\"end\":38834,\"start\":38828},{\"end\":39118,\"start\":39112},{\"end\":39135,\"start\":39129},{\"end\":39152,\"start\":39146},{\"end\":39169,\"start\":39163},{\"end\":39386,\"start\":39382},{\"end\":39397,\"start\":39390},{\"end\":39404,\"start\":39399},{\"end\":39663,\"start\":39656},{\"end\":39679,\"start\":39673},{\"end\":39696,\"start\":39689},{\"end\":39713,\"start\":39705},{\"end\":39733,\"start\":39726},{\"end\":39751,\"start\":39744},{\"end\":39775,\"start\":39759},{\"end\":39787,\"start\":39782},{\"end\":39802,\"start\":39796},{\"end\":39809,\"start\":39804},{\"end\":40190,\"start\":40183},{\"end\":40204,\"start\":40197},{\"end\":40217,\"start\":40211},{\"end\":40234,\"start\":40225},{\"end\":40247,\"start\":40242},{\"end\":40262,\"start\":40257},{\"end\":40277,\"start\":40271},{\"end\":40295,\"start\":40285},{\"end\":40596,\"start\":40593},{\"end\":40604,\"start\":40601},{\"end\":40621,\"start\":40612},{\"end\":40635,\"start\":40629},{\"end\":40648,\"start\":40644},{\"end\":40668,\"start\":40660},{\"end\":40683,\"start\":40675},{\"end\":40698,\"start\":40693},{\"end\":40710,\"start\":40706},{\"end\":40726,\"start\":40719},{\"end\":41131,\"start\":41129},{\"end\":41145,\"start\":41141},{\"end\":41160,\"start\":41155},{\"end\":41175,\"start\":41170},{\"end\":41197,\"start\":41185},{\"end\":41480,\"start\":41478},{\"end\":41492,\"start\":41489},{\"end\":41504,\"start\":41502},{\"end\":41756,\"start\":41752},{\"end\":41764,\"start\":41761},{\"end\":41780,\"start\":41774},{\"end\":41788,\"start\":41786},{\"end\":41804,\"start\":41797},{\"end\":41814,\"start\":41811},{\"end\":42198,\"start\":42194},{\"end\":42210,\"start\":42207},{\"end\":42223,\"start\":42219},{\"end\":42240,\"start\":42231},{\"end\":42248,\"start\":42244},{\"end\":42270,\"start\":42257},{\"end\":42274,\"start\":42272},{\"end\":42612,\"start\":42609},{\"end\":42623,\"start\":42619},{\"end\":42641,\"start\":42636},{\"end\":42655,\"start\":42648},{\"end\":42665,\"start\":42663},{\"end\":42675,\"start\":42672},{\"end\":42688,\"start\":42684},{\"end\":42702,\"start\":42697},{\"end\":42713,\"start\":42709},{\"end\":42730,\"start\":42723},{\"end\":43139,\"start\":43136},{\"end\":43163,\"start\":43154},{\"end\":43177,\"start\":43172},{\"end\":43189,\"start\":43187},{\"end\":43202,\"start\":43200},{\"end\":43214,\"start\":43212},{\"end\":43590,\"start\":43587},{\"end\":43602,\"start\":43600},{\"end\":43612,\"start\":43610},{\"end\":43626,\"start\":43622},{\"end\":43638,\"start\":43636},{\"end\":43907,\"start\":43903},{\"end\":43916,\"start\":43913},{\"end\":43928,\"start\":43925},{\"end\":43939,\"start\":43936},{\"end\":43954,\"start\":43950},{\"end\":43969,\"start\":43964},{\"end\":43982,\"start\":43979},{\"end\":43992,\"start\":43990},{\"end\":44005,\"start\":44003},{\"end\":44017,\"start\":44015},{\"end\":44296,\"start\":44289},{\"end\":44310,\"start\":44302},{\"end\":44324,\"start\":44320},{\"end\":44337,\"start\":44330},{\"end\":44349,\"start\":44345},{\"end\":44624,\"start\":44619},{\"end\":44635,\"start\":44631},{\"end\":44647,\"start\":44644},{\"end\":44661,\"start\":44657},{\"end\":44674,\"start\":44670},{\"end\":44687,\"start\":44683},{\"end\":44699,\"start\":44695},{\"end\":44875,\"start\":44870},{\"end\":44891,\"start\":44885},{\"end\":44904,\"start\":44899},{\"end\":44919,\"start\":44912},{\"end\":44930,\"start\":44926},{\"end\":44944,\"start\":44940},{\"end\":44963,\"start\":44958},{\"end\":44974,\"start\":44970},{\"end\":44983,\"start\":44981},{\"end\":45000,\"start\":44988}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b0\"},\"end\":28851,\"start\":28633},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":218971783},\"end\":29384,\"start\":28853},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b2\"},\"end\":29705,\"start\":29386},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":52967399},\"end\":30171,\"start\":29707},{\"attributes\":{\"doi\":\"arXiv:2103.10360\",\"id\":\"b4\"},\"end\":30567,\"start\":30173},{\"attributes\":{\"id\":\"b5\"},\"end\":30781,\"start\":30569},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b6\"},\"end\":31145,\"start\":30783},{\"attributes\":{\"doi\":\"arXiv:2101.00027\",\"id\":\"b7\"},\"end\":31628,\"start\":31147},{\"attributes\":{\"doi\":\"10.5281/zenodo.5371628\",\"id\":\"b8\"},\"end\":32281,\"start\":31630},{\"attributes\":{\"doi\":\"arXiv:2009.03300\",\"id\":\"b9\"},\"end\":32661,\"start\":32283},{\"attributes\":{\"doi\":\"arXiv:2211.10017\",\"id\":\"b10\"},\"end\":33041,\"start\":32663},{\"attributes\":{\"id\":\"b11\"},\"end\":33546,\"start\":33043},{\"attributes\":{\"id\":\"b12\"},\"end\":33859,\"start\":33548},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b13\"},\"end\":34311,\"start\":33861},{\"attributes\":{\"doi\":\"arXiv:2305.17888\",\"id\":\"b14\"},\"end\":34814,\"start\":34313},{\"attributes\":{\"id\":\"b15\"},\"end\":35254,\"start\":34816},{\"attributes\":{\"id\":\"b16\"},\"end\":35471,\"start\":35256},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":184487878},\"end\":35861,\"start\":35473},{\"attributes\":{\"doi\":\"arXiv:2106.08295\",\"id\":\"b18\"},\"end\":36241,\"start\":35863},{\"attributes\":{\"doi\":\"arXiv:1606.06031\",\"id\":\"b19\"},\"end\":36770,\"start\":36243},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":204838007},\"end\":37293,\"start\":36772},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":198893658},\"end\":37638,\"start\":37295},{\"attributes\":{\"doi\":\"arXiv:2211.05100\",\"id\":\"b22\"},\"end\":38165,\"start\":37640},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":202565587},\"end\":38716,\"start\":38167},{\"attributes\":{\"doi\":\"arXiv:2306.11695\",\"id\":\"b24\"},\"end\":39026,\"start\":38718},{\"attributes\":{\"id\":\"b25\"},\"end\":39323,\"start\":39028},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1545102},\"end\":39649,\"start\":39325},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b27\"},\"end\":40147,\"start\":39651},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13756489},\"end\":40585,\"start\":40149},{\"attributes\":{\"doi\":\"arXiv:2206.07682\",\"id\":\"b29\"},\"end\":41036,\"start\":40587},{\"attributes\":{\"id\":\"b30\"},\"end\":41369,\"start\":41038},{\"attributes\":{\"id\":\"b31\"},\"end\":41650,\"start\":41371},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":253708271},\"end\":42111,\"start\":41652},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":195069387},\"end\":42555,\"start\":42113},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":227127479},\"end\":43034,\"start\":42557},{\"attributes\":{\"doi\":\"arXiv:2206.01861\",\"id\":\"b35\"},\"end\":43468,\"start\":43036},{\"attributes\":{\"id\":\"b36\"},\"end\":43819,\"start\":43470},{\"attributes\":{\"id\":\"b37\"},\"end\":44226,\"start\":43821},{\"attributes\":{\"doi\":\"arXiv:1905.07830\",\"id\":\"b38\"},\"end\":44547,\"start\":44228},{\"attributes\":{\"id\":\"b39\"},\"end\":44862,\"start\":44549},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b40\"},\"end\":45314,\"start\":44864}]", "bib_title": "[{\"end\":28890,\"start\":28853},{\"end\":29787,\"start\":29707},{\"end\":35543,\"start\":35473},{\"end\":36853,\"start\":36772},{\"end\":37356,\"start\":37295},{\"end\":38229,\"start\":38167},{\"end\":39372,\"start\":39325},{\"end\":40174,\"start\":40149},{\"end\":41740,\"start\":41652},{\"end\":42185,\"start\":42113},{\"end\":42600,\"start\":42557}]", "bib_author": "[{\"end\":28649,\"start\":28635},{\"end\":28667,\"start\":28649},{\"end\":28686,\"start\":28667},{\"end\":28903,\"start\":28892},{\"end\":28918,\"start\":28903},{\"end\":28930,\"start\":28918},{\"end\":28947,\"start\":28930},{\"end\":28963,\"start\":28947},{\"end\":28982,\"start\":28963},{\"end\":29002,\"start\":28982},{\"end\":29016,\"start\":29002},{\"end\":29031,\"start\":29016},{\"end\":29046,\"start\":29031},{\"end\":29400,\"start\":29386},{\"end\":29412,\"start\":29400},{\"end\":29428,\"start\":29412},{\"end\":29446,\"start\":29428},{\"end\":29803,\"start\":29789},{\"end\":29819,\"start\":29803},{\"end\":29831,\"start\":29819},{\"end\":29851,\"start\":29831},{\"end\":30187,\"start\":30173},{\"end\":30199,\"start\":30187},{\"end\":30209,\"start\":30199},{\"end\":30220,\"start\":30209},{\"end\":30234,\"start\":30220},{\"end\":30247,\"start\":30234},{\"end\":30257,\"start\":30247},{\"end\":30262,\"start\":30257},{\"end\":30657,\"start\":30642},{\"end\":30671,\"start\":30657},{\"end\":30881,\"start\":30866},{\"end\":30905,\"start\":30881},{\"end\":30918,\"start\":30905},{\"end\":30928,\"start\":30918},{\"end\":31222,\"start\":31213},{\"end\":31239,\"start\":31222},{\"end\":31250,\"start\":31239},{\"end\":31268,\"start\":31250},{\"end\":31282,\"start\":31268},{\"end\":31298,\"start\":31282},{\"end\":31311,\"start\":31298},{\"end\":31322,\"start\":31311},{\"end\":31335,\"start\":31322},{\"end\":31350,\"start\":31335},{\"end\":31691,\"start\":31682},{\"end\":31705,\"start\":31691},{\"end\":31722,\"start\":31705},{\"end\":31733,\"start\":31722},{\"end\":31749,\"start\":31733},{\"end\":31765,\"start\":31749},{\"end\":31783,\"start\":31765},{\"end\":31796,\"start\":31783},{\"end\":31811,\"start\":31796},{\"end\":31831,\"start\":31811},{\"end\":31844,\"start\":31831},{\"end\":31860,\"start\":31844},{\"end\":31871,\"start\":31860},{\"end\":31884,\"start\":31871},{\"end\":31894,\"start\":31884},{\"end\":31906,\"start\":31894},{\"end\":31916,\"start\":31906},{\"end\":32298,\"start\":32283},{\"end\":32312,\"start\":32298},{\"end\":32327,\"start\":32312},{\"end\":32337,\"start\":32327},{\"end\":32769,\"start\":32754},{\"end\":32781,\"start\":32769},{\"end\":32794,\"start\":32781},{\"end\":32816,\"start\":32794},{\"end\":33123,\"start\":33107},{\"end\":33140,\"start\":33123},{\"end\":33153,\"start\":33140},{\"end\":33188,\"start\":33153},{\"end\":33207,\"start\":33188},{\"end\":33221,\"start\":33207},{\"end\":33250,\"start\":33221},{\"end\":33262,\"start\":33250},{\"end\":33635,\"start\":33627},{\"end\":33649,\"start\":33635},{\"end\":33663,\"start\":33649},{\"end\":33675,\"start\":33663},{\"end\":33688,\"start\":33675},{\"end\":33698,\"start\":33688},{\"end\":33873,\"start\":33861},{\"end\":33883,\"start\":33873},{\"end\":33896,\"start\":33883},{\"end\":33908,\"start\":33896},{\"end\":33922,\"start\":33908},{\"end\":33934,\"start\":33922},{\"end\":33945,\"start\":33934},{\"end\":33957,\"start\":33945},{\"end\":33975,\"start\":33957},{\"end\":33993,\"start\":33975},{\"end\":34002,\"start\":33993},{\"end\":34399,\"start\":34387},{\"end\":34412,\"start\":34399},{\"end\":34429,\"start\":34412},{\"end\":34442,\"start\":34429},{\"end\":34456,\"start\":34442},{\"end\":34471,\"start\":34456},{\"end\":34485,\"start\":34471},{\"end\":34512,\"start\":34485},{\"end\":34527,\"start\":34512},{\"end\":34902,\"start\":34890},{\"end\":34915,\"start\":34902},{\"end\":34932,\"start\":34915},{\"end\":34945,\"start\":34932},{\"end\":34959,\"start\":34945},{\"end\":34974,\"start\":34959},{\"end\":34988,\"start\":34974},{\"end\":35015,\"start\":34988},{\"end\":35030,\"start\":35015},{\"end\":35331,\"start\":35320},{\"end\":35345,\"start\":35331},{\"end\":35359,\"start\":35345},{\"end\":35559,\"start\":35545},{\"end\":35576,\"start\":35559},{\"end\":35596,\"start\":35576},{\"end\":35609,\"start\":35596},{\"end\":35877,\"start\":35863},{\"end\":35897,\"start\":35877},{\"end\":35913,\"start\":35897},{\"end\":35933,\"start\":35913},{\"end\":35950,\"start\":35933},{\"end\":35970,\"start\":35950},{\"end\":36258,\"start\":36243},{\"end\":36277,\"start\":36258},{\"end\":36297,\"start\":36277},{\"end\":36308,\"start\":36297},{\"end\":36324,\"start\":36308},{\"end\":36341,\"start\":36324},{\"end\":36357,\"start\":36341},{\"end\":36371,\"start\":36357},{\"end\":36386,\"start\":36371},{\"end\":36397,\"start\":36386},{\"end\":36869,\"start\":36855},{\"end\":36883,\"start\":36869},{\"end\":36897,\"start\":36883},{\"end\":36912,\"start\":36897},{\"end\":36927,\"start\":36912},{\"end\":36943,\"start\":36927},{\"end\":36955,\"start\":36943},{\"end\":36963,\"start\":36955},{\"end\":36976,\"start\":36963},{\"end\":37377,\"start\":37358},{\"end\":37387,\"start\":37377},{\"end\":37401,\"start\":37387},{\"end\":37420,\"start\":37401},{\"end\":37426,\"start\":37420},{\"end\":37662,\"start\":37640},{\"end\":37679,\"start\":37662},{\"end\":37692,\"start\":37679},{\"end\":37708,\"start\":37692},{\"end\":37721,\"start\":37708},{\"end\":37736,\"start\":37721},{\"end\":37762,\"start\":37736},{\"end\":37781,\"start\":37762},{\"end\":37796,\"start\":37781},{\"end\":37803,\"start\":37796},{\"end\":38243,\"start\":38231},{\"end\":38254,\"start\":38243},{\"end\":38264,\"start\":38254},{\"end\":38276,\"start\":38264},{\"end\":38288,\"start\":38276},{\"end\":38302,\"start\":38288},{\"end\":38313,\"start\":38302},{\"end\":38327,\"start\":38313},{\"end\":38336,\"start\":38327},{\"end\":38798,\"start\":38785},{\"end\":38810,\"start\":38798},{\"end\":38821,\"start\":38810},{\"end\":38836,\"start\":38821},{\"end\":39120,\"start\":39107},{\"end\":39137,\"start\":39120},{\"end\":39154,\"start\":39137},{\"end\":39171,\"start\":39154},{\"end\":39388,\"start\":39374},{\"end\":39399,\"start\":39388},{\"end\":39406,\"start\":39399},{\"end\":39665,\"start\":39651},{\"end\":39681,\"start\":39665},{\"end\":39698,\"start\":39681},{\"end\":39715,\"start\":39698},{\"end\":39735,\"start\":39715},{\"end\":39753,\"start\":39735},{\"end\":39777,\"start\":39753},{\"end\":39789,\"start\":39777},{\"end\":39804,\"start\":39789},{\"end\":39811,\"start\":39804},{\"end\":40192,\"start\":40176},{\"end\":40206,\"start\":40192},{\"end\":40219,\"start\":40206},{\"end\":40236,\"start\":40219},{\"end\":40249,\"start\":40236},{\"end\":40264,\"start\":40249},{\"end\":40279,\"start\":40264},{\"end\":40297,\"start\":40279},{\"end\":40598,\"start\":40587},{\"end\":40606,\"start\":40598},{\"end\":40623,\"start\":40606},{\"end\":40637,\"start\":40623},{\"end\":40650,\"start\":40637},{\"end\":40670,\"start\":40650},{\"end\":40685,\"start\":40670},{\"end\":40700,\"start\":40685},{\"end\":40712,\"start\":40700},{\"end\":40728,\"start\":40712},{\"end\":41133,\"start\":41125},{\"end\":41147,\"start\":41133},{\"end\":41162,\"start\":41147},{\"end\":41177,\"start\":41162},{\"end\":41199,\"start\":41177},{\"end\":41482,\"start\":41470},{\"end\":41494,\"start\":41482},{\"end\":41506,\"start\":41494},{\"end\":41758,\"start\":41742},{\"end\":41766,\"start\":41758},{\"end\":41782,\"start\":41766},{\"end\":41790,\"start\":41782},{\"end\":41806,\"start\":41790},{\"end\":41816,\"start\":41806},{\"end\":42200,\"start\":42187},{\"end\":42212,\"start\":42200},{\"end\":42225,\"start\":42212},{\"end\":42242,\"start\":42225},{\"end\":42250,\"start\":42242},{\"end\":42272,\"start\":42250},{\"end\":42276,\"start\":42272},{\"end\":42614,\"start\":42602},{\"end\":42625,\"start\":42614},{\"end\":42643,\"start\":42625},{\"end\":42657,\"start\":42643},{\"end\":42667,\"start\":42657},{\"end\":42677,\"start\":42667},{\"end\":42690,\"start\":42677},{\"end\":42704,\"start\":42690},{\"end\":42715,\"start\":42704},{\"end\":42732,\"start\":42715},{\"end\":43141,\"start\":43129},{\"end\":43165,\"start\":43141},{\"end\":43179,\"start\":43165},{\"end\":43191,\"start\":43179},{\"end\":43204,\"start\":43191},{\"end\":43216,\"start\":43204},{\"end\":43592,\"start\":43580},{\"end\":43604,\"start\":43592},{\"end\":43614,\"start\":43604},{\"end\":43628,\"start\":43614},{\"end\":43640,\"start\":43628},{\"end\":43909,\"start\":43895},{\"end\":43918,\"start\":43909},{\"end\":43930,\"start\":43918},{\"end\":43941,\"start\":43930},{\"end\":43956,\"start\":43941},{\"end\":43971,\"start\":43956},{\"end\":43984,\"start\":43971},{\"end\":43994,\"start\":43984},{\"end\":44007,\"start\":43994},{\"end\":44019,\"start\":44007},{\"end\":44298,\"start\":44283},{\"end\":44312,\"start\":44298},{\"end\":44326,\"start\":44312},{\"end\":44339,\"start\":44326},{\"end\":44351,\"start\":44339},{\"end\":44626,\"start\":44614},{\"end\":44637,\"start\":44626},{\"end\":44649,\"start\":44637},{\"end\":44663,\"start\":44649},{\"end\":44676,\"start\":44663},{\"end\":44689,\"start\":44676},{\"end\":44701,\"start\":44689},{\"end\":44877,\"start\":44864},{\"end\":44893,\"start\":44877},{\"end\":44906,\"start\":44893},{\"end\":44921,\"start\":44906},{\"end\":44932,\"start\":44921},{\"end\":44946,\"start\":44932},{\"end\":44965,\"start\":44946},{\"end\":44976,\"start\":44965},{\"end\":44985,\"start\":44976},{\"end\":45002,\"start\":44985}]", "bib_venue": "[{\"end\":38445,\"start\":38399},{\"end\":29107,\"start\":29046},{\"end\":29515,\"start\":29462},{\"end\":29930,\"start\":29851},{\"end\":30348,\"start\":30278},{\"end\":30640,\"start\":30569},{\"end\":30864,\"start\":30783},{\"end\":31211,\"start\":31147},{\"end\":31680,\"start\":31630},{\"end\":32452,\"start\":32353},{\"end\":32752,\"start\":32663},{\"end\":33105,\"start\":33043},{\"end\":33625,\"start\":33548},{\"end\":34064,\"start\":34018},{\"end\":34385,\"start\":34313},{\"end\":34888,\"start\":34816},{\"end\":35318,\"start\":35256},{\"end\":35659,\"start\":35609},{\"end\":36030,\"start\":35986},{\"end\":36485,\"start\":36413},{\"end\":37016,\"start\":36976},{\"end\":37451,\"start\":37426},{\"end\":37875,\"start\":37819},{\"end\":38397,\"start\":38336},{\"end\":38783,\"start\":38718},{\"end\":39105,\"start\":39028},{\"end\":39480,\"start\":39406},{\"end\":39872,\"start\":39827},{\"end\":40358,\"start\":40297},{\"end\":40787,\"start\":40744},{\"end\":41123,\"start\":41038},{\"end\":41468,\"start\":41371},{\"end\":41860,\"start\":41816},{\"end\":42325,\"start\":42276},{\"end\":42783,\"start\":42732},{\"end\":43127,\"start\":43036},{\"end\":43578,\"start\":43470},{\"end\":43893,\"start\":43821},{\"end\":44281,\"start\":44228},{\"end\":44612,\"start\":44549},{\"end\":45062,\"start\":45018}]"}}}, "year": 2023, "month": 12, "day": 17}