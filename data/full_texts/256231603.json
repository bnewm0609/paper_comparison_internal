{"id": 256231603, "updated": "2023-02-11 14:20:37.284", "metadata": {"title": "Learning quadrupedal locomotion on deformable terrain", "authors": "[{\"first\":\"Suyoung\",\"last\":\"Choi\",\"middle\":[]},{\"first\":\"Gwanghyeon\",\"last\":\"Ji\",\"middle\":[]},{\"first\":\"Jeongsoo\",\"last\":\"Park\",\"middle\":[]},{\"first\":\"Hyeongjun\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Juhyeok\",\"last\":\"Mun\",\"middle\":[]},{\"first\":\"Jeong\",\"last\":\"Lee\",\"middle\":[\"Hyun\"]},{\"first\":\"Jemin\",\"last\":\"Hwangbo\",\"middle\":[]}]", "venue": "Science Robotics", "journal": "Science Robotics", "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Simulation-based reinforcement learning approaches are leading the next innovations in legged robot control. However, the resulting control policies are still not applicable on soft and deformable terrains, especially at high speed. The primary reason is that reinforcement learning approaches, in general, are not effective beyond the data distribution: The agent cannot perform well in environments that it has not experienced. To this end, we introduce a versatile and computationally efficient granular media model for reinforcement learning. Our model can be parameterized to represent diverse types of terrain from very soft beach sand to hard asphalt. In addition, we introduce an adaptive control architecture that can implicitly identify the terrain properties as the robot feels the terrain. The identified parameters are then used to boost the locomotion performance of the legged robot. We applied our techniques to the Raibo robot, a dynamic quadrupedal robot developed in-house. The trained networks demonstrated high-speed locomotion capabilities on deformable terrains: The robot was able to run on soft beach sand at 3.03 meters per second although the feet were completely buried in the sand during the stance phase. We also demonstrate its ability to generalize to different terrains by presenting running experiments on vinyl tile flooring, athletic track, grass, and a soft air mattress. Description An adaptive locomotion strategy and terrain simulation enable agile and robust quadrupedal locomotion on deformable terrains.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": "36696473", "pubmedcentral": null, "dblp": "journals/scirobotics/ChoiJPKMLH23", "doi": "10.1126/scirobotics.ade2256"}}, "content": {"source": {"pdf_hash": "1499579caf04760d58c650c20e2f4129e49a4806", "pdf_src": "Science", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "c2ed0d630836ce624e4f3e8c7a636f3b31335565", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1499579caf04760d58c650c20e2f4129e49a4806.txt", "contents": "\nLearning quadrupedal locomotion on deformable terrain\n\n\nSuyoung Choi \nGwanghyeon Ji \nJeongsoo Park \nHyeongjun Kim \nJeongJuhyeok Mun \nHyun Lee \nJemin Hwangbo \nLearning quadrupedal locomotion on deformable terrain\nA N I M A L R O B O T S\nSimulation-based reinforcement learning approaches are leading the next innovations in legged robot control. However, the resulting control policies are still not applicable on soft and deformable terrains, especially at high speed. The primary reason is that reinforcement learning approaches, in general, are not effective beyond the data distribution: The agent cannot perform well in environments that it has not experienced. To this end, we introduce a versatile and computationally efficient granular media model for reinforcement learning. Our model can be parameterized to represent diverse types of terrain from very soft beach sand to hard asphalt. In addition, we introduce an adaptive control architecture that can implicitly identify the terrain properties as the robot feels the terrain. The identified parameters are then used to boost the locomotion performance of the legged robot. We applied our techniques to the Raibo robot, a dynamic quadrupedal robot developed in-house. The trained networks demonstrated high-speed locomotion capabilities on deformable terrains: The robot was able to run on soft beach sand at 3.03 meters per second although the feet were completely buried in the sand during the stance phase. We also demonstrate its ability to generalize to different terrains by presenting running experiments on vinyl tile flooring, athletic track, grass, and a soft air mattress.\n\nINTRODUCTION\n\nRecent studies on legged robots have predominantly focused on locomotion on rigid ground, and, consequently, the resulting controllers have compromising performance on soft and deformable terrains. Because a large portion of the terrestrial surface is covered by nonrigid substrates, such as sand, soil, and vegetation, locomotion on soft terrains is a necessary skill for many applications of legged robots. At the same time, traversing such terrain is more suited to legged locomotion as opposed to wheeled mobility, which often encounters sinkage and slippages (1). Because soft earthy substrates might yield even under a small load, legged robots should be able to adapt to different terrain characteristics as they feel the terrain, as animals do (2)(3)(4). Although recent advances in legged robotics have led to more versatile robots that can traverse a wider range of terrains (5)(6)(7)(8), most of the existing works on control assumed that the terrain is rigid, and such an assumption inevitably leads to compromising locomotion performance on soft substrates (9).\n\nThere have been efforts to control legged robots on highly soft and deformable terrains. Some of them took advantage of known properties of the terrain (10)(11)(12) or estimated the ground reaction force with a nonparametric model trained with data from prior experiences (13). However, because the ground properties and prior experiences are often not available in the outdoor environments, demonstrations of those approaches have been confined to laboratory environments. The issue stems from the difficulty of predicting the terrain dynamics. In particular, terrain properties, such as compliance and deformability, vary with the weather and the surface conditions, and such differences are indistinguishable even to a highly advanced vision system. Furthermore, the complexity of a combined model that describes the interactions between the floating-based robot and the terrain leads to an impractically complex control architecture. As a result, contemporary research on integrating the terrain model into the legged robotic control has primarily been limited to morphologically simple robots, e.g., one-dimensional (1D) hopper (10,12,13).\n\nRising recently, the application of model-free reinforcement learning (RL) to legged robotics has demonstrated extensive generalization to unseen environments (5)(6)(7)(14)(15)(16)(17)(18) using simulated environments. Specifically, quadrupedal locomotion controllers trained on rigid rough terrain exhibited robust locomotion capabilities on various challenging terrains, even including several deformable terrains such as mud and snow (5,16). One of the core ideas that led to such a success is the privileged learning (PL) framework. In this framework, a teacher policy is trained with privileged access to states only available in the simulation. A student policy learns to imitate the teacher's encoding of privileged information and act accordingly. Another key feature was the randomization of physical parameters during the training, which is often referred to as domain randomization (19). It can be combined with adaptation methods (18,20) such that the policy can be adaptive to a new environment from the training domain distribution. However, because it has a prior bias toward the trained environments, it fails when the new environment is considerably different from the training domain distribution. Because all of the aforementioned works used only hard-contact simulation for training, their performance was highly compromised on soft terrain as the speed of the robot increased. In such conditions, the feet are completely buried into the ground, and rigid-body simulators cannot generate meaningful data for a sim-to-real transfer.\n\nOne of the primary reasons for such a trend is a lack of an efficient simulation pipeline for soft and deformable terrain simulation. The leading simulators that these controllers used, such as PyBullet (21), MuJoCo (22), and RaiSim (23), cannot capture the complex dynamics of soft and deformable terrains. They are based on rigid body dynamics or a variation thereof and cannot model the deformation of complex terrains such as a sand beach. Consequently, the necessity of an accurate and fast simulation solution for such terrains arises.\n\nIn the granular physics community, there have been multiple efforts to simulate soft substrates. Specifically, granular media (GM)-a collection of discrete solid particles-has been studied to model the naturally occurring soft ground. Unfortunately, a unified solution to simulate intrusions on diverse soft and deformable terrains in 3D has not been found and is still under active research. One of the promising directions for GM simulation is the discrete element method (24,25), which models individual grains and their interactions. This approach requires a prohibitively long computation time, which makes it inappropriate for RL. Some other branches of terramechanics use reduced-order models, such as the unidirectional spring model (12) and viscoplastic model (11). They have a lower computation cost, but the results might be substantially different from the reality because they cannot capture the hydrodynamic-like nature of GM.\n\nMore promising approaches build on granular resistive force theory (RFT), which is computationally efficient and provides more accurate results in predicting the bulk reaction using empirically derived and experimentally verified models (26)(27)(28). Among them, (26,28) proposed models describing the force exerted on the surface element and predicted the reaction by integrating force elements over the colliding surface. Although numerical integration can provide an accurate estimate of the net force and torque, summing up all reaction force elements over the submerged area involves a large amount of computation, which may obstruct the application of the data-driven method.\n\nIn contrast, the granular cone model proposed by Aguilar and Goldman (27) computes a bulk reaction depending on the intruder's kinematics while accounting for transient granular dynamics, which can be salient in the rapid intrusion. The model eliminates the need for integration over surface, offering a computationally efficient way to approximate the granular intrusion as a point interaction. Although the model cannot explain the rate dependency of granular intrusion captured in (28), its effect is relatively small in the typical operating range of legged machines.\n\nDespite the computational merit, the granular cone model is deficient in simulating general locomotion because it only addresses the vertical reaction for the intruder of a specific shape. To work around this, we further extended and modified the model to efficiently simulate a general multilegged robot under assumptions admissible for shallow penetrations. Specifically, we approximated the interactions between the nonrigid substrates and the intruder as a point contact and modeled the tangential forces as Coulomb friction. We solved this model using the block Gauss-Seidel method to handle multicontact scenarios. The solver used a filter designed to smooth the normal reaction to prevent a positional drift of the intruder. In addition, we introduced the horizontal stroke-resistive (HSR) force model, which describes the impeding reaction exerted by the GM when the intruder travels horizontally. These modifications allowed us to generate sufficient data while maintaining the desired accuracy for a sim-to-real transfer.\n\nHowever, good simulation alone is not sufficient to dynamically control a legged robot because we do not have accurate information about the terrain properties a priori. These properties cannot be estimated reliably with a vision system because they sometimes do not exhibit distinct visual features. To resolve this unperceivable nature of the terrain properties, we used domain randomization of the terrain properties and an adaptive locomotion strategy that uses haptic information from the feet. The strategy is realized with the proposed control architecture, which processes proprioceptive data describing the robot's internal states. It comprises the recurrent encoding module to compress the sensor observation stream and the explicit estimation module for veiled state variables. The controller encodes the history of the observation in both explicit and implicit ways simultaneously to identify the terrain properties and to adapt the control strategy accordingly. Large-scale randomization on simulated terrain characteristics fosters the discerning ability for surroundings and produces a terrain-agnostic controller adaptable to a broad spectrum of environments.\n\nWe followed the presented approach to successfully train a controller for a quadruped robot, Raibo, a dynamic and versatile quadrupedal robot. The robot can run over diverse deformable terrains, including soft beach sand, athletic track, vinyl tile, soft air mattress, and grass field. Compared with the previous controllers, our controller achieves a lower failure rate, higher maximum speed, and lower cost of transport (CoT).\n\n\nRESULTS\n\nThe presented controller trained on simulated deformable terrains has been deployed on the real robot Raibo (fig. S1) in diverse environments. It controls four legs, each of which contains three joints: hip abduction/adduction, hip flexion/extension (HFE), and knee flexion/extension (KFE). Detailed specifications for our target hardware are provided in section S2. Movie 1 contains the results and a brief explanation of our work, and Fig. 1 shows examples of the test environments.\n\nIn the following sections, we used a learned state estimator (7) to estimate the state and related properties thereof. The estimation performance can be found in section S4.\n\n\nBeach sand\n\nThe trained controller demonstrated dynamic locomotion skills over dry and wet beach sand (Movie 1 and Fig. 2). On the dry sand, the feet (6-cm diameter) were sunk entirely into the sand during the stance phase, while the robot was running at high speed. On this terrain, our terrain-agnostic controller was able to achieve forward body speed up to 3.03 m/s and to turn robustly at a yaw rate of around 0.92 rad/s. The wet sand was generally firmer and required higher stress to yield, showing characteristics close to those of the rigid ground. As shown in movie S1, the controller could continue walking on the terrain transitioning from dry to wet sand, and it achieved a similar maximum speed of around 3.0 m/s after the transition. Traversing along the shore, the robot encountered both inclining and declining slopes up to about 4\u00b0and irregular pits and bumps on the surfaces. Although both situations were not modeled in the simulation, the policy could robustly run on those surfaces. Figure 2 (A and B) illustrates the case when the front-left foot encountered a loosely packed region while the robot was running at a speed of around 2.5 m/s. The robot lost its balance because the sand beneath the front-left foot did not generate a sufficient reaction force. It immediately swung the front-right foot with a high clearance and placed it on the next contact point where the reaction force could be stably generated. This sequence of actions could stabilize the robot in a fraction of a second and enable it to achieve near the commanded locomotion speed.\n\nOn the dry beach sand, the proposed method was compared with PL, a state-of-the-art training method for a quadrupedal robot (5,6,16). Using the PL method, we trained two baseline policies as performance references for the presented controller: PL/Env-Rigid and PL/Env-GM. The former was trained in randomized rigid rough terrains, where the hard contact occurs between foot and ground. In contrast, the latter experienced the proposed deformable terrain simulation during training. For training Teacher/Env-GM, the teacher policy of PL/Env-GM, we provided terrain parameters of the simulation as the privileged information. To further study the contribution of simulated terrain characteristics, we additionally trained three ablated controllers by replacing the proposed simulation with uneven rigid terrain, the GM simulation without inertial drag forces, and a spring-damper terrain model. We refer to them as Env-Rigid, Env-Quasistatic, and Env-Linear, respectively. For these ablated controllers, training methods other than the simulation were kept the same as the presented, full-featured controller (Ours). In all controller training, the objective defined by the reward function remained unchanged: to track the desired linear velocity and yaw rate of the torso while achieving high robustness and energy efficiency. Detailed descriptions for the baseline and ablated controllers can be found in section S3.\n\nWe quantitatively evaluated how learning frameworks and training environments affect the performance of the controllers with respect to the objectives. The policies deployed on the real robot underwent five repetitions of a 5-m flat sand track for each commanded forward velocity ranging from 0.5 to 4.0 m/s. The corresponding results on energy efficiency, success rate, foot clearance, torque usage, and speed can be found in Fig. 2 (C to F) and Table 1. In each experiment, the robot was placed 1 m before the start line. Any body-ground contact or joint position limit violation was considered a failure for safety. Throughout the controller evaluations, we used the safety cable to prevent the hardware from being damaged but took care to keep the measurement unaffected. The foot clearances presented in Fig. 2E were calculated as interquartile ranges (IQRs) of foot heights in the sagittal plane, reconstructed from the torso's orientation and joint positions. Figure 2D shows the success rate of the task for each command velocity and controller. On the experimented sand terrain, the presented controller exhibited stable locomotion performance over all command velocities without a single failure. On the contrary, PL/ Env-Rigid achieved an overall success rate of only 37.5%. Unexpectedly, the success rate of this controller versus the commanded speed appears as a bell shape in the plot. The policy achieved high success rates for command velocities between 2.0 and 2.5 m/s but low success rates for higher and lower command velocities. The main reason for terminations in the low command range was insufficient foot lifting, which created a stiff sand pile in front of the feet. In the high command velocity range, on the other hand, severe sinking of the rear stance feet often caused the torso to heavily deviate from its standard orientation and eventually led the robot to failure. PL/Env-GM performed better, achieving an increased success rate of 72.5% and a zero failure rate up to 2.5 m/s command. As shown in Fig. 2E, the policy resulted in much higher foot clearance than the other controllers and safely traversed the sand terrain at low velocities. However, at high command velocities, such motions made the policy susceptible to unmodeled effects. As shown in fig. S5, because the policy has used joint angles close to the limit while producing high clearances, the joint limits were often violated.\n\nNext, we compared the forward locomotion speed achieved by each controller on the sand. The average forward velocity over the 5-m sand track and the top speed after the acceleration phase were measured using a stopwatch and recorded videos. For the latter measurement, the speed was averaged over the last gait cycle before it reached the goal line. Table 1 reports the maximum speed achieved by different controllers. The presented controller outperformed the others by a large margin. Compared with PL/ Env-Rigid and PL/Env-GM, the maximum speed of Ours was 83.3 and 50.7% faster, respectively.\n\nLast, the energy consumption during the locomotion of each controller was analyzed. We computed the CoT for each trial as P/mgV x , where the average power P was calculated as a sum of mechanical power P mech \u00bc P t \u00f0 \u03c4 \ufffd _ q\u00de\u0394t=T and the corresponding rate\nof heat dissipation P heat \u00bc P t \u00f0 \u03c4 \ufffd \u03c4=k 2 m \u00de\u0394t=T. \u03c4 is the joint torques, _\nq is the joint velocities, mg denotes the weight of the robot, T is the time duration from start to the goal line, and V x is the corresponding average forward velocity. We measured \u03c4 and _ q for time t from motor drives with a period of \u0394t. The motor constant k m was provided by the manufacturer. Figure 2C presents the CoTs versus average velocities for different controllers. The presented controller recorded considerably lower CoTs than the controllers trained with PL. The measured torques from each joint, while the robot was running at the average speed nearest to 1 m/s, are shown in Fig. 2F. For the flexion/extension joints, i.e., KFE and HFE joints, the positive direction is for the extension, and the negative direction corresponds to the flexion. Ours used lower peak torques than the other controllers, which contributes to its low CoTs.\n\nThe ablated controllers were evaluated under the same objectives, and the corresponding results are shown in Fig. 2 (C to F) and Table 1. Env-Rigid recorded a 50% overall success rate. Failures occurred at high-speed commands because of similar reasons to PL/ Env-Rigid. Meanwhile, although the low foot clearances had caused the foot to move inside the sand, the penetration was not severe, and the policy could succeed in the task at low speeds. To measure the penetration, we alternatively calculated the average rate of mechanical work done by leg extensions,\nP E mech \u00bc P t \u00f0\u00bd\u03c4 FE \ufffd \u00fe \ufffd \u00bd _ q FE \ufffd \u00fe\n\u00de\u0394t=T, because most of the energy was consumed by the sand during penetration. [x] + denotes an element-wise operation, max(0, x), and \u03c4 FE and _ q FE are joint torques and velocities for flexion/extension joints, respectively. P E mech was 52.5% larger for the PL/Env-Rigid than the Env-Rigid when the average speed was around 1 m/s (Fig. 2F), which suggested a shallower penetration depth of the latter for low-speed locomotion. Env-Quasistatic could complete the task with commands from 1.0 to 3.0 m/s. However, it failed to control the robot safely at velocity commands higher than this range. Although the safety was compromised, the policy achieved a slightly higher energy efficiency than the full-featured controller there. Third, substituting the proposed simulation with the randomized spring-damper terrain made the controller unstable on the yielding terrain. In most cases, Env-Linear did not generate sufficient torques to walk and let the torso fall on the ground. The resulting success rate of 17.5% was the lowest among all alternatives that we tested.\n\nSo far, we have quantitatively analyzed the performance gaps among various controllers on the forward locomotion. However, this does not fully reflect the actual performance gaps that we have encountered during experiments. The controllers not trained in the deformable terrain simulation were very unstable, and terminations often occurred immediately after the initialization or when the robot was walking back to the starting position for a reset. All such scenarios were not recorded as a failure. Furthermore, failures of those trained on the rigid ground or spring-damper system often involved fiercely oscillating motions, which could damage the robot. The presented controller was the only one that could perform all tasks successively without a system reinitialization. We did not observe any unstable oscillations throughout the experiments.\n\n\nVarious terrains\n\nWe further investigated the performance of the presented controller on vinyl tile, athletic track, grass, and an air mattress, as shown in Fig. 3 (A to C and H), to test the controller's adaptability to different ground stiffness. The vinyl tile-covered cement floor was the closest to the ideal rigid ground, having negligible compliance. The porous athletic track was made of polyurethane-bound rubber granules and had lower surface stiffness than the vinyl tiles. The grassy terrain was even softer than the athletic track, and its intrusion dynamics was noticeably different from that of GM. The air mattress allowed much deeper penetrations but differed from our GM simulation because the reaction forces are exerted during lifting as well. In addition, the reaction forces on each foot are highly coupled through the air pressure, and the control problem becomes more challenging.\n\nAlthough none of the tested terrains was simulated during training, the presented controller could generalize to the tested terrains:\n\nThe robot could achieve forward locomotion speed up to 3.13, 3.62, and 3.76 m/s on grass (movie S2), athletic track (movie S3), and vinyl tile (movie S4), respectively. In addition, in all terrains, the robot stably maintained the turning speed of around 0.94 rad/s. Because of the size limitation, we tested only yaw turning on the air mattress, as shown in movie S5. The presented controller could maintain balance and turn at 0.71 rad/s under the command of 1.0 rad/s and at 1.54 rad/s under the command of 2.0 rad/s on the air mattress.\n\nOn the vinyl tile, running track, and grass field, we quantitatively compared the presented controller with Env-Rigid. To this end, we set up a 5-m straight running track on each terrain and repeated the same running test done on the sand. Measurements were obtained the same way, except that we initialized each controller at least 5 m before the start line to evaluate them after a sufficient acceleration period. The corresponding results are shown in Fig. 3 (D and E).\n\nOn all three terrains, both controllers showed comparable locomotion speeds up to the commanded speed of 3.0 m/s, as shown in Fig. 3E. At higher command velocities, however, Env-Rigid experienced substantial degradation in performance on the athletic track and grass. It showed 6.1 and 10.1% lower maximum speed than our controller on the running track and grass, respectively. In particular, Env-Rigid recorded an apparent decline in speed when the command was increased to 3.5 m/s on the grass. In this case, we found that the policy produced an abnormally large foot clearance ( fig. S6), and, consequently, the running speed was lowered. The same problem drove the robot to fail at the command at 4.0 m/s. On the vinyl tile, we could not observe a meaningful difference between the policies.\n\nNext, we compared the CoTs of the controllers on multiple terrains as illustrated in Fig. 3D. On all terrains, including the beach sand (Fig. 2C), the presented controller exhibited comparable or lower CoT than Env-Rigid. However, the CoT increased steeply at high speeds on the running track and grass. We hypothesize that the inaccuracies of the proposed model in simulating the nongranular terrain caused the policy to act more conservatively with higher foot clearances ( fig. S6). More detailed descriptions of this phenomenon are provided in section S5. On all terrains, the CoTs of the presented controller were less sensitive to the ground softness than the CoTs of Env-Rigid.\n\nTo visualize how the presented controller encodes the differences of the terrains in the latent space, we conducted t-distributed stochastic neighbor embedding (t-SNE) on the hidden states h t condensed by the recurrent encoder. Figure 3F shows the result of t-SNE for the commanded controller (1 m/s) deployed on the above three terrains and the dry sand. We found that embedded hidden states distributed distinctly for different terrains and thus carried sufficient information about the terrain type. The band shape of the distribution for each terrain represents the cyclic nature of the gait, as shown in fig. S7.\n\nOn the air mattress, the yaw rate of 1 rad/s was commanded after the robot was initialized on the mattress, as shown in Fig. 3G. We considered more than a half turn a success, and five clockwise and five counterclockwise trials were conducted. To compare the energy efficiency of different controllers, we defined the cost of turning (CoTu) as P/mgl c \u03c9 z , which is analogous to the CoT for turning motions. \u03c9 z is a yaw rate, and l c represents the characteristic length, which was set to the default distance between two diagonal feet. Four controllers were deployed in this setting: Ours, Env-Rigid, PL/Env-GM, and PL/Env-Rigid. Results are shown in Fig. 3H.\n\nThe presented controller was the only one that could perform five turns in a row in each direction. The other controllers exhibited very unstable motions on this terrain. In particular, PL/Env-Rigid could not make even a half turn. Env-Rigid and PL/Env-GM could perform more than half a turn but at a much lower energy efficiency than Ours. This test and the results are illustrated in movie S5.\n\nWe further evaluated the presented controller's ability to adapt to different terrain characteristics during locomotion. In this experiment, the controller encountered a sudden terrain transition from a rigid brick road to a soft memory foam mattress and could stably traverse over it with a command at 0.5 m/s, as shown in movie S6 and fig. S8. In-depth analyses of this experiment are available in section S6.\n\n\nSimulated deformable terrains\n\nWe deployed the controllers on the proposed GM simulation to meticulously investigate the behavioral adaptation under the terrain variation. Test environments were constructed from an equally spaced grid of two stiffness parameters, flat-surface resistive stress (\u03c3 flat ), and conical-surface resistive stress (\u03c3 cone ), where the ranges were 1.0 to 10.0 MN/m 3 and 0.15 to 0.6 MN/m 3 , respectively. The other parameters were kept constant at default values presented in table S3. For each environment, controllers were tested 360 times with a randomly sampled velocity command for 4 s.\n\nIn these simulated environments, we evaluated the policies trained with the proposed simulation or hard contact with reward function and foot clearance to see how locomotion performance and walking behavior change under different terrain characteristics. The clearance was calculated as an average foot height from the mean penetration depth of stance feet. Figure 4 (A to D) and table S1 show the results. Env-Rigid and PL/Env-Rigid showed miserable reward degradation on compliant grounds and did not display a noticeable adaptive behavior in clearance. The foot clearances of Ours and PL/Env-GM adapted flexibly to terrain characteristics (Fig. 4D). Meanwhile, Ours outperformed PL/Env-GM in terms of reward for all conditions tested and used lower foot clearance, which indicates efficient motion. Section S7 provides detailed measurement methods and comparisons.\n\nWe explored an alternative architecture for the encoder from the same simulation settings. We replaced the recurrent encoding module with a time-convolutional network and examined the resulting policy. The recurrent module exhibited a higher overall reward, and the policies with a time-convolutional network did not display a noticeable adaptation to each test terrain. Figure S10 shows the results, and corresponding descriptions are available in section S8.\n\nWe next studied how much each input affects the output in the multilayer perceptron (MLP) actor module included in a trained controller. In the presented controller, the actor determines the subsequent motion while taking the desired velocity command cmd, the encoded observation history h t , and the estimated states e t . Instead of directly examining it, we trained oracle policies, which are policies trained with additional inputs unobservable in the actual application, for further analyses. Oracle(H) policy has the actor that takes parameters for the deformable terrain simulation \u0398 T in addition to our proposed scheme. Oracle(O)'s actor module similarly takes \u0398 T , yet h t is replaced by the current observation o t of current sensor data. Detailed descriptions are provided in section S3. We evaluated the above oracles and Teacher/Env-GM, which also has direct access to \u0398 T , with the same metrics as the other policies. In addition, to understand the input-output relation of the actor modules, we conducted attribution analyses on Oracle(H) and Oracle(O). We simulated each policy for 4 s in 10,800 randomized environments as we did for the training (sections S15 and S16) and calculated integrated gradients (29) and group feature ablations on the actor modules. An averaged input was used for the reference for calculating them, and we took only the magnitudes of the attributions to represent the sensitivity of the output to the input. The results are presented in Fig. 4 (E to G).\n\nThe attribution results implied that a module performing explicit state estimations was the key feature that contributes substantially to the final performance. As shown in Fig. 4E, for both policies, the attribution calculated by group feature ablation was much higher for the estimation group e t compared with the terrain parameter group \u0398 T . Such a high dependency on e t suggested that the estimated features provided the network with decisive features in determining actions. Despite the privileged access to the ground-truth features, Teacher/Env-GM could not reach the reward of the presented controller and the other two oracle policies (Fig. 4, A and  B). These observations demonstrated the efficacy of a concurrently trained state estimation module.\n\nAnother feature of the proposed method is the implicitly encoded observation history, transferred to the actor along with the state estimations. Among Oracle(H) and Oracle(O), the one with the encoded history outperformed the other by a meaningful margin. Oracle(H) exhibited the most skilled locomotion among all policies, as presented in Fig. 4 (A and B). Removing the encoded history from the input resulted in degeneration and recorded a similar mean reward with the presented policy (Ours).\n\nOracle(H) manifests less attribution for the terrain parameters than Oracle(O), as shown in Fig. 4E. In addition, the attribution of terrain parameters was generally more sensitive to some parameters that were used for the simulation when the encoded history was absent, as presented in Fig. 4F and fig. S11. These findings suggested that, when both the encoded history and the true terrain parameters are given to the actor, the actor considerably relied upon the former to get terrain-specific information.\n\nIn the last part, we investigated integrated gradients of each terrain parameter from the same simulation for Oracle(O) to study how much each terrain characteristic affects the following action. Figure 4G shows the result. It revealed that the depth-dependent parts of the ground reaction force corresponding to parameters \u03c3 flat and \u03c3 cone were more substantial than the other parts, such as the inertial force of the ground. Section S9 provides more analyses.\n\n\nDISCUSSION\n\nThe presented controller demonstrated a high level of agility and robustness on various deformable terrains. Our controller stably drove the quadrupedal robot over dry beach sand even when encountering unexpected obstacles such as slopes and loosely packed regions during high-speed locomotion. It substantially outperformed the baseline and ablated controllers regarding success rates, maximum speed, and energy efficiency at all velocity commands. The learned locomotion skill could also adapt to artificial environments or nongranular substrates, although the training environment was confined to a granular model. Its performance was relatively insensitive to wide terrain variations and could generalize to an air mattress that requires fastidious control.\n\nThe ability to acclimate to different ground characteristics primarily emanates from the appropriate terrain distribution for training. Training policies without ground deformation resulted in apparent degradation in performance as the test terrain softness increased. Notably, such degeneration occurred even on the athletic track, where the deformation appears to be negligible. Even when the ground compliance was tackled, neither relying on a simple spring-damper model nor taking only the quasistatic forces could enhance the controller much on the deformable terrain. Our method, established on the appropriate terrain interaction physics, could well approximate the reality, resulting in a performant controller.\n\nBefore this work, simulation techniques for compliant and yielding terrains were not fast enough for RL because they primarily focused on accuracy. Because the computation time made them impractical options, earlier research works had to use rigid ground simulators (5). The proposed methodology enables simulating diverse compliant contact efficiently with admissible accuracy and permits the agent to experience relevant contact scenarios. Consequently, our model tightens the gap between the feasible simulation for training and the physical world so that the RL agent can cultivate adaptability to various nonrigid terrains.\n\nIn addition, this work found that a complementary relationship between the contact model and the training framework could be established through domain randomization. The randomization of terrain parameters compensated for the imperfections in the contact model. Diverse contact scenarios roughly close to the actual reaction of the deformable terrain benefited a resulting controller to generalize well across diverse nonrigid terrains without knowing their properties a priori. We see that the approach is not limited to our application but can be used in other practices where the accurate prediction is too costly.\n\nThe suitable policy architecture for traversing deformable terrain was also essential. Simulation results and the attribution study revealed the efficacies of having distinct input groups of our controller: the explicit state estimations and the implicit history encoding. In the case of latter, we found that a trained policy considerably depended on the encoded history even when the terrain parameters were explicitly given. This means that our recurrent encoding structure effectively condensed the temporal data such that the subsequent actor module could use them easily.\n\nOur recurrent control architecture outperformed the common PL by a wide margin. The performance of the student PL policy was upper-bounded by the teacher policy in an imitation learning setting. On the other hand, our training pipeline is end-to-end and does not suffer from such limitations. Furthermore, our implicit encoding of history conveyed well the terrain-related information (Fig. 3F), and the resulting controller performed better than the teacher policy Teacher/Env-GM.\n\nThere are limitations left to be addressed for future work. We assumed that the contact occurs at a point for fast calculation, and the contact model could admirably approximate the reality in the regime where the intruder is a small convex body and the penetration is shallow. However, if the colliding area between the intruder and the ground increases, the violation of the assumption would severely lower the model accuracy. For applications beyond the current regime, such as flat-plate intruders on the sandy terrain, the contact model should address multiple points or surfaces to maintain accuracy.\n\nAnother direction is to widen the model's capability to simulate various terrains. Although the proposed simulation provided fruitful experiences for the policy, we found some type of terrains where the proposed method yielded slightly higher CoT (Fig. 3D), as discussed in section S5. Enlarging the model capability to other environments, such as grass that has firm ground below, will remedy this problem. In addition, the proposed model is currently insufficient to simulate sloped ground, and the consequences were often observed in the outdoor sandy terrain. To this end, we plan to build a model that predicts granular flow and computes the dynamic terrain normal vector accordingly. Last, our model has to be extended to include propulsive and impeding forces while the feet move upward, which can be observed in many types of nongranular terrains such as mudflats and swamps.\n\nThe presented controller is still blind-it is not using exteroceptive sensors. Although visual features do not convey sufficient information in most cases, they can be combined with the proposed work to further improve the locomotion performance. The fusion of haptic and exteroceptive information for terrain classification can be very challenging because the spatial correlation of the terrain properties has to be taken into account. However, we believe that such a fusion can lead to animal-like locomotion skills in broader wild environments.\n\n\nMATERIALS AND METHODS\n\n\nSimulation\n\nOur work focuses on modifying the contact solver and the underlying contact model to simulate various types of deformable ground. The proposed model and solver find contact forces for each colliding point of the system. Our model has its basis in the model proposed by Aguilar and Goldman (27), but we introduce several simplifications to make the simulation fast and stable. The resulting contact solver implemented on RaiSim (23) enables an RL agent to experience a wide range of terrain characteristics, whereby a robust locomotion policy can be trained. We provide a generic method for robot simulation in section S10, and the following sections describe contact with deformable terrain.\n\n\nTerrain model\n\nThe model introduced by Aguilar and Goldman (27) focuses on the cone-shape jammed grains beneath the intruder to account for the quasistatic and inertial drag force, where the latter comprises a term proportional to squared velocity and an added-mass effect of GM (30,31). It produces a reaction force depending on the accumulation dynamics of the growing cone of densely packed grains, as illustrated in Fig. 5A. The model explains the depth dependency of the granular stiffness by splitting the stress on the conical and flat areas. Section S11 includes explanations and implementation details for the terrain model, including the following modifications.\n\nBecause the original model only explains the interactions of cylindrically shaped objects with GM, it cannot explain the reaction forces on arbitrarily shaped robot feet. Therefore, we assume that intrusions of an arbitrary-shaped intruder would form a similar jamming cone within the predictable range, as discussed by Aguilar and Goldman (27). We substitute the top radius of the developing cone with the hydraulic radius of the cross-section A sub of the intruder, i.e., the intersecting area at z = 0. Although this might lead to a compromise in simulation accuracy, our goal is not to model the terrain exactly; it is to match the distribution of the models with the reality such that our randomized terrains can produce a realistic data distribution for RL. Therefore, we randomize the model parameters such that the ranges of sinkages and slippages in simulation match those in reality.\n\nTo further mimic the deformation characteristics often found in natural soft substrates, we assume that a penetration leaves the ground deformed and that the ground cannot exert force while the intruder moves up. Our model simulates these by conditioning the force to be exerted only on the downward locomotor and absent during upward movement.\n\n\nContact model\n\nTo rapidly simulate the interactions with soft substrates, we approximate the surface contact between a foot and the deformable terrain as a point contact. In other words, we define a contact model to make the resultant force of the terrain model apply to a single contact point. The point is assumed to be the deepest spot of the plunged intruder. Because the terrain model calculates the normal reaction depending on the kinematics, not the shape of the colliding surface, it is suitable for point contact approximation. On the other hand, this conversion yields several problems. First, there is no existing model describing the tangential force for a point contact. The granular cone model describes only the force in the normal direction, and other models based on surface contact (26,28) require integration to get the sum of horizontal forces exerted on the surface elements.\n\nWe assume the Coulomb friction at the contact point between the ground and the intruder to model the tangential force (Fig. 5B). We pose that single Coulomb friction calculated from the resultant lift force can produce enough approximate experiences for training. This assumption reduces the computational complexity, which is beneficial for our data-driven method. We exclude the inertial force terms when computing the tangential forces to further reduce the errors due to the point contact assumption. Agarwal et al. (28) proposed a surface contact model where the inertial force proportional to the squared velocity is only exerted in the surface-normal direction. Because the inertial force is typically the biggest just near the ground surface, including it to produce the friction force will yield an inordinate tangential drag there and result in a considerable dissimilarity with the surface contact model. Rather than taking this difference, we randomized the friction coefficient so that the trained policy can learn robust behaviors. Our model also does not consider static friction because the substance easily yields and flows.\n\nIn addition, to capture the resistive force when the buried foot is dragged horizontally, we introduce the HSR force model (Fig. 5C). It assumes the impact crater as a truncated cone centered at the point where the maximum penetration occurs and the intruder shape as a sphere of a radius r f . Having the height of z max , the cone geometry is parameterized with the radius of the bottom circle r c and the incline angle \u03b3 c . The HSR force model defines the planar reaction force exerted on the intruder when it moves horizontally beyond the crater cone boundary. It uses a simple spring-damper-like model to represent HSR force F HSR based on the horizontal travel distance d HSR as\nd HSR \u00bc k p t \u00c0 p max k \u00fe r f sin \u03b3 c \u00c0 r c \u00c0 z max \u00c0 z t \u00fe r f tan\u00f01\u00deF HSR \u00bc k h \u00f0d HSR \u00fe \u03b2 d z t \u00de \u00fe b h _ p t\u00f02\u00de\nwhere k h is HSR stiffness, p t is the current planar position of the foot, b h is HSR damping factor, and \u03b2 d is the depth dependency scaling factor. p max is the position of the crater center at the start of the penetration. The reaction force F HSR increases as the foot moves deeper into the ground and further away from p max . The force starts acting on the lateral side of the foot against the intruding direction as it penetrates the ground. Considering the plastic deformation on the surface where the horizontal penetration already took place, the force is exerted on the intruder only when d HSR, t > max d HSR, t0 : t \u2212 1 , where t 0 represents the time when the horizontal penetration starts. The point of application is located in the intruder's moving direction. We also randomize the stiffness and the damping factor so that the resulting terrains have a realistic distribution.\n\nThe last issue we address in our model is a drifting phenomenon, which makes the intruder slide on the horizontal plane. Because it originates from the integration error, we use an exponential moving average (EMA) filter for the normal force resulting from the terrain model. Details on this phenomenon and the filter design are available in section S12.\n\n\nContact solver\n\nWe used the projected Gauss-Seidel solver to calculate impulses on each contact instance of the system. Algorithm S1 describes the solver processing N contact instances on the robot. The solver searches for impulses from multiple contacts by updating each contact in turn. The tangential impulse is initialized with a zero vector, and the normal impulse is fixed to the output of the EMA filter. For each update of impulses, the relative contact velocity of the next time step is predicted and used to correct the tangential impulse. Before accepting the new value, the solver checks the friction cone constraint and projects the tangential impulse to the cone's surface if the constraint is violated. The solver iterates updating the impulses until the total amount of an update converges under the predefined threshold 1 \u00d7 10 \u22125 N\u22c5s. Further details are available in section S10.\n\nA good granular model for RL should be fast and able to simulate diverse contact scenarios at the same time. In this light, we compared the proposed method with dynamic RFT (DRFT) (28) by dropping cylindrical objects on each terrain model with randomized parameters. We describe in detail the experiment setup and results in section S13. Our simulation method was about 20 times faster than the DRFT method and made the final positions of the cylinders more widely distributed, as shown in fig. S12. We could not train a controller with the DRFT model because of its prohibitively expensive computation cost. However, we argue that the controller trained with our model will generalize to a wider range of terrains because our model can generate more diverse experiences.\n\n\nTraining\n\nWe used the proposed contact model and solver to build various granular terrain environments with randomized parameters, ranging from ceramic-level stiffness to beach-sand softness. The locomotion problem is formulated as a Markov decision process, as described in section S14. In what follows, we describe details of the training method.\n\n\nController structure\n\nOne of the biggest challenges in such environments is state estimation. Because most state estimators for legged robots rely on a binary contact state, they are compromised in deformable terrains. To this end, we used the learned state estimator presented by Ji et al. (7), which is trained concurrently with the actor. In this way, the estimator network can predict the terrain deformation and adjust its state estimates accordingly. Fig. 6. Overview of the simulation, training, and control method. The training uses the proposed deformable terrain simulation. The policy module maps the current observation, and the desired velocity command to the PD targets for the low-level controller. Given the policy output, the simulation calculates the next state of the robot. During the interactions between the environment and the agent, rewards and ground truths for estimations are stored. An RL algorithm and a supervised learning (SL) objective are used to update the policy module. We apply the domain randomization and curriculum learning to facilitate the training.\n\nOur overall control architecture is shown in Fig. 6. The controller comprises a long short-term memory (32) encoder, MLP estimator, and MLP actor. The encoder observes the current sensor data o t \u00bc \u00f0 \u03c9 t ; \u03c8 t ; q t ; _ q t \u00de, where \u03c9 t is the angular velocity, \u03c8 t is the base orientation, and q t and _ q t are the joint position and velocity. A recurrent structure helps form a compact representation for the history data, which is essential for estimating the terrain properties. The output encoded vector h t is fed to the MLP estimator, which predicts the body height z body , the foot heights z f , and the linear velocity of the robot torso with respect to the body frame V. Given the predicted state and the encoded history, the actor network takes a concatenated vector i t = (cmd, h t , e t ) and produces an action a t that specifies the joint position targets at 100 Hz. cmd represents a 3D vector of the desired velocity, having two elements for the desired translational velocity and one for the desired yaw rate, and e t is an estimation vector e t = (z body , V, z f ). Last, the low-level positionalderivative (PD) controller tracks a t at 4 kHz with fixed gains and zero joint velocity target. Implementation details are provided in section S3.\n\nWe set several important kinematic states to be estimated, but the parameters for the simulation were excluded. It is challenging to infer the terrain parameters from the proprioceptive observation alone because their contributions to the granular reaction force overlap and act synthetically. Instead, we provide an encoded vector to the actor network, enabling it to take account of terrain characteristics implicitly. This approach can be considered a mixture of providing an actor with a latent vector that contains the history information (5,6,16) and using the explicit state estimation module (7).\n\n\nTraining in simulation\n\nDuring training, the algorithm gathered the 4 s of transition data from 360 parallel simulation environments to train the encoder, estimator, and actor networks. We used proximal policy optimization (33) to train the networks, and the estimator and encoder were further subjected to supervised learning. Supervised learning uses the mean squared error with the ground truth from the simulation as a loss function. The net training time for the presented policy was about 50 hours.\n\nTo facilitate the training, we manipulate the initial state distribution to resemble practical scenarios. We make the environment reset in two ways: hard and soft reset. The former simulates the initialization of the robot in a new environment, and we harshly disturb the robot state for this case. On the other hand, the latter represents the case of terrain transitions during control. With this condition, the robot inherits the previous states. Further details for the environment resets are provided in section S15.\n\nWe extensively use domain randomization techniques for the two goals: a seamless sim-to-real transfer and applications in diverse environments (19,34). We broadly randomized terrain parameters so that each simulated agent underwent different intrusion scenarios, and the abundant terrain-foot interactions could be collected. The randomization ranges are presented in table S3. We also noisify the observation to address the sensor noises and uncertainties in the dynamics parameters, such as kinematic positions of the feet, to make the controller more robust. Section S16 describes how each component is randomized.\n\nReward functions are formulated such that the agent gets a high reward as it tracks the commands with energy-efficient and robust motions. It does not specify the gait pattern as trot, although some components prompt symmetric locomotion behaviors. We also use curriculum learning to induce desired behaviors relatively quickly. Reward functions and the curriculum are described in detail in sections S17 and S18, respectively.\n\n\nSupplementary Materials\n\nThis PDF file includes: Sections S1 to S18 Figs. S1 to S12 Tables S1 to S4 Algorithm S1 References (35)(36)(37)(38)(39) Other Supplementary Material for this manuscript includes the following: Movies S1 to S6\n\nMovie 1 .\n1Agile robotic locomotion of Raibo over deformable terrains. Choi et al., Sci. Robot. 8, eade2256 (2023) 25 January 2023 2 of 14\n\nFig. 1 .of 14 Fig. 2 .\n1142Quadrupedal robot on various terrain. The proposed controller was tested on diverse terrains. Choi et al., Sci. Robot. 8, eade2256 (2023) 25 January 2023 3 Deployment on the soft beach sand. The presented controller could traverse robustly on dry beach sand despite some unpredictable obstacles. (A) A sequence of recovering motions from the loosely packed region suddenly encountered by the front-left foot while running at 2.5 m/s. (B) A sequence of the recovery motions reconstructed using state estimation. For (A) and (B), the four right images correspond to 0.27, 0.37, 0.62, and 0.78 s after the time of the leftmost image. (C) CoTs for different average velocities and controllers. CoTs were measured, while the robot was walking on a 5-m sand track and presented as dots. Curves result from the nonlinear fitting of ae bx + cx + b to CoTs. (D) Success rates for different commanded velocities. Each success rate was evaluated over five trials. (E) Foot clearances for different commanded velocities. To measure how high the resultant controllers lifted the feet, we computed the IQRs of the foot heights projected on the sagittal plane. The line graph represents the expectation of IQR over feet and successful trials, and the caps represent the maximum and minimum IQRs among them. (F) The measured torques of the two right legs during forward walking (around 1.0 m/s). Choi et al., Sci. Robot. 8, eade2256 (2023) 25 January 2023 4 of 14\n\nFig. 3 .\n3Evaluation on various terrain. The presented controller was evaluated on vinyl tile (A), athletic track (B), grassy terrain (C), and an air mattress (G). For the former three terrains, the presented controller was compared with the one trained with the rigid contact. (D) CoTs for different average velocities on each terrain. Dots represent the measured CoTs, and curves result from the same fitting withFig. 2D. (E) Measured average velocities against the commanded velocities on each terrain. Caps represent the minimum and maximum speeds given the velocity command. (F) The t-SNE visualization for the encoded latent vectors that were collected on the different terrains using the presented controller. (H) Success rates and CoTus for yaw turning on the air mattress. The success rate was computed from five positive and five negative turns, and the caps represent the maximum and minimum CoTus over successful trials.\n\nFig. 4 .\n4Evaluation in simulation. The controllers were further quantitatively evaluated in the proposed deformable terrain simulation. (A and B) Mean rewards computed with eq. S17 for different ground stiffnesses. (C and D) Mean foot clearances for different ground stiffnesses. They are computed as a time average of the foot height with respect to the average maximum penetration of stance feet z g . The black area represents parameter sets that could not achieve a higher success rate than the criteria, 10%. (E) Group feature ablation results for each group provided to the two oracle policies. (F) Group feature ablation of terrain parameter \u0398 T for different stiffness parameter \u03c3 flat used for the simulation. (E) and (F) only present the magnitude of each attribution to represent the output's sensitivity to the input. (G) Averaged magnitude of integrated gradient attributions on each terrain parameter for three different joints of the legs.\n\nFig. 5 .\n5Contact model definition. (A) The terrain model predicts a vertical component of a ground reaction force on the basis of the intruder's penetration depth and velocity. The calculation involves the developing granular cone beneath the intruder, as Aguilar and Goldman(27)proposed. (B) The surface contact between the intruder and the adjacent substrates is approximated as a point contact at the deepest point. The bulk tangential force from the substrates is assumed to be Coulomb friction. (C) The HSR force model is introduced to simulate the reaction from the substrates when the intruder moves horizontally in the substrates. The force is computed on the basis of the travel distance d HSR and the current penetration depth z t .\n\nTable 1 .\n1Maximum average speed while traversing 5 m and maximum speed after acceleration on beach sand.Controller \n\nMaximum average speed over 5 m (m/s) \nMaximum speed after acceleration (m/s) \nTraining \nframework \n\nTrained \nenvironment \n\nOurs \nGM (Ours) \n2.427 \n3.028 \n\nRigid \n1.163 \n1.234 \n\nQuasistatic \n1.866 \n1.959 \n\nLinear \n1.101 \n1.658 \n\nPL \nGM (Ours) \n1.838 \n2.009 \n\nRigid \n1.567 \n1.652 \n\nChoi et al., Sci. Robot. 8, eade2256 (2023) 25 January 2023 \n5 of 14 \n\n\nH Kolvenbach, P Arm, E Hampp, A Dietsche, V Bickel, B Sun, C Meyer, M Hutter, arXiv:2106.01974Traversing steep and granular martian analog slopes with a dynamic quadrupedal robot. H. Kolvenbach, P. Arm, E. Hampp, A. Dietsche, V. Bickel, B. Sun, C. Meyer, M. Hutter, Tra- versing steep and granular martian analog slopes with a dynamic quadrupedal robot. arXiv:2106.01974 (2021).\n\nIt's just sand between the toes: How particle size and shape variation affect running performance and kinematics in a generalist lizard. P Bergmann, K J Pettinelli, M E Crockett, E G Schaper, J. Exp. Biol. 220Pt 20P. Bergmann, K. J. Pettinelli, M. E. Crockett, E. G. Schaper, It's just sand between the toes: How particle size and shape variation affect running performance and kinematics in a generalist lizard. J. Exp. Biol. 220(Pt 20), 3706-3716 (2017).\n\nUtilization of granular solidification during terrestrial locomotion of hatchling sea turtles. N Mazouchova, N Gravish, A Savu, D I Goldman, Biol. Lett. 6N. Mazouchova, N. Gravish, A. Savu, D. I. Goldman, Utilization of granular solidification during terrestrial locomotion of hatchling sea turtles. Biol. Lett. 6, 398-401 (2010).\n\nSidewinding with minimal slip: Snake and robot ascent of sandy slopes. H Marvi, C Gong, N Gravish, H Astley, M Travers, R L Hatton, J R Mendelson, Iii , H Choset, D L Hu, D I Goldman, Science. 346H. Marvi, C. Gong, N. Gravish, H. Astley, M. Travers, R. L. Hatton, J. R. Mendelson III, H. Choset, D. L. Hu, D. I. Goldman, Sidewinding with minimal slip: Snake and robot ascent of sandy slopes. Science 346, 224-229 (2014).\n\nLearning quadrupedal locomotion over challenging terrain. J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, Sci. Robot. 55986J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, M. Hutter, Learning quadrupedal locomotion over challenging terrain. Sci. Robot. 5, eabc5986 (2020).\n\nRMA: Rapid motor adaptation for legged robots. A Kumar, Z Fu, D Pathak, J Malik, Proceedings of the Robotics: Science and Systems. the Robotics: Science and SystemsA. Kumar, Z. Fu, D. Pathak, J. Malik, RMA: Rapid motor adaptation for legged robots, in Proceedings of the Robotics: Science and Systems (2021).\n\nConcurrent training of a control policy and a state estimator for dynamic and robust legged locomotion. G Ji, J Mun, H Kim, J Hwangbo, IEEE Robot. Autom. Lett. 7G. Ji, J. Mun, H. Kim, J. Hwangbo, Concurrent training of a control policy and a state esti- mator for dynamic and robust legged locomotion. IEEE Robot. Autom. Lett. 7, 4630-4637 (2022).\n\nHigh-speed bounding with the MIT Cheetah 2: Control design and experiments. H W Park, P M Wensing, S Kim, Int. J. Robot. Res. 36H. W. Park, P. M. Wensing, S. Kim, High-speed bounding with the MIT Cheetah 2: Control design and experiments. Int. J. Robot. Res. 36, 167-192 (2017).\n\nSensitive dependence of the motion of a legged robot on granular media. C Li, P B Umbanhowar, H Komsuoglu, D E Koditschek, D I Goldman, Proc. Natl. Acad. Sci. U.S.A. 106C. Li, P. B. Umbanhowar, H. Komsuoglu, D. E. Koditschek, D. I. Goldman, Sensitive depen- dence of the motion of a legged robot on granular media. Proc. Natl. Acad. Sci. U.S.A. 106, 3029-3034 (2009).\n\nTractable terrain-aware motion planning on granular media: An impulsive jumping study. C M Hubicki, J J Aguilar, D I Goldman, A D Ames, Proceedings of the 2016 IEEE/ RSJ International Conference on Intelligent Robots and Systems. the 2016 IEEE/ RSJ International Conference on Intelligent Robots and SystemsIEEEC. M. Hubicki, J. J. Aguilar, D. I. Goldman, A. D. Ames, Tractable terrain-aware motion planning on granular media: An impulsive jumping study, in Proceedings of the 2016 IEEE/ RSJ International Conference on Intelligent Robots and Systems (IEEE, 2016), pp. 3887-3892.\n\nCompliant terrain legged locomotion using a viscoplastic approach. V Vasilopoulos, I S Paraskevas, E G Papadopoulos, Proceedings of the 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems. the 2014 IEEE/RSJ International Conference on Intelligent Robots and SystemsIEEEV. Vasilopoulos, I. S. Paraskevas, E. G. Papadopoulos, Compliant terrain legged locomotion using a viscoplastic approach, in Proceedings of the 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, 2014), pp. 4849-4854.\n\nThe soft-landing problem: Minimizing energy loss by a legged robot impacting yielding terrain. D J Lynch, K M Lynch, P B Umbanhowar, IEEE Robot. Autom. Lett. 5D. J. Lynch, K. M. Lynch, P. B. Umbanhowar, The soft-landing problem: Minimizing energy loss by a legged robot impacting yielding terrain. IEEE Robot. Autom. Lett. 5, 3658-3665 (2020).\n\nLearning terrain dynamics: A gaussian process modeling and optimal control adaptation framework applied to robotic jumping. A H Chang, C M Hubicki, J J Aguilar, D I Goldman, A D Ames, P A Vela, IEEE Trans. Control Syst. Technol. 29A. H. Chang, C. M. Hubicki, J. J. Aguilar, D. I. Goldman, A. D. Ames, P. A. Vela, Learning terrain dynamics: A gaussian process modeling and optimal control adaptation framework applied to robotic jumping. IEEE Trans. Control Syst. Technol. 29, 1581-1596 (2021).\n\nBlind bipedal stair traversal via sim-to-real reinforcement learning. J Siekmann, K Green, J Warila, A Fern, J Hurst, Proceedings of the Robotics: Science and Systems. the Robotics: Science and SystemsJ. Siekmann, K. Green, J. Warila, A. Fern, J. Hurst, Blind bipedal stair traversal via sim-to-real reinforcement learning, in Proceedings of the Robotics: Science and Systems (2021).\n\nLearning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, Sci. Robot. 45872J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, M. Hutter, Learning agile and dynamic motor skills for legged robots. Sci. Robot. 4, eaau5872 (2019).\n\nLearning robust perceptive locomotion for quadrupedal robots in the wild. T Miki, J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, Sci. Robot. 72822T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, M. Hutter, Learning robust perceptive locomotion for quadrupedal robots in the wild. Sci. Robot. 7, eabk2822 (2022).\n\nLearning to walk via deep reinforcement learning. T Haarnoja, S Ha, A Zhou, J Tan, G Tucker, S Levine, Proceedings of the Robotics: Science and Systems. the Robotics: Science and SystemsT. Haarnoja, S. Ha, A. Zhou, J. Tan, G. Tucker, S. Levine, Learning to walk via deep rein- forcement learning, in Proceedings of the Robotics: Science and Systems (2019).\n\nLearning fast adaptation with meta strategy optimization. W Yu, J Tan, Y Bai, E Coumans, S Ha, IEEE Robot. Autom. Lett. 5W. Yu, J. Tan, Y. Bai, E. Coumans, S. Ha, Learning fast adaptation with meta strategy opti- mization. IEEE Robot. Autom. Lett. 5, 2950-2957 (2020).\n\nSim-to-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P , Proceedings of the 2018 IEEE International Conference on Robotics and Automation. the 2018 IEEE International Conference on Robotics and AutomationIEEEX. B. Peng, M. Andrychowicz, W. Zaremba, P. Abbeel, Sim-to-real transfer of robotic control with dynamics randomization, in Proceedings of the 2018 IEEE International Conference on Robotics and Automation (IEEE, 2018), pp. 3803-3810.\n\nFast online adaptation in robotics through meta-learning embeddings of simulated priors. R Kaushik, T Anne, J Mouret, Proceedings of the 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems. the 2020 IEEE/RSJ International Conference on Intelligent Robots and SystemsIEEER. Kaushik, T. Anne, J. Mouret, Fast online adaptation in robotics through meta-learning embeddings of simulated priors, in Proceedings of the 2020 IEEE/RSJ International Confer- ence on Intelligent Robots and Systems (IEEE, 2020), pp. 5269-5276.\n\nPybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, pybullet.orgE. Coumans, Y. Bai, Pybullet, a python module for physics simulation for games, robotics and machine learning (2016);pybullet.org.\n\nMuJoCo: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, Proceedings of the 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. the 2012 IEEE/RSJ International Conference on Intelligent Robots and SystemsE. Todorov, T. Erez, Y. Tassa, MuJoCo: A physics engine for model-based control, in Pro- ceedings of the 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, 2012), pp. 5026-5033.\n\nPer-contact iteration method for solving contact dynamics. J Hwangbo, J Lee, M Hutter, IEEE Robot. Autom. Lett. 3J. Hwangbo, J. Lee, M. Hutter, Per-contact iteration method for solving contact dynamics. IEEE Robot. Autom. Lett. 3, 895-902 (2018).\n\nA discrete numerical model for granular assemblies. P A Cundall, O D L Strack, Geotechnique. 29P. A. Cundall, O. D. L. Strack, A discrete numerical model for granular assemblies. Geo- technique 29, 47-65 (1979).\n\nT P\u00f6schel, T Schwager, Computational Granular Dynamics: Models and Algorithms. Springer-VerlagT. P\u00f6schel, T. Schwager, Computational Granular Dynamics: Models and Algorithms (Springer-Verlag, 2005).\n\nA terradynamics of legged locomotion on granular media. C Li, T Zhang, D I Goldman, Science. 339C. Li, T. Zhang, D. I. Goldman, A terradynamics of legged locomotion on granular media. Science 339, 1408-1412 (2013).\n\nRobophysical study of jumping dynamics on granular media. J Aguilar, D I Goldman, Nat. Phys. 12J. Aguilar, D. I. Goldman, Robophysical study of jumping dynamics on granular media. Nat. Phys. 12, 278-283 (2016).\n\nSurprising simplicity in the modeling of dynamic granular intrusion. S Agarwal, A Karsai, D I Goldman, K Kamrin, Sci. Adv. 7631S. Agarwal, A. Karsai, D. I. Goldman, K. Kamrin, Surprising simplicity in the modeling of dynamic granular intrusion. Sci. Adv. 7, eabe0631 (2021).\n\nM Sundararajan, A Taly, Q Yan, arXiv:1703.01365Axiomatic attribution for deep networks. M. Sundararajan, A. Taly, Q. Yan, Axiomatic attribution for deep networks, arXiv:1703. 01365 (2017).\n\nA hydrodynamic model of locomotion in the Basilisk lizard. J W Glasheen, T A Mcmahon, Nature. 380J. W. Glasheen, T. A. McMahon, A hydrodynamic model of locomotion in the Basilisk lizard. Nature 380, 340-342 (1996).\n\nDrag force scaling for penetration into granular media. H Katsuragi, D Durian, Phys. Rev. E. 8752208H. Katsuragi, D. Durian, Drag force scaling for penetration into granular media. Phys. Rev. E 87, 052208 (2013).\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural Comput. 9S. Hochreiter, J. Schmidhuber, Long short-term memory. Neural Comput. 9, 1735-1780 (1997).\n\nJ Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, Proximal policy optimization algorithms, arXiv:1707.06347 (2017).\n\nLearning dexterous in-hand manipulation. M Andrychowicz, B Baker, M Chociej, R J Zefowicz, B Mcgrew, J Pachocki, A Petron, M Plappert, G Powell, A Ray, J Schneider, S Sidor, J Tobin, P Welinder, L Weng, W Zaremba, Int. J. Robot. Res. 39M. Andrychowicz, B. Baker, M. Chociej, R. J. Zefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, W. Zaremba, Learning dexterous in-hand manipulation. Int. J. Robot. Res. 39, 3-20 (2020).\n\nThe effects of adding noise during backpropagation training on a generalization performance. G An, Neural Comput. 8G. An, The effects of adding noise during backpropagation training on a generalization performance. Neural Comput. 8, 643-674 (1996).\n\nMitigating drift in time series data with noise augmentation. T Fields, G Hsieh, J Chenou, Proceedings of the 2019 International Conference on Computational Science and Computational Intelligence. the 2019 International Conference on Computational Science and Computational IntelligenceIEEET. Fields, G. Hsieh, J. Chenou, Mitigating drift in time series data with noise augmentation, in Proceedings of the 2019 International Conference on Computational Science and Com- putational Intelligence (IEEE, 2019), pp. 227-230.\n\nMini cheetah: A platform for pushing the limits of dynamic quadruped control. B Katz, J D Carlo, S Kim, Proceedings of the 2019 IEEE International Conference on Robotics and Automation. the 2019 IEEE International Conference on Robotics and AutomationIEEEB. Katz, J. D. Carlo, S. Kim, Mini cheetah: A platform for pushing the limits of dynamic quadruped control, in Proceedings of the 2019 IEEE International Conference on Robotics and Automation (IEEE, 2019), pp. 6295-6301.\n\nAn empirical evaluation of generic convolutional and recurrent networks for sequence modeling. S Bai, J Z Kolter, V Koltun, arXiv:1803.01271[cs.LGS. Bai, J. Z. Kolter, V. Koltun, An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv:1803.01271 [cs.LG] (4 Mar 2018).\n\nUnified force law for granular impact cratering. H Katsuragi, D Durian, Nat. Phys. 3H. Katsuragi, D. Durian, Unified force law for granular impact cratering. Nat. Phys. 3, 420-423 (2007).\n", "annotations": {"author": "[{\"end\":70,\"start\":57},{\"end\":85,\"start\":71},{\"end\":100,\"start\":86},{\"end\":115,\"start\":101},{\"end\":133,\"start\":116},{\"end\":143,\"start\":134},{\"end\":158,\"start\":144}]", "publisher": null, "author_last_name": "[{\"end\":69,\"start\":65},{\"end\":84,\"start\":82},{\"end\":99,\"start\":95},{\"end\":114,\"start\":111},{\"end\":132,\"start\":129},{\"end\":142,\"start\":139},{\"end\":157,\"start\":150}]", "author_first_name": "[{\"end\":64,\"start\":57},{\"end\":81,\"start\":71},{\"end\":94,\"start\":86},{\"end\":110,\"start\":101},{\"end\":128,\"start\":121},{\"end\":138,\"start\":134},{\"end\":149,\"start\":144}]", "author_affiliation": null, "title": "[{\"end\":54,\"start\":1},{\"end\":212,\"start\":159}]", "venue": null, "abstract": "[{\"end\":1645,\"start\":237}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2228,\"start\":2225},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2416,\"start\":2413},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2419,\"start\":2416},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2422,\"start\":2419},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2549,\"start\":2546},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2552,\"start\":2549},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2555,\"start\":2552},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2558,\"start\":2555},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2734,\"start\":2731},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2893,\"start\":2889},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2897,\"start\":2893},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2901,\"start\":2897},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3013,\"start\":3009},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3874,\"start\":3870},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3877,\"start\":3874},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3880,\"start\":3877},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4045,\"start\":4042},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4048,\"start\":4045},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4051,\"start\":4048},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4055,\"start\":4051},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4059,\"start\":4055},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4063,\"start\":4059},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4067,\"start\":4063},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4071,\"start\":4067},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4323,\"start\":4320},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4326,\"start\":4323},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4780,\"start\":4776},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4829,\"start\":4825},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4832,\"start\":4829},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5643,\"start\":5639},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5656,\"start\":5652},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5673,\"start\":5669},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6457,\"start\":6453},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6460,\"start\":6457},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6724,\"start\":6720},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6752,\"start\":6748},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7162,\"start\":7158},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7166,\"start\":7162},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7170,\"start\":7166},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7188,\"start\":7184},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7191,\"start\":7188},{\"end\":7677,\"start\":7653},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8092,\"start\":8088},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13194,\"start\":13191},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13196,\"start\":13194},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13199,\"start\":13196},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30050,\"start\":30046},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34325,\"start\":34322},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":39204,\"start\":39200},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":39424,\"start\":39420},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":39427,\"start\":39424},{\"end\":40159,\"start\":40135},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":41862,\"start\":41858},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":41865,\"start\":41862},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":42480,\"start\":42476},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":50083,\"start\":50080},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":50085,\"start\":50083},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":50088,\"start\":50085},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":50139,\"start\":50136},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":50370,\"start\":50366},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":51318,\"start\":51314},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":51321,\"start\":51318},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":52348,\"start\":52344},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":52352,\"start\":52348},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":52356,\"start\":52352},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":52360,\"start\":52356},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":52364,\"start\":52360}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":52593,\"start\":52454},{\"attributes\":{\"id\":\"fig_1\"},\"end\":54069,\"start\":52594},{\"attributes\":{\"id\":\"fig_2\"},\"end\":55003,\"start\":54070},{\"attributes\":{\"id\":\"fig_3\"},\"end\":55960,\"start\":55004},{\"attributes\":{\"id\":\"fig_4\"},\"end\":56705,\"start\":55961},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":57175,\"start\":56706}]", "paragraph": "[{\"end\":2735,\"start\":1661},{\"end\":3881,\"start\":2737},{\"end\":5434,\"start\":3883},{\"end\":5977,\"start\":5436},{\"end\":6919,\"start\":5979},{\"end\":7602,\"start\":6921},{\"end\":8175,\"start\":7604},{\"end\":9208,\"start\":8177},{\"end\":10385,\"start\":9210},{\"end\":10815,\"start\":10387},{\"end\":11311,\"start\":10827},{\"end\":11486,\"start\":11313},{\"end\":13065,\"start\":11501},{\"end\":14483,\"start\":13067},{\"end\":16910,\"start\":14485},{\"end\":17508,\"start\":16912},{\"end\":17766,\"start\":17510},{\"end\":18701,\"start\":17847},{\"end\":19266,\"start\":18703},{\"end\":20377,\"start\":19308},{\"end\":21230,\"start\":20379},{\"end\":22137,\"start\":21251},{\"end\":22272,\"start\":22139},{\"end\":22814,\"start\":22274},{\"end\":23288,\"start\":22816},{\"end\":24085,\"start\":23290},{\"end\":24771,\"start\":24087},{\"end\":25391,\"start\":24773},{\"end\":26055,\"start\":25393},{\"end\":26452,\"start\":26057},{\"end\":26865,\"start\":26454},{\"end\":27487,\"start\":26899},{\"end\":28356,\"start\":27489},{\"end\":28818,\"start\":28358},{\"end\":30322,\"start\":28820},{\"end\":31086,\"start\":30324},{\"end\":31583,\"start\":31088},{\"end\":32093,\"start\":31585},{\"end\":32557,\"start\":32095},{\"end\":33333,\"start\":32572},{\"end\":34054,\"start\":33335},{\"end\":34684,\"start\":34056},{\"end\":35304,\"start\":34686},{\"end\":35883,\"start\":35306},{\"end\":36366,\"start\":35885},{\"end\":36974,\"start\":36368},{\"end\":37859,\"start\":36976},{\"end\":38408,\"start\":37861},{\"end\":39138,\"start\":38447},{\"end\":39813,\"start\":39156},{\"end\":40708,\"start\":39815},{\"end\":41054,\"start\":40710},{\"end\":41954,\"start\":41072},{\"end\":43097,\"start\":41956},{\"end\":43784,\"start\":43099},{\"end\":44795,\"start\":43901},{\"end\":45151,\"start\":44797},{\"end\":46051,\"start\":45170},{\"end\":46824,\"start\":46053},{\"end\":47175,\"start\":46837},{\"end\":48269,\"start\":47200},{\"end\":49534,\"start\":48271},{\"end\":50140,\"start\":49536},{\"end\":50647,\"start\":50167},{\"end\":51169,\"start\":50649},{\"end\":51788,\"start\":51171},{\"end\":52217,\"start\":51790},{\"end\":52453,\"start\":52245}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17846,\"start\":17767},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19307,\"start\":19267},{\"attributes\":{\"id\":\"formula_2\"},\"end\":43855,\"start\":43785},{\"attributes\":{\"id\":\"formula_3\"},\"end\":43900,\"start\":43855}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":14939,\"start\":14932},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":17269,\"start\":17262},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":18839,\"start\":18832}]", "section_header": "[{\"end\":1659,\"start\":1647},{\"end\":10825,\"start\":10818},{\"end\":11499,\"start\":11489},{\"end\":21249,\"start\":21233},{\"end\":26897,\"start\":26868},{\"end\":32570,\"start\":32560},{\"end\":38432,\"start\":38411},{\"end\":38445,\"start\":38435},{\"end\":39154,\"start\":39141},{\"end\":41070,\"start\":41057},{\"end\":45168,\"start\":45154},{\"end\":46835,\"start\":46827},{\"end\":47198,\"start\":47178},{\"end\":50165,\"start\":50143},{\"end\":52243,\"start\":52220},{\"end\":52464,\"start\":52455},{\"end\":52617,\"start\":52595},{\"end\":54079,\"start\":54071},{\"end\":55013,\"start\":55005},{\"end\":55970,\"start\":55962},{\"end\":56716,\"start\":56707}]", "table": "[{\"end\":57175,\"start\":56812}]", "figure_caption": "[{\"end\":52593,\"start\":52466},{\"end\":54069,\"start\":52622},{\"end\":55003,\"start\":54081},{\"end\":55960,\"start\":55015},{\"end\":56705,\"start\":55972},{\"end\":56812,\"start\":56718}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11270,\"start\":11264},{\"end\":11610,\"start\":11604},{\"end\":12502,\"start\":12494},{\"end\":14918,\"start\":14912},{\"end\":15301,\"start\":15294},{\"end\":15461,\"start\":15452},{\"end\":16523,\"start\":16516},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16778,\"start\":16771},{\"end\":18155,\"start\":18146},{\"end\":18448,\"start\":18441},{\"end\":18818,\"start\":18812},{\"end\":19651,\"start\":19642},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21396,\"start\":21390},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23287,\"start\":23271},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23423,\"start\":23416},{\"end\":23880,\"start\":23872},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24179,\"start\":24172},{\"end\":24232,\"start\":24223},{\"end\":24570,\"start\":24563},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25011,\"start\":25002},{\"end\":25390,\"start\":25383},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25520,\"start\":25513},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26054,\"start\":26047},{\"end\":26798,\"start\":26791},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27855,\"start\":27847},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28140,\"start\":28131},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28739,\"start\":28729},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30321,\"start\":30306},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30504,\"start\":30497},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30989,\"start\":30971},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31444,\"start\":31428},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31684,\"start\":31677},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31892,\"start\":31872},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32300,\"start\":32291},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":36279,\"start\":36270},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":37232,\"start\":37223},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":39568,\"start\":39561},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":42083,\"start\":42074},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":43231,\"start\":43222},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46551,\"start\":46543},{\"end\":47641,\"start\":47635},{\"end\":48322,\"start\":48316}]", "bib_author_first_name": "[{\"end\":57178,\"start\":57177},{\"end\":57192,\"start\":57191},{\"end\":57199,\"start\":57198},{\"end\":57208,\"start\":57207},{\"end\":57220,\"start\":57219},{\"end\":57230,\"start\":57229},{\"end\":57237,\"start\":57236},{\"end\":57246,\"start\":57245},{\"end\":57695,\"start\":57694},{\"end\":57707,\"start\":57706},{\"end\":57709,\"start\":57708},{\"end\":57723,\"start\":57722},{\"end\":57725,\"start\":57724},{\"end\":57737,\"start\":57736},{\"end\":57739,\"start\":57738},{\"end\":58111,\"start\":58110},{\"end\":58125,\"start\":58124},{\"end\":58136,\"start\":58135},{\"end\":58144,\"start\":58143},{\"end\":58146,\"start\":58145},{\"end\":58419,\"start\":58418},{\"end\":58428,\"start\":58427},{\"end\":58436,\"start\":58435},{\"end\":58447,\"start\":58446},{\"end\":58457,\"start\":58456},{\"end\":58468,\"start\":58467},{\"end\":58470,\"start\":58469},{\"end\":58480,\"start\":58479},{\"end\":58482,\"start\":58481},{\"end\":58497,\"start\":58494},{\"end\":58501,\"start\":58500},{\"end\":58511,\"start\":58510},{\"end\":58513,\"start\":58512},{\"end\":58519,\"start\":58518},{\"end\":58521,\"start\":58520},{\"end\":58828,\"start\":58827},{\"end\":58835,\"start\":58834},{\"end\":58846,\"start\":58845},{\"end\":58860,\"start\":58859},{\"end\":58870,\"start\":58869},{\"end\":59092,\"start\":59091},{\"end\":59101,\"start\":59100},{\"end\":59107,\"start\":59106},{\"end\":59117,\"start\":59116},{\"end\":59459,\"start\":59458},{\"end\":59465,\"start\":59464},{\"end\":59472,\"start\":59471},{\"end\":59479,\"start\":59478},{\"end\":59780,\"start\":59779},{\"end\":59782,\"start\":59781},{\"end\":59790,\"start\":59789},{\"end\":59792,\"start\":59791},{\"end\":59803,\"start\":59802},{\"end\":60056,\"start\":60055},{\"end\":60062,\"start\":60061},{\"end\":60064,\"start\":60063},{\"end\":60078,\"start\":60077},{\"end\":60091,\"start\":60090},{\"end\":60093,\"start\":60092},{\"end\":60107,\"start\":60106},{\"end\":60109,\"start\":60108},{\"end\":60440,\"start\":60439},{\"end\":60442,\"start\":60441},{\"end\":60453,\"start\":60452},{\"end\":60455,\"start\":60454},{\"end\":60466,\"start\":60465},{\"end\":60468,\"start\":60467},{\"end\":60479,\"start\":60478},{\"end\":60481,\"start\":60480},{\"end\":61001,\"start\":61000},{\"end\":61017,\"start\":61016},{\"end\":61019,\"start\":61018},{\"end\":61033,\"start\":61032},{\"end\":61035,\"start\":61034},{\"end\":61566,\"start\":61565},{\"end\":61568,\"start\":61567},{\"end\":61577,\"start\":61576},{\"end\":61579,\"start\":61578},{\"end\":61588,\"start\":61587},{\"end\":61590,\"start\":61589},{\"end\":61940,\"start\":61939},{\"end\":61942,\"start\":61941},{\"end\":61951,\"start\":61950},{\"end\":61953,\"start\":61952},{\"end\":61964,\"start\":61963},{\"end\":61966,\"start\":61965},{\"end\":61977,\"start\":61976},{\"end\":61979,\"start\":61978},{\"end\":61990,\"start\":61989},{\"end\":61992,\"start\":61991},{\"end\":62000,\"start\":61999},{\"end\":62002,\"start\":62001},{\"end\":62381,\"start\":62380},{\"end\":62393,\"start\":62392},{\"end\":62402,\"start\":62401},{\"end\":62412,\"start\":62411},{\"end\":62420,\"start\":62419},{\"end\":62755,\"start\":62754},{\"end\":62766,\"start\":62765},{\"end\":62773,\"start\":62772},{\"end\":62788,\"start\":62787},{\"end\":62801,\"start\":62800},{\"end\":62812,\"start\":62811},{\"end\":62822,\"start\":62821},{\"end\":63099,\"start\":63098},{\"end\":63107,\"start\":63106},{\"end\":63114,\"start\":63113},{\"end\":63125,\"start\":63124},{\"end\":63139,\"start\":63138},{\"end\":63149,\"start\":63148},{\"end\":63399,\"start\":63398},{\"end\":63411,\"start\":63410},{\"end\":63417,\"start\":63416},{\"end\":63425,\"start\":63424},{\"end\":63432,\"start\":63431},{\"end\":63442,\"start\":63441},{\"end\":63765,\"start\":63764},{\"end\":63771,\"start\":63770},{\"end\":63778,\"start\":63777},{\"end\":63785,\"start\":63784},{\"end\":63796,\"start\":63795},{\"end\":64046,\"start\":64045},{\"end\":64048,\"start\":64047},{\"end\":64056,\"start\":64055},{\"end\":64072,\"start\":64071},{\"end\":64083,\"start\":64082},{\"end\":64562,\"start\":64561},{\"end\":64573,\"start\":64572},{\"end\":64581,\"start\":64580},{\"end\":65103,\"start\":65102},{\"end\":65114,\"start\":65113},{\"end\":65315,\"start\":65314},{\"end\":65326,\"start\":65325},{\"end\":65334,\"start\":65333},{\"end\":65779,\"start\":65778},{\"end\":65790,\"start\":65789},{\"end\":65797,\"start\":65796},{\"end\":66020,\"start\":66019},{\"end\":66022,\"start\":66021},{\"end\":66033,\"start\":66032},{\"end\":66037,\"start\":66034},{\"end\":66181,\"start\":66180},{\"end\":66192,\"start\":66191},{\"end\":66437,\"start\":66436},{\"end\":66443,\"start\":66442},{\"end\":66452,\"start\":66451},{\"end\":66454,\"start\":66453},{\"end\":66655,\"start\":66654},{\"end\":66666,\"start\":66665},{\"end\":66668,\"start\":66667},{\"end\":66878,\"start\":66877},{\"end\":66889,\"start\":66888},{\"end\":66899,\"start\":66898},{\"end\":66901,\"start\":66900},{\"end\":66912,\"start\":66911},{\"end\":67085,\"start\":67084},{\"end\":67101,\"start\":67100},{\"end\":67109,\"start\":67108},{\"end\":67334,\"start\":67333},{\"end\":67336,\"start\":67335},{\"end\":67348,\"start\":67347},{\"end\":67350,\"start\":67349},{\"end\":67547,\"start\":67546},{\"end\":67560,\"start\":67559},{\"end\":67729,\"start\":67728},{\"end\":67743,\"start\":67742},{\"end\":67866,\"start\":67865},{\"end\":67878,\"start\":67877},{\"end\":67888,\"start\":67887},{\"end\":67900,\"start\":67899},{\"end\":67911,\"start\":67910},{\"end\":68146,\"start\":68145},{\"end\":68162,\"start\":68161},{\"end\":68171,\"start\":68170},{\"end\":68182,\"start\":68181},{\"end\":68184,\"start\":68183},{\"end\":68196,\"start\":68195},{\"end\":68206,\"start\":68205},{\"end\":68218,\"start\":68217},{\"end\":68228,\"start\":68227},{\"end\":68240,\"start\":68239},{\"end\":68250,\"start\":68249},{\"end\":68257,\"start\":68256},{\"end\":68270,\"start\":68269},{\"end\":68279,\"start\":68278},{\"end\":68288,\"start\":68287},{\"end\":68300,\"start\":68299},{\"end\":68308,\"start\":68307},{\"end\":68703,\"start\":68702},{\"end\":68922,\"start\":68921},{\"end\":68932,\"start\":68931},{\"end\":68941,\"start\":68940},{\"end\":69460,\"start\":69459},{\"end\":69468,\"start\":69467},{\"end\":69470,\"start\":69469},{\"end\":69479,\"start\":69478},{\"end\":69954,\"start\":69953},{\"end\":69961,\"start\":69960},{\"end\":69963,\"start\":69962},{\"end\":69973,\"start\":69972},{\"end\":70222,\"start\":70221},{\"end\":70235,\"start\":70234}]", "bib_author_last_name": "[{\"end\":57189,\"start\":57179},{\"end\":57196,\"start\":57193},{\"end\":57205,\"start\":57200},{\"end\":57217,\"start\":57209},{\"end\":57227,\"start\":57221},{\"end\":57234,\"start\":57231},{\"end\":57243,\"start\":57238},{\"end\":57253,\"start\":57247},{\"end\":57704,\"start\":57696},{\"end\":57720,\"start\":57710},{\"end\":57734,\"start\":57726},{\"end\":57747,\"start\":57740},{\"end\":58122,\"start\":58112},{\"end\":58133,\"start\":58126},{\"end\":58141,\"start\":58137},{\"end\":58154,\"start\":58147},{\"end\":58425,\"start\":58420},{\"end\":58433,\"start\":58429},{\"end\":58444,\"start\":58437},{\"end\":58454,\"start\":58448},{\"end\":58465,\"start\":58458},{\"end\":58477,\"start\":58471},{\"end\":58492,\"start\":58483},{\"end\":58508,\"start\":58502},{\"end\":58516,\"start\":58514},{\"end\":58529,\"start\":58522},{\"end\":58832,\"start\":58829},{\"end\":58843,\"start\":58836},{\"end\":58857,\"start\":58847},{\"end\":58867,\"start\":58861},{\"end\":58877,\"start\":58871},{\"end\":59098,\"start\":59093},{\"end\":59104,\"start\":59102},{\"end\":59114,\"start\":59108},{\"end\":59123,\"start\":59118},{\"end\":59462,\"start\":59460},{\"end\":59469,\"start\":59466},{\"end\":59476,\"start\":59473},{\"end\":59487,\"start\":59480},{\"end\":59787,\"start\":59783},{\"end\":59800,\"start\":59793},{\"end\":59807,\"start\":59804},{\"end\":60059,\"start\":60057},{\"end\":60075,\"start\":60065},{\"end\":60088,\"start\":60079},{\"end\":60104,\"start\":60094},{\"end\":60117,\"start\":60110},{\"end\":60450,\"start\":60443},{\"end\":60463,\"start\":60456},{\"end\":60476,\"start\":60469},{\"end\":60486,\"start\":60482},{\"end\":61014,\"start\":61002},{\"end\":61030,\"start\":61020},{\"end\":61048,\"start\":61036},{\"end\":61574,\"start\":61569},{\"end\":61585,\"start\":61580},{\"end\":61601,\"start\":61591},{\"end\":61948,\"start\":61943},{\"end\":61961,\"start\":61954},{\"end\":61974,\"start\":61967},{\"end\":61987,\"start\":61980},{\"end\":61997,\"start\":61993},{\"end\":62007,\"start\":62003},{\"end\":62390,\"start\":62382},{\"end\":62399,\"start\":62394},{\"end\":62409,\"start\":62403},{\"end\":62417,\"start\":62413},{\"end\":62426,\"start\":62421},{\"end\":62763,\"start\":62756},{\"end\":62770,\"start\":62767},{\"end\":62785,\"start\":62774},{\"end\":62798,\"start\":62789},{\"end\":62809,\"start\":62802},{\"end\":62819,\"start\":62813},{\"end\":62829,\"start\":62823},{\"end\":63104,\"start\":63100},{\"end\":63111,\"start\":63108},{\"end\":63122,\"start\":63115},{\"end\":63136,\"start\":63126},{\"end\":63146,\"start\":63140},{\"end\":63156,\"start\":63150},{\"end\":63408,\"start\":63400},{\"end\":63414,\"start\":63412},{\"end\":63422,\"start\":63418},{\"end\":63429,\"start\":63426},{\"end\":63439,\"start\":63433},{\"end\":63449,\"start\":63443},{\"end\":63768,\"start\":63766},{\"end\":63775,\"start\":63772},{\"end\":63782,\"start\":63779},{\"end\":63793,\"start\":63786},{\"end\":63799,\"start\":63797},{\"end\":64053,\"start\":64049},{\"end\":64069,\"start\":64057},{\"end\":64080,\"start\":64073},{\"end\":64570,\"start\":64563},{\"end\":64578,\"start\":64574},{\"end\":64588,\"start\":64582},{\"end\":65111,\"start\":65104},{\"end\":65118,\"start\":65115},{\"end\":65323,\"start\":65316},{\"end\":65331,\"start\":65327},{\"end\":65340,\"start\":65335},{\"end\":65787,\"start\":65780},{\"end\":65794,\"start\":65791},{\"end\":65804,\"start\":65798},{\"end\":66030,\"start\":66023},{\"end\":66044,\"start\":66038},{\"end\":66189,\"start\":66182},{\"end\":66201,\"start\":66193},{\"end\":66440,\"start\":66438},{\"end\":66449,\"start\":66444},{\"end\":66462,\"start\":66455},{\"end\":66663,\"start\":66656},{\"end\":66676,\"start\":66669},{\"end\":66886,\"start\":66879},{\"end\":66896,\"start\":66890},{\"end\":66909,\"start\":66902},{\"end\":66919,\"start\":66913},{\"end\":67098,\"start\":67086},{\"end\":67106,\"start\":67102},{\"end\":67113,\"start\":67110},{\"end\":67345,\"start\":67337},{\"end\":67358,\"start\":67351},{\"end\":67557,\"start\":67548},{\"end\":67567,\"start\":67561},{\"end\":67740,\"start\":67730},{\"end\":67755,\"start\":67744},{\"end\":67875,\"start\":67867},{\"end\":67885,\"start\":67879},{\"end\":67897,\"start\":67889},{\"end\":67908,\"start\":67901},{\"end\":67918,\"start\":67912},{\"end\":68159,\"start\":68147},{\"end\":68168,\"start\":68163},{\"end\":68179,\"start\":68172},{\"end\":68193,\"start\":68185},{\"end\":68203,\"start\":68197},{\"end\":68215,\"start\":68207},{\"end\":68225,\"start\":68219},{\"end\":68237,\"start\":68229},{\"end\":68247,\"start\":68241},{\"end\":68254,\"start\":68251},{\"end\":68267,\"start\":68258},{\"end\":68276,\"start\":68271},{\"end\":68285,\"start\":68280},{\"end\":68297,\"start\":68289},{\"end\":68305,\"start\":68301},{\"end\":68316,\"start\":68309},{\"end\":68706,\"start\":68704},{\"end\":68929,\"start\":68923},{\"end\":68938,\"start\":68933},{\"end\":68948,\"start\":68942},{\"end\":69465,\"start\":69461},{\"end\":69476,\"start\":69471},{\"end\":69483,\"start\":69480},{\"end\":69958,\"start\":69955},{\"end\":69970,\"start\":69964},{\"end\":69980,\"start\":69974},{\"end\":70232,\"start\":70223},{\"end\":70242,\"start\":70236}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2106.01974\",\"id\":\"b0\"},\"end\":57555,\"start\":57177},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":8775418},\"end\":58013,\"start\":57557},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":13968923},\"end\":58345,\"start\":58015},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":23364137},\"end\":58767,\"start\":58347},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":224828219},\"end\":59042,\"start\":58769},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":235650916},\"end\":59352,\"start\":59044},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":246822591},\"end\":59701,\"start\":59354},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1297846},\"end\":59981,\"start\":59703},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":8912273},\"end\":60350,\"start\":59983},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":11854605},\"end\":60931,\"start\":60352},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":18983404},\"end\":61468,\"start\":60933},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":202565470},\"end\":61813,\"start\":61470},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":225374235},\"end\":62308,\"start\":61815},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":234762823},\"end\":62693,\"start\":62310},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":58031572},\"end\":63022,\"start\":62695},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":246063414},\"end\":63346,\"start\":63024},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":57189150},\"end\":63704,\"start\":63348},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":203593300},\"end\":63974,\"start\":63706},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3707478},\"end\":64470,\"start\":63976},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":212644474},\"end\":65009,\"start\":64472},{\"attributes\":{\"id\":\"b20\"},\"end\":65262,\"start\":65011},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5230692},\"end\":65717,\"start\":65264},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3742121},\"end\":65965,\"start\":65719},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":128484753},\"end\":66178,\"start\":65967},{\"attributes\":{\"id\":\"b24\"},\"end\":66378,\"start\":66180},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":5625139},\"end\":66594,\"start\":66380},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1680191},\"end\":66806,\"start\":66596},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":231666267},\"end\":67082,\"start\":66808},{\"attributes\":{\"doi\":\"arXiv:1703.01365\",\"id\":\"b28\"},\"end\":67272,\"start\":67084},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4316842},\"end\":67488,\"start\":67274},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":40055519},\"end\":67702,\"start\":67490},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1915014},\"end\":67863,\"start\":67704},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b32\"},\"end\":68102,\"start\":67865},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":51894399},\"end\":68607,\"start\":68104},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":49741739},\"end\":68857,\"start\":68609},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":216043454},\"end\":69379,\"start\":68859},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":199541175},\"end\":69856,\"start\":69381},{\"attributes\":{\"doi\":\"arXiv:1803.01271[cs.LG\",\"id\":\"b37\"},\"end\":70170,\"start\":69858},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":119340448},\"end\":70359,\"start\":70172}]", "bib_title": "[{\"end\":57692,\"start\":57557},{\"end\":58108,\"start\":58015},{\"end\":58416,\"start\":58347},{\"end\":58825,\"start\":58769},{\"end\":59089,\"start\":59044},{\"end\":59456,\"start\":59354},{\"end\":59777,\"start\":59703},{\"end\":60053,\"start\":59983},{\"end\":60437,\"start\":60352},{\"end\":60998,\"start\":60933},{\"end\":61563,\"start\":61470},{\"end\":61937,\"start\":61815},{\"end\":62378,\"start\":62310},{\"end\":62752,\"start\":62695},{\"end\":63096,\"start\":63024},{\"end\":63396,\"start\":63348},{\"end\":63762,\"start\":63706},{\"end\":64043,\"start\":63976},{\"end\":64559,\"start\":64472},{\"end\":65312,\"start\":65264},{\"end\":65776,\"start\":65719},{\"end\":66017,\"start\":65967},{\"end\":66434,\"start\":66380},{\"end\":66652,\"start\":66596},{\"end\":66875,\"start\":66808},{\"end\":67331,\"start\":67274},{\"end\":67544,\"start\":67490},{\"end\":67726,\"start\":67704},{\"end\":68143,\"start\":68104},{\"end\":68700,\"start\":68609},{\"end\":68919,\"start\":68859},{\"end\":69457,\"start\":69381},{\"end\":70219,\"start\":70172}]", "bib_author": "[{\"end\":57191,\"start\":57177},{\"end\":57198,\"start\":57191},{\"end\":57207,\"start\":57198},{\"end\":57219,\"start\":57207},{\"end\":57229,\"start\":57219},{\"end\":57236,\"start\":57229},{\"end\":57245,\"start\":57236},{\"end\":57255,\"start\":57245},{\"end\":57706,\"start\":57694},{\"end\":57722,\"start\":57706},{\"end\":57736,\"start\":57722},{\"end\":57749,\"start\":57736},{\"end\":58124,\"start\":58110},{\"end\":58135,\"start\":58124},{\"end\":58143,\"start\":58135},{\"end\":58156,\"start\":58143},{\"end\":58427,\"start\":58418},{\"end\":58435,\"start\":58427},{\"end\":58446,\"start\":58435},{\"end\":58456,\"start\":58446},{\"end\":58467,\"start\":58456},{\"end\":58479,\"start\":58467},{\"end\":58494,\"start\":58479},{\"end\":58500,\"start\":58494},{\"end\":58510,\"start\":58500},{\"end\":58518,\"start\":58510},{\"end\":58531,\"start\":58518},{\"end\":58834,\"start\":58827},{\"end\":58845,\"start\":58834},{\"end\":58859,\"start\":58845},{\"end\":58869,\"start\":58859},{\"end\":58879,\"start\":58869},{\"end\":59100,\"start\":59091},{\"end\":59106,\"start\":59100},{\"end\":59116,\"start\":59106},{\"end\":59125,\"start\":59116},{\"end\":59464,\"start\":59458},{\"end\":59471,\"start\":59464},{\"end\":59478,\"start\":59471},{\"end\":59489,\"start\":59478},{\"end\":59789,\"start\":59779},{\"end\":59802,\"start\":59789},{\"end\":59809,\"start\":59802},{\"end\":60061,\"start\":60055},{\"end\":60077,\"start\":60061},{\"end\":60090,\"start\":60077},{\"end\":60106,\"start\":60090},{\"end\":60119,\"start\":60106},{\"end\":60452,\"start\":60439},{\"end\":60465,\"start\":60452},{\"end\":60478,\"start\":60465},{\"end\":60488,\"start\":60478},{\"end\":61016,\"start\":61000},{\"end\":61032,\"start\":61016},{\"end\":61050,\"start\":61032},{\"end\":61576,\"start\":61565},{\"end\":61587,\"start\":61576},{\"end\":61603,\"start\":61587},{\"end\":61950,\"start\":61939},{\"end\":61963,\"start\":61950},{\"end\":61976,\"start\":61963},{\"end\":61989,\"start\":61976},{\"end\":61999,\"start\":61989},{\"end\":62009,\"start\":61999},{\"end\":62392,\"start\":62380},{\"end\":62401,\"start\":62392},{\"end\":62411,\"start\":62401},{\"end\":62419,\"start\":62411},{\"end\":62428,\"start\":62419},{\"end\":62765,\"start\":62754},{\"end\":62772,\"start\":62765},{\"end\":62787,\"start\":62772},{\"end\":62800,\"start\":62787},{\"end\":62811,\"start\":62800},{\"end\":62821,\"start\":62811},{\"end\":62831,\"start\":62821},{\"end\":63106,\"start\":63098},{\"end\":63113,\"start\":63106},{\"end\":63124,\"start\":63113},{\"end\":63138,\"start\":63124},{\"end\":63148,\"start\":63138},{\"end\":63158,\"start\":63148},{\"end\":63410,\"start\":63398},{\"end\":63416,\"start\":63410},{\"end\":63424,\"start\":63416},{\"end\":63431,\"start\":63424},{\"end\":63441,\"start\":63431},{\"end\":63451,\"start\":63441},{\"end\":63770,\"start\":63764},{\"end\":63777,\"start\":63770},{\"end\":63784,\"start\":63777},{\"end\":63795,\"start\":63784},{\"end\":63801,\"start\":63795},{\"end\":64055,\"start\":64045},{\"end\":64071,\"start\":64055},{\"end\":64082,\"start\":64071},{\"end\":64086,\"start\":64082},{\"end\":64572,\"start\":64561},{\"end\":64580,\"start\":64572},{\"end\":64590,\"start\":64580},{\"end\":65113,\"start\":65102},{\"end\":65120,\"start\":65113},{\"end\":65325,\"start\":65314},{\"end\":65333,\"start\":65325},{\"end\":65342,\"start\":65333},{\"end\":65789,\"start\":65778},{\"end\":65796,\"start\":65789},{\"end\":65806,\"start\":65796},{\"end\":66032,\"start\":66019},{\"end\":66046,\"start\":66032},{\"end\":66191,\"start\":66180},{\"end\":66203,\"start\":66191},{\"end\":66442,\"start\":66436},{\"end\":66451,\"start\":66442},{\"end\":66464,\"start\":66451},{\"end\":66665,\"start\":66654},{\"end\":66678,\"start\":66665},{\"end\":66888,\"start\":66877},{\"end\":66898,\"start\":66888},{\"end\":66911,\"start\":66898},{\"end\":66921,\"start\":66911},{\"end\":67100,\"start\":67084},{\"end\":67108,\"start\":67100},{\"end\":67115,\"start\":67108},{\"end\":67347,\"start\":67333},{\"end\":67360,\"start\":67347},{\"end\":67559,\"start\":67546},{\"end\":67569,\"start\":67559},{\"end\":67742,\"start\":67728},{\"end\":67757,\"start\":67742},{\"end\":67877,\"start\":67865},{\"end\":67887,\"start\":67877},{\"end\":67899,\"start\":67887},{\"end\":67910,\"start\":67899},{\"end\":67920,\"start\":67910},{\"end\":68161,\"start\":68145},{\"end\":68170,\"start\":68161},{\"end\":68181,\"start\":68170},{\"end\":68195,\"start\":68181},{\"end\":68205,\"start\":68195},{\"end\":68217,\"start\":68205},{\"end\":68227,\"start\":68217},{\"end\":68239,\"start\":68227},{\"end\":68249,\"start\":68239},{\"end\":68256,\"start\":68249},{\"end\":68269,\"start\":68256},{\"end\":68278,\"start\":68269},{\"end\":68287,\"start\":68278},{\"end\":68299,\"start\":68287},{\"end\":68307,\"start\":68299},{\"end\":68318,\"start\":68307},{\"end\":68708,\"start\":68702},{\"end\":68931,\"start\":68921},{\"end\":68940,\"start\":68931},{\"end\":68950,\"start\":68940},{\"end\":69467,\"start\":69459},{\"end\":69478,\"start\":69467},{\"end\":69485,\"start\":69478},{\"end\":69960,\"start\":69953},{\"end\":69972,\"start\":69960},{\"end\":69982,\"start\":69972},{\"end\":70234,\"start\":70221},{\"end\":70244,\"start\":70234}]", "bib_venue": "[{\"end\":59208,\"start\":59175},{\"end\":60659,\"start\":60582},{\"end\":61219,\"start\":61143},{\"end\":62511,\"start\":62478},{\"end\":63534,\"start\":63501},{\"end\":64233,\"start\":64168},{\"end\":64759,\"start\":64683},{\"end\":65511,\"start\":65435},{\"end\":69145,\"start\":69056},{\"end\":69632,\"start\":69567},{\"end\":57355,\"start\":57271},{\"end\":57761,\"start\":57749},{\"end\":58166,\"start\":58156},{\"end\":58538,\"start\":58531},{\"end\":58889,\"start\":58879},{\"end\":59173,\"start\":59125},{\"end\":59512,\"start\":59489},{\"end\":59827,\"start\":59809},{\"end\":60147,\"start\":60119},{\"end\":60580,\"start\":60488},{\"end\":61141,\"start\":61050},{\"end\":61626,\"start\":61603},{\"end\":62042,\"start\":62009},{\"end\":62476,\"start\":62428},{\"end\":62841,\"start\":62831},{\"end\":63168,\"start\":63158},{\"end\":63499,\"start\":63451},{\"end\":63824,\"start\":63801},{\"end\":64166,\"start\":64086},{\"end\":64681,\"start\":64590},{\"end\":65100,\"start\":65011},{\"end\":65433,\"start\":65342},{\"end\":65829,\"start\":65806},{\"end\":66058,\"start\":66046},{\"end\":66257,\"start\":66203},{\"end\":66471,\"start\":66464},{\"end\":66687,\"start\":66678},{\"end\":66929,\"start\":66921},{\"end\":67170,\"start\":67131},{\"end\":67366,\"start\":67360},{\"end\":67581,\"start\":67569},{\"end\":67770,\"start\":67757},{\"end\":67975,\"start\":67936},{\"end\":68336,\"start\":68318},{\"end\":68721,\"start\":68708},{\"end\":69054,\"start\":68950},{\"end\":69565,\"start\":69485},{\"end\":69951,\"start\":69858},{\"end\":70253,\"start\":70244}]"}}}, "year": 2023, "month": 12, "day": 17}