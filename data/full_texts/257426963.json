{"id": 257426963, "updated": "2023-10-05 11:27:24.227", "metadata": {"title": "Three New Validators and a Large-Scale Benchmark Ranking for Unsupervised Domain Adaptation", "authors": "[{\"first\":\"Kevin\",\"last\":\"Musgrave\",\"middle\":[]},{\"first\":\"Serge\",\"last\":\"Belongie\",\"middle\":[]},{\"first\":\"Ser-Nam\",\"last\":\"Lim\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Changes to hyperparameters can have a dramatic effect on model accuracy. Thus, the tuning of hyperparameters plays an important role in optimizing machine-learning models. An integral part of the hyperparameter-tuning process is the evaluation of model checkpoints, which is done through the use of\"validators\". In a supervised setting, these validators evaluate checkpoints by computing accuracy on a validation set that has labels. In contrast, in an unsupervised setting, the validation set has no such labels. Without any labels, it is impossible to compute accuracy, so validators must estimate accuracy instead. But what is the best approach to estimating accuracy? In this paper, we consider this question in the context of unsupervised domain adaptation (UDA). Specifically, we propose three new validators, and we compare and rank them against five other existing validators, on a large dataset of 1,000,000 checkpoints. Extensive experimental results show that two of our proposed validators achieve state-of-the-art performance in various settings. Finally, we find that in many cases, the state-of-the-art is obtained by a simple baseline method. To the best of our knowledge, this is the largest empirical study of UDA validators to date. Code is available at https://www.github.com/KevinMusgrave/powerful-benchmarker.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2208.07360", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": null}}, "content": {"source": {"pdf_hash": "bdb22c36ffb604f5aafec1be3b738b4761644bcf", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2208.07360v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "94fdb8b23e070eefc8a37727eb7da62408f94655", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bdb22c36ffb604f5aafec1be3b738b4761644bcf.txt", "contents": "\nThree New Validators and a Large-Scale Benchmark Ranking for Unsupervised Domain Adaptation\n\n\nKevin Musgrave \nUniversity of Copenhagen\n\n\nCornell Tech \nUniversity of Copenhagen\n\n\nSerge Belongie \nUniversity of Copenhagen\n\n\nSer-Nam Lim \nUniversity of Copenhagen\n\n\nMeta Ai \nUniversity of Copenhagen\n\n\nThree New Validators and a Large-Scale Benchmark Ranking for Unsupervised Domain Adaptation\n\nChanges to hyperparameters can have a dramatic effect on model accuracy. Thus, the tuning of hyperparameters plays an important role in optimizing machine-learning models. An integral part of the hyperparameter-tuning process is the evaluation of model checkpoints, which is done through the use of \"validators\". In a supervised setting, these validators evaluate checkpoints by computing accuracy on a validation set that has labels. In contrast, in an unsupervised setting, the validation set has no such labels. Without any labels, it is impossible to compute accuracy, so validators must estimate accuracy instead. But what is the best approach to estimating accuracy? In this paper, we consider this question in the context of unsupervised domain adaptation (UDA). Specifically, we propose three new validators, and we compare and rank them against five other existing validators, on a large dataset of 1,000,000 checkpoints. Extensive experimental results show that two of our proposed validators achieve state-of-the-art performance in various settings. Finally, we find that in many cases, the state-of-the-art is obtained by a simple baseline method. To the best of our knowledge, this is the largest empirical study of UDA validators to date. Code is available at github.com/KevinMusgrave/powerful-benchmarker.\n\nIntroduction\n\nMachine learning models are improving at a dramatic rate as a result of advances in model architectures [14,50], optimization [17,25], training algorithms [13,15], and scaling [3]. One commonality among these advances is that they require hyperparameter tuning to maximize model accuracy. Indeed, hyperparameter tuning is a crucial component of any machine-learning pipeline.\n\nConsider supervised learning, in which a curated dataset is first divided into training and validation samples. Training samples are used to iteratively update model parameters towards the goal of maximizing accuracy on a specific task.\n\nDuring this process, snapshots (\"checkpoints\") of the model are periodically evaluated by measuring their accuracy on the validation samples. Typically, each training run is brought to an end as soon as checkpoint accuracy plateaus. Once training is complete, the best checkpoint is selected as the \"representative\" for the currently-used hyperparameters, and a new training run begins with a new set of hyperparameters. The process of trying out new hyperparameters is repeated as many times as desired, and at the end of this hyperparameter tuning, the best of the best checkpoints is selected as the final model. Now consider a class of unsupervised learning where the validation samples are unlabeled. This setting is significantly more challenging, because without labels, accuracy on the validation set cannot be directly measured. Instead, only estimates of accuracy are possible, where the estimates are expressed as a set of validation scores produced by a \"validator\". Ideally, the validation scores are perfectly correlated with actual accuracy, but in reality, the correlation may be low. A low correlation yields poor estimates of accuracy, resulting in the selection of sub-optimal checkpoints and hyperparameters. To avoid this, researchers and engineers need to use only the most reliable validators. But which validators are the most reliable? That is the question we attempt to answer in this paper, in the context of unsupervised domain adaptation (UDA). During UDA training, the model has access to both labeled \"source\" data and unlabeled \"target\" data. The goal is to maximize accuracy on the unlabeled target data, but it is impossible to directly measure this. Thus, UDA validators are needed to estimate target-domain accuracy.\n\nUnfortunately, this is not a highly-researched subject. Most existing UDA papers are about algorithms, not validators. Furthermore, most of these papers select checkpoints using the \"oracle\" validator [27], which directly computes target-domain accuracy by accessing target-domain labels. This violates the main assumption of UDA, which is that the target domain does not have labels at all. Hence, the oracle validator cannot be used in real-world applications. Moreover, existing papers that actually do analyze UDA val-idators, evaluate them on checkpoint sets that are too small and homogeneous to accurately reveal which validators are most reliable.\n\nIn this paper, we present the largest empirical study of UDA validators to date:\n\n\u2022 We introduce three new validators, two of which achieve state-of-the-art performance in various settings.\n\n\u2022 We benchmark and rank our proposed validators against five existing validators on a large dataset of 1,000,000 checkpoints generated by ten UDA algorithms and 100 hyperparameter settings per algorithm.\n\nThe paper is organized as follows:\n\n\u2022 Sections 1.1 and 1.2 provide an overview of existing validators, and the three new validators that we propose.\n\n\u2022 Section 2 explains our experiment methodology.\n\n\u2022 Section 3 presents our experiment results.\n\n\u2022 The Appendix provides additional explanations, experiment details, and results.\n\n\nExisting validators Source accuracy\n\nThis is simply the model's accuracy on the source domain:\nAccuracy = 1 N N i=1 1(arg max(p i ) = y i )(1)\nwhere 1 is the indicator function, N is the size of the source dataset, p i is the ith prediction vector, and y i is the label for the ith dataset sample. The assumption here is that the source and target domains are similar enough that high source-accuracy implies high target-accuracy.\n\nReverse validation [59,11] This method consists of two steps. First it trains a model via UDA on the source (S) and target (T ) data, and uses this model to create pseudo-labels for T . Next, it trains a reverse model via UDA on T and S, where T is the pseudo-labeled target data, and S is the \"unlabeled\" source data. The final score is the accuracy of the reverse model on S. One disadvantage of this approach is that it trains two models, doubling the required training time, but still producing only a single usable model. Furthermore, all it does is make it easier to choose between training runs (i.e. for tuning hyperparameters). So selecting the best forward-model checkpoint requires using another validator that can compute scores per checkpoint.\n\n\nEntropy\n\nThis measures the \"confidence\" of the model:\nEntropy = 1 N N i=1 H(p i )(2)H(p i ) = \u2212 C j=1 p ij log p ij(3)\nwhere H(p i ) is the entropy of the ith prediction vector, C is the number of classes, and N is the size of the target dataset. An accurate model will output prediction vectors that have a single large value corresponding with the correct class for each sample. This produces a low entropy score, indicating high confidence. However, this method fails if a model is incorrectly confident. For example, the model might incorrectly classify all samples in the dataset as belonging to the same class.\n\nDeep embedded validation (DEV) [57] This computes a classification loss for every source validation sample, and weights each loss based on the probability that the sample belongs to the target domain. The probability comes from a domain classifier trained on source and target data.\nDEV = L + \u03b7W \u2212 \u03b7 (4) \u03b7 = Cov(L, W ) V ar(W )(5)\nwhere L contains the weighted loss for each source validation sample, W contains the weight of each loss, and L and W are the mean of L and W respectively. One practical issue with DEV is that its scores are unbounded. Very large values can occur if W has low variance, or if L and W have high covariance.\n\nProxy risk [7] This method evaluates checkpoints using a \"check\" model. The check model predicts both class label and domain, and is trained on the transfer task using an algorithm such as DANN [11], with an additional \"disagreement\" loss term on the target samples:\nD = 1 B B i=1 \u2212||x i \u2212 m i || 2(6)\nwhere B is the batch size, x i and m i are the ith prediction vector of the checkpoint and the check model respectively, and ||.|| 2 is the L2 norm function. If the check model maintains a low DANN loss, but obtains outputs that differ from the checkpoint, then the checkpoint likely has low accuracy on the target domain. The disadvantage of this method is that it requires training a DANN-like model for every checkpoint, increasing total training time from O(epochs), to O(epochs 2 ).\n\nEnsemble-based model selection (EMS) [34] This uses a linear regressor trained on 5 signals: target entropy, target diversity, silhoutte & Calinski-Harabasz scores on the target features, source accuracy, and time-consistent pseudo-labels. EMS differs from other methods because it requires a dataset of {signal, ground truth accuracy} pairs to train the regressor. These pairs have to be collected by training a model on a domain adaptation task that has labeled target data. A drawback of this method is that the regressor may overfit and not generalize to our actual UDA task.\n\nSoft neighborhood density (SND) [38] This computes the entropy of the softmaxed target similarity matrix:\nSND = H(softmax \u03c4 ( X))(7)X = F T F(8)\nwhere H is the entropy function, softmax \u03c4 is the softmax function with temperature \u03c4 , X is the similarity matrix, F is the set of L2 normalized target feature vectors, and X is X with the diagonal entries removed. A high SND score means that each feature is close to many other features, which can indicate good clustering. The caveat of SND is that it assumes the model has not mapped all target features into a single cluster. A single cluster would result in a high SND score, but low accuracy.\n\n\nNew validators\n\nHere we explain our proposed validators, which include modifications of existing methods.\n\nBatch nuclear-norm maximization (BNM) [8] BNM is a UDA algorithm which aims to generate predictions that are both diverse and confident. It approaches this via singular value decomposition:\nBNM = ||P || *(9)\nwhere P is the N \u00d7 C prediction matrix (N is the dataset size and C is the number of classes), and ||P || * is the nuclear norm (the sum of the singular values) of P . This simple loss function is highly effective at training UDA models, which leads us to wonder if its numerical value is a proxy for target domain accuracy. We propose using BNM as a validator by applying the BNM loss function to all of the prediction vectors of the source and/or target domain. A drawback of BNM is that the computation can be expensive for large datasets with many classes, though fast approximations do exist [9].\n\nClassAMI [34] proposed using the silhouette score of the target features clustered with k-means (we call this \"ClassSS\"). However, this approach is entirely dependent on the cluster labels. We propose computing the Adjusted Mutual Information (AMI) between cluster labels and the predicted labels, so that the model's predictions are accounted for:\n\nClassAMI = AMI(X, kmeans(F ).labels) (10)\nX i = arg max p i(11)\nwhere X is the predicted labels for the target data, p i is the ith prediction vector, and F is the set of target features.\n\n\nDEV with normalization (DEVN)\n\nOne practical concern with DEV is that \u03b7 can become very large if W has low variance, or if L and W have high covariance. To avoid this, we propose max-normalizing the weights:\nW max = V \u2212 V + 1 (12) V = W max W(13)\nIn the above equations, W is the vector of unnormalized weights, and V is the mean of V .\n\n\nExperiment Methodology\n\nTo allow for efficient benchmarking, we created a dataset of feature vectors extracted from model checkpoints, that could be easily loaded and used as input to all validators. At a high level, the benchmarking process consisted of these steps:\n\n1. Create a dataset of checkpoints:\n\n\u2022 For each UDA algorithm, randomly create 100 hyperparameter settings. \u2022 For each hyperparameter setting, train a model on a UDA task for a fixed number of iterations, and save a model checkpoint at regular intervals. Each checkpoint consists of only the features and logits of the model.\n\n\nCalculate each validator's performance:\n\n\u2022 For every checkpoint, compute the validator's score and the target-domain accuracy. (We are able to compute ground-truth accuracy because we have access to the labels. In a real-world application, the target-domain accuracy cannot be computed.)\n\n\u2022 Compute a rank correlation between the validator's scores and the target-domain accuracies.\n\nIn the rest of this section, we will describe the above steps in detail.\n\n\nCreating the dataset of checkpoints\n\nWe ran experiments on 31 transfer tasks:\n\n\u2022 MNIST: 1 task between MNIST and MNISTM [11].\n\n\u2022 Office31 [36]: 6 tasks between 3 domains (Amazon, DSLR, Webcam).\n\n\u2022 OfficeHome [51]: 12 tasks between 4 domains (Art, Clipart, Product, Real).\n\n\u2022 DomainNet126 [31,37]: 12 tasks between 4 domains (Clipart, Painting, Real, Sketch).\n\nFor the MNIST\u2192MNISTM task, each training run used a LeNet-like model as the trunk, pretrained on MNIST. For Office31, OfficeHome, and DomainNet126, we used a ResNet50 [14] pretrained [52] on ImageNet [35], and finetuned this model on every domain. Then for every task, we started each training run using the model finetuned on the source domain (i.e. the source-only model). We followed this procedure using 10 UDA algorithms (see Table 1), all implemented in PyTorch [30].\n\nFor each UDA algorithm/task pair, we ran 100 steps of random hyperparameter search using Optuna [1]. This full search was run using two different feature layers (the 3rd and 2nd last layers), with the exception of DomainNet126, for which we used just the 2nd-last layer. Each training run lasted for a fixed number of epochs. Features and logits for both source and target datasets were saved at regular intervals, 20 times per training run. The final result was 1,000,000 checkpoints: 10 algorithms * 100 steps of hyperparameter search * 20 checkpoints per training run * (19 tasks * 2 feature layers + 12 tasks * 1 feature layer).\n\n\nSelecting the validators to benchmark\n\nWe benchmarked the validators described in Sections 1.1 and 1.2, excluding those that are impractical to apply on a per-checkpoint basis. Computing scores per-checkpoint is preferred because it allows for faster feedback during training, and a greater likelihood of finding the optimal model.\n\n\nAlgorithm\n\nType of algorithm ATDOC [20] Pseudo labeling BNM [8] BSP [6] SVD loss CDAN [22] DANN [11] GVB [10] Adversarial IM [43] MCC [16] Info max MCD [40] Multiple classifier discrepancy MMD [21] Feature distance As well, it is how checkpoint selection is usually done in the supervised setting. The validation methods we excluded are:\n\n\u2022 Reverse validation, which is typically applied per training run rather than per checkpoint.\n\n\u2022 Proxy risk, which requires training a full UDA model per checkpoint. This increases total training-time complexity to O(epochs 2 ), and is therefore not practical to use on a large scale.\n\n\u2022 EMS, which requires access to a separate dataset with ground-truth target-labels.\n\nEach validator can have multiple variants by changing its parameters. For example, BNM, Entropy, ClassAMI, and ClassSS can be applied to the source, target, or both domains, and DEV, DEVN, and SND can be applied to features or logits. We tested 35 validator variations in total. (See the appendix for details.)\n\n\nMeasuring validator performance\n\nUsing our newly-created dataset of checkpoints, we need to measure each validator's checkpoint-ranking ability. An ideal validator will rank highly the checkpoints with the highest ground-truth target-domain accuracies. In other words, an ideal validator will achieve a high rank correlation between its validation scores and the ground-truth target-domain accuracies. For this purpose, the Spearman rank correlation is one possible metric. However, the Spearman correlation treats all samples equally, whereas we are more interested in the samples with high validation scores or high target-domain accuracies.\n\nFor example, consider a hypothetical set of validation scores that are perfectly correlated with accuracy, with the exception of the highest score that breaks the trend and returns a model with 0% accuracy. The set of scores with perfect correlation is useless, because ultimately, only the model with the highest validation score is selected. In this example, that model has 0% accuracy.\n\nThus, to account for this type of scenario, we use the weighted Spearman correlation (WSC) [2] to give more weight to the samples with high validation scores or high target-domain accuracies (see Figure 1). The weighted Spearman correlation is defined as:\nWSC = N i=1 w i (x i \u2212 x)(y i \u2212 y) N i=1 w i (x i \u2212 x) 2 N i=1 w i (y i \u2212 y) 2 (14) x = N i=1 w i x i N i=1 w i (15) y = N i=1 w i y i N i=1 w i(16)\nwhere w i is the weight of each pair, N is the number of pairs, and x i and y i are the weighted rank of sample i in the x and y variables. Now let's compute the weighted rank for the ith sample. In this equation we will use x as the variable, but the same equation applies for the y variable:\nx i = a i + b i (17) a i = N k=1 w k 1(rank k < rank i ) (18) b i = t + 1 2 w i (19) w i = 1 t N k=1 w k 1(rank k = rank i )(20)\n1 is the indicator function, and t is the number of samples that have the same rank as sample i. The above equations show how to compute the weighted ranks given a set of weights and ranks, but we still need a method for computing the weights themselves, w i . To emphasize samples with a high validation score or high target-domain accuracy, we set w i as:\nw i,v = rank(v(x i )) max 1\u2264k\u2264N rank(v(x k ))(21)w i,a = rank(a(x i )) max 1\u2264k\u2264N rank(a(x k ))(22)w i = max(w i,v , w i,a ) 2(23)\nwhere\n\n\u2022 v(x i ) and a(x i ) are the validation score and targetdomain accuracy of sample x i , respectively.\n\n\u2022 rank(.) is the integer rank obtained by dense-ranking all values, such that the lowest value has a rank of 1.\n\n\u2022 max 1\u2264k\u2264N rank(.) is the maximum rank of all N scores.\n\nThis formulation satisfies our goal of emphasizing samples with high validation scores or high target-domain accuracies. It does this by making w increase quadratically with increasing validation-score ranks or increasing target-domain accuracy ranks.\n\nTo measure a validator's performance across multiple transfer tasks, we use the average weighted Spearman correlation:\nAvg WSC across tasks = 1 T T i=1 WSC(A i , V i )(24)\nwhere A i is the set of target domain accuracies for task i, V i is the set of validator scores for task i, and T is the number of tasks. The comparison of algorithm/validator pairs is a special case where WSC cannot be used. For example, a pair with a high WSC could result in relatively low model accuracy because the algorithm itself performs poorly relative to other algorithms. Thus, for comparing algorithm/validator pairs, we use the average accuracy of each pair's top N training runs (AATN):\nR i = max v\u2208Vi v (25) S i = arg max v\u2208Vi v (26) D = arg sort(R) (27) Avg Acc of Top N = 1 N N i=1\nAcc(C Di,S D i ) (28) where:\n\n\u2022 C i is the ith training run, and C i,j is the jth checkpoint   Figure 1a a much lower score, because there are many high validation scores corresponding with low accuracies. This means that during checkpoint selection, there is a high chance of selecting a low-accuracy checkpoint. In Figure 1c, the worst accuracies correspond with the highest validation scores, and in Figure 1d, accuracies ranging from 40% to 70% all have roughly the same validation score. The Spearman correlation treats all points equally and produces misleading scores for our purposes. In contrast, our weighted Spearman correlation emphasizes the samples with high validation scores, and heavily penalizes these two examples.\nof C i \u2022 V i is the set of validation scores for C i \u2022 R i is the maximum validation score obtained in C i \u2022 S i is the index of R i within C i \u2022 D\n\u2022 Because it relies on more data, WSC is less affected by noise than AATN (see Figure 2). Consequently, if we repeated all our experiments, the WSC ranking of validators would remain quite consistent across our experiments. In contrast, the AATN ranking would vary considerably, as it is more affected by noise.\n\n\u2022 AATN requires a choice of N, and that choice can only be arbitrary, because there is no way to predict which N value will prove most useful.\n\n\nResults\n\nWe followed the experiment methodology described in Section 2 and found the following results:\n\n\u2022 The top 3 algorithm/validator pairs all use the proposed ClassAMI validator (see Table 2).\n\n\u2022 The proposed DEVN validator outperforms DEV on datasets where source validation accuracy performs well (Office31 and OfficeHome). See Figure 3.\n\n\u2022 The proposed BNM validator is the top validator on datasets where source validation accuracy performs poorly (MNIST and DomainNet126). See Figure 3.\n\n\u2022 Source validation accuracy (the baseline method) has the highest average performance for six of the UDA algorithms, and two of the datasets (Office31 and Of-ficeHome). See Figure 3.\n\n\u2022 SND consistently underperforms all of the other validators. See Figure 3. This represents the variance caused by randomness in experiments. The y-axis is the spearman correlation between the ranking of the validators in the noise-added setting and the ranking of the validators in the original setting. As more noise is added, the rankings obtained by WSC remain highly correlated with the original ranking. In contrast, AATN is significantly less consistent, even when N=100.\n\n\u2022 When using the oracle validator (a validator that can directly compute target domain accuracy), most UDA algorithms outperform the source-only model (see Table 3a). However, when using non-oracle validators,  accuracy drops by several percentage points on average (see Table 2). As a result, the UDA algorithms sometimes only match or even degrade the accuracy of the source-only model (see Table 3b). In these cases, it is better to leave the model as-is, instead of training it using UDA algorithms.\n\n\nConclusion\n\nHere are the main takeaways from our benchmark of UDA validators:\n\n\u2022 Practitioners looking for the best model accuracy should use MCC, BNM, or IM, paired with the ClassAMI validator.\n\n\u2022 Researchers creating new UDA algorithms should use ClassAMI or source validation accuracy as the validator, because they are the best-performing validators on 9 out of 10 UDA algorithms. That said, a new UDA algorithm may require a different kind of validator if it is significantly different from the 10 algorithms that we tested.\n\n\u2022 Validators almost always pick sub-optimal checkpoints, which causes accuracy to drop from the oracle level by several percentage points on average. In some cases, this means that UDA algorithms perform worse than untrained models (see Table 3). Thus, there is much room for improvement in the accuracy of UDA validators.\n\nTo unlock the full potential of UDA algorithms and models, more research is needed to improve validator accuracy and consistency. We hope our large-scale benchmark study, and our three new validators, will serve as a useful reference for future research in this area.      \n\n\nA. Validator parameters explained\n\n\nDEV\n\n\nFeatures\n\nThe discriminator is trained on feature vectors.\n\n\nLogits\n\nThe discriminator is trained on logits.\n\n\nPreds\n\nThe discriminator is trained on prediction vectors.\n\n\nDEVN\n\nFeatures, max normalization The discriminator is trained on feature vectors. The sample weights are max-normalized. Logits, max normalization The discriminator is trained on logits. The sample weights are max-normalized.\n\n\nPreds, max normalization\n\nThe discriminator is trained on prediction vectors. The sample weights are max-normalized. The similarity matrix is derived from target features. Softmax temperature is 0.1. Features, \u03c4 = 0.5\n\nThe similarity matrix is derived from target features. Softmax temperature is 0.5. Logits, \u03c4 = 0.05\n\nThe similarity matrix is derived from target logits. Softmax temperature is 0.05. Logits, \u03c4 = 0. 1 The similarity matrix is derived from target logits. Softmax temperature is 0.1 Logits, \u03c4 = 0.5\n\nThe similarity matrix is derived from target logits. Softmax temperature is 0.5 Preds, \u03c4 = 0.05\n\nThe similarity matrix is derived from target predictions. Softmax temperature is 0.05 Preds, \u03c4 = 0. 1 The similarity matrix is derived from target predictions. Softmax temperature is 0.1 Preds, \u03c4 = 0.5\n\nThe similarity matrix is derived from target predictions. Softmax temperature is 0.5      (a) SND applied to the checkpoints of the CDAN algorithm, on the OfficeHome Clipart \u2192 Real task. The weighted Spearman correlation is -81.9. This is SND applied to the prediction vectors, with \u03c4 = 0.05.\nBNM \u03bb bnm \u03bb L [0,1] [0,1] BSP \u03bb bsp \u03bb L log([1e-6,1]) [0,1] CDAN \u03bb D \u03bb G \u03bb L [0,1] [0,1] [0,1] DANN \u03bb D \u03bb grl \u03bb L [0,1] log([0.1,10]) [0,1] GVB \u03bb D \u03bb B G \u03bb B D \u03bb grl [0,1] [0,1] [0,1] log([0.1,10]) IM \u03bb imax \u03bb L [0,1] [0,1] MMD \u03bb F \u03bb L \u03b3 exp [0,1] [0,1] int([1,8]) MCC \u03bb mcc T mcc \u03bb L [0,1] [0.2,5] [0,1] MCD N mcd \u03bb L \u03bb disc int([1,10]) [0,1] [0,1]x, 2 \u22121 x, 2 0 x, 2 1 x, 2 2 x}, where x is the base bandwidth.\n(b) SND applied to the checkpoints of the MMD algorithm, on the OfficeHome Product \u2192 Clipart task. The weighted Spearman correlation is -3.1. This is SND applied to the feature vectors, with \u03c4 = 0.5.  Figure 8: These plots show the checkpoints of the MCC algorithm, using the ClassAMI validator, on a task from each dataset. ClassAMI is the best validator for MCC, as measured by average WSC across tasks (excluding the MNIST task). MCC/ClassAMI is the best performing algorithm/validator pair as measured by AATN across tasks (excluding the MNIST task). Note how MNIST is an outlier in terms of results, dataset attributes, and model architecture. : These plots show the checkpoints of the ATDOC algorithm, using the BNM validator, on a task from each dataset. BNM is the best validator for ATDOC, as measured by average WSC across tasks (excluding the MNIST task). ATDOC/BNM is the 4th-best performing algorithm/validator pair as measured by AATN across tasks (excluding the MNIST task).  Figure 10: These plots show the checkpoints of the DANN algorithm, using the Accuracy (Source Val) validator, on a task from each dataset. Accuracy (Source Val) is the best validator for DANN, as measured by average WSC across tasks (excluding the MNIST task). DANN/Accuracy is the 5th-best performing algorithm/validator pair as measured by AATN across tasks (excluding the MNIST task). Note how the untrained (source-only) model is ranked the highest for both the OfficeHome and DomainNet126 tasks.  All combined Table 11  Table 22  Table 33  ATDOC  Table 12  Table 23  Table 34  BNM  Table 13  Table 24  Table 34  BSP  Table 14  Table 25  Table 34  CDAN  Table 15  Table 26  Table 34  DANN  Table 16  Table 27  Table 34  GVB  Table 17  Table 28  Table 34  IM  Table 18  Table 29  Table 34  MCC  Table 19  Table 30  Table 34  MCD  Table 20  Table 31  Table 34  MMD  Table 21  Table 32  Table 34 What the green coloring means\n\n\nD. MNIST correlation bar plot\n\n\nE. Correlation tables\n\nFor all tables, the green coloring indicates better performance. The greener the cell color, the better the performance, compared to the Source Val Accuracy validator. The best value per column is bolded. The Mean and Std columns are the mean and standard deviation of all task or algorithm columns. A high mean and low standard deviation reflect good performance.                                      \n\n\nE.1. Weighted Spearman Correlation for Office31 and OfficeHome\n\n\nE.2. Weighted Spearman Correlation for DomainNet126\n\n\nE.3. Weighted Spearman Correlation for MNIST\n\n\nF. Summary of UDA Algorithms\n\nThe goal of unsupervised domain adaptation (UDA) is to adapt a model trained on labeled source data, for use on unlabeled target data. Applications of UDA include:\n\n\u2022 semantic segmentation [47] \u2022 object detection [28] \u2022 natural language processing [33] There are also other types of domain adaptation, including:\n\n\u2022 semi-supervised [37] \u2022 multi-source [31] \u2022 partial [29,41,4] \u2022 universal [56] \u2022 source-free [19] In this paper, we focus on UDA for image classification, because it is well-studied and often used as a foundation for other domain adaptation subfields. Here we provide a summary of UDA algorithms by category:\n\n\u2022 Adversarial methods use a GAN where the generator outputs feature vectors. The discriminator's goal is to correctly classify features as coming from the source or target domain, while the generator tries to minimize the discriminator's accuracy. Examples:\n-DANN [11]\n-Domain Confusion [48] -ADDA [49] -CDAN [22] -VADA [44] \u2022 Feature distance losses encourage source and target features to have similar distributions. Examples:\n\n-MMD [21] -CORAL [46] -JMMD [24] \u2022 Maximum classifier discrepancy methods use a generator and multiple classifiers in an adversarial setup. The classifiers' goal is to maximize the difference between their prediction vectors (i.e. after softmax) for the target domain data, while the generator's goal is to minimize this discrepancy. Examples:\n-MCD [40]\n-SWD [18] -STAR [26] \u2022 Information maximization methods use the entropy or mutual information of prediction vectors. Examples:\n\n-ITL [43] -MCC [16] -SENTRY [32] \u2022 SVD losses apply singular value decomposition to the source and/or target features. Examples:\n-BSP [6]\n-BNM [8] \u2022 Image generation methods use a decoder model to generate source/target -like images from feature vectors, usually as part of of an adversarial method. Examples:\n\n-DRCN [12] -GTA [42] \u2022 Pseudo-labeling methods generate labels for the unlabeled target-domain data, to transform the problem from unsupervised to supervised. This is also known as self-supervised learning. Examples:\n\n-ATDA [39] -ATDOC [20] \u2022 Mixup augmentations create training data and features that are a blend between source and target domains. Examples:\n\n-DM-ADA [54] -DMRL [53] \u2022 Other notable methods that are more difficult to categorize include:\n-RTN [23]\n-AFN [55] -DSBN [5] -SymNets [58] -GVB [10] \n\nFigure 1 :\n1These scatter plots show the advantage of the weighted Spearman correlation (WSC) over the Spearman correlation (SC). The Spearman correlation gives roughly the same score forFigures 1a and 1b.In contrast, our weighted Spearman correlation gives\n\nFigure 2 :\n2In this plot, the x-axis is the standard deviation of the random noise added to the target-domain accuracies.\n\nFigure 3 :\n3Each validator's average WSC across tasks (equation 24) within Office31, OfficeHome, and DomainNet126 (see the appendix for MNIST). The error bars represent the standard deviation across transfer tasks. The correlations are computed using the checkpoints of all UDA algorithms.\n\n\n(Source train predictions) Source Train + Target Entropy(Source train predictions) + Entropy(Target predictions) Source Val Entropy(Source validation predictions) Source Val + Target Entropy(Source validation predictions) + Entropy(Target predictions) Target Entropy(Target predictions) SND Features, \u03c4 = 0.05 The similarity matrix is derived from target features. Softmax temperature is 0.05. Features, \u03c4 = 0.1\n\nFigure 4 :Figure 5 :\n45k atdoc Number of nearest neighbors to retrieve for computing pseudolabels in ATDOC N mcd Number of times the MCD generator is updated per batch T mcc Softmax temperature used by MCC We used the Adjusted Mutual Information (ClassAMI) instead of the Silhouette Score (ClassSS) to achieve a significant improvement for the class clustering validation method. These plots are for the OfficeHome Real \u2192 Clipart task. DEV can produce scores approaching infinity(Figure 5a). Our proposed method, DEVN, fixes this problem by normalizing the sample weights.(Figure 5b). These plots are for the OfficeHome Clipart \u2192 Art task.\n\nFigure 6 : 3 Figure 7 :\n637Examples of the SND validator being a poor predictor of accuracy.(a) DomainNet126 Sketch \u2192 Clipart, MCC algorithm, Accuracy (Source Val) validator, WSC = 53.0 (b) DomainNet126 Painting \u2192 Sketch, IM Algorithm, Accuracy (Source Val) validator, WSC = 38.What causes the low WSC for the Accuracy (Source Val) validator on DomainNet126 tasks? Although source validation accuracy ranks most checkpoints correctly, it incorrectly ranks the untrained model as the best. In the above plots, the untrained model is represented by the dot furthest to the right. (a) MNIST \u2192 MNISTM, WSC = -55.4 (b) Office31 Amazon \u2192 DSLR, WSC = 72.7 (c) OfficeHome Art \u2192 Clipart, WSC = 90.1 (d) DomainNet126 Clipart \u2192 Painting, WSC = 95.1\n\n7 Figure 9\n79Amazon \u2192 DSLR, WSC = 93.7 (c) OfficeHome Art \u2192 Clipart, WSC = 88.7 (d) DomainNet126 Clipart \u2192 Painting, WSC = 96.\n\n\nAmazon \u2192 DSLR, WSC = 84.4 (c) OfficeHome Art \u2192 Clipart, WSC = 78.5 (d) DomainNet126 Clipart \u2192 Painting, WSC = 53.5\n\nFigure 11 :\n11Each validator's WSC on the MNIST \u2192 MNISTM transfer task. The correlations are computed using the checkpoints of all UDA algorithms.\n\n\n, \u03c4 = 0.5 -80.9 -85.6 -75.6 -41.2 -80.3 -45.5 -82.8 -81.0 -71.3 -82.5 -82.1 -81.1 -80.4 -77.9 -77.6 -68.9 -77.0 -70.5 -74.6 11.9 Logits, \u03c4 = 0.05 -83.1 -82.8 -88.5 -64.0 -87.0 -54.6 -89.5 -88.4 -88.5 -87.9 -88.8 -89.7 -88.9 -89.8 -88.7 -86.9 -89.6 -84.2 -84.5 9.3 Logits, \u03c4 = 0.1 -89.8 -91.1 -87.9 -79.2 -90.2 -73.2 -90.5 -90.6 -91.6 -89.1 -91.7 -91.6 -89.1 -90.8 -92.3 -89.9 -91.4 -90.3 -88.9 4.7 Logits, \u03c4 = 0.5 -89.0 -90.0 -88.1 -65.5 -89.4 -65.6 -89.0 -88.4 -87.1 -88.3 -89.6 -89.0 -89.3 -87.2 -88.1 -83.8 -85.1 -83.1 -85.3 7.2 Preds, \u03c4 = 0.05 -84.4 -83.7 -86.7 -68.7 -86.1 -47.1 -86.3 -88.3 -84.8 -85.4 -84.5 -85.6 -85.6 -84.8 -82.5 -79.0 -83.7 -76.6 -81.3 9.4 Preds, \u03c4 = 0.1 -88.7 -90.0 -85.0 -87.0 -87.1 -67.0 -89.4 -90.6 -93.0 -88.5 -92.1 -92.3 -87.6 -89.7 -92.0 -86.9 -91.1 -88.3 -88.1 5.6 Preds, \u03c4 = 0.5 -82.6 -86.0 -69.7 -59.2 -72.1 -75.0 -83.8 -83.4 -74.7 -86.7 -82.6 -80.7 -82.8 -79.7 -78.1 -84.5 -79.7 -79.5 -78.9 6.6\n\n\n, \u03c4 = 0.05 -76.6 -74.6 -84.7 -41.7 -84.5 -19.8 -88.3 -86.2 -84.6 -85.2 -81.8 -87.9 -87.3 -89.2 -86.0 -81.5 -88.3 -83.4 -78.4 17.7 Logits, \u03c4 = 0.1 -85.8 -89.0 -86.7 -69.8 -91.1 -42.8 -89.8 -90.7 -91.1 -88.2 -90.0 -91.2 -89.3 -91.0 -92.1 -86.8 -91.4 -89.5 -85.9 11.5 Logits, \u03c4 = 0.5 -89.3 -91.6 -92.1 -70.3 -92.5 -52.0 -87.2 -90.4 -91.3 -89.1 -90.0 -90.8 -89.4 -85.5 -90.8 -84.8 -83.6 -89.7 -86.1 9.7 Preds, \u03c4 = 0.05 -79.8 -81.2 -91.4 -47.7 -90.0 3.8 -87.5 -86.9 -77.4 -79.0 -73.4 -84.2 -86.0 -85.1 -80.8 -69.3 -83.1 -80.2 -75.5 21.5 Preds, \u03c4 = 0.1 -85.9 -90.5 -91.7 -70.0 -93.9 -31.0 -90.7 -90.4 -90.8 -87.2 -90.3 -91.3 -89.2 -92.2 -91.8 -82.3 -92.1 -87.5 -85.5 14.2 Preds, \u03c4 = 0.5 -82.3 -80.0 -48.9 -53.9 -51.2 -79.7 -72.6 -81.5 -79.8 -81.0 -75.3 -76.6 -74.8 -59.5 -73.3 -90.4 -70.1 -87.0 -73.2 11.8\n\n\n, \u03c4 = 0.05 -63.0 -67.9 -88.0 -62.7 -84.9 -52.3 -92.2 -83.9 -85.8 -82.5 -87.3 -89.4 -91.6 -88.6 -86.4 -84.1 -88.0 -86.5 -81.4 11.2 Logits, \u03c4 = 0.1 -76.5 -86.0 -85.5 -72.5 -91.1 -60.0 -94.1 -87.0 -84.3 -83.9 -88.8 -90.1 -88.1 -93.0 -91.8 -84.9 -93.1 -89.1 -85.5 8.2 Logits, \u03c4 = 0.5 -84.0 -90.8 -87.0 -61.7 -91.0 -63.4 -90.3 -81.0 -85.5 -81.0 -87.4 -84.5 -85.6 -84.3 -80.4 -80.4 -76.2 -82.2 -82.0 7.9 Preds, \u03c4 = 0.05 -63.8 -65.2 -91.6 -64.9 -85.7 -31.8 -83.4 -82.3 -86.4 -84.8 -79.9 -81.9 -87.9 -77.3 -71.7 -73.3 -73.4 -77.3 -75.7 13.3 Preds, \u03c4 = 0.1 -80.1 -87.7 -93.0 -93.3 -95.6 -61.0 -97.2 -92.4 -95.9 -84.8 -92.3 -94.6 -89.3 -96.4 -96.4 -82.7 -95.6 -89.2 -89.9 8.5 Preds, \u03c4 = 0.5 -73.8 -87.2 -56.0 -49.9 -65.0 -78.7 -82.4 -72.6 -64.9 -85.7 -79.6 -77.0 -78.7 -75.6 -69.6 -85.2 -72.5 -81.6 -74.2 9.8\n\n\n, \u03c4 = 0.05 -68.0 -66.5 -88.2 -49.0 -81.2 -42.5 -87.8 -80.4 -85.2 -85.4 -82.1 -82.1 -90.1 -89.3 -83.2 -84.2 -86.9 -79.0 -78.4 13.1 Logits, \u03c4 = 0.1 -87.5 -90.8 -88.7 -72.7 -90.2 -70.2 -93.9 -89.1 -91.8 -90.2 -91.7 -90.2 -90.1 -94.6 -93.5 -92.7 -96.0 -92.5 -89.2 6.6 Logits, \u03c4 = 0.5 -92.0 -93.7 -90.2 -51.9 -87.8 -67.3 -87.8 -84.0 -88.8 -85.4 -87.9 -83.8 -85.0 -88.2 -89.4 -88.0 -84.3 -86.9 -84.6 9.6 Preds, \u03c4 = 0.05 -69.2 -66.0 -73.2 -53.6 -73.1 -33.7 -81.0 -83.2 -83.9 -82.9 -76.8 -75.1 -82.7 -78.8 -74.8 -77.5 -75.4 -73.7 -73.0 11.9 Preds, \u03c4 = 0.1 -83.2 -87.5 -73.0 -88.6 -78.0 -64.2 -89.0 -87.4 -94.9 -88.4 -91.4 -91.0 -85.2 -92.1 -93.2 -87.1 -93.6 -88.7 -86.5 7.6 Preds, \u03c4 = 0.5 -83.7 -89.9 -46.3 -47.2 -54.7 -68.1 -84.6 -79.2 -75.4 -86.8 -82.0 -75.3 -75.4 -85.2 -81.2 -88.8 -81.3 -86.5 -76.2 13.2\n\n\n, \u03c4 = 0.5 -92.3 -91.5 -89.1 -71.5 -90.5 -66.5 -90.4 -91.9 -91.0 -90.5 -90.1 -89.1 -91.3 -87.8 -91.6 -86.9 -80.3 -88.1 -87.3 7.0 Preds, \u03c4 = 0.05 -83.3 -84.6 -92.4 -65.4 -90.9 -27.2 -90.4 -80.4 -75.9 -77.0 -77.0 -82.5 -79.1 -84.0 -77.7 -74.7 -79.6 -71.3 -77.4 13.9 Preds, \u03c4 = 0.1 -88.4 -89.0 -90.0 -78.3 -91.7 -49.8 -94.5 -88.4 -90.0 -87.6 -91.1 -91.6 -88.4 -92.5 -90.3 -85.9 -93.6 -87.3 -87.1 9.7 Preds, \u03c4 = 0.5 -82.0 -85.0 -52.6 -43.1 -60.9 -71.9 -80.4 -82.6 -79.4 -86.9 -82.8 -78.2 -78.7 -65.0 -76.3 -91.1 -59.5 -84.1 -74.5 12.7\n\n\nLogits, \u03c4 = 0.5 -81.6 -78.9 -81.0 -42.3 -81.7 -54.6 -77.0 -79.5 -72.6 -83.1 -81.6 -78.1 -79.8 -71.3 -75.4 -73.0 -64.7 -73.2 -73.9 10.3 Preds, \u03c4 = 0.05 -73.2 -54.8 -91.2 -34.1 -90.0 7.7 -69.8 -68.5 -64.2 -65.8 -53.5 -51.4 -61.5 -56.6 -44.3 -39.1 -49.8 -40.6 -55.6 21.7 Preds, \u03c4 = 0.1 -88.0 -87.3 -90.7 -91.0 -90.4 -53.6 -91.7 -91.9 -96.2 -82.7 -92.2 -91.7 -84.8 -90.7 -94.1 -75.6 -88.0 -83.2 -86.9 9.3 Preds, \u03c4 = 0.5 -59.7 -68.7 -14.8 -26.0 -11.1 -64.8 -42.2 -61.9 -35.0 -76.1 -59.5 -42.5 -49.2 -49.6 -47.7 -72.5 -52.2 -66.5 -50.0 18.3\n\n\n, \u03c4 = 0.05 -75.7 -91.4 -84.2 -84.6 -90.9 -73.5 -89.0 -93.9 -84.0 -90.9 -82.7 -85.4 -85.5 6.0 Logits, \u03c4 = 0.1 -75.5 -90.5 -84.4 -84.3 -90.5 -73.3 -88.9 -94.0 -83.7 -90.5 -82.0 -85.2 -85.2 6.0 Logits, \u03c4 = 0.5 -75.7 -87.1 -80.8 -81.4 -85.5 -71.8 -84.9 -81.9 -80.7 -85.8 -78.2 -85.4 -81.6 4.4 Preds, \u03c4 = 0.05 -74.1 -91.5 -84.0 -83.9 -89.3 -72.4 -88.2 -89.7 -83.7 -89.4 -81.5 -84.9 -84.4 5.8 Preds, \u03c4 = 0.1 -75.3 -91.7 -84.4 -84.2 -90.7 -72.5 -89.5 -95.4 -84.3 -90.9 -82.7 -85.4 -85.6 6.4 Preds, \u03c4 = 0.5 -72.5 -86.6 -78.6 -77.7 -78.4 -69.3 -82.5 -75.5 -78.2 -84.2 -78.0 -84.0 -78.8 4.8\n\n\n, \u03c4 = 0.05 -92.3 -91.2 -92.7 -95.0 -92.2 -83.5 -92.5 -91.8 -89.4 -92.7 -92.3 -90.0 -91.3 2.7 Logits, \u03c4 = 0.1 -90.9 -88.8 -90.8 -92.7 -89.1 -81.5 -89.7 -88.6 -87.2 -91.4 -89.2 -84.0 -88.7 3.0 Logits, \u03c4 = 0.5 -88.9 -86.4 -87.3 -89.3 -84.7 -79.3 -86.8 -83.2 -85.6 -87.4 -86.5 -80.1 -85.5 3.0 Preds, \u03c4 = 0.05 -87.7 -88.4 -88.1 -89.3 -90.6 -82.8 -84.8 -83.6 -85.5 -88.0 -87.9 -86.9 -87.0 2.2 Preds, \u03c4 = 0.1 -92.5 -92.6 -92.0 -95.1 -92.1 -83.9 -93.0 -94.6 -90.8 -93.0 -92.6 -92.7 -92.1 2.7 Preds, \u03c4 = 0.5 -90.1 -87.5 -83.8 -81.0 -76.0 -77.3 -85.7 -86.8 -85.6 -86.8 -87.6 -81.9 -84.2 4.1\n\nTable 1 :\n1The 10 UDA algorithms used to create the dataset of feature vectors.\n\n\ncontains the indices to sort R in descending order\u2022 Acc computes target-domain accuracy Should we use AATN instead of WSC, to measure a validator's performance in general? No, for these reasons:\u2022 WSC represents the complete behavior of each validator, because it takes all checkpoints into account, not just the top N.(a) A synthetic example of a \npoor validator. \nSC: 91.6. WSC: 67.0. \n\n(b) A synthetic example of a \nbetter validator. \nSC: 91.6. WSC: 93.9. \n\n(c) ClassSS validator, Office-\nHome Art \u2192 Real. \nSC: 58.5. WSC: 0.0. \n\n(d) Entropy validator, Office-\nHome Real \u2192 Art. \nSC: 84.5. WSC: 24.7. \n\n\n\nTable 2 :\n2This table shows the validator that scores the highest \naverage WSC across Office31, OfficeHome, and Domain-\nNet126, for each UDA algorithm. We exclude MNIST from \nthe calculation because it is an outlier in terms of results, \ndataset attributes, and model architecture. The AATN col-\numn is the average AATN with N = 5 (eq. 28). The Val -\nOracle column is the average drop in AATN when using the \nlisted validator instead of the oracle validator. \n\n\n\n\nThe accuracy of UDA algorithms when using the algorithm/validator pairs shown inTable 2.Office31 \n\nOfficeHome \nMM \nAD AW \nDA DW WA \nWD \nAC \nAP \nAR \nCA \nCP \nCR \nPA \nPC \nPR \nRA \nRC \nRP \n\nSource only 57.6 82.7 80.4 69.6 94.3 71.5 \n99.0 41.2 68.6 76.7 60.2 67.6 70.5 60.0 42.8 76.2 68.8 44.7 79.1 \nATDOC \n60.4 88.8 84.4 72.1 96.6 72.3 \n99.6 47.5 72.3 76.8 63.9 74.6 74.0 65.9 47.9 78.4 73.6 52.2 80.9 \nBNM \n63.3 89.3 91.3 73.5 97.4 74.8 100.0 52.3 74.2 79.9 67.0 74.2 76.7 66.8 51.9 80.4 72.6 56.9 81.9 \nBSP \n57.7 85.3 80.5 69.7 96.4 71.5 \n99.9 43.9 68.6 76.7 60.3 67.6 70.5 60.3 42.8 76.3 69.6 45.9 79.1 \nCDAN \n91.6 88.2 91.3 72.7 96.6 74.1 \n99.8 52.3 70.9 77.6 62.6 69.0 72.5 64.0 53.2 79.9 72.0 57.1 81.3 \nDANN \n92.2 89.6 92.0 72.7 97.1 74.2 \n99.9 53.2 71.3 78.0 63.0 69.5 72.6 64.6 52.6 79.7 73.1 58.1 81.7 \nGVB \n78.8 90.2 91.7 71.5 95.9 74.6 100.0 52.9 70.4 78.3 65.4 71.2 74.5 64.9 53.4 81.1 74.2 56.9 82.3 \nIM \n63.2 89.1 91.3 72.8 96.7 75.0 \n99.9 52.7 73.4 80.3 67.0 74.1 76.5 66.2 51.3 80.9 73.1 56.5 81.9 \nMCC \n69.0 93.3 92.8 73.2 97.7 75.2 100.0 55.1 74.7 81.1 69.7 75.9 77.6 68.3 54.5 82.6 75.3 58.0 83.5 \nMCD \n68.1 88.1 83.3 69.6 98.8 71.5 100.0 43.3 68.8 76.7 60.8 68.5 70.9 61.8 44.9 77.3 72.2 49.4 80.6 \nMMD \n71.6 87.8 88.0 72.1 97.3 72.1 100.0 50.5 71.1 77.4 64.2 69.7 72.1 64.9 48.7 78.8 72.3 52.8 80.5 \n\n(a) The accuracy of UDA algorithms when using the oracle validator. \n\nOffice31 \nOfficeHome \nMM \nAD AW \nDA DW WA \nWD \nAC \nAP \nAR \nCA \nCP \nCR \nPA \nPC \nPR \nRA \nRC \nRP \n\nSource only 57.6 82.7 80.4 69.6 94.3 71.5 \n99.0 41.2 68.6 76.7 60.2 67.6 70.5 60.0 42.8 76.2 68.8 44.7 79.1 \nATDOC \n58.7 85.5 83.8 68.8 95.7 69.5 \n97.8 44.6 71.6 75.1 60.1 73.1 70.6 64.5 42.4 76.6 71.0 49.5 79.7 \nBNM \n24.8 81.6 82.6 71.7 94.3 72.8 \n96.3 49.5 68.0 77.5 60.2 69.6 73.8 61.6 50.5 78.7 67.4 55.8 78.6 \nBSP \n18.2 82.7 80.3 69.3 94.2 43.8 \n96.8 41.2 68.6 76.6 56.7 58.5 65.2 54.7 35.7 74.8 67.3 44.7 79.1 \nCDAN \n57.6 82.2 87.0 69.6 94.3 68.5 \n95.3 41.2 68.6 76.4 56.4 67.9 68.3 62.2 50.0 78.8 70.2 44.7 79.1 \nDANN \n57.6 84.1 87.5 69.5 94.4 68.4 \n97.5 41.2 68.6 76.2 56.7 66.2 70.4 60.8 49.9 77.9 69.8 44.7 79.1 \nGVB \n57.6 82.3 84.4 69.5 94.3 54.9 \n94.9 41.2 68.6 76.7 60.4 64.2 70.6 61.9 50.0 78.5 71.3 44.7 79.1 \nIM \n22.0 81.4 85.9 70.3 94.2 72.7 \n96.5 51.9 68.9 78.2 61.5 69.4 73.2 62.9 49.6 78.7 69.0 55.2 77.9 \nMCC \n49.9 84.9 87.5 69.7 95.3 72.6 \n99.3 52.3 72.8 80.1 62.8 71.8 74.9 63.3 53.2 80.6 69.2 56.7 80.6 \nMCD \n57.4 82.7 80.3 69.4 94.3 45.4 100.0 41.2 68.6 76.7 56.6 66.3 68.6 60.6 41.6 74.4 68.8 44.7 79.1 \nMMD \n57.6 79.7 79.9 69.6 94.3 61.5 \n96.5 41.2 69.0 76.8 57.9 65.4 69.2 62.6 45.8 76.8 70.7 44.7 79.1 \n\n(b) CP \nCR \nCS \nPC \nPR \nPS \nRC \nRP \nRS \nSC \nSP \nSR \n\nSource only 36.4 44.1 46.9 50.9 63.2 45.5 56.9 61.4 47.9 56.4 46.8 46.8 \nATDOC \n41.4 46.9 50.0 56.2 65.1 54.6 60.2 64.6 53.6 62.3 53.4 54.5 \nBNM \n45.5 56.3 54.9 59.6 68.9 57.6 64.4 65.1 58.3 64.8 57.1 61.2 \nBSP \n38.5 45.2 46.9 51.8 63.2 51.2 57.9 61.4 49.1 60.1 50.3 50.4 \nCDAN \n44.8 53.6 52.9 56.0 64.6 55.7 62.6 63.5 58.5 62.0 55.3 58.3 \nDANN \n44.6 53.8 52.4 56.4 64.6 56.0 62.2 63.6 57.7 62.2 54.9 58.0 \nGVB \n44.5 53.5 53.1 56.0 63.9 57.0 64.4 64.0 59.8 61.5 55.1 58.2 \nIM \n45.2 55.1 54.6 59.1 68.9 57.6 63.7 64.6 58.0 64.7 56.2 60.8 \nMCC \n47.8 57.4 56.8 62.5 70.0 59.1 67.5 67.3 61.6 66.9 59.0 63.2 \nMCD \n40.8 47.9 49.7 55.2 64.6 53.7 61.8 64.6 53.1 61.5 54.1 54.9 \nMMD \n42.4 49.2 50.9 57.3 65.2 55.7 61.9 64.3 56.1 62.3 54.4 54.8 \n\n(c) The accuracy of UDA algorithms on DomainNet126, when using the oracle validator. \n\nCP \nCR \nCS \nPC \nPR \nPS \nRC \nRP \nRS \nSC \nSP \nSR \n\nSource only 36.4 44.1 46.9 50.9 63.2 45.5 56.9 61.4 47.9 56.4 46.8 46.8 \nATDOC \n40.7 46.7 49.8 55.6 62.8 53.3 59.7 63.9 53.1 61.8 53.4 53.6 \nBNM \n44.6 55.9 54.4 58.6 68.7 55.8 63.7 63.8 57.8 63.9 56.0 60.1 \nBSP \n36.4 44.1 46.9 50.9 63.2 45.5 56.9 61.4 47.9 56.4 46.8 46.7 \nCDAN \n36.4 44.1 46.9 50.9 63.2 45.5 56.9 61.4 47.9 56.4 46.8 46.7 \nDANN \n36.4 44.1 46.9 50.9 63.2 45.5 56.9 61.4 47.9 56.4 46.8 46.7 \nGVB \n36.4 44.1 46.9 50.9 63.2 45.5 56.9 61.4 47.9 56.4 46.8 46.7 \nIM \n44.7 52.2 54.1 57.1 68.7 55.8 63.2 63.1 57.5 64.0 55.7 59.8 \nMCC \n47.4 56.4 56.3 61.7 69.2 58.4 66.5 66.8 60.7 66.7 58.4 61.7 \nMCD \n36.4 44.1 46.9 50.9 63.2 45.5 56.9 61.4 47.9 56.4 46.8 46.7 \nMMD \n36.4 44.1 46.9 50.9 63.2 45.5 56.9 61.4 47.9 56.4 46.8 46.7 \n\n(d) The accuracy of UDA algorithms on DomainNet126, when using the algorithm/validator pairs shown in Table 2. \n\n\n\nTable 3 :\n3Even the best validators tend to pick sub-optimal checkpoints, which in many cases causes UDA algorithms to \nperform worse than untrained models. These tables show the accuracy of UDA algorithms when using the oracle validator \n(Tables 3a and 3c) and non-oracle validators (Tables 3b and 3d). Each value is the average target-domain accuracy of the \ntop 5 training runs, as determined by the validator (see equation 28). Green cells have an average accuracy greater than the \nsource-only model. A stronger green color indicates higher accuracy. The color scheme is shared between the two tables, i.e. \nthe colors in Tables 3b and 3d are on the same scale as in Tables 3a and 3c respectively. Bold indicates the highest value per \ncolumn, per table. Bolding in Tables 3b and 3d is independent of the bolding in Tables 3a and 3c. \n\n\n\nTable 4 :\n4Each validator has its own settings. In the other tables and figures in this paper, we use short descriptions to indicate what settings are used. This table explains what those short descriptions mean. Note that we L2-normalized the inputs to ClassSS because it performed worse with un-normalized inputs.Validator \nParameters \nExplanation \n\nAccuracy \nSource Train \nAccuracy(Source train predictions) \nSource Val \nAccuracy(Source validation predictions) \n\nBNM \n\nSource Train \nBNM(Source train predictions) \nSource Train + Target \nBNM(Source train predictions) + BNM(Target predictions) \nSource Val \nBNM(Source validation predictions) \nSource Val + Target \nBNM(Source validation predictions) + BNM(Target predictions) \nTarget \nBNM(Target predictions) \n\nClassAMI \n\nSource + Target Features \nClassAMI(concat(Source train features, Target features)) \nSource + Target Logits \nClassAMI(concat(Source train logits, Target logits)) \nTarget Features \nClassAMI(Target features) \nTarget Logits \nClassAMI(Target logits) \n\nClassSS \n\nSource + Target Features \nClassSS(concat(Source train normalized features, Target normalized features)) \nSource + Target Logits \nClassSS(concat(Source train normalized logits, Target normalized logits)) \nTarget Features \nClassSS(Target normalized features) \nTarget Logits \nClassSS(Target normalized logits) \n\n\n\nTable 5 :\n5The validator parameters categorized by function.Validators Parameters \n\nAccuracy, BNM, Entropy Dataset splits \nClassAMI, ClassSS Dataset splits, Choice of feature vector \nDEV, DEVN Choice of feature vector \nSND Choice of feature vector, softmax temperature \n\n\n\nTable 6 :\n6A list of the models used in our experiments. We created the dataset of model outputs using two feature layers: FL3 and FL6. Every checkpoint contains the feature layer (FL3 or FL6) and the logits. The discriminator is used only for adversarial methods. It receives the feature layer as input, but keeps the same depth regardless of feature layer.Layers \nFeature name \n\nTrunk \nLeNet or ResNet50 \n\nClassifier \n\nLinear(256) \nReLU() \nDropout(0.5) \nLinear(128) \nReLU() \nDropout(0.5) \nLinear(num_cls) \nSoftmax() \n\nFL3 \n\nFL6 \nLogits \nPreds \n\nDiscriminator \n\nLinear(2048) \nReLU() \nLinear(2048) \nReLU() \nLinear(1) \n\n\n\nTable 7 :\n7A list of various experiment settings. The learning rate (lr) is one of the hyperparameters. The same lr is used by trunk, classifier, and discriminator.Category \nSettings \n\nOptimizer \n\nAdam [17] \nWeight decay of 1e-4 \nlr \u2208 log([1e-5,0.1]) \n\nLR scheduler \n\nOne Cycle [45] \n5% warmup period \nlr init = lr max /100 \nlr f inal = 0 \nCosine annealing \nBatch size \n64 source + 64 target \n\nEpochs / checkpoint interval \n\nDigits: 100 / 5 \nOffice31 (W and D as target): 2000 / 100 \nOffice31 (A as target): 200 / 10 \nOfficeHome: 200 / 10 \nDomainNet126: 40 / 2 \n\nTraining image transforms \n\nResize(256) \nRandomCrop(224) \nRandomHorizontalFlip() \nNormalize() \n\nValidation image transforms \n\nResize(256) \nCenterCrop(224) \nNormalize() \n\nMNIST image transforms \n\nResize(32) \nGrayscaleToRGB() \nNormalize() \n\n\n\nTable 8 :\n8A list of the hyperparameter search settings used in the experiment.Algorithm Hyperparameter \nSearch space \n\nATDOC \n\n\u03bb atdoc \nk atdoc \n\u03bb L \n\n[0,1] \nint([5, 25], step=5) \n[0,1] \n\n\n\nTable 9 :\n9Description of every hyperparameter that is mentioned inTable 8.Hyperparameter \nDescription \n\n\u03bb atdoc \nATDOC loss weight \n\u03bb bnm \nBNM loss weight \n\u03bb bsp \nBSP loss weight \n\u03bb disc \nClassifier discrepancy loss weight for MCD \n\u03bb grl \nGradient reversal weight, i.e. gradients are multiplied by \u2212\u03bb grl \n\u03bb imax \nInformation maximization loss weight \n\u03bb mcc \nMCC loss weight \n\u03bb B G \nGenerator bridge loss weight for GVB \n\u03bb B D \nDiscriminator bridge loss weight for GVB \n\u03bb D \nDiscriminator loss weight \n\u03bb F \nFeature distance loss weight \n\u03bb G \nGenerator loss weight \n\u03bb L \nSource classification loss weight \n\u03b3 exp \nExponent of the bandwidth multiplier for MMD. For example, if \u03b3 exp = 2, then the \nbandwidths used will be {2 \u22122 \n\nTable 10 :\n10Summary of tables that show the weighted Spearman correlation of the benchmarked validators.Algorithm \nOffice31 & OfficeHome DomainNet126 MNIST\u2192MNISTM (MM) \n\n\n\nTable 11 :\n11The weighted Spearman correlation of each validator/task pair, using the checkpoints of all algorithms.AD \nAW \nDA \nDW \nWA \nWD \nAC \nAP \nAR \nCA \nCP \nCR \nPA \nPC \nPR \nRA \nRC \nRP Mean \nStd \n\nAccuracy \nSource Train \n66.3 \n71.1 \n79.2 \n67.3 \n81.2 \n65.2 \n61.6 \n63.5 \n56.1 \n52.7 \n54.0 \n55.5 \n71.8 \n69.6 \n72.5 \n54.7 \n70.8 \n62.5 \n65.3 \n8.3 \nSource Val \n82.5 \n83.5 \n81.1 \n80.2 \n81.8 \n73.7 \n86.9 \n94.5 \n96.4 \n80.7 \n85.4 \n86.0 \n92.2 \n90.4 \n94.8 \n92.3 \n84.9 \n94.0 \n86.7 \n6.2 \n\nBNM \n\nSource Train \n61.2 \n65.5 \n55.7 \n43.1 \n57.3 \n40.4 \n51.4 \n45.8 \n30.2 \n33.3 \n32.4 \n31.9 \n59.0 \n59.1 \n55.9 \n29.8 \n59.3 \n41.1 \n47.4 11.9 \nSource Train + Target \n57.7 \n69.4 \n56.6 \n53.7 \n66.5 \n40.1 \n65.2 \n57.4 \n49.8 \n51.0 \n55.9 \n54.7 \n58.6 \n65.7 \n63.4 \n40.2 \n71.7 \n51.2 \n57.2 \n8.7 \nSource Val \n63.4 \n65.6 \n54.1 \n55.4 \n60.2 \n49.4 \n79.9 \n74.7 \n62.1 \n35.1 \n37.5 \n34.0 \n68.7 \n71.4 \n63.7 \n55.5 \n80.4 \n65.2 \n59.8 13.6 \nSource Val + Target \n61.5 \n70.0 \n58.7 \n58.0 \n67.2 \n47.8 \n75.1 \n69.6 \n59.5 \n47.2 \n50.3 \n47.6 \n61.6 \n68.5 \n64.4 \n46.7 \n76.6 \n60.3 \n60.6 \n9.4 \nTarget \n55.6 \n66.9 \n51.9 \n54.3 \n65.8 \n40.0 \n62.8 \n57.2 \n51.4 \n52.6 \n59.4 \n59.5 \n57.4 \n64.1 \n63.6 \n40.9 \n70.7 \n52.1 \n57.0 \n8.0 \n\nClassAMI \n\nSource + Target Features \n64.8 \n75.7 \n75.2 \n69.2 \n79.5 \n67.9 \n84.2 \n75.3 \n76.2 \n69.7 \n73.7 \n77.1 \n76.9 \n85.5 \n81.6 \n65.5 \n89.5 \n78.8 \n75.9 \n6.6 \nSource + Target Logits \n64.2 \n74.8 \n73.3 \n68.0 \n78.2 \n66.0 \n82.7 \n72.5 \n73.7 \n66.7 \n69.8 \n74.9 \n73.8 \n84.3 \n79.6 \n63.9 \n89.3 \n78.0 \n74.1 \n6.9 \nTarget Features \n-13.6 -25.6 \n-9.5 \n15.5 \n-8.6 \n20.0 -40.6 -34.7 -28.0 -37.3 -35.4 -32.1 -28.7 -28.0 -22.8 -26.5 -27.5 -20.4 -21.3 16.3 \nTarget Logits \n-17.0 -29.7 -13.1 \n9.7 -13.7 \n15.1 -50.4 -37.9 -29.4 -40.9 -38.3 -33.9 -33.8 -37.6 -27.3 -29.6 -35.9 -22.2 -25.9 16.5 \n\nClassSS \n\nSource + Target Features \n24.1 \n36.2 \n18.6 \n48.0 \n29.3 \n35.4 -14.4 -14.8 \n-0.8 -29.9 \n-6.6 -10.7 -12.9 \n7.3 \n9.6 \n-4.0 \n13.5 \n14.7 \n7.9 20.7 \nSource + Target Logits \n29.2 \n34.4 \n15.2 \n44.2 \n29.2 \n35.7 \n-4.1 \n-4.1 \n6.4 -18.3 \n5.6 \n2.4 \n-6.4 \n13.1 \n16.3 \n5.8 \n16.0 \n22.7 \n13.5 16.3 \nTarget Features \n36.9 \n50.9 \n19.0 \n52.8 \n25.9 \n42.6 -30.3 -16.6 \n-0.5 -29.6 \n2.0 \n-1.9 -27.1 -10.8 \n6.5 \n-3.8 \n-2.4 \n18.8 \n7.4 25.8 \nTarget Logits \n37.6 \n53.4 \n14.6 \n53.5 \n25.2 \n43.1 -13.9 \n-7.5 \n6.3 -20.8 \n11.7 \n7.7 -21.2 \n3.1 \n12.7 \n4.9 \n9.1 \n25.6 \n13.6 22.1 \n\nDEV \n\nFeatures \n-39.0 -48.3 \n28.0 \n53.0 \n24.8 \n35.0 \n16.9 \n23.3 \n11.8 -26.1 -29.2 -22.2 -22.7 -14.2 \n-9.4 -26.4 -10.3 \n1.0 \n-3.0 27.6 \nLogits \n-18.1 -22.7 \n40.9 \n63.8 \n47.4 \n44.0 \n44.4 \n48.2 \n44.1 \n24.9 \n25.8 \n37.6 \n16.4 \n18.9 \n37.4 \n6.3 \n39.1 \n43.7 \n30.1 22.2 \nPreds \n-37.4 -52.3 -52.4 -25.1 -47.7 -18.8 -34.2 -24.1 -29.1 -43.8 \n46.9 \n50.7 -43.1 \n40.9 \n47.9 -48.7 \n50.1 \n49.0 \n-9.5 41.4 \n\nDEVN \n\nFeatures, max normalization \n77.7 \n71.9 \n77.8 \n78.1 \n70.8 \n72.3 \n80.2 \n92.6 \n94.9 \n74.5 \n77.0 \n78.5 \n89.2 \n82.4 \n90.5 \n86.4 \n74.6 \n92.4 \n81.2 \n7.6 \nLogits, max normalization \n77.7 \n73.9 \n74.2 \n78.2 \n68.8 \n71.3 \n80.1 \n92.3 \n94.7 \n73.7 \n77.5 \n78.8 \n88.0 \n81.0 \n89.5 \n86.5 \n73.5 \n92.7 \n80.7 \n7.8 \nPreds, max normalization \n78.0 \n74.2 \n69.8 \n77.2 \n62.0 \n71.7 \n79.6 \n92.0 \n94.7 \n66.0 \n69.3 \n72.5 \n86.9 \n87.5 \n90.0 \n86.1 \n75.1 \n94.0 \n79.3 \n9.8 \n\nEntropy \n\nSource Train \n50.9 \n54.8 \n35.2 \n41.3 \n42.0 \n39.9 \n46.7 \n39.5 \n22.7 \n27.9 \n27.7 \n27.9 \n49.1 \n54.0 \n48.8 \n24.7 \n55.3 \n36.7 \n40.3 10.5 \nSource Train + Target \n51.7 \n57.0 \n19.3 \n46.3 \n37.2 \n48.8 \n53.7 \n49.7 \n43.4 \n43.5 \n47.4 \n46.6 \n47.9 \n53.9 \n55.3 \n37.0 \n61.9 \n45.9 \n47.0 \n9.1 \nSource Val \n30.4 \n35.0 \n26.8 \n43.1 \n39.3 \n44.1 \n53.8 \n48.9 \n49.3 \n25.7 \n29.8 \n27.7 \n55.4 \n61.3 \n55.3 \n49.6 \n66.5 \n55.4 \n44.3 12.4 \nSource Val + Target \n46.5 \n46.3 \n18.3 \n46.2 \n36.5 \n47.7 \n51.6 \n48.8 \n47.2 \n39.5 \n43.4 \n42.2 \n48.3 \n53.9 \n55.0 \n41.7 \n61.8 \n49.8 \n45.8 \n8.8 \nTarget \n46.5 \n49.5 \n8.3 \n45.3 \n30.1 \n50.5 \n49.4 \n47.0 \n43.3 \n44.5 \n49.7 \n50.2 \n43.6 \n49.2 \n52.7 \n36.8 \n57.6 \n44.9 \n44.4 10.5 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-84.9 -86.4 -84.9 -58.5 -86.7 -51.3 -89.1 -89.6 -89.5 -87.8 -88.5 -90.4 -88.5 -89.6 -90.5 -87.1 -88.9 -84.3 -84.2 10.6 \nFeatures, \u03c4 = 0.1 \n-88.9 -90.8 -84.2 -70.9 -87.1 -68.7 -88.1 -89.2 -89.0 -88.1 -90.4 -90.6 -87.5 -88.2 -90.5 -86.6 -88.2 -88.3 -86.\n\nTable 12 :\n12The weighted Spearman correlation of each validator/task pair for ATDOC.AD \nAW \nDA \nDW \nWA \nWD \nAC \nAP \nAR \nCA \nCP \nCR \nPA \nPC \nPR \nRA \nRC \nRP Mean \nStd \n\nAccuracy \nSource Train \n72.4 \n75.9 \n82.8 \n83.5 \n78.3 \n89.7 \n61.6 \n64.8 \n48.6 \n70.8 \n40.3 \n57.2 \n88.6 \n63.9 \n80.1 \n43.2 \n64.0 \n66.6 \n68.5 14.2 \nSource Val \n87.6 \n92.1 \n89.3 \n92.2 \n81.0 \n90.0 \n92.6 \n93.5 \n94.1 \n86.5 \n83.1 \n84.0 \n92.2 \n80.7 \n92.1 \n93.9 \n91.1 \n95.2 \n89.5 \n4.5 \n\nBNM \n\nSource Train \n80.6 \n84.5 \n77.2 \n79.2 \n78.6 \n77.2 \n78.5 \n81.9 \n51.3 \n81.0 \n51.0 \n77.7 \n92.7 \n79.1 \n87.5 \n26.8 \n78.2 \n68.6 \n74.0 15.4 \nSource Train + Target \n94.3 \n93.9 \n83.8 \n88.5 \n86.8 \n92.7 \n89.3 \n94.4 \n91.6 \n95.3 \n92.8 \n94.2 \n96.6 \n91.6 \n97.4 \n89.3 \n88.9 \n92.6 \n91.9 \n3.4 \nSource Val \n85.7 \n88.4 \n80.4 \n84.9 \n82.5 \n79.8 \n92.6 \n94.8 \n92.5 \n85.7 \n74.8 \n86.5 \n95.3 \n89.7 \n93.2 \n83.2 \n89.4 \n90.5 \n87.2 \n5.5 \nSource Val + Target \n93.7 \n93.3 \n83.3 \n87.8 \n85.7 \n90.4 \n92.2 \n95.1 \n93.6 \n93.6 \n89.0 \n92.8 \n96.7 \n92.4 \n96.7 \n91.2 \n90.1 \n93.4 \n91.7 \n3.4 \nTarget \n93.7 \n93.4 \n84.7 \n89.5 \n85.6 \n92.5 \n88.7 \n94.1 \n92.9 \n96.1 \n95.9 \n95.2 \n95.7 \n91.8 \n97.8 \n91.8 \n88.7 \n94.2 \n92.4 \n3.5 \n\nClassAMI \n\nSource + Target Features \n70.6 \n82.2 \n77.6 \n86.7 \n84.2 \n87.8 \n80.1 \n85.9 \n71.3 \n82.5 \n76.1 \n84.7 \n87.4 \n78.3 \n88.6 \n39.3 \n71.9 \n72.2 \n78.2 11.1 \nSource + Target Logits \n72.9 \n83.1 \n76.1 \n87.2 \n82.7 \n88.0 \n77.4 \n86.5 \n71.8 \n81.7 \n74.2 \n81.5 \n87.1 \n72.6 \n87.8 \n42.5 \n69.6 \n74.3 \n77.6 10.5 \nTarget Features \n-21.6 -36.6 -13.3 \n13.0 -10.7 \n20.2 -49.7 -38.9 -36.0 -42.3 -24.7 -39.4 -18.4 -19.1 -17.3 -29.8 -60.4 -27.4 -25.1 19.5 \nTarget Logits \n-20.5 -38.2 \n-4.2 \n12.7 \n-7.8 \n25.3 -53.7 -30.5 -32.5 -41.0 -19.2 -33.9 -16.3 -24.5 -15.3 -18.4 -62.8 -17.6 -22.1 20.7 \n\nClassSS \n\nSource + Target Features \n43.7 \n43.2 \n12.3 \n50.7 \n15.9 \n65.0 -39.9 -12.1 \n-4.0 -37.9 -15.5 -31.5 \n12.1 -22.7 \n17.5 \n-5.0 -42.4 \n9.1 \n3.2 31.7 \nSource + Target Logits \n47.2 \n22.1 \n4.1 \n49.0 \n16.7 \n55.4 -38.3 -10.6 \n4.8 -23.0 \n4.9 -14.0 \n16.1 -14.8 \n27.1 \n-6.8 -43.7 \n11.9 \n6.0 27.4 \nTarget Features \n23.1 \n31.9 \n19.0 \n50.2 \n21.0 \n63.0 -52.0 -11.8 \n-7.3 -40.1 \n30.8 -12.1 -19.0 -30.1 \n-1.7 \n-4.3 -43.2 \n6.8 \n1.3 31.3 \nTarget Logits \n30.7 \n46.2 \n7.0 \n53.8 \n19.3 \n54.9 -42.5 -14.6 \n-1.8 -35.1 \n26.1 \n-9.1 -17.4 -24.2 \n7.2 \n-7.9 -38.6 \n4.9 \n3.3 29.6 \n\nDEV \n\nFeatures \n-14.7 \n-1.1 \n48.8 \n73.7 \n3.5 \n41.3 \n34.2 \n42.9 \n15.9 \n1.4 -14.4 \n6.3 \n17.4 \n21.7 \n36.4 -31.6 \n-7.2 \n-0.9 \n15.2 26.1 \nLogits \n-16.9 -11.0 \n53.9 \n80.1 \n37.0 \n12.9 \n56.0 \n64.9 \n35.3 \n69.4 \n50.1 \n82.1 \n76.3 \n43.5 \n83.1 \n0.6 \n64.0 \n49.3 \n46.1 30.5 \nPreds \n-39.7 -51.0 -34.2 -27.0 -33.2 -57.9 -41.1 -28.4 -35.7 -42.3 \n60.1 \n78.4 -33.3 \n68.3 \n68.1 -59.3 \n66.7 \n36.4 \n-5.8 50.0 \n\nDEVN \n\nFeatures, max normalization \n88.0 \n92.4 \n90.3 \n93.0 \n81.3 \n88.4 \n93.4 \n93.8 \n93.0 \n89.2 \n84.5 \n86.2 \n94.3 \n82.4 \n94.1 \n92.4 \n91.7 \n95.3 \n90.2 \n4.1 \nLogits, max normalization \n88.3 \n92.9 \n90.6 \n93.5 \n81.9 \n87.5 \n93.8 \n94.2 \n93.4 \n88.2 \n84.9 \n86.7 \n94.1 \n82.5 \n93.9 \n91.7 \n91.6 \n95.1 \n90.3 \n4.1 \nPreds, max normalization \n88.8 \n92.8 \n89.8 \n92.0 \n81.2 \n88.0 \n94.9 \n94.9 \n94.7 \n86.9 \n86.4 \n89.0 \n94.0 \n83.7 \n94.6 \n92.3 \n92.0 \n95.7 \n90.6 \n4.1 \n\nEntropy \n\nSource Train \n72.6 \n79.0 \n75.7 \n77.8 \n77.3 \n74.5 \n80.6 \n83.5 \n54.0 \n79.1 \n54.7 \n79.8 \n88.5 \n81.3 \n87.2 \n29.6 \n78.3 \n70.9 \n73.6 13.9 \nSource Train + Target \n77.2 \n78.4 \n64.6 \n78.8 \n79.9 \n80.0 \n75.2 \n82.6 \n71.6 \n84.6 \n77.7 \n85.9 \n81.0 \n71.8 \n78.5 \n40.4 \n54.8 \n64.1 \n73.7 11.1 \nSource Val \n66.1 \n78.3 \n75.5 \n79.9 \n80.2 \n75.7 \n81.5 \n85.0 \n83.0 \n82.0 \n73.6 \n83.9 \n89.9 \n88.8 \n90.5 \n79.2 \n73.0 \n81.4 \n80.4 \n6.1 \nSource Val + Target \n77.7 \n77.2 \n65.5 \n78.2 \n79.4 \n78.7 \n73.1 \n78.9 \n73.6 \n83.7 \n77.2 \n84.0 \n81.0 \n71.9 \n77.6 \n49.3 \n54.6 \n66.9 \n73.8 \n9.1 \nTarget \n75.7 \n73.9 \n53.7 \n75.5 \n74.6 \n80.0 \n65.3 \n74.5 \n67.6 \n81.4 \n74.9 \n79.5 \n76.1 \n64.6 \n70.7 \n38.6 \n47.1 \n57.3 \n68.4 11.7 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-89.5 -85.9 -85.7 -90.2 -86.0 -89.1 -85.8 -89.4 -91.1 -84.1 -92.8 -90.3 -83.1 -88.1 -91.1 -90.7 -87.8 -91.6 -88.5 \n2.7 \nFeatures, \u03c4 = 0.1 \n-88.7 -85.5 -84.6 -90.0 -85.6 -89.2 -84.8 -88.1 -88.4 -83.3 -91.7 -90.1 -81.6 -85.4 -88.7 -83.8 -85.3 -89.7 -86.9 \n2.7 \nFeatures, \u03c4 = 0.5 \n-86.5 -85.1 -83.3 -84.7 -83.6 -79.7 -84.7 -84.3 -78.6 -81.8 -85.7 -86.7 -80.6 -83.0 -83.5 -68.2 -83.1 -81.5 -82.5 \n4.1 \nLogits, \u03c4 = 0.05 \n-90.4 -85.6 -87.9 -93.8 -88.6 -92.2 -86.0 -91.0 -95.1 -85.0 -94.9 -92.0 -83.3 -90.8 -91.9 -92.8 -89.9 -93.8 -90.3 \n3.4 \nLogits, \u03c4 = 0.1 \n-90.7 -85.6 -85.6 -93.8 -87.6 -93.5 -85.3 -90.4 -93.7 -84.4 -94.3 -91.4 -83.0 -89.6 -90.9 -92.4 -88.3 -92.9 -89.6 \n3.5 \nLogits, \u03c4 = 0.5 \n-88.3 -84.8 -85.6 -90.2 -87.4 -87.7 -84.9 -85.7 -84.4 -82.7 -88.0 -88.2 -82.3 -85.6 -85.3 -76.7 -84.8 -84.8 -85.4 \n2.9 \nPreds, \u03c4 = 0.05 \n-90.3 -85.4 -88.5 -92.9 -87.9 -90.0 -84.1 -90.3 -91.6 -84.9 -92.2 -91.5 -83.5 -89.4 -90.5 -86.7 -88.4 -90.8 -88.8 \n2.8 \nPreds, \u03c4 = 0.1 \n-90.7 -85.8 -88.5 -95.4 -88.4 -96.0 -84.7 -91.8 -95.6 -85.7 -95.5 -93.0 -84.1 -90.4 -92.4 -94.9 -89.3 -95.3 -91.0 \n4.0 \nPreds, \u03c4 = 0.5 \n-87.8 -84.2 -82.7 -87.4 -84.7 -83.6 -82.4 -86.5 -82.2 -83.1 -86.9 -87.8 -80.9 -84.8 -83.1 -79.2 -84.3 -84.2 -84.2 \n2.3 \n\n\n\nTable 13 :\n13The weighted Spearman correlation of each validator/task pair for BNM.AD \nAW \nDA \nDW \nWA \nWD \nAC \nAP \nAR \nCA \nCP \nCR \nPA \nPC \nPR \nRA \nRC \nRP Mean \nStd \n\nAccuracy \nSource Train \n65.3 \n70.4 \n85.1 \n64.6 \n82.1 \n53.8 \n64.7 \n69.1 \n57.9 \n49.2 \n51.7 \n49.7 \n67.8 \n66.1 \n61.3 \n49.3 \n67.4 \n49.3 \n62.5 10.5 \nSource Val \n80.2 \n88.9 \n84.4 \n81.4 \n86.4 \n63.7 \n84.2 \n93.0 \n95.4 \n83.8 \n86.1 \n88.6 \n92.6 \n91.8 \n95.1 \n93.0 \n85.6 \n95.2 \n87.2 \n7.4 \n\nBNM \n\nSource Train \n48.2 \n52.4 \n46.5 \n28.8 \n54.8 \n15.1 \n41.0 \n35.2 \n13.4 \n10.8 \n5.1 \n17.6 \n44.1 \n42.1 \n29.1 \n7.5 \n40.9 \n18.3 \n30.6 16.0 \nSource Train + Target \n45.2 \n58.3 \n47.4 \n38.2 \n63.1 \n6.3 \n51.0 \n50.8 \n40.9 \n32.4 \n37.9 \n46.3 \n42.1 \n53.2 \n49.1 \n19.9 \n61.9 \n36.6 \n43.4 13.7 \nSource Val \n50.1 \n55.4 \n45.3 \n45.1 \n58.4 \n26.9 \n72.8 \n64.5 \n55.0 \n9.9 \n11.2 \n21.6 \n58.7 \n61.7 \n47.9 \n44.9 \n75.4 \n51.6 \n47.6 18.5 \nSource Val + Target \n46.9 \n57.5 \n50.1 \n45.9 \n64.6 \n18.5 \n63.9 \n59.6 \n51.3 \n26.9 \n27.9 \n37.6 \n46.3 \n56.4 \n50.3 \n29.1 \n68.3 \n45.9 \n47.1 13.8 \nTarget \n44.0 \n56.9 \n43.2 \n37.5 \n62.2 \n4.5 \n48.0 \n50.3 \n41.9 \n34.7 \n41.1 \n51.3 \n40.7 \n52.4 \n49.9 \n20.1 \n61.5 \n36.9 \n43.2 13.6 \n\nClassAMI \n\nSource + Target Features \n61.1 \n77.3 \n83.0 \n70.3 \n88.2 \n53.8 \n82.1 \n78.6 \n82.4 \n61.9 \n75.1 \n77.6 \n72.3 \n86.9 \n85.4 \n66.7 \n92.2 \n74.1 \n76.1 10.0 \nSource + Target Logits \n59.9 \n74.4 \n78.0 \n68.4 \n86.6 \n50.1 \n77.7 \n74.8 \n79.1 \n59.1 \n72.4 \n74.9 \n68.5 \n84.3 \n82.7 \n65.0 \n91.5 \n71.8 \n73.3 10.1 \nTarget Features \n-15.1 -21.5 \n8.4 \n19.3 \n-0.8 \n28.3 -37.4 -48.2 -27.5 -25.1 -23.6 -33.8 -21.1 -27.7 -31.6 \n-8.9 -32.6 -42.9 -19.0 20.3 \nTarget Logits \n-10.7 -25.0 \n12.2 \n16.3 \n0.1 \n27.9 -42.7 -42.4 -23.5 -23.6 -24.6 -33.6 -25.2 -32.0 -25.5 \n-8.0 -35.2 -41.5 -18.7 20.3 \n\nClassSS \n\nSource + Target Features \n15.9 \n21.9 \n33.9 \n41.1 \n45.1 \n12.0 -22.8 \n-5.2 \n4.5 -18.6 \n1.9 \n-2.9 -16.0 \n8.3 \n6.7 \n-2.4 \n9.5 \n1.4 \n7.5 18.5 \nSource + Target Logits \n5.9 \n25.9 \n24.9 \n36.7 \n43.2 \n12.2 -15.0 \n-2.8 \n7.0 -10.1 \n12.6 \n10.8 -11.0 \n2.6 \n5.6 \n2.4 \n7.2 \n1.7 \n8.9 15.2 \nTarget Features \n35.5 \n46.5 \n35.0 \n44.9 \n44.3 \n17.5 -42.0 \n-6.0 \n8.6 -11.1 \n13.7 \n8.0 -23.1 -12.6 \n7.5 \n-5.6 \n-9.1 \n7.3 \n8.8 24.4 \nTarget Logits \n21.5 \n52.2 \n25.6 \n45.8 \n42.0 \n18.4 -34.3 \n-5.6 \n10.0 -10.5 \n21.1 \n17.0 -19.5 \n-5.7 \n5.5 \n-0.7 \n-2.1 \n7.9 \n10.5 22.0 \n\nDEV \n\nFeatures \n-15.5 -18.1 \n23.6 \n51.9 \n30.9 \n47.0 \n17.7 \n30.0 \n21.5 -25.3 -30.2 -16.6 -19.8 \n-9.9 -18.3 -12.0 \n-9.8 \n-3.1 \n2.4 25.2 \nLogits \n17.4 \n20.7 \n56.2 \n64.8 \n60.9 \n59.8 \n43.1 \n62.1 \n76.0 \n14.9 \n14.6 \n39.1 \n10.4 \n10.5 \n36.0 \n16.2 \n42.3 \n62.4 \n39.3 21.7 \nPreds \n-24.9 -39.5 -58.2 -24.7 -53.7 \n-8.4 -41.9 -25.0 -32.6 -39.4 \n61.5 \n73.8 -35.8 \n58.7 \n72.6 -47.9 \n72.0 \n75.0 \n-1.0 50.8 \n\nDEVN \n\nFeatures, max normalization \n76.0 \n81.1 \n72.8 \n77.6 \n76.8 \n60.8 \n74.8 \n89.0 \n91.3 \n76.6 \n76.9 \n83.8 \n89.3 \n84.1 \n92.7 \n87.4 \n76.2 \n92.6 \n81.1 \n8.2 \nLogits, max normalization \n74.8 \n82.2 \n69.2 \n77.8 \n76.1 \n60.2 \n74.9 \n89.0 \n91.0 \n74.9 \n76.9 \n83.9 \n88.2 \n83.1 \n92.6 \n87.8 \n74.4 \n92.2 \n80.5 \n8.5 \nPreds, max normalization \n73.4 \n82.6 \n67.9 \n77.8 \n73.1 \n61.2 \n74.1 \n88.9 \n91.9 \n71.4 \n71.0 \n80.2 \n87.2 \n87.2 \n91.3 \n87.2 \n74.9 \n94.4 \n79.7 \n9.2 \n\nEntropy \n\nSource Train \n40.9 \n47.3 \n27.2 \n28.6 \n44.5 \n16.9 \n36.8 \n35.2 \n9.5 \n3.6 \n3.6 \n14.9 \n33.7 \n40.7 \n25.5 \n4.5 \n38.5 \n13.8 \n25.9 14.4 \nSource Train + Target \n46.3 \n56.9 \n32.4 \n42.3 \n50.3 \n27.2 \n39.1 \n47.1 \n42.3 \n25.5 \n35.1 \n42.2 \n29.8 \n43.0 \n46.4 \n27.8 \n55.8 \n35.6 \n40.3 \n9.2 \nSource Val \n25.5 \n42.2 \n32.2 \n37.0 \n49.8 \n23.4 \n41.8 \n39.2 \n48.8 \n-4.1 \n8.4 \n15.9 \n39.2 \n50.1 \n41.5 \n40.6 \n63.2 \n41.3 \n35.3 15.8 \nSource Val + Target \n40.5 \n52.2 \n33.0 \n41.5 \n51.3 \n26.0 \n37.8 \n42.9 \n46.1 \n19.3 \n28.7 \n35.9 \n30.1 \n43.8 \n46.2 \n32.0 \n57.1 \n37.6 \n39.0 \n9.6 \nTarget \n45.4 \n55.4 \n26.4 \n41.5 \n46.6 \n28.2 \n35.3 \n43.9 \n43.2 \n28.7 \n39.1 \n47.3 \n25.8 \n41.1 \n46.8 \n28.8 \n53.7 \n35.4 \n39.\n\nTable 14 :\n14The weighted Spearman correlation of each validator/task pair for BSP.AD \nAW \nDA \nDW \nWA \nWD \nAC \nAP \nAR \nCA \nCP \nCR \nPA \nPC \nPR \nRA \nRC \nRP Mean \nStd \n\nAccuracy \nSource Train \n67.6 \n52.8 \n62.8 \n75.4 \n67.0 \n79.2 \n38.0 \n35.6 \n37.2 \n31.4 \n20.3 \n30.7 \n67.9 \n36.0 \n52.0 \n39.4 \n50.1 \n40.8 \n49.1 16.8 \nSource Val \n87.3 \n87.9 \n73.0 \n85.3 \n64.2 \n83.2 \n96.7 \n97.5 \n97.1 \n70.1 \n78.1 \n81.1 \n89.9 \n89.7 \n93.3 \n96.7 \n91.6 \n95.7 \n86.6 \n9.7 \n\nBNM \n\nSource Train \n44.0 \n6.9 \n21.4 \n9.8 \n8.4 \n26.6 \n35.3 \n37.9 \n29.6 \n57.9 \n47.4 \n50.0 \n64.1 \n39.6 \n52.6 \n57.9 \n45.4 \n48.8 \n38.0 17.1 \nSource Train + Target \n32.2 \n7.0 \n20.5 \n16.8 \n13.1 \n27.4 \n36.4 \n42.5 \n46.1 \n66.6 \n63.2 \n63.1 \n62.1 \n39.0 \n57.6 \n63.0 \n43.0 \n54.1 \n41.9 18.6 \nSource Val \n41.0 \n9.3 \n25.6 \n13.7 \n11.1 \n25.2 \n40.9 \n43.4 \n47.2 \n59.8 \n54.2 \n51.6 \n65.4 \n46.0 \n57.5 \n67.2 \n48.3 \n57.7 \n42.5 17.8 \nSource Val + Target \n33.2 \n6.8 \n23.1 \n15.6 \n12.7 \n25.8 \n38.6 \n42.5 \n46.2 \n65.0 \n61.2 \n60.3 \n63.2 \n41.4 \n57.6 \n64.1 \n44.3 \n55.4 \n42.1 18.4 \nTarget \n25.6 \n2.9 \n6.2 \n19.3 \n9.2 \n26.1 \n31.8 \n40.3 \n43.8 \n66.2 \n61.2 \n62.0 \n57.1 \n32.6 \n56.3 \n58.6 \n35.0 \n51.1 \n38.1 19.8 \n\nClassAMI \n\nSource + Target Features \n74.5 \n52.8 \n21.6 \n49.3 \n25.1 \n70.1 \n46.6 \n54.7 \n62.0 \n71.1 \n60.5 \n73.6 \n86.6 \n32.2 \n82.2 \n83.5 \n44.3 \n71.2 \n59.0 19.1 \nSource + Target Logits \n80.3 \n51.5 \n31.6 \n54.4 \n30.9 \n72.9 \n47.7 \n58.8 \n64.8 \n72.1 \n63.6 \n76.8 \n89.0 \n33.8 \n83.7 \n83.9 \n49.7 \n72.5 \n62.1 17.9 \nTarget Features \n-55.3 -48.1 -49.4 -24.6 -39.3 \n-9.5 -50.0 -30.8 -14.2 -44.9 -55.1 -44.2 -37.0 -52.8 -11.2 -36.4 -29.4 -27.1 -36.6 14.5 \nTarget Logits \n-50.7 -48.8 -51.1 -28.2 -43.0 \n-9.7 -67.3 -38.6 -20.7 -52.1 -57.7 -49.0 -44.5 -64.0 -23.6 -41.5 -50.5 -41.1 -43.5 14.5 \n\nClassSS \n\nSource + Target Features \n-57.6 -35.3 -29.5 \n10.1 -38.3 \n12.1 -81.7 -76.2 -67.4 -75.1 -73.2 -70.4 -62.1 -77.3 -67.4 -69.8 -67.2 -68.3 -55.3 27.5 \nSource + Target Logits \n-39.0 -33.6 -28.1 \n0.3 -26.5 \n26.9 -74.4 -58.4 -44.6 -65.0 -62.6 -55.4 -53.7 -71.7 -55.0 -54.1 -56.5 -50.3 -44.5 24.7 \nTarget Features \n-23.0 \n6.0 -26.6 \n14.3 -34.7 \n26.9 -77.5 -67.3 -53.9 -68.6 -44.7 -51.1 -60.7 -72.6 -55.1 -54.2 -67.7 -51.1 -42.3 29.9 \nTarget Logits \n-21.7 \n14.6 -23.6 \n19.2 -24.0 \n38.2 -63.0 -50.5 -33.5 -62.6 -31.5 -38.1 -55.8 -61.3 -42.8 -36.4 -53.6 -24.7 -30.6 28.2 \n\nDEV \n\nFeatures \n-62.4 -59.1 -23.4 \n1.3 -42.8 -41.9 -26.0 -31.2 -38.8 -56.5 -48.3 -46.1 -53.1 -50.6 -44.6 -61.9 -24.7 -42.4 -41.8 15.8 \nLogits \n-43.1 -43.3 -11.5 \n14.5 -11.0 -31.3 \n1.2 -16.3 -28.5 \n-9.1 \n6.0 \n19.7 -22.6 -20.0 \n-4.2 -28.7 \n9.5 \n-6.4 -12.5 18.0 \nPreds \n-49.2 -56.3 -44.0 -31.3 -42.1 -38.1 -47.9 -31.2 -39.0 -46.1 \n-2.9 \n0.9 -47.3 -17.2 -27.6 -49.5 -10.1 -17.1 -33.1 16.8 \n\nDEVN \n\nFeatures, max normalization \n87.4 \n84.8 \n60.8 \n76.4 \n40.4 \n81.7 \n96.3 \n97.8 \n97.7 \n79.3 \n82.4 \n84.3 \n92.8 \n91.0 \n95.1 \n97.1 \n92.2 \n96.1 \n85.2 14.3 \nLogits, max normalization \n88.0 \n84.4 \n63.8 \n80.2 \n41.6 \n82.0 \n95.4 \n96.3 \n96.9 \n77.1 \n78.9 \n82.3 \n91.8 \n88.7 \n93.5 \n97.0 \n93.4 \n97.5 \n84.9 13.7 \nPreds, max normalization \n87.8 \n85.1 \n59.5 \n80.3 \n32.0 \n81.8 \n94.6 \n93.7 \n94.4 \n71.6 \n80.8 \n82.9 \n90.2 \n89.0 \n93.7 \n95.5 \n86.5 \n93.5 \n82.9 15.2 \n\nEntropy \n\nSource Train \n31.5 \n4.3 \n15.8 \n6.5 \n6.9 \n21.7 \n32.5 \n36.9 \n30.8 \n56.6 \n48.7 \n49.5 \n61.1 \n38.5 \n52.0 \n56.9 \n42.5 \n49.0 \n35.6 17.8 \nSource Train + Target \n26.1 \n4.3 \n4.2 \n11.7 \n5.0 \n22.0 \n32.0 \n38.1 \n42.1 \n62.2 \n60.0 \n59.9 \n57.7 \n35.8 \n54.0 \n58.1 \n38.4 \n49.3 \n36.7 20.0 \nSource Val \n23.8 \n3.3 \n13.1 \n7.1 \n6.5 \n19.4 \n32.9 \n36.8 \n41.6 \n57.0 \n52.6 \n49.9 \n60.5 \n43.0 \n54.6 \n60.8 \n40.7 \n50.8 \n36.4 19.0 \nSource Val + Target \n22.0 \n0.8 \n3.0 \n11.0 \n3.4 \n20.1 \n29.0 \n35.3 \n40.1 \n60.4 \n57.7 \n57.2 \n56.1 \n35.2 \n53.0 \n55.7 \n34.8 \n47.1 \n34.6 20.0 \nTarget \n18.3 \n-2.3 \n-4.8 \n13.5 \n0.6 \n20.2 \n24.3 \n34.2 \n38.9 \n58.5 \n56.1 \n56.4 \n48.7 \n28.3 \n51.2 \n50.0 \n28.0 \n43.5 \n31.3 20.1 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-84.7 -82.0 -77.9 -62.9 -76.5 -52.3 -80.4 -84.2 -84.8 -82.4 -81.6 -84.9 -83.3 -81.0 -83.2 -78.2 -77.5 -81.5 -78.8 \n8.1 \nFeatures, \u03c4 = 0.1 \n-86.1 -84.6 -78.7 -73.2 -78.7 -71.9 -77.5 -84.7 -85.7 -81.6 -82.6 -86.6 -82.4 -76.2 -85.4 -76.1 -69.3 -82.9 -80.\n\nTable 15 :\n15The weighted Spearman correlation of each validator/task pair for CDAN.AD \nAW \nDA \nDW \nWA \nWD \nAC \nAP \nAR \nCA \nCP \nCR \nPA \nPC \nPR \nRA \nRC \nRP Mean \nStd \n\nAccuracy \nSource Train \n63.9 \n78.3 \n80.0 \n61.7 \n85.6 \n48.7 \n65.5 \n70.5 \n59.0 \n34.7 \n55.0 \n59.3 \n64.0 \n76.3 \n76.2 \n52.6 \n74.3 \n57.5 \n64.6 12.4 \nSource Val \n84.4 \n82.1 \n80.1 \n79.9 \n85.4 \n66.2 \n81.7 \n97.7 \n93.0 \n66.7 \n85.2 \n87.7 \n94.2 \n90.5 \n96.5 \n92.9 \n81.5 \n95.0 \n85.6 \n8.8 \n\nBNM \n\nSource Train \n44.1 \n69.1 \n33.7 \n29.4 \n54.9 \n35.0 \n49.4 \n31.4 \n20.7 \n6.9 \n26.4 \n35.5 \n39.1 \n69.7 \n60.4 \n14.8 \n56.4 \n30.6 \n39.3 17.3 \nSource Train + Target \n33.9 \n56.4 \n35.4 \n40.0 \n68.1 \n29.4 \n69.1 \n38.5 \n22.2 \n11.4 \n36.2 \n46.6 \n32.3 \n74.6 \n61.4 \n19.2 \n71.6 \n35.4 \n43.4 18.6 \nSource Val \n59.4 \n82.1 \n27.6 \n49.2 \n56.8 \n43.5 \n88.8 \n79.0 \n58.1 \n8.5 \n30.0 \n39.1 \n55.7 \n86.8 \n74.5 \n54.2 \n90.3 \n70.3 \n58.6 22.5 \nSource Val + Target \n46.5 \n73.5 \n34.9 \n50.0 \n65.8 \n40.0 \n85.2 \n67.5 \n49.8 \n11.0 \n35.6 \n45.0 \n36.8 \n79.9 \n67.9 \n32.7 \n83.0 \n58.0 \n53.5 19.8 \nTarget \n30.8 \n53.5 \n33.4 \n39.7 \n68.6 \n28.6 \n68.8 \n39.5 \n23.4 \n12.7 \n38.9 \n49.9 \n32.0 \n74.4 \n61.9 \n19.6 \n71.5 \n37.0 \n43.6 18.4 \n\nClassAMI \n\nSource + Target Features \n45.6 \n61.4 \n49.8 \n40.7 \n77.6 \n48.1 \n79.1 \n58.2 \n36.1 \n23.6 \n48.0 \n63.1 \n49.7 \n84.1 \n77.3 \n55.7 \n90.1 \n64.8 \n58.5 17.4 \nSource + Target Logits \n42.8 \n57.8 \n43.2 \n40.0 \n71.6 \n45.0 \n75.4 \n54.7 \n30.6 \n19.7 \n41.3 \n59.5 \n44.1 \n82.5 \n74.8 \n52.6 \n88.6 \n62.4 \n54.8 18.0 \nTarget Features \n-20.0 -27.8 -22.2 -18.3 -12.9 \n-8.8 -53.5 -11.3 -43.9 -57.8 -48.7 -44.9 -52.5 -38.0 \n-7.5 -34.6 -19.1 -40.4 -31.2 16.3 \nTarget Logits \n-20.2 -26.8 -20.7 -20.1 -11.7 \n-8.1 -55.4 \n-8.4 -41.5 -58.0 -47.2 -43.3 -46.8 -40.4 \n-2.1 -33.5 -20.0 -39.3 -30.2 16.6 \n\nClassSS \n\nSource + Target Features \n25.0 \n33.9 \n10.3 \n20.2 \n40.0 \n8.0 \n16.4 \n-1.1 -10.2 -37.4 \n-4.7 \n0.0 -31.1 \n22.6 \n25.4 \n-8.0 \n45.7 \n10.0 \n9.2 21.9 \nSource + Target Logits \n25.1 \n23.6 \n3.6 \n9.3 \n35.3 \n7.1 \n18.1 \n2.3 -10.2 -27.4 \n2.1 \n5.7 -23.2 \n31.8 \n31.3 \n8.5 \n51.7 \n16.8 \n11.8 19.6 \nTarget Features \n32.3 \n44.0 \n11.8 \n23.4 \n41.5 \n12.7 \n17.7 \n3.6 \n-9.8 -33.8 \n4.4 \n10.9 -28.7 \n23.6 \n29.6 \n0.4 \n45.7 \n16.1 \n13.6 21.7 \nTarget Logits \n30.4 \n44.2 \n4.2 \n23.8 \n35.3 \n11.3 \n24.8 \n5.3 -10.5 -28.3 \n7.8 \n12.7 -29.4 \n32.8 \n31.7 \n10.0 \n51.3 \n20.8 \n15.5 21.6 \n\nDEV \n\nFeatures \n-32.2 -52.6 \n1.0 \n41.9 \n37.9 \n37.8 \n21.5 \n34.6 \n28.8 -14.5 -28.4 -23.2 -27.5 \n-2.8 \n6.9 -22.6 \n-9.8 \n11.9 \n0.5 27.9 \nLogits \n-7.3 -10.4 \n16.7 \n64.0 \n54.6 \n54.7 \n53.4 \n69.6 \n75.8 \n23.7 \n31.2 \n40.6 \n6.3 \n26.9 \n40.4 \n3.7 \n35.2 \n50.3 \n35.0 25.1 \nPreds \n-30.3 -43.9 -56.1 -25.3 -44.1 \n-9.1 -24.8 -27.4 -32.3 -37.9 \n55.5 \n58.1 -44.7 \n46.1 \n67.6 -43.9 \n77.0 \n69.8 \n-2.5 47.3 \n\nDEVN \n\nFeatures, max normalization \n82.7 \n72.2 \n57.4 \n76.2 \n67.1 \n66.4 \n64.6 \n96.4 \n91.0 \n52.1 \n75.8 \n80.9 \n91.1 \n73.2 \n92.5 \n88.7 \n66.1 \n93.0 \n77.1 12.9 \nLogits, max normalization \n81.0 \n71.9 \n55.4 \n77.1 \n63.0 \n64.4 \n64.7 \n95.9 \n89.3 \n50.7 \n76.7 \n82.7 \n89.2 \n71.8 \n90.5 \n87.9 \n65.3 \n92.3 \n76.1 13.1 \nPreds, max normalization \n80.7 \n71.9 \n54.7 \n77.1 \n59.8 \n65.1 \n64.4 \n96.3 \n89.5 \n35.3 \n54.0 \n64.9 \n86.9 \n88.9 \n91.6 \n88.1 \n68.2 \n91.0 \n73.8 16.2 \n\nEntropy \n\nSource Train \n43.8 \n60.2 \n13.3 \n24.2 \n38.2 \n32.0 \n51.5 \n28.9 \n17.5 \n6.4 \n23.6 \n36.1 \n26.9 \n68.2 \n55.6 \n12.4 \n57.5 \n25.9 \n34.6 17.6 \nSource Train + Target \n32.9 \n49.7 \n14.2 \n17.9 \n45.5 \n32.9 \n66.4 \n37.7 \n13.4 \n13.0 \n31.4 \n42.6 \n23.2 \n70.5 \n57.0 \n21.0 \n71.0 \n35.0 \n37.5 18.8 \nSource Val \n58.1 \n70.1 \n13.0 \n26.1 \n35.6 \n35.3 \n83.2 \n63.2 \n42.5 \n8.1 \n29.6 \n38.8 \n42.7 \n82.5 \n69.3 \n51.6 \n86.5 \n65.7 \n50.1 23.0 \nSource Val + Target \n47.2 \n63.4 \n13.7 \n20.6 \n44.9 \n34.6 \n73.6 \n50.1 \n28.8 \n12.8 \n33.6 \n43.3 \n25.5 \n73.5 \n61.5 \n30.8 \n77.5 \n50.2 \n43.6 19.8 \nTarget \n29.2 \n45.9 \n11.0 \n15.9 \n45.1 \n33.0 \n65.8 \n38.5 \n14.5 \n16.1 \n34.6 \n45.7 \n21.3 \n69.5 \n57.3 \n21.8 \n70.6 \n36.2 \n37.3 18.7 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-73.4 -79.9 -80.9 -59.5 -87.8 -50.6 -92.8 -88.9 -86.7 -86.0 -88.5 -90.6 -88.5 -90.8 -91.4 -86.0 -89.7 -88.0 -83.3 11.1 \nFeatures, \u03c4 = 0.1 \n-77.2 -87.5 -79.8 -65.9 -88.4 -63.3 -89.4 -84.1 -83.6 -83.9 -88.6 -89.2 -85.3 -88.2 -88.6 -84.2 -86.9 -88.5 -83.5 \n7.4 \n\n\nTable 16 :\n16The weighted Spearman correlation of each validator/task pair for DANN.AD \nAW \nDA \nDW \nWA \nWD \nAC \nAP \nAR \nCA \nCP \nCR \nPA \nPC \nPR \nRA \nRC \nRP Mean \nStd \n\nAccuracy \nSource Train \n70.6 \n80.2 \n78.8 \n58.8 \n81.4 \n48.7 \n65.8 \n63.4 \n58.5 \n34.2 \n49.2 \n44.1 \n55.2 \n75.1 \n71.4 \n47.7 \n74.3 \n54.8 \n61.8 13.4 \nSource Val \n84.4 \n82.0 \n80.2 \n80.2 \n81.1 \n71.1 \n78.5 \n96.4 \n96.8 \n75.2 \n82.5 \n78.8 \n90.8 \n88.7 \n94.5 \n91.6 \n70.6 \n91.8 \n84.2 \n8.0 \n\nBNM \n\nSource Train \n59.5 \n72.6 \n40.0 \n21.3 \n54.1 \n23.9 \n49.5 \n34.1 \n19.9 \n9.8 \n29.1 \n20.2 \n31.8 \n67.3 \n63.0 \n15.6 \n68.5 \n26.0 \n39.2 19.9 \nSource Train + Target \n39.2 \n58.1 \n34.8 \n31.0 \n64.5 \n22.3 \n71.7 \n33.9 \n28.5 \n16.4 \n37.0 \n25.1 \n25.9 \n75.6 \n58.4 \n21.8 \n77.4 \n30.4 \n41.8 19.6 \nSource Val \n72.3 \n84.2 \n40.2 \n49.4 \n58.0 \n50.4 \n92.0 \n77.0 \n72.2 \n15.5 \n36.3 \n28.7 \n55.4 \n85.6 \n79.3 \n60.3 \n89.2 \n74.2 \n62.2 21.5 \nSource Val + Target \n54.8 \n76.4 \n41.0 \n49.0 \n65.9 \n43.8 \n90.9 \n65.5 \n64.1 \n16.4 \n39.0 \n28.8 \n31.9 \n82.7 \n69.5 \n38.8 \n89.1 \n61.4 \n56.1 20.8 \nTarget \n34.4 \n53.7 \n32.3 \n30.6 \n65.0 \n22.0 \n71.7 \n34.4 \n30.7 \n18.5 \n40.3 \n28.1 \n25.7 \n75.4 \n58.6 \n22.6 \n77.3 \n32.1 \n41.9 19.1 \n\nClassAMI \n\nSource + Target Features \n58.9 \n69.0 \n77.6 \n34.8 \n84.6 \n58.7 \n89.8 \n70.1 \n72.3 \n70.8 \n63.4 \n70.6 \n75.7 \n91.1 \n85.5 \n74.6 \n92.5 \n73.2 \n73.0 13.6 \nSource + Target Logits \n54.0 \n64.1 \n72.0 \n32.1 \n82.3 \n55.4 \n88.7 \n57.5 \n61.3 \n64.2 \n51.1 \n66.0 \n68.4 \n89.6 \n83.8 \n70.4 \n91.3 \n68.5 \n67.8 15.0 \nTarget Features \n-10.4 \n-4.1 \n7.7 \n5.8 \n18.7 \n19.6 \n-7.3 -23.0 \n8.0 -14.0 -25.3 \n-6.9 \n10.6 \n12.1 \n18.4 \n4.9 \n3.7 \n8.9 \n1.5 13.2 \nTarget Logits \n-46.6 -36.4 -30.9 -27.3 -27.6 -18.5 -58.0 -62.9 -48.3 -55.3 -61.0 -42.6 -50.0 -38.9 -37.0 -45.3 -42.0 -34.0 -42.4 12.0 \n\nClassSS \n\nSource + Target Features \n36.5 \n54.1 \n6.2 \n23.0 \n41.7 \n18.8 \n2.2 -16.0 -11.8 -38.1 -20.8 -25.5 -18.2 \n31.0 \n21.3 -13.6 \n29.6 \n11.3 \n7.3 25.6 \nSource + Target Logits \n33.4 \n55.0 \n1.6 \n22.4 \n42.9 \n16.9 \n15.8 -13.0 \n-4.6 -28.7 \n-5.6 -11.8 -18.8 \n37.4 \n26.6 \n-4.3 \n41.9 \n13.8 \n12.3 23.5 \nTarget Features \n31.9 \n54.5 \n-1.2 \n24.1 \n32.6 \n25.3 -32.1 -30.5 -29.3 -52.3 -32.8 -29.6 -52.3 \n-4.5 \n2.5 -33.9 \n0.1 \n0.6 \n-7.1 30.6 \nTarget Logits \n29.9 \n52.0 \n2.2 \n22.8 \n44.5 \n23.2 \n9.1 -11.6 \n-0.2 -19.5 \n1.2 \n-1.9 -24.7 \n35.2 \n28.2 \n-4.0 \n33.2 \n18.6 \n13.2 21.4 \n\nDEV \n\nFeatures \n-22.0 -29.5 \n33.9 \n48.8 \n55.1 \n52.3 \n23.7 \n38.0 \n33.4 -19.4 -35.3 -36.0 -24.6 \n-5.3 -15.7 -24.6 -16.5 \n-6.5 \n2.8 31.9 \nLogits \n-2.3 \n4.9 \n40.8 \n61.5 \n47.5 \n64.7 \n54.9 \n70.8 \n79.1 \n30.4 \n33.1 \n22.8 \n11.7 \n21.6 \n38.9 \n2.2 \n39.3 \n42.5 \n36.9 23.2 \nPreds \n-22.8 -42.4 -51.7 -26.7 -39.2 \n-5.9 -23.5 -19.0 -25.9 -35.4 \n50.5 \n33.8 -39.4 \n25.5 \n64.2 -44.0 \n55.2 \n67.5 \n-4.4 40.4 \n\nDEVN \n\nFeatures, max normalization \n80.7 \n72.8 \n63.4 \n75.8 \n61.2 \n70.5 \n58.6 \n93.5 \n94.9 \n64.1 \n70.2 \n70.3 \n83.7 \n66.5 \n84.2 \n85.7 \n50.1 \n85.7 \n74.0 12.1 \nLogits, max normalization \n80.5 \n71.8 \n57.6 \n76.4 \n54.0 \n69.6 \n57.5 \n93.0 \n94.5 \n65.2 \n73.5 \n76.7 \n82.1 \n62.9 \n82.1 \n85.4 \n48.1 \n85.9 \n73.2 13.0 \nPreds, max normalization \n80.1 \n72.4 \n55.2 \n77.0 \n46.2 \n70.1 \n58.3 \n93.5 \n94.8 \n47.8 \n48.0 \n49.0 \n79.0 \n83.1 \n87.9 \n85.6 \n54.7 \n89.3 \n70.7 16.8 \n\nEntropy \n\nSource Train \n50.4 \n55.0 \n13.8 \n15.1 \n31.4 \n19.4 \n42.0 \n25.9 \n10.0 \n2.0 \n18.0 \n15.1 \n16.1 \n59.0 \n58.6 \n5.5 \n61.7 \n15.8 \n28.6 19.7 \nSource Train + Target \n34.4 \n44.5 \n14.1 \n10.0 \n47.1 \n30.2 \n61.6 \n30.3 \n15.4 \n14.4 \n27.1 \n21.0 \n19.5 \n68.0 \n57.5 \n18.3 \n69.8 \n23.0 \n33.7 19.1 \nSource Val \n56.2 \n64.1 \n26.9 \n19.9 \n41.4 \n37.8 \n77.3 \n58.6 \n51.1 \n9.1 \n26.4 \n27.9 \n41.1 \n76.8 \n76.3 \n49.1 \n78.0 \n57.0 \n48.6 20.7 \nSource Val + Target \n48.1 \n59.2 \n16.7 \n13.8 \n48.4 \n37.0 \n71.2 \n45.4 \n38.9 \n15.0 \n29.9 \n26.1 \n23.3 \n71.9 \n64.4 \n30.6 \n76.8 \n42.2 \n42.2 19.6 \nTarget \n28.7 \n40.0 \n10.3 \n7.9 \n46.9 \n31.7 \n60.9 \n30.8 \n17.4 \n19.8 \n31.0 \n25.0 \n19.6 \n67.7 \n57.9 \n19.5 \n69.6 \n24.7 \n33.9 18.6 \n\n\n\nTable 17 :\n17The weighted Spearman correlation of each validator/task pair for GVB.AD \nAW \nDA \nDW \nWA \nWD \nAC \nAP \nAR \nCA \nCP \nCR \nPA \nPC \nPR \nRA \nRC \nRP Mean \nStd \n\nAccuracy \nSource Train \n69.3 \n71.8 \n82.1 \n63.0 \n82.1 \n67.5 \n51.7 \n52.4 \n50.3 \n42.9 \n49.7 \n45.4 \n69.8 \n66.0 \n61.2 \n49.7 \n64.5 \n52.3 \n60.6 11.6 \nSource Val \n87.2 \n84.3 \n86.9 \n81.5 \n83.5 \n76.7 \n89.4 \n96.6 \n97.1 \n80.8 \n86.8 \n84.3 \n93.6 \n91.7 \n94.4 \n86.6 \n84.7 \n91.9 \n87.7 \n5.5 \n\nBNM \n\nSource Train \n41.1 \n50.4 \n17.5 \n26.7 \n14.4 \n35.5 \n19.6 \n-4.6 \n-3.8 \n7.4 \n9.7 \n0.9 \n33.3 \n42.6 \n17.2 \n16.1 \n30.6 \n5.0 \n20.0 15.9 \nSource Train + Target \n50.8 \n65.7 \n32.2 \n36.8 \n53.7 \n38.1 \n54.4 \n22.9 \n34.6 \n34.9 \n40.7 \n35.3 \n44.6 \n67.7 \n52.7 \n41.5 \n67.5 \n39.6 \n45.2 12.6 \nSource Val \n52.0 \n63.8 \n26.9 \n49.9 \n31.4 \n49.3 \n70.6 \n52.9 \n52.9 \n6.9 \n12.3 \n1.7 \n45.2 \n68.5 \n39.9 \n48.0 \n72.5 \n52.1 \n44.3 20.4 \nSource Val + Target \n53.2 \n72.4 \n33.7 \n49.0 \n50.2 \n46.6 \n67.5 \n42.0 \n48.2 \n27.2 \n30.6 \n21.5 \n46.1 \n70.5 \n49.6 \n45.1 \n72.3 \n48.4 \n48.6 14.5 \nTarget \n49.1 \n62.3 \n29.5 \n37.3 \n56.5 \n37.8 \n56.2 \n25.8 \n38.8 \n38.1 \n47.1 \n41.7 \n44.3 \n67.5 \n54.2 \n43.1 \n68.9 \n43.2 \n46.7 11.8 \n\nClassAMI \n\nSource + Target Features \n65.1 \n78.6 \n64.7 \n42.5 \n74.8 \n52.3 \n83.9 \n53.7 \n64.9 \n58.2 \n68.7 \n69.1 \n71.1 \n85.1 \n71.5 \n68.8 \n90.1 \n72.0 \n68.6 11.7 \nSource + Target Logits \n64.5 \n76.6 \n63.7 \n41.3 \n73.5 \n50.8 \n82.5 \n51.7 \n62.4 \n56.4 \n62.2 \n67.2 \n68.2 \n83.4 \n69.4 \n67.6 \n90.2 \n71.0 \n66.8 11.9 \nTarget Features \n8.0 -13.6 \n-9.2 \n6.9 \n5.1 \n9.2 -44.5 -39.2 -52.5 -41.5 -39.2 -22.7 -24.9 -46.2 -35.2 -44.5 -39.2 -45.3 -26.0 21.0 \nTarget Logits \n8.6 -13.7 \n-9.5 \n5.9 \n2.8 \n7.7 -45.2 -40.1 -53.6 -41.8 -39.8 -23.7 -24.6 -46.5 -35.7 -44.8 -39.4 -45.6 -26.6 20.8 \n\nClassSS \n\nSource + Target Features \n36.4 \n58.4 \n33.1 \n37.5 \n49.4 \n35.3 \n34.1 \n6.2 \n4.6 \n-7.3 \n15.3 \n8.4 \n-1.7 \n51.8 \n30.8 \n19.6 \n59.0 \n35.4 \n28.1 19.7 \nSource + Target Logits \n47.3 \n65.5 \n32.2 \n36.4 \n54.1 \n39.0 \n44.8 \n15.8 \n15.3 \n13.9 \n29.6 \n29.2 \n10.0 \n57.7 \n42.6 \n33.6 \n64.7 \n42.7 \n37.5 16.4 \nTarget Features \n57.6 \n72.6 \n39.5 \n39.9 \n54.1 \n39.2 \n35.2 \n16.6 \n16.2 \n11.6 \n36.5 \n29.4 \n1.5 \n51.3 \n42.8 \n30.7 \n61.2 \n47.9 \n38.0 17.9 \nTarget Logits \n61.5 \n75.7 \n37.1 \n38.8 \n56.5 \n42.5 \n42.0 \n23.7 \n24.7 \n26.1 \n45.8 \n43.8 \n6.1 \n53.6 \n50.1 \n40.9 \n65.0 \n52.4 \n43.7 16.2 \n\nDEV \n\nFeatures \n-31.1 -46.0 \n43.1 \n51.9 \n35.7 \n30.2 \n8.7 \n24.5 \n23.0 -25.2 -23.6 -13.8 -14.1 -19.9 -16.3 -32.5 -18.9 \n-9.7 \n-1.9 28.6 \nLogits \n-20.0 -16.3 \n51.1 \n67.4 \n53.7 \n52.9 \n48.5 \n55.8 \n68.1 \n15.7 \n25.2 \n24.3 \n12.6 \n23.6 \n29.7 \n6.9 \n34.1 \n40.1 \n31.8 25.0 \nPreds \n-38.1 -48.6 -56.5 -29.0 -48.7 -13.2 -28.8 -19.3 -30.7 -38.2 \n58.2 \n51.6 -49.2 \n53.3 \n51.0 -43.7 \n61.5 \n62.5 \n-5.9 45.3 \n\nDEVN \n\nFeatures, max normalization \n85.6 \n76.9 \n81.6 \n78.0 \n70.4 \n75.8 \n79.3 \n94.3 \n94.3 \n69.2 \n75.3 \n73.5 \n90.4 \n81.9 \n87.2 \n78.0 \n72.4 \n86.6 \n80.6 \n7.5 \nLogits, max normalization \n83.1 \n74.5 \n79.4 \n78.3 \n68.6 \n74.4 \n79.2 \n94.0 \n93.8 \n66.1 \n75.7 \n71.6 \n89.1 \n80.6 \n87.5 \n77.7 \n71.7 \n87.3 \n79.6 \n7.9 \nPreds, max normalization \n82.9 \n74.2 \n77.0 \n77.8 \n60.3 \n75.0 \n79.4 \n94.5 \n95.2 \n62.7 \n59.4 \n58.5 \n88.6 \n86.5 \n82.1 \n77.6 \n73.0 \n89.1 \n77.4 11.1 \n\nEntropy \n\nSource Train \n38.3 \n45.8 \n1.8 \n25.6 \n1.5 \n33.7 \n17.1 \n-8.0 \n-7.7 \n5.1 \n7.2 \n0.0 \n27.2 \n39.1 \n10.6 \n13.9 \n27.7 \n3.3 \n15.7 16.3 \nSource Train + Target \n51.6 \n66.7 \n4.9 \n25.2 \n37.7 \n36.7 \n52.2 \n21.7 \n29.6 \n31.4 \n35.9 \n28.9 \n37.9 \n61.4 \n46.7 \n40.3 \n62.2 \n38.2 \n39.4 15.1 \nSource Val \n45.6 \n58.0 \n8.4 \n29.2 \n18.8 \n39.3 \n50.0 \n26.1 \n36.3 \n2.3 \n9.5 \n0.5 \n33.6 \n60.9 \n32.3 \n39.9 \n60.6 \n45.5 \n33.2 18.7 \nSource Val + Target \n49.9 \n67.5 \n6.0 \n26.2 \n38.4 \n38.6 \n52.8 \n23.5 \n33.7 \n25.3 \n30.7 \n22.0 \n37.5 \n62.2 \n45.0 \n41.5 \n63.9 \n42.9 \n39.3 15.6 \nTarget \n52.4 \n65.3 \n-0.6 \n24.6 \n38.3 \n36.9 \n53.5 \n24.7 \n32.1 \n35.9 \n42.7 \n35.1 \n38.0 \n60.3 \n49.0 \n42.8 \n63.6 \n42.0 \n40.9 15.4 \n\n\n\nTable 18 :\n18The weighted Spearman correlation of each validator/task pair for IM.AD \nAW \nDA \nDW \nWA \nWD \nAC \nAP \nAR \nCA \nCP \nCR \nPA \nPC \nPR \nRA \nRC \nRP Mean \nStd \n\nAccuracy \nSource Train \n54.7 \n82.2 \n83.4 \n78.8 \n84.8 \n68.6 \n64.7 \n72.3 \n65.0 \n34.4 \n36.7 \n47.9 \n60.4 \n71.1 \n72.2 \n43.1 \n69.9 \n47.2 \n63.2 15.4 \nSource Val \n81.8 \n88.2 \n86.8 \n86.8 \n82.1 \n78.4 \n77.9 \n91.5 \n95.7 \n72.3 \n83.1 \n84.7 \n86.1 \n91.4 \n94.8 \n93.0 \n78.6 \n95.2 \n86.0 \n6.6 \n\nBNM \n\nSource Train \n50.6 \n72.7 \n61.8 \n41.3 \n67.3 \n36.3 \n56.9 \n57.3 \n22.2 \n27.3 \n17.5 \n35.1 \n60.9 \n60.5 \n54.5 \n10.4 \n57.4 \n23.8 \n45.2 18.3 \nSource Train + Target \n41.9 \n69.1 \n61.5 \n48.3 \n68.3 \n30.7 \n60.5 \n60.8 \n39.9 \n35.5 \n38.6 \n53.3 \n56.3 \n63.4 \n54.3 \n16.1 \n65.4 \n32.9 \n49.8 14.7 \nSource Val \n44.3 \n67.4 \n61.9 \n47.1 \n69.4 \n40.6 \n77.4 \n70.9 \n52.6 \n25.6 \n19.3 \n36.1 \n70.8 \n72.8 \n60.3 \n31.3 \n76.2 \n44.9 \n53.8 17.8 \nSource Val + Target \n41.6 \n66.7 \n63.5 \n51.1 \n71.4 \n36.6 \n70.5 \n66.5 \n49.6 \n32.9 \n30.7 \n46.9 \n60.6 \n66.3 \n57.3 \n21.5 \n70.5 \n40.0 \n52.5 15.3 \nTarget \n40.4 \n66.3 \n58.8 \n48.1 \n67.2 \n29.6 \n54.0 \n58.5 \n40.0 \n34.8 \n41.3 \n55.6 \n54.8 \n60.3 \n53.4 \n15.6 \n63.6 \n33.0 \n48.6 13.8 \n\nClassAMI \n\nSource + Target Features \n56.1 \n83.4 \n83.1 \n85.4 \n85.1 \n73.6 \n90.2 \n82.6 \n81.6 \n61.1 \n66.7 \n76.6 \n75.2 \n88.2 \n82.6 \n60.5 \n92.1 \n67.7 \n77.3 10.5 \nSource + Target Logits \n54.1 \n81.7 \n79.0 \n83.7 \n80.2 \n71.6 \n86.7 \n78.1 \n76.9 \n55.7 \n63.0 \n74.1 \n70.4 \n87.1 \n78.0 \n55.4 \n91.1 \n64.9 \n74.0 11.0 \nTarget Features \n-17.0 -36.3 -11.4 \n21.3 -16.3 \n-8.0 \n-8.9 -47.9 -26.3 -26.9 -38.3 -33.5 -40.8 -36.4 -46.0 -15.4 -31.0 -29.1 -24.9 16.5 \nTarget Logits \n-10.4 -42.3 \n-6.3 \n17.4 \n-5.3 \n-0.6 -18.5 -49.1 \n-5.4 -23.7 -30.5 -24.4 -43.6 -41.2 -44.5 \n-4.1 -34.2 -20.3 -21.5 18.2 \n\nClassSS \n\nSource + Target Features \n9.9 \n25.5 \n24.5 \n48.9 \n19.3 \n21.7 -27.2 -31.5 \n-1.8 -37.2 -31.5 -11.3 -15.8 \n-1.2 \n4.8 \n-8.8 -12.6 -25.2 \n-2.8 23.4 \nSource + Target Logits \n0.7 \n6.2 \n11.4 \n37.5 \n4.7 \n13.1 -23.7 -28.9 \n-9.4 -34.0 -29.4 -12.8 -12.2 \n-1.0 \n-1.2 -15.3 -21.2 -26.7 \n-7.9 18.0 \nTarget Features \n23.1 \n37.3 \n20.4 \n50.2 \n12.1 \n28.1 -53.5 -34.9 \n-5.7 -41.5 -32.7 \n-6.1 -42.3 -39.1 \n-7.9 -11.7 -40.2 -20.5 \n-9.2 30.5 \nTarget Logits \n12.8 \n46.6 \n8.2 \n52.9 \n-0.4 \n19.9 -46.9 -31.0 -14.7 -40.1 -33.6 -12.7 -37.4 -31.1 -14.2 -18.4 -37.2 -23.4 -11.2 28.4 \n\nDEV \n\nFeatures \n-15.4 \n-6.8 \n37.7 \n57.2 \n34.0 \n58.4 \n15.8 \n43.3 \n42.0 -10.2 -18.3 -12.3 -24.3 -12.0 \n3.2 \n-3.2 \n-9.7 \n17.6 \n10.9 26.9 \nLogits \n30.9 \n48.5 \n51.8 \n68.8 \n53.8 \n74.7 \n43.5 \n64.6 \n77.8 \n32.6 \n38.6 \n52.2 \n35.5 \n29.5 \n53.5 \n28.0 \n58.5 \n68.1 \n50.6 15.5 \nPreds \n-24.0 -30.5 -51.0 -30.9 -49.0 -13.9 -43.5 -26.9 -33.8 -31.0 \n55.5 \n72.1 -23.9 \n73.9 \n59.2 -50.9 \n75.8 \n53.1 \n-1.1 47.9 \n\nDEVN \n\nFeatures, max normalization \n79.4 \n82.8 \n86.4 \n85.4 \n77.9 \n76.6 \n75.4 \n92.9 \n92.5 \n73.5 \n81.0 \n85.2 \n86.6 \n89.1 \n93.0 \n89.5 \n76.0 \n94.7 \n84.3 \n6.6 \nLogits, max normalization \n76.8 \n82.7 \n85.2 \n85.4 \n76.1 \n76.3 \n74.9 \n92.6 \n92.4 \n71.5 \n79.8 \n84.3 \n83.2 \n87.8 \n92.1 \n89.3 \n72.1 \n94.2 \n83.2 \n7.2 \nPreds, max normalization \n77.7 \n82.6 \n84.6 \n85.4 \n73.7 \n76.9 \n73.2 \n91.9 \n93.3 \n67.8 \n72.2 \n82.1 \n83.0 \n90.7 \n89.4 \n88.2 \n71.8 \n94.2 \n82.2 \n8.0 \n\nEntropy \n\nSource Train \n34.5 \n57.3 \n38.1 \n37.7 \n55.1 \n32.0 \n54.6 \n47.5 \n22.3 \n20.4 \n13.8 \n32.1 \n49.9 \n55.4 \n46.2 \n7.9 \n55.5 \n13.9 \n37.5 15.9 \nSource Train + Target \n36.5 \n53.3 \n23.2 \n43.3 \n43.9 \n42.5 \n47.7 \n43.5 \n39.6 \n28.6 \n30.6 \n47.6 \n41.5 \n49.0 \n40.7 \n22.4 \n55.0 \n21.7 \n39.5 10.0 \nSource Val \n-11.3 \n16.0 \n26.1 \n30.5 \n50.6 \n31.2 \n39.3 \n27.2 \n41.3 \n10.4 \n10.2 \n26.9 \n50.9 \n57.8 \n40.7 \n27.4 \n54.7 \n25.1 \n30.8 17.2 \nSource Val + Target \n17.7 \n31.3 \n20.9 \n40.5 \n43.1 \n38.3 \n39.6 \n34.2 \n40.5 \n22.3 \n23.5 \n41.3 \n41.4 \n48.4 \n38.5 \n23.6 \n51.6 \n22.1 \n34.4 10.0 \nTarget \n32.1 \n44.6 \n8.4 \n41.1 \n33.0 \n43.8 \n38.7 \n37.7 \n38.6 \n26.6 \n32.0 \n49.4 \n32.6 \n42.8 \n35.3 \n22.8 \n49.2 \n20.2 \n34.9 10.2 \n\n\n\nTable 19 :\n19The weighted Spearman correlation of each validator/task pair for MCC.AD \nAW \nDA \nDW \nWA \nWD \nAC \nAP \nAR \nCA \nCP \nCR \nPA \nPC \nPR \nRA \nRC \nRP Mean \nStd \n\nAccuracy \nSource Train \n74.8 \n76.7 \n84.4 \n75.3 \n87.7 \n70.4 \n55.9 \n65.7 \n56.3 \n51.7 \n61.9 \n55.1 \n70.4 \n63.6 \n72.1 \n57.7 \n71.3 \n61.2 \n67.3 10.0 \nSource Val \n78.3 \n84.7 \n84.3 \n83.6 \n86.4 \n78.0 \n83.9 \n89.7 \n94.8 \n84.5 \n89.9 \n87.6 \n91.4 \n89.3 \n94.8 \n86.7 \n81.3 \n91.8 \n86.7 \n4.8 \n\nBNM \n\nSource Train \n66.7 \n69.1 \n58.9 \n47.2 \n66.9 \n44.3 \n42.8 \n36.5 \n20.7 \n14.3 \n31.2 \n21.3 \n49.9 \n43.8 \n55.3 \n33.2 \n48.4 \n32.9 \n43.5 15.8 \nSource Train + Target \n52.1 \n66.5 \n52.0 \n53.4 \n62.8 \n26.8 \n63.4 \n34.0 \n41.7 \n24.9 \n48.3 \n39.9 \n39.0 \n49.6 \n54.9 \n31.7 \n59.2 \n40.6 \n46.7 12.2 \nSource Val \n70.0 \n72.9 \n55.3 \n51.8 \n67.3 \n51.4 \n84.3 \n58.6 \n54.7 \n15.3 \n37.1 \n25.3 \n63.5 \n66.4 \n66.3 \n61.5 \n77.9 \n58.8 \n57.7 16.9 \nSource Val + Target \n62.5 \n69.4 \n55.4 \n56.0 \n67.7 \n43.7 \n75.6 \n49.8 \n51.2 \n22.9 \n45.0 \n34.9 \n45.4 \n54.6 \n60.3 \n42.7 \n68.3 \n51.3 \n53.2 12.8 \nTarget \n49.0 \n62.2 \n47.2 \n52.4 \n60.5 \n23.4 \n59.3 \n30.1 \n40.4 \n24.2 \n49.8 \n42.5 \n37.2 \n47.1 \n53.5 \n31.2 \n57.9 \n39.5 \n44.9 11.8 \n\nClassAMI \n\nSource + Target Features \n72.7 \n77.8 \n75.5 \n86.7 \n82.6 \n85.1 \n90.1 \n76.2 \n78.2 \n67.2 \n79.8 \n78.0 \n75.8 \n87.9 \n84.4 \n69.5 \n92.8 \n79.3 \n80.0 \n6.7 \nSource + Target Logits \n74.1 \n79.4 \n71.6 \n86.0 \n81.7 \n82.5 \n88.4 \n70.5 \n74.8 \n62.7 \n76.4 \n74.6 \n72.1 \n85.7 \n82.1 \n66.3 \n91.8 \n78.0 \n77.7 \n7.5 \nTarget Features \n-7.1 -28.8 \n1.5 \n34.3 -18.6 \n42.3 -52.5 -38.9 -31.2 -26.0 -30.4 -18.4 -15.2 -23.6 -24.4 -32.5 \n-2.5 -27.6 -16.7 23.1 \nTarget Logits \n-10.0 -28.8 \n-0.2 \n31.6 -21.7 \n41.1 -54.4 -40.6 -32.3 -27.4 -30.3 -17.1 -21.2 -26.6 -26.6 -33.4 \n-5.4 -29.5 -18.5 22.9 \n\nClassSS \n\nSource + Target Features \n42.8 \n45.5 \n36.0 \n55.8 \n51.9 \n40.2 \n20.6 \n5.8 \n21.0 \n1.7 \n34.7 \n16.7 \n12.8 \n28.5 \n25.5 \n21.8 \n43.2 \n33.7 \n29.9 15.0 \nSource + Target Logits \n47.6 \n43.7 \n31.5 \n52.7 \n53.9 \n42.8 \n24.2 \n14.9 \n26.4 \n9.3 \n43.8 \n23.4 \n18.4 \n32.4 \n27.3 \n26.5 \n49.9 \n39.9 \n33.8 13.1 \nTarget Features \n58.4 \n55.8 \n34.7 \n58.9 \n52.5 \n46.5 \n10.2 \n9.8 \n23.3 \n13.8 \n47.8 \n27.2 \n11.4 \n19.7 \n27.3 \n23.5 \n36.6 \n37.8 \n33.1 16.6 \nTarget Logits \n62.6 \n59.5 \n30.0 \n60.1 \n53.7 \n49.9 \n17.3 \n16.8 \n26.0 \n18.4 \n54.3 \n30.2 \n17.5 \n23.1 \n29.8 \n27.0 \n42.7 \n44.1 \n36.8 16.0 \n\nDEV \n\nFeatures \n-22.6 -23.9 \n41.5 \n56.4 \n45.4 \n60.2 \n34.6 \n27.4 \n21.7 -19.7 -22.6 -10.8 -17.2 \n-8.0 -10.8 -12.3 \n3.1 \n11.2 \n8.5 28.4 \nLogits \n13.0 \n21.1 \n58.1 \n66.8 \n60.6 \n72.9 \n50.4 \n64.7 \n73.7 \n31.2 \n35.0 \n46.6 \n21.4 \n22.8 \n61.7 \n36.1 \n48.3 \n71.3 \n47.5 19.4 \nPreds \n-24.2 -43.2 -54.6 -29.1 -42.8 -11.3 -30.4 -18.4 -31.4 -39.6 \n80.2 \n75.7 -39.0 \n63.4 \n78.1 -46.1 \n85.6 \n79.4 \n2.9 53.5 \n\nDEVN \n\nFeatures, max normalization \n69.7 \n77.2 \n81.5 \n80.9 \n75.8 \n75.9 \n76.4 \n82.5 \n90.0 \n77.3 \n84.9 \n80.6 \n83.7 \n78.2 \n89.0 \n77.2 \n67.8 \n87.0 \n79.8 \n5.8 \nLogits, max normalization \n70.9 \n76.8 \n79.1 \n81.1 \n74.3 \n75.5 \n76.4 \n82.5 \n90.1 \n77.1 \n85.5 \n80.7 \n82.4 \n76.7 \n89.2 \n76.2 \n66.9 \n87.2 \n79.4 \n6.0 \nPreds, max normalization \n69.3 \n76.8 \n77.3 \n80.8 \n73.1 \n76.1 \n76.2 \n82.0 \n90.6 \n73.0 \n82.8 \n78.7 \n83.0 \n84.6 \n91.4 \n76.4 \n69.5 \n90.8 \n79.6 \n6.6 \n\nEntropy \n\nSource Train \n56.1 \n57.6 \n42.4 \n47.7 \n62.4 \n45.4 \n44.6 \n36.7 \n22.2 \n8.4 \n27.0 \n19.5 \n46.4 \n44.0 \n53.4 \n31.0 \n47.0 \n33.3 \n40.3 14.0 \nSource Train + Target \n61.7 \n61.6 \n20.9 \n52.4 \n47.8 \n56.2 \n52.3 \n41.8 \n43.8 \n25.2 \n47.5 \n36.4 \n44.2 \n42.8 \n55.9 \n44.3 \n52.4 \n45.8 \n46.3 10.6 \nSource Val \n14.1 \n28.6 \n29.0 \n35.0 \n53.5 \n42.9 \n52.4 \n36.1 \n47.0 \n4.7 \n30.4 \n19.2 \n53.8 \n55.0 \n59.6 \n58.8 \n62.3 \n52.2 \n40.8 16.5 \nSource Val + Target \n49.1 \n47.0 \n18.8 \n47.1 \n47.4 \n49.4 \n49.1 \n37.6 \n44.9 \n20.1 \n43.2 \n31.8 \n45.3 \n43.0 \n56.5 \n49.0 \n53.8 \n47.7 \n43.4 10.0 \nTarget \n60.0 \n54.0 \n9.6 \n49.8 \n41.6 \n56.4 \n46.9 \n37.4 \n42.0 \n26.6 \n49.5 \n39.7 \n41.4 \n38.4 \n53.2 \n44.3 \n49.4 \n44.3 \n43.6 11.3 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-82.8 -83.1 -83.3 -49.0 -85.6 -31.3 -91.2 -86.9 -86.9 -87.0 -85.8 -87.9 -86.5 -87.2 -87.7 -86.7 -85.3 -83.2 -81.0 14.9 \nFeatures, \u03c4 = 0.1 \n-88.2 -87.9 -83.7 -67.2 -86.9 -55.4 -89.8 -89.9 -88.8 -89.2 -90.6 -88.5 -87.4 -86.5 -90.1 -88.1 -84.3 -88.7 -85.1 \n8.8 \nFeatures, \u03c4 = 0.5 \n-85.2 -84.6 -79.5 -37.9 -83.6 -32.3 -85.1 -86.3 -76.5 -84.6 -86.8 -80.5 -85.0 -78.5 -81.6 -75.8 -67.1 -72.6 -75.7 15.3 \nLogits, \u03c4 = 0.05 \n-77.7 -80.3 -84.5 -56.8 -83.6 -36.2 -92.5 -83.2 -85.1 -85.3 -84.4 -86.2 -82.8 -86.6 -84.2 -84.1 -88.2 -82.6 -80.2 12.8 \nLogits, \u03c4 = 0.1 \n-88.7 -89.4 -84.3 -79.8 -88.0 -65.2 -93.6 -89.7 -91.4 -89.7 -91.7 -90.6 -89.9 -90.6 -91.0 -89.8 -90.8 -90.4 -88.\n\nTable 20 :\n20The weighted Spearman correlation of each validator/task pair for MCD.AD \nAW \nDA \nDW \nWA \nWD \nAC \nAP \nAR \nCA \nCP \nCR \nPA \nPC \nPR \nRA \nRC \nRP Mean \nStd \n\nAccuracy \nSource Train \n79.3 \n77.0 \n75.5 \n73.0 \n83.4 \n76.3 \n79.8 \n74.2 \n51.9 \n65.5 \n54.6 \n52.0 \n79.1 \n74.1 \n67.4 \n63.6 \n82.2 \n66.4 \n70.9 \n9.7 \nSource Val \n84.4 \n86.2 \n87.3 \n69.1 \n85.4 \n80.5 \n97.0 \n98.9 \n98.8 \n86.6 \n87.7 \n86.5 \n96.0 \n94.9 \n94.1 \n95.5 \n93.8 \n95.5 \n89.9 \n7.4 \n\nBNM \n\nSource Train \n78.4 \n71.5 \n67.6 \n65.5 \n57.8 \n80.9 \n76.8 \n62.6 \n32.5 \n73.4 \n41.4 \n51.6 \n80.1 \n64.2 \n58.6 \n60.4 \n80.7 \n48.1 \n64.0 13.6 \nSource Train + Target \n78.3 \n77.0 \n50.2 \n84.0 \n49.1 \n86.9 \n72.4 \n68.8 \n57.3 \n79.8 \n60.8 \n68.4 \n79.8 \n57.5 \n72.1 \n62.1 \n78.1 \n62.1 \n69.1 11.1 \nSource Val \n78.9 \n69.6 \n67.9 \n77.6 \n55.5 \n81.2 \n88.8 \n84.1 \n69.3 \n72.0 \n49.5 \n55.0 \n82.4 \n73.0 \n67.7 \n74.3 \n90.0 \n73.4 \n72.8 10.9 \nSource Val + Target \n79.9 \n75.4 \n60.7 \n82.1 \n51.9 \n84.2 \n80.7 \n80.4 \n67.6 \n76.7 \n55.9 \n62.1 \n81.0 \n62.6 \n71.2 \n67.8 \n82.1 \n69.1 \n71.7 \n9.7 \nTarget \n72.7 \n77.3 \n18.8 \n83.7 \n37.9 \n86.3 \n66.6 \n68.4 \n61.9 \n81.6 \n65.6 \n78.1 \n78.9 \n53.9 \n74.1 \n62.0 \n74.5 \n62.4 \n66.9 16.3 \n\nClassAMI \n\nSource + Target Features \n80.3 \n81.7 \n29.6 \n76.7 \n39.9 \n88.6 \n80.4 \n83.2 \n77.7 \n87.7 \n73.3 \n79.0 \n89.6 \n73.8 \n84.1 \n89.4 \n91.2 \n89.8 \n77.6 16.2 \nSource + Target Logits \n81.6 \n81.3 \n32.9 \n77.0 \n42.3 \n89.6 \n85.7 \n84.2 \n78.2 \n89.3 \n72.0 \n80.8 \n90.3 \n77.0 \n84.2 \n91.1 \n92.5 \n91.8 \n79.0 15.8 \nTarget Features \n-21.3 -52.7 -47.2 \n3.3 -49.8 \n29.0 -74.1 -61.8 -68.6 -70.5 -62.4 -67.0 -62.2 -55.9 -60.7 -62.5 -64.4 -52.5 -50.1 26.4 \nTarget Logits \n-25.0 -52.3 -51.0 \n5.0 -49.5 \n20.0 -76.5 -56.7 -63.1 -69.9 -59.3 -65.0 -58.8 -59.3 -57.8 -59.7 -67.4 -43.8 -49.4 24.5 \n\nClassSS \n\nSource + Target Features \n38.4 \n16.1 -10.5 \n56.2 \n-2.2 \n69.0 -21.2 \n1.0 \n-8.3 -15.5 \n-7.5 -15.6 \n2.2 \n0.2 \n15.5 \n16.4 \n18.8 \n21.9 \n9.7 24.1 \nSource + Target Logits \n45.5 \n26.8 -10.3 \n59.4 -14.6 \n72.6 -19.1 \n14.4 \n-7.5 \n-8.4 \n1.9 \n-4.5 \n-5.9 \n-1.1 \n13.9 \n23.3 \n4.0 \n25.1 \n12.0 25.3 \nTarget Features \n31.3 \n46.6 -22.1 \n71.8 -27.2 \n70.8 -40.6 \n-3.2 \n-3.2 -23.9 \n-3.9 -13.0 -25.6 -22.4 \n8.4 \n8.7 -12.7 \n24.3 \n3.6 32.3 \nTarget Logits \n35.7 \n34.9 -22.3 \n64.3 -33.9 \n74.2 -35.1 \n4.5 \n-5.1 -23.8 \n2.7 \n-8.6 -34.6 -15.7 \n6.0 \n12.9 \n-9.7 \n25.5 \n4.0 31.5 \n\nDEV \n\nFeatures \n-49.2 -62.6 \n11.2 \n56.8 \n13.2 \n63.5 \n30.0 \n54.8 \n41.1 -34.6 -30.1 -23.0 -13.0 -11.0 \n-6.7 -23.8 \n3.1 \n-3.4 \n0.9 35.8 \nLogits \n-4.5 \n-5.5 \n24.7 \n60.6 \n40.8 \n69.0 \n65.4 \n75.7 \n74.1 \n44.4 \n53.3 \n64.4 \n57.0 \n27.3 \n61.5 \n39.9 \n63.0 \n63.7 \n48.6 23.7 \nPreds \n-26.8 -42.3 -30.9 -21.8 -31.8 \n4.4 -21.2 -13.9 -19.1 -16.0 \n86.2 \n76.8 \n-7.2 \n68.8 \n76.1 -34.3 \n87.4 \n82.0 \n12.0 48.9 \n\nDEVN \n\nFeatures, max normalization \n83.7 \n86.7 \n88.3 \n66.9 \n83.2 \n80.5 \n97.2 \n98.5 \n97.6 \n90.9 \n82.8 \n83.8 \n96.7 \n94.2 \n91.2 \n93.3 \n94.1 \n91.6 \n88.9 \n7.7 \nLogits, max normalization \n84.3 \n84.2 \n88.6 \n66.7 \n84.0 \n80.6 \n97.3 \n98.6 \n98.1 \n89.1 \n79.7 \n82.5 \n96.2 \n93.9 \n89.5 \n92.4 \n93.9 \n92.9 \n88.5 \n7.9 \nPreds, max normalization \n83.4 \n84.2 \n88.1 \n66.1 \n82.0 \n79.7 \n96.9 \n98.7 \n98.3 \n84.6 \n64.1 \n75.2 \n95.7 \n93.3 \n86.8 \n92.6 \n94.3 \n94.7 \n86.6 10.1 \n\nEntropy \n\nSource Train \n62.5 \n49.7 \n-8.0 \n62.5 \n4.2 \n79.3 \n68.1 \n57.2 \n29.1 \n69.3 \n35.2 \n45.1 \n76.7 \n62.5 \n55.6 \n59.2 \n77.6 \n47.0 \n51.8 23.3 \nSource Train + Target \n57.5 \n41.6 -48.6 \n75.3 -42.9 \n86.6 \n51.6 \n55.4 \n48.3 \n75.9 \n50.4 \n56.7 \n73.2 \n49.4 \n62.7 \n59.3 \n70.9 \n57.6 \n49.0 35.3 \nSource Val \n28.8 \n20.7 -34.6 \n67.1 -22.7 \n76.8 \n56.9 \n56.7 \n49.7 \n66.0 \n43.3 \n46.3 \n76.8 \n66.2 \n62.3 \n70.8 \n78.6 \n65.6 \n48.6 31.4 \nSource Val + Target \n51.1 \n27.4 -49.3 \n74.9 -44.0 \n82.8 \n51.6 \n55.3 \n50.3 \n72.8 \n48.1 \n53.3 \n73.1 \n50.2 \n63.1 \n62.4 \n71.9 \n60.9 \n47.5 35.6 \nTarget \n45.4 \n38.0 -50.8 \n68.6 -48.8 \n85.3 \n47.2 \n53.6 \n49.0 \n78.3 \n52.0 \n62.6 \n71.6 \n44.6 \n63.0 \n58.8 \n67.8 \n56.7 \n46.8 36.2 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-88.1 -88.2 -82.2 -71.3 -84.0 -66.6 -91.6 -92.6 -93.4 -86.9 -91.7 -91.4 -83.5 -88.4 -92.5 -90.9 -88.7 -90.0 -86.8 \n7.1 \nFeatures, \u03c4 = 0.1 \n-89.3 -87.9 -80.3 -85.9 -81.2 -87.7 -90.2 -91.5 -92.2 -87.0 -91.7 -91.1 -82.8 -85.8 -91.6 -90.4 -87.0 -91.4 -88.1 \n3.6 \nFeatures, \u03c4 = 0.5 \n-80.6 -85.8 -80.0 -49.8 -75.7 -54.2 -89.6 -80.2 -80.8 -86.1 -82.0 -83.1 -82.7 -82.5 -80.4 -80.5 -85.8 -78.3 -78.8 10.0 \nLogits, \u03c4 = 0.05 \n-90.0 -88.8 -87.5 -65.0 -89.6 -62.4 -93.6 -95.0 -95.8 -87.8 -94.2 -93.4 -84.8 -91.5 -93.9 -93.4 -90.4 -93.9 -88.4 \n9.2 \nLogits, \u03c4 = 0.1 \n-91.5 -89.2 -87.2 -75.7 -89.1 -84.1 -93.0 -93.4 -94.3 -87.7 -91.8 -91.9 -84.8 -90.5 -92.4 -91.8 -89.5 -91.8 -89.4 \n4.3 \nLogits, \u03c4 = 0.5 \n-86.6 -88.5 -89.2 -42.6 -89.3 -56.6 -93.6 -90.0 -92.2 -88.8 -90.0 -90.5 -88.8 -91.2 -90.9 -89.1 -89.2 -87.1 -85.2 12.9 \nPreds, \u03c4 = 0.05 \n-86.9 -86.8 -82.2 -74.0 -82.6 -54.3 -91.3 -91.4 -92.3 -86.1 -87.9 -89.1 -81.2 -85.8 -87.8 -87.4 -85.5 -83.1 -84.2 \n8.4 \nPreds, \u03c4 = 0.1 \n-91.8 -89.0 -80.9 -91.6 -81.5 -80.1 -90.5 -93.1 -95.8 -87.1 -93.3 -93.6 -82.8 -86.8 -94.4 -89.4 -88.4 -89.8 -88.9 \n4.7 \nPreds, \u03c4 = 0.5 \n-79.2 -85.4 -77.1 -54.5 -72.0 -72.7 -89.8 -82.3 -80.4 -86.7 -81.9 -83.3 -83.0 -87.1 -80.9 -85.7 -90.1 -82.5 -80.8 \n8.0 \n\n\n\nTable 21 :\n21The weighted Spearman correlation of each validator/task pair for MMD.AD \nAW \nDA \nDW \nWA \nWD \nAC \nAP \nAR \nCA \nCP \nCR \nPA \nPC \nPR \nRA \nRC \nRP Mean \nStd \n\nAccuracy \nSource Train \n52.4 \n55.8 \n80.5 \n62.2 \n78.5 \n66.8 \n50.1 \n55.9 \n46.4 \n35.3 \n44.8 \n40.4 \n59.3 \n58.5 \n67.6 \n47.5 \n57.2 \n51.0 \n56.1 11.7 \nSource Val \n80.9 \n88.0 \n84.8 \n77.8 \n81.4 \n76.5 \n92.0 \n95.5 \n97.1 \n78.8 \n84.3 \n82.1 \n92.7 \n94.4 \n94.7 \n94.0 \n90.9 \n96.6 \n87.9 \n6.9 \n\nBNM \n\nSource Train \n31.7 \n29.5 \n20.0 \n31.1 \n16.0 \n37.5 \n4.8 \n4.0 -11.3 \n-9.3 \n-2.8 -10.0 \n14.3 \n23.6 \n33.0 \n0.4 \n12.9 \n3.7 \n12.7 15.6 \nSource Train + Target \n33.7 \n41.8 \n30.9 \n51.0 \n32.7 \n50.8 \n45.4 \n39.4 \n39.9 \n12.8 \n30.3 \n34.8 \n24.9 \n45.4 \n57.0 \n28.4 \n48.7 \n34.5 \n37.9 10.6 \nSource Val \n45.3 \n47.4 \n15.7 \n48.2 \n13.2 \n55.2 \n80.8 \n67.8 \n64.9 \n-3.8 \n5.2 \n-1.6 \n44.7 \n51.4 \n58.0 \n54.4 \n72.2 \n63.1 \n43.5 25.4 \nSource Val + Target \n38.7 \n45.3 \n28.3 \n53.2 \n27.3 \n55.2 \n69.6 \n61.2 \n60.5 \n8.9 \n21.8 \n22.1 \n29.8 \n48.8 \n58.8 \n40.0 \n61.6 \n54.1 \n43.6 16.8 \nTarget \n33.0 \n41.2 \n31.5 \n53.8 \n35.5 \n52.3 \n46.4 \n42.1 \n44.8 \n15.5 \n38.2 \n43.4 \n25.0 \n45.7 \n58.9 \n30.1 \n49.4 \n38.2 \n40.3 10.5 \n\nClassAMI \n\nSource + Target Features \n51.7 \n60.9 \n77.0 \n65.6 \n69.1 \n75.9 \n86.3 \n73.6 \n78.4 \n64.5 \n71.7 \n75.1 \n72.2 \n87.5 \n85.2 \n74.7 \n90.5 \n81.3 \n74.5 \n9.7 \nSource + Target Logits \n49.3 \n58.6 \n73.5 \n65.5 \n63.5 \n75.3 \n85.3 \n70.9 \n74.9 \n61.0 \n67.2 \n73.1 \n68.9 \n86.3 \n83.7 \n72.0 \n90.0 \n80.4 \n72.2 10.3 \nTarget Features \n61.7 \n73.8 \n64.2 \n70.6 \n65.2 \n77.7 \n62.1 \n61.1 \n63.1 \n35.6 \n48.4 \n70.6 \n57.4 \n55.5 \n82.1 \n64.8 \n81.8 \n72.9 \n64.9 11.2 \nTarget Logits \n57.8 \n66.6 \n66.8 \n69.9 \n61.3 \n75.2 \n55.2 \n66.5 \n70.1 \n39.1 \n59.7 \n69.8 \n53.5 \n51.8 \n80.4 \n64.6 \n77.6 \n77.1 \n64.6 10.3 \n\nClassSS \n\nSource + Target Features \n13.5 \n10.5 \n26.9 \n50.2 \n22.5 \n42.4 -24.1 -17.0 \n-6.6 -45.0 -13.7 -24.0 -13.8 \n-5.6 \n24.2 \n-1.7 \n6.9 \n12.8 \n3.2 24.0 \nSource + Target Logits \n19.0 \n17.6 \n19.3 \n50.3 \n25.0 \n44.3 -15.7 \n-9.4 \n-4.4 -36.6 \n-5.9 -11.1 \n-4.4 \n3.4 \n29.2 \n6.9 \n8.4 \n15.8 \n8.4 21.1 \nTarget Features \n26.6 \n32.8 \n24.6 \n50.9 \n25.1 \n49.2 -23.1 -21.5 \n6.8 -37.4 \n6.1 \n3.4 \n-6.8 \n-6.1 \n33.1 \n4.2 \n-1.5 \n18.4 \n10.3 23.8 \nTarget Logits \n29.0 \n34.8 \n17.1 \n50.7 \n23.7 \n51.9 -17.9 -19.0 \n3.8 -36.8 \n6.3 \n10.1 \n-3.7 \n-0.7 \n32.2 \n7.5 \n1.0 \n16.3 \n11.5 22.8 \n\nDEV \n\nFeatures \n-10.7 \n-2.6 \n26.0 \n52.5 \n-4.5 \n56.8 \n15.1 \n18.9 \n6.8 -16.6 -20.2 -22.4 -19.9 -16.7 \n5.1 \n-5.8 \n15.1 \n20.4 \n5.4 22.9 \nLogits \n-9.7 \n-9.4 \n48.6 \n60.6 \n38.3 \n69.9 \n19.8 \n33.8 \n49.7 \n-4.7 \n8.5 \n13.0 -18.0 \n-8.9 \n22.8 \n0.4 \n7.2 \n24.4 \n19.2 25.6 \nPreds \n-38.1 -51.6 -67.5 -34.5 -60.0 -11.9 -43.2 -34.8 -34.2 -49.9 \n39.5 \n37.0 -50.2 \n18.0 \n56.5 -48.0 \n31.8 \n53.5 -16.0 41.4 \n\nDEVN \n\nFeatures, max normalization \n81.0 \n84.6 \n77.9 \n74.3 \n48.8 \n74.3 \n82.8 \n91.7 \n95.8 \n63.7 \n67.1 \n64.1 \n86.4 \n88.7 \n89.8 \n89.2 \n79.8 \n94.6 \n79.7 12.1 \nLogits, max normalization \n76.5 \n83.9 \n73.6 \n74.0 \n47.3 \n73.5 \n82.7 \n91.7 \n95.8 \n63.5 \n67.1 \n65.2 \n85.0 \n88.3 \n88.8 \n90.0 \n79.6 \n94.8 \n79.0 12.3 \nPreds, max normalization \n78.6 \n84.2 \n69.2 \n74.0 \n42.4 \n74.0 \n82.7 \n91.1 \n94.8 \n56.7 \n49.1 \n53.7 \n84.6 \n87.0 \n84.9 \n89.4 \n78.9 \n92.2 \n76.0 15.3 \n\nEntropy \n\nSource Train \n29.8 \n26.5 \n1.7 \n31.4 \n-3.8 \n39.8 \n-2.1 \n-2.3 -15.5 -10.0 \n-2.2 \n-8.4 \n6.6 \n19.2 \n27.5 \n-2.1 \n10.8 \n0.9 \n8.2 16.1 \nSource Train + Target \n27.7 \n37.6 \n12.1 \n43.0 \n18.2 \n55.9 \n35.6 \n35.7 \n31.6 \n14.4 \n26.0 \n29.6 \n24.8 \n40.7 \n54.8 \n31.1 \n48.0 \n37.5 \n33.6 12.0 \nSource Val \n34.1 \n44.2 \n8.0 \n40.1 \n7.1 \n52.1 \n55.0 \n47.9 \n49.0 \n-5.2 \n6.5 \n0.8 \n37.5 \n49.2 \n54.4 \n50.6 \n65.6 \n61.6 \n36.6 22.0 \nSource Val + Target \n34.0 \n41.9 \n12.8 \n43.7 \n18.3 \n56.4 \n43.4 \n42.5 \n42.0 \n12.3 \n23.6 \n25.1 \n27.7 \n42.9 \n56.3 \n38.6 \n54.3 \n50.0 \n37.0 13.7 \nTarget \n27.9 \n38.5 \n9.1 \n44.1 \n20.0 \n58.7 \n36.1 \n38.0 \n34.7 \n18.8 \n33.4 \n38.8 \n25.2 \n40.7 \n56.8 \n33.1 \n48.6 \n41.3 \n35.8 12.3 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-74.6 -70.4 -73.4 -22.2 -73.7 -11.0 -74.2 -83.4 -78.5 -77.4 -75.9 -79.6 -79.2 -76.1 -80.7 -73.9 -70.3 -67.9 -69.0 19.0 \nFeatures, \u03c4 = 0.1 \n-77.9 -75.8 -73.3 -41.3 -73.5 -46.2 -70.2 -85.2 -76.4 -80.9 -80.7 -80.3 -76.8 -66.9 -78.4 -75.3 -70.6 -76.6 -72.6 11.1 \nFeatures, \u03c4 = 0.5 \n-36.8 -49.7 -31.0 \n32.9 -48.1 \n21.4 -24.2 -57.1 -38.9 -48.3 -44.3 -30.9 -16.3 \n-3.1 -25.8 -22.1 -20.7 -39.5 -26.8 23.2 \nLogits, \u03c4 = 0.05 \n-63.0 -50.5 -83.0 -22.3 -73.5 -11.2 -73.5 -67.1 -67.3 -70.4 -68.7 -64.2 -73.2 -83.7 -75.7 -64.1 -73.7 -60.1 -63.6 18.3 \nLogits, \u03c4 = 0.1 \n-77.7 -79.3 -83.8 -69.2 -81.7 -61.6 -82.2 -85.7 -82.3 -81.6 -85.0 -82.2 -82.4 -86.9 -88.8 -78.3 -84.2 -82.1 -80.\n\nTable 22 :\n22The weighted Spearman correlation of each validator/task pair, using the checkpoints of all algorithms.CP \nCR \nCS \nPC \nPR \nPS \nRC \nRP \nRS \nSC \nSP \nSR Mean \nStd \n\nAccuracy \nSource Train \n84.3 \n82.9 \n81.9 \n74.6 \n67.6 \n65.2 \n75.5 \n84.9 \n68.1 \n76.3 \n78.7 \n76.0 \n76.3 \n6.4 \nSource Val \n75.7 \n81.3 \n82.2 \n74.2 \n88.3 \n64.8 \n76.2 \n89.6 \n70.8 \n70.6 \n68.0 \n63.9 \n75.5 \n8.1 \n\nBNM \n\nSource Train \n82.5 \n84.1 \n81.2 \n85.1 \n74.5 \n82.6 \n86.0 \n84.6 \n83.2 \n85.5 \n83.8 \n85.1 \n83.2 \n2.9 \nSource Train + Target \n84.5 \n87.0 \n85.1 \n84.9 \n83.8 \n85.1 \n84.2 \n73.1 \n84.7 \n84.3 \n85.0 \n92.3 \n84.5 \n4.1 \nSource Val \n82.8 \n81.8 \n80.4 \n83.9 \n85.3 \n81.1 \n81.4 \n80.5 \n79.7 \n82.6 \n80.9 \n86.8 \n82.3 \n2.0 \nSource Val + Target \n83.3 \n83.6 \n82.8 \n83.9 \n84.4 \n83.3 \n83.3 \n74.0 \n83.6 \n82.6 \n82.7 \n89.1 \n83.1 \n3.2 \nTarget \n82.0 \n84.8 \n83.7 \n82.5 \n81.8 \n83.2 \n82.9 \n70.8 \n83.6 \n81.5 \n82.7 \n91.4 \n82.6 \n4.4 \n\nClassAMI \n\nSource + Target Features \n80.0 \n73.5 \n78.2 \n82.0 \n52.3 \n82.5 \n82.8 \n78.0 \n86.5 \n79.7 \n81.8 \n81.2 \n78.2 \n8.4 \nSource + Target Logits \n83.2 \n77.7 \n82.6 \n82.0 \n58.7 \n82.8 \n84.7 \n80.5 \n87.0 \n80.5 \n82.7 \n84.3 \n80.6 \n7.0 \nTarget Features \n-45.1 -47.4 -44.3 -45.2 -50.9 -56.3 -40.0 -34.2 -38.9 -47.9 -48.6 -58.3 -46.4 \n6.6 \nTarget Logits \n-43.8 -19.6 -22.7 -44.4 -25.6 -38.7 -38.4 -31.9 -25.0 -47.0 -47.7 -35.0 -35.0 \n9.5 \n\nClassSS \n\nSource + Target Features \n25.9 \n30.8 \n15.2 \n35.2 \n44.2 \n33.9 \n45.7 \n63.7 \n49.0 \n42.2 \n33.6 \n35.4 \n37.9 11.8 \nSource + Target Logits \n27.9 -14.2 -15.1 \n40.5 \n-6.1 \n-9.6 \n19.1 \n25.7 \n20.9 \n9.3 \n-8.9 -18.0 \n6.0 19.4 \nTarget Features \n2.9 \n22.4 \n-3.3 \n9.4 \n38.1 \n12.0 \n23.9 \n46.7 \n24.7 \n13.5 \n11.4 \n23.9 \n18.8 13.6 \nTarget Logits \n2.8 -18.6 -26.7 \n12.5 \n-9.8 -21.3 \n25.1 \n46.6 \n1.2 \n14.4 \n12.9 -23.6 \n1.3 21.4 \n\nDEV \n\nFeatures \n33.6 \n28.1 \n36.3 \n25.3 \n31.5 \n30.5 \n23.6 \n51.3 \n33.1 \n5.7 \n28.5 \n17.3 \n28.7 10.5 \nLogits \n28.5 \n23.5 \n19.6 \n11.6 \n21.8 \n28.3 \n25.4 \n46.0 \n29.3 \n34.1 \n32.7 \n44.3 \n28.8 \n9.3 \nPreds \n65.5 \n62.0 \n63.8 \n61.5 \n53.4 \n81.6 \n49.5 \n77.3 \n80.0 \n51.5 \n76.9 \n67.7 \n65.9 10.7 \n\nDEVN \n\nFeatures, max normalization \n56.2 \n59.8 \n62.9 \n60.8 \n77.5 \n51.0 \n66.2 \n83.5 \n56.2 \n56.6 \n49.7 \n41.4 \n60.2 11.1 \nLogits, max normalization \n57.2 \n61.7 \n62.9 \n61.3 \n77.3 \n51.3 \n66.4 \n83.6 \n56.9 \n56.2 \n49.6 \n40.5 \n60.4 11.2 \nPreds, max normalization \n57.0 \n62.1 \n66.4 \n61.0 \n87.4 \n49.7 \n65.3 \n83.1 \n53.9 \n61.1 \n51.7 \n32.8 \n61.0 13.9 \n\nEntropy \n\nSource Train \n75.5 \n78.1 \n79.3 \n80.4 \n71.8 \n76.1 \n79.1 \n81.3 \n78.6 \n79.7 \n72.9 \n73.6 \n77.2 \n3.0 \nSource Train + Target \n66.5 \n69.1 \n73.2 \n75.9 \n75.7 \n68.3 \n73.0 \n72.0 \n69.9 \n76.0 \n64.4 \n69.1 \n71.1 \n3.7 \nSource Val \n67.4 \n67.1 \n71.8 \n72.7 \n74.9 \n66.4 \n71.6 \n75.2 \n71.2 \n72.0 \n60.6 \n64.3 \n69.6 \n4.3 \nSource Val + Target \n63.3 \n64.8 \n69.9 \n72.3 \n73.6 \n64.3 \n70.3 \n70.7 \n66.7 \n71.8 \n59.3 \n64.3 \n67.6 \n4.2 \nTarget \n58.9 \n62.4 \n66.7 \n70.6 \n71.9 \n61.2 \n67.9 \n68.4 \n59.8 \n70.9 \n56.6 \n62.9 \n64.\n\nTable 23 :\n23The weighted Spearman correlation of each validator/task pair for ATDOC.CP \nCR \nCS \nPC \nPR \nPS \nRC \nRP \nRS \nSC \nSP \nSR Mean \nStd \n\nAccuracy \nSource Train \n91.7 \n95.4 \n94.2 \n93.8 \n87.7 \n94.4 \n95.9 \n94.0 \n93.4 \n91.7 \n92.8 \n95.0 \n93.3 \n2.1 \nSource Val \n91.2 \n96.8 \n97.9 \n93.5 \n97.2 \n91.5 \n95.5 \n95.4 \n93.8 \n90.0 \n87.1 \n92.4 \n93.5 \n3.1 \n\nBNM \n\nSource Train \n96.6 \n96.8 \n96.2 \n94.6 \n95.6 \n96.2 \n97.9 \n97.0 \n97.1 \n97.5 \n97.4 \n97.7 \n96.7 \n0.9 \nSource Train + Target \n97.3 \n97.5 \n97.3 \n95.3 \n97.0 \n97.0 \n98.6 \n96.9 \n98.2 \n98.4 \n98.6 \n98.3 \n97.5 \n0.9 \nSource Val \n96.5 \n97.5 \n97.1 \n95.0 \n97.5 \n96.3 \n97.9 \n97.1 \n97.3 \n98.0 \n97.6 \n98.1 \n97.2 \n0.8 \nSource Val + Target \n97.0 \n97.7 \n97.3 \n95.0 \n97.5 \n96.7 \n98.6 \n97.0 \n98.3 \n98.4 \n98.3 \n98.4 \n97.5 \n1.0 \nTarget \n96.7 \n97.9 \n96.8 \n94.2 \n96.9 \n96.0 \n97.8 \n95.0 \n97.2 \n98.5 \n98.9 \n98.5 \n97.0 \n1.4 \n\nClassAMI \n\nSource + Target Features \n91.5 \n86.1 \n84.1 \n92.5 \n93.2 \n94.3 \n94.8 \n93.8 \n94.8 \n88.6 \n96.6 \n93.5 \n92.0 \n3.6 \nSource + Target Logits \n93.6 \n92.1 \n86.7 \n93.4 \n92.8 \n95.0 \n95.6 \n93.8 \n95.5 \n90.9 \n96.8 \n95.7 \n93.5 \n2.6 \nTarget Features \n-46.6 -67.6 -64.4 -64.2 -56.3 -54.9 -56.2 -31.8 -59.7 -66.7 -60.2 -66.6 -57.9 \n9.8 \nTarget Logits \n-47.6 -26.4 -21.7 -64.5 \n15.5 \n-5.7 -56.5 -31.9 -40.6 -66.7 -60.5 -21.9 -35.7 24.2 \n\nClassSS \n\nSource + Target Features \n-30.3 -10.3 \n-3.1 \n9.5 \n31.7 \n8.7 \n23.1 \n71.7 \n43.8 \n43.2 \n-1.0 \n8.6 \n16.3 26.7 \nSource + Target Logits \n-29.5 -60.3 -50.8 \n12.4 -43.1 -36.6 -32.8 \n-6.5 -17.8 -40.0 -50.1 -50.6 -33.8 20.1 \nTarget Features \n-56.2 -27.4 -33.2 -30.9 \n12.3 -32.4 -12.3 \n40.6 -18.1 \n-7.4 -36.9 -19.3 -18.4 24.2 \nTarget Logits \n-57.5 -65.4 -62.6 -28.1 -48.6 -52.4 -13.4 \n35.6 -48.4 -10.3 -36.8 -57.0 -37.1 28.0 \n\nDEV \n\nFeatures \n54.2 \n39.4 \n68.0 \n59.9 \n61.6 \n21.4 \n67.8 \n79.7 \n71.9 \n27.6 \n27.6 -24.8 \n46.2 28.3 \nLogits \n33.3 \n16.6 -24.4 \n22.9 \n10.0 \n-3.9 \n66.0 \n79.9 \n60.1 \n67.1 \n57.4 \n16.4 \n33.4 31.1 \nPreds \n43.5 \n26.1 \n73.7 \n67.2 \n87.9 \n91.7 \n68.5 \n96.3 \n94.1 \n70.1 \n84.1 \n57.3 \n71.7 20.5 \n\nDEVN \n\nFeatures, max normalization \n93.1 \n97.1 \n98.2 \n94.5 \n97.3 \n93.2 \n96.1 \n95.5 \n94.5 \n92.0 \n89.6 \n94.1 \n94.6 \n2.3 \nLogits, max normalization \n93.7 \n97.5 \n98.4 \n95.2 \n97.7 \n93.6 \n96.1 \n95.6 \n94.5 \n91.9 \n89.6 \n93.5 \n94.8 \n2.4 \nPreds, max normalization \n93.0 \n97.3 \n98.2 \n93.9 \n97.7 \n93.1 \n95.8 \n95.7 \n94.0 \n90.0 \n88.9 \n93.1 \n94.2 \n2.8 \n\nEntropy \n\nSource Train \n77.1 \n85.1 \n93.3 \n89.4 \n94.1 \n81.3 \n93.7 \n96.3 \n91.9 \n95.0 \n81.0 \n78.9 \n88.1 \n6.7 \nSource Train + Target \n60.5 \n72.5 \n88.3 \n81.5 \n92.3 \n71.1 \n88.7 \n93.3 \n82.8 \n92.6 \n74.2 \n66.0 \n80.3 10.8 \nSource Val \n70.4 \n79.1 \n91.8 \n84.3 \n93.1 \n74.1 \n93.0 \n95.3 \n91.0 \n93.5 \n74.1 \n67.5 \n83.9 \n9.9 \nSource Val + Target \n55.2 \n66.5 \n85.8 \n77.3 \n90.2 \n66.0 \n87.5 \n92.2 \n81.3 \n90.8 \n69.5 \n59.2 \n76.8 12.5 \nTarget \n37.6 \n48.1 \n75.7 \n67.8 \n86.6 \n56.2 \n78.5 \n87.8 \n66.4 \n86.5 \n63.4 \n49.\n\nTable 24 :\n24The weighted Spearman correlation of each validator/task pair for BNM.CP \nCR \nCS \nPC \nPR \nPS \nRC \nRP \nRS \nSC \nSP \nSR Mean \nStd \n\nAccuracy \nSource Train \n78.8 \n78.0 \n68.6 \n66.6 \n77.6 \n48.2 \n75.3 \n73.6 \n52.8 \n71.0 \n57.0 \n73.2 \n68.4 \n9.9 \nSource Val \n68.1 \n73.9 \n61.7 \n53.4 \n65.4 \n51.7 \n70.7 \n76.5 \n57.3 \n59.9 \n40.3 \n43.8 \n60.2 11.0 \n\nBNM \n\nSource Train \n66.3 \n76.0 \n47.2 \n66.5 \n71.8 \n64.1 \n83.3 \n59.0 \n77.1 \n61.5 \n51.2 \n73.0 \n66.4 10.2 \nSource Train + Target \n78.1 \n89.1 \n61.2 \n60.8 \n71.5 \n66.0 \n70.8 \n34.7 \n76.5 \n58.1 \n63.1 \n84.5 \n67.9 13.7 \nSource Val \n74.1 \n81.9 \n50.6 \n56.1 \n67.7 \n59.8 \n76.1 \n47.5 \n75.4 \n49.8 \n53.9 \n75.8 \n64.1 11.9 \nSource Val + Target \n77.3 \n85.7 \n57.4 \n57.8 \n68.2 \n62.9 \n70.8 \n35.9 \n75.9 \n53.9 \n59.9 \n79.8 \n65.5 13.1 \nTarget \n78.4 \n89.8 \n62.3 \n58.4 \n68.7 \n65.4 \n69.0 \n31.7 \n75.7 \n57.1 \n63.8 \n85.7 \n67.2 14.5 \n\nClassAMI \n\nSource + Target Features \n95.3 \n94.5 \n92.1 \n84.7 \n94.5 \n81.9 \n93.0 \n87.3 \n94.9 \n86.9 \n87.3 \n91.7 \n90.4 \n4.3 \nSource + Target Logits \n94.9 \n95.5 \n90.2 \n79.5 \n92.1 \n77.9 \n93.6 \n82.1 \n92.9 \n83.6 \n83.1 \n91.0 \n88.0 \n6.1 \nTarget Features \n-20.5 -29.7 -27.2 -35.1 -36.2 -43.9 -21.8 -19.0 -11.2 -20.2 -44.3 -56.9 -30.5 12.6 \nTarget Logits \n-20.6 \n2.1 -11.8 -36.0 -10.1 -27.5 -21.6 -20.4 \n1.9 -21.1 -45.3 -24.1 -19.5 13.2 \n\nClassSS \n\nSource + Target Features \n46.1 \n41.0 \n29.6 \n38.4 \n76.8 \n27.5 \n63.1 \n56.5 \n40.8 \n61.8 \n27.4 \n57.4 \n47.2 15.2 \nSource + Target Logits \n51.0 \n-1.3 \n9.7 \n48.8 \n18.3 \n-1.5 \n41.6 \n24.2 \n26.6 \n55.3 \n-7.7 -18.0 \n20.6 23.8 \nTarget Features \n15.2 \n32.4 \n-3.0 \n5.1 \n66.4 \n1.0 \n26.6 \n26.0 \n14.2 \n6.7 \n11.9 \n53.8 \n21.4 20.2 \nTarget Logits \n15.0 \n-8.5 -14.6 \n8.5 \n11.5 -19.6 \n29.2 \n29.4 \n2.4 \n11.5 \n16.4 -21.4 \n5.0 16.8 \n\nDEV \n\nFeatures \n33.8 \n23.2 \n46.8 \n20.8 \n31.1 \n34.3 \n16.3 \n38.4 \n15.8 \n9.0 \n40.3 \n36.2 \n28.8 11.1 \nLogits \n33.6 \n17.2 \n33.2 \n5.5 \n25.0 \n28.4 \n6.9 \n31.7 \n19.6 \n38.9 \n25.2 \n49.7 \n26.2 12.2 \nPreds \n59.6 \n53.8 \n57.4 \n59.3 \n66.8 \n78.0 \n50.8 \n54.8 \n60.1 \n57.7 \n65.2 \n65.6 \n60.8 \n7.0 \n\nDEVN \n\nFeatures, max normalization \n54.2 \n60.9 \n43.1 \n43.2 \n52.6 \n45.1 \n66.5 \n68.4 \n55.5 \n46.2 \n21.9 \n29.8 \n49.0 13.2 \nLogits, max normalization \n54.7 \n62.5 \n43.4 \n43.5 \n53.3 \n45.7 \n66.5 \n69.1 \n55.5 \n46.0 \n22.0 \n29.1 \n49.3 13.5 \nPreds, max normalization \n57.5 \n61.5 \n47.1 \n41.8 \n74.1 \n44.5 \n63.0 \n63.9 \n54.0 \n46.2 \n26.8 \n20.3 \n50.1 15.0 \n\nEntropy \n\nSource Train \n64.3 \n75.0 \n54.7 \n61.6 \n73.5 \n63.2 \n77.0 \n57.2 \n69.8 \n51.8 \n37.4 \n78.3 \n63.7 11.6 \nSource Train + Target \n64.2 \n75.9 \n55.0 \n52.9 \n68.3 \n49.5 \n63.1 \n39.9 \n54.1 \n45.9 \n40.6 \n68.2 \n56.5 11.1 \nSource Val \n58.8 \n72.2 \n48.2 \n43.9 \n62.5 \n45.1 \n64.6 \n43.6 \n58.5 \n36.7 \n35.2 \n60.2 \n52.5 11.3 \nSource Val + Target \n59.1 \n72.1 \n49.4 \n47.0 \n62.3 \n43.2 \n59.6 \n38.0 \n50.2 \n40.7 \n37.1 \n59.2 \n51.5 10.5 \nTarget \n56.0 \n70.5 \n43.9 \n47.6 \n61.9 \n37.8 \n54.4 \n36.1 \n38.9 \n43.0 \n33.2 \n57.6 \n48.4 11.1 \n\n\n\nTable 25 :\n25The weighted Spearman correlation of each validator/task pair for BSP.CP \nCR \nCS \nPC \nPR \nPS \nRC \nRP \nRS \nSC \nSP \nSR Mean \nStd \n\nAccuracy \nSource Train \n92.5 \n89.5 \n79.1 \n93.7 \n60.6 \n88.9 \n93.2 \n87.7 \n89.8 \n83.4 \n92.4 \n85.8 \n86.4 \n8.8 \nSource Val \n95.0 \n98.8 \n97.9 \n98.2 \n98.1 \n91.0 \n97.3 \n98.2 \n94.0 \n94.1 \n95.1 \n95.6 \n96.1 \n2.3 \n\nBNM \n\nSource Train \n85.7 \n80.2 \n90.8 \n86.6 \n76.9 \n91.5 \n88.6 \n83.9 \n80.0 \n94.7 \n90.6 \n89.1 \n86.6 \n5.2 \nSource Train + Target \n83.8 \n78.6 \n88.8 \n81.3 \n73.5 \n90.5 \n85.1 \n80.0 \n72.9 \n93.6 \n89.5 \n88.3 \n83.8 \n6.4 \nSource Val \n81.8 \n76.1 \n87.3 \n82.3 \n70.1 \n90.3 \n87.2 \n81.6 \n78.0 \n93.2 \n87.7 \n86.5 \n83.5 \n6.2 \nSource Val + Target \n81.2 \n75.3 \n86.7 \n79.7 \n69.2 \n89.7 \n85.2 \n79.6 \n73.9 \n92.9 \n87.6 \n86.4 \n82.3 \n6.8 \nTarget \n77.4 \n70.6 \n84.7 \n72.9 \n66.2 \n88.3 \n82.0 \n75.7 \n65.0 \n92.1 \n87.2 \n85.4 \n78.9 \n8.6 \n\nClassAMI \n\nSource + Target Features \n46.2 \n25.8 \n24.6 \n88.7 \n2.6 \n91.7 \n90.5 \n77.5 \n89.6 \n72.4 \n82.9 \n54.8 \n62.3 29.6 \nSource + Target Logits \n61.0 \n39.6 \n50.7 \n91.9 \n25.6 \n91.0 \n92.4 \n83.0 \n91.8 \n76.7 \n90.4 \n60.5 \n71.2 22.3 \nTarget Features \n-73.2 -69.2 -75.3 -61.2 -75.3 -80.2 -67.8 -62.0 -70.0 -74.6 -75.0 -77.6 -71.8 \n5.7 \nTarget Logits \n-74.3 -67.9 -73.7 -61.2 -69.6 -80.2 -67.4 -59.1 -70.3 -72.9 -74.5 -76.3 -70.6 \n5.8 \n\nClassSS \n\nSource + Target Features \n-26.9 -48.6 -63.4 -34.9 -13.5 -39.3 -29.2 \n-9.1 -33.3 -29.0 -17.6 -28.0 -31.1 14.3 \nSource + Target Logits \n-27.6 -49.8 -59.1 -32.4 -20.6 -36.7 -24.2 -10.3 -31.8 -23.0 -21.2 -29.3 -30.5 12.7 \nTarget Features \n-37.0 -50.4 -66.3 -37.9 -11.5 -46.0 -46.6 -27.2 -42.8 -37.6 -31.0 -43.1 -39.8 12.8 \nTarget Logits \n-40.0 -51.5 -61.2 -37.0 -16.8 -45.4 -44.4 -31.2 -42.9 -35.4 -30.1 -42.3 -39.8 10.8 \n\nDEV \n\nFeatures \n61.1 \n37.5 \n21.0 \n49.5 \n53.6 \n2.2 \n52.2 \n81.7 \n85.3 \n20.6 \n12.4 \n3.2 \n40.0 27.3 \nLogits \n63.5 \n60.4 -11.8 \n29.0 \n52.5 \n3.1 \n32.7 \n46.2 \n13.6 \n18.9 \n36.2 \n16.4 \n30.1 22.2 \nPreds \n71.4 \n77.0 \n25.3 \n52.0 \n60.6 \n67.1 \n36.2 \n89.4 \n96.7 \n34.9 \n86.1 \n85.5 \n65.2 22.7 \n\nDEVN \n\nFeatures, max normalization \n95.1 \n98.9 \n98.2 \n98.3 \n98.4 \n90.9 \n97.4 \n98.5 \n94.1 \n93.8 \n95.1 \n95.6 \n96.2 \n2.4 \nLogits, max normalization \n95.2 \n98.9 \n98.3 \n98.4 \n98.6 \n90.8 \n97.6 \n98.6 \n94.6 \n93.8 \n95.1 \n95.7 \n96.3 \n2.4 \nPreds, max normalization \n95.2 \n98.6 \n97.3 \n98.2 \n96.3 \n89.0 \n97.1 \n98.9 \n95.3 \n83.9 \n93.7 \n93.0 \n94.7 \n4.2 \n\nEntropy \n\nSource Train \n78.9 \n73.4 \n87.9 \n74.5 \n70.6 \n87.7 \n82.4 \n76.8 \n68.0 \n92.3 \n86.8 \n86.1 \n80.4 \n7.5 \nSource Train + Target \n73.1 \n66.1 \n83.4 \n65.5 \n64.0 \n84.0 \n77.8 \n71.3 \n59.8 \n90.5 \n84.0 \n82.2 \n75.1 \n9.4 \nSource Val \n71.8 \n65.3 \n82.1 \n65.5 \n61.2 \n83.7 \n79.4 \n72.7 \n64.5 \n89.9 \n82.0 \n80.7 \n74.9 \n8.9 \nSource Val + Target \n67.6 \n60.7 \n80.0 \n58.9 \n57.8 \n80.9 \n75.8 \n68.5 \n57.0 \n89.0 \n81.0 \n78.6 \n71.3 10.5 \nTarget \n62.4 \n54.9 \n76.9 \n49.7 \n54.2 \n76.0 \n70.3 \n62.6 \n46.5 \n87.4 \n79.2 \n74.8 \n66.2 12.5 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-86.4 -85.1 -93.4 -90.2 -82.6 -81.6 -88.0 -80.0 -78.3 -91.4 -82.9 -87.2 -85.6 \n4.5 \nFeatures, \u03c4 = 0.1 \n-82.4 -76.9 -92.3 -85.6 -69.8 -79.2 -84.3 -74.5 -75.0 -89.7 -80.8 -82.4 -81.1 \n6.2 \nFeatures, \u03c4 = 0.5 \n-79.0 -76.3 -86.2 -80.7 -70.3 -73.8 -78.7 -70.4 -74.3 -83.6 -79.2 -80.6 -77.8 \n4.7 \nLogits, \u03c4 = 0.05 \n-93.8 -95.5 -95.7 -95.6 -93.6 -82.9 -91.7 -92.3 -83.1 -94.2 -91.7 -92.8 -91.9 \n4.2 \nLogits, \u03c4 = 0.1 \n-93.6 -95.1 -95.0 -95.0 -92.7 -82.0 -91.2 -92.0 -82.2 -94.3 -91.1 -92.4 -91.4 \n4.4 \nLogits, \u03c4 = 0.5 \n-93.8 -93.0 -93.7 -93.4 -90.6 -82.9 -89.8 -91.1 -81.4 -92.4 -90.5 -91.5 -90.3 \n3.9 \nPreds, \u03c4 = 0.05 \n-93.9 -93.3 -93.2 -92.6 -91.8 -83.8 -89.8 -91.3 -83.6 -92.4 -90.6 -91.9 -90.7 \n3.3 \nPreds, \u03c4 = 0.1 \n-94.0 -94.0 -94.3 -93.6 -92.8 -83.7 -90.7 -92.2 -83.2 -93.1 -90.6 -92.3 -91.2 \n3.7 \nPreds, \u03c4 = 0.5 \n-94.6 -93.2 -89.9 -90.9 -80.7 -84.2 -87.1 -90.8 -83.7 -90.7 -91.6 -91.9 -89.1 \n4.1 \n\n\n\nTable 26 :\n26The weighted Spearman correlation of each validator/task pair for CDAN.CP \nCR \nCS \nPC \nPR \nPS \nRC \nRP \nRS \nSC \nSP \nSR Mean \nStd \n\nAccuracy \nSource Train \n83.6 \n83.8 \n83.4 \n84.4 \n50.5 \n61.7 \n72.1 \n80.4 \n62.0 \n81.8 \n83.9 \n78.8 \n75.5 10.9 \nSource Val \n65.2 \n60.4 \n74.0 \n69.6 \n96.9 \n42.1 \n68.9 \n85.6 \n49.8 \n60.5 \n54.2 \n52.2 \n65.0 14.8 \n\nBNM \n\nSource Train \n78.1 \n80.6 \n75.2 \n86.8 \n46.8 \n72.5 \n80.4 \n73.9 \n76.5 \n80.7 \n77.6 \n81.9 \n75.9 \n9.5 \nSource Train + Target \n82.4 \n87.1 \n82.5 \n87.2 \n49.1 \n74.5 \n86.0 \n63.3 \n89.2 \n81.9 \n85.1 \n91.1 \n79.9 11.7 \nSource Val \n78.3 \n79.6 \n75.3 \n87.1 \n60.6 \n74.0 \n77.1 \n73.7 \n74.2 \n79.5 \n74.0 \n82.1 \n76.3 \n6.1 \nSource Val + Target \n81.6 \n84.4 \n80.9 \n88.3 \n55.8 \n76.4 \n85.0 \n64.9 \n87.4 \n81.1 \n82.0 \n87.5 \n79.6 \n9.4 \nTarget \n82.9 \n86.4 \n83.2 \n86.1 \n48.0 \n73.6 \n85.4 \n61.4 \n89.4 \n81.5 \n84.9 \n93.1 \n79.6 12.3 \n\nClassAMI \n\nSource + Target Features \n84.0 \n80.6 \n82.0 \n87.4 \n30.8 \n76.9 \n93.4 \n80.1 \n91.2 \n81.4 \n89.2 \n92.9 \n80.8 15.9 \nSource + Target Logits \n84.8 \n80.5 \n82.1 \n86.6 \n29.8 \n73.6 \n92.8 \n78.9 \n89.1 \n81.1 \n89.1 \n93.6 \n80.2 16.2 \nTarget Features \n-46.4 \n-2.3 -17.8 -45.5 -58.6 -67.3 -23.6 -23.4 -21.8 -52.1 -26.1 -56.0 -36.7 19.2 \nTarget Logits \n-45.6 \n13.2 \n6.8 -44.8 -46.9 -50.4 -22.0 -23.4 -13.8 -51.8 -26.1 -39.0 -28.6 21.0 \n\nClassSS \n\nSource + Target Features \n35.7 \n55.2 \n52.8 \n64.2 \n19.3 \n36.0 \n75.2 \n61.7 \n77.2 \n39.4 \n49.5 \n49.0 \n51.3 16.3 \nSource + Target Logits \n38.8 \n35.5 \n18.5 \n67.2 -12.7 -24.5 \n73.6 \n36.6 \n66.6 \n24.6 \n7.4 \n-3.9 \n27.3 30.7 \nTarget Features \n31.3 \n51.0 \n43.5 \n28.6 \n14.4 \n10.0 \n47.3 \n46.3 \n64.5 \n12.9 \n36.2 \n43.6 \n35.8 16.2 \nTarget Logits \n31.4 \n29.0 \n10.3 \n31.8 -17.0 -35.8 \n46.1 \n46.9 \n51.5 \n13.6 \n37.5 \n-7.8 \n19.8 26.5 \n\nDEV \n\nFeatures \n40.0 \n1.1 \n30.9 \n21.5 \n36.0 \n8.7 \n30.3 \n42.7 \n31.0 \n-1.1 \n20.5 \n14.5 \n23.0 14.1 \nLogits \n38.7 \n14.1 \n35.4 \n14.2 \n31.9 \n24.2 \n35.3 \n48.3 \n33.6 \n44.1 \n28.8 \n43.1 \n32.6 10.4 \nPreds \n65.1 \n59.9 \n68.8 \n70.6 \n40.5 \n70.0 \n58.9 \n65.3 \n78.0 \n51.8 \n75.5 \n72.5 \n64.7 10.2 \n\nDEVN \n\nFeatures, max normalization \n47.2 \n31.4 \n50.0 \n62.3 \n94.8 \n26.0 \n63.1 \n79.2 \n37.9 \n48.2 \n36.0 \n35.2 \n50.9 19.7 \nLogits, max normalization \n48.1 \n33.1 \n51.5 \n62.0 \n95.1 \n26.0 \n63.2 \n80.2 \n38.2 \n48.0 \n35.9 \n34.9 \n51.4 19.7 \nPreds, max normalization \n44.3 \n37.1 \n54.4 \n61.2 \n93.4 \n21.7 \n59.4 \n73.9 \n35.7 \n49.7 \n33.9 \n30.6 \n49.6 19.4 \n\nEntropy \n\nSource Train \n80.6 \n85.6 \n78.4 \n86.7 \n46.1 \n72.3 \n78.4 \n72.1 \n75.8 \n81.0 \n78.7 \n70.4 \n75.5 10.1 \nSource Train + Target \n74.6 \n83.3 \n72.3 \n84.9 \n44.2 \n66.6 \n81.0 \n65.0 \n84.1 \n79.6 \n81.7 \n75.9 \n74.4 11.1 \nSource Val \n73.4 \n78.8 \n72.7 \n85.0 \n48.3 \n68.0 \n75.7 \n70.1 \n74.6 \n77.7 \n74.0 \n69.4 \n72.3 \n8.5 \nSource Val + Target \n73.3 \n81.8 \n71.7 \n84.3 \n44.9 \n64.1 \n79.8 \n65.0 \n83.0 \n78.4 \n79.5 \n74.1 \n73.3 10.6 \nTarget \n71.7 \n81.3 \n70.8 \n82.8 \n42.1 \n61.6 \n78.8 \n63.3 \n84.2 \n78.6 \n81.4 \n76.1 \n72.7 11.6 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-96.9 -97.4 -98.5 -97.0 -96.7 -96.2 -97.5 -95.3 -97.0 -96.0 -96.6 -96.4 -96.8 \n0.8 \nFeatures, \u03c4 = 0.1 \n-93.6 -92.2 -96.1 -95.0 -95.4 -95.3 -91.9 -92.1 -91.7 -95.8 -94.9 -94.3 -94.0 \n1.6 \nFeatures, \u03c4 = 0.5 \n-70.2 -63.9 -55.0 -61.0 -78.5 -74.2 -30.9 -38.5 -27.6 -66.2 -49.8 -74.4 -57.5 16.6 \nLogits, \u03c4 = 0.05 \n-95.1 -89.6 -95.1 -95.8 -87.0 -93.0 -94.8 -91.8 -95.0 -94.6 -94.6 -92.5 -93.3 \n2.5 \nLogits, \u03c4 = 0.1 \n-93.3 -89.0 -93.9 -95.3 -86.6 -92.4 -91.7 -90.4 -92.3 -94.3 -92.8 -91.6 -92.0 \n2.3 \nLogits, \u03c4 = 0.5 \n-75.8 -52.0 -76.8 -86.0 -79.6 -86.1 -56.4 -46.8 -54.2 -81.5 -70.3 -75.1 -70.1 13.4 \nPreds, \u03c4 = 0.05 \n-91.1 -72.7 -87.9 -89.7 -86.2 -92.6 -84.0 -74.3 -85.2 -86.4 -87.6 -86.9 -85.4 \n5.8 \nPreds, \u03c4 = 0.1 \n-97.3 -98.5 -98.2 -97.4 -92.4 -94.5 -97.5 -93.1 -98.1 -95.9 -97.8 -96.5 -96.4 \n2.0 \nPreds, \u03c4 = 0.5 \n-67.6 -56.8 -42.3 -65.7 -70.0 -73.2 -50.3 -69.6 -40.1 -78.5 -65.6 -76.6 -63.0 12.3 \n\n\n\nTable 27 :\n27The weighted Spearman correlation of each validator/task pair for DANN.CP \nCR \nCS \nPC \nPR \nPS \nRC \nRP \nRS \nSC \nSP \nSR Mean \nStd \n\nAccuracy \nSource Train \n76.9 \n87.2 \n76.1 \n60.6 \n67.4 \n66.9 \n74.3 \n68.7 \n58.4 \n56.5 \n86.3 \n82.7 \n71.8 10.0 \nSource Val \n53.5 \n64.6 \n74.1 \n63.1 \n94.7 \n57.6 \n68.7 \n91.1 \n50.4 \n55.3 \n52.6 \n40.7 \n63.9 15.5 \n\nBNM \n\nSource Train \n66.3 \n90.2 \n71.3 \n68.5 \n66.7 \n77.1 \n86.2 \n69.3 \n76.7 \n56.4 \n89.4 \n87.4 \n75.5 10.4 \nSource Train + Target \n73.5 \n92.7 \n83.6 \n76.6 \n68.2 \n84.5 \n93.3 \n63.2 \n87.9 \n66.2 \n88.6 \n94.3 \n81.1 10.6 \nSource Val \n63.0 \n85.0 \n76.7 \n76.6 \n85.4 \n76.3 \n87.9 \n75.2 \n74.5 \n69.4 \n78.8 \n82.1 \n77.6 \n6.7 \nSource Val + Target \n69.9 \n90.0 \n82.0 \n79.2 \n80.8 \n84.6 \n93.1 \n66.4 \n86.1 \n69.4 \n83.8 \n89.4 \n81.2 \n8.3 \nTarget \n72.8 \n89.3 \n84.7 \n76.9 \n67.3 \n85.8 \n93.2 \n62.0 \n88.0 \n68.0 \n87.1 \n93.0 \n80.7 10.4 \n\nClassAMI \n\nSource + Target Features \n22.0 \n7.8 \n45.3 \n66.9 \n30.5 \n43.5 \n63.2 \n78.0 \n44.9 \n70.9 \n55.8 \n61.2 \n49.2 20.1 \nSource + Target Logits \n29.1 \n14.8 \n52.3 \n66.3 \n24.9 \n47.2 \n65.3 \n79.5 \n48.4 \n71.1 \n57.4 \n63.4 \n51.7 19.0 \nTarget Features \n-59.5 -61.3 -57.0 -51.0 -48.2 -56.4 -43.5 -42.1 -55.1 -47.5 -66.3 -58.7 -53.9 \n7.1 \nTarget Logits \n-56.8 -54.7 -46.1 -49.0 -46.0 -50.2 -35.2 -39.5 -46.5 -46.8 -64.5 -52.2 -48.9 \n7.4 \n\nClassSS \n\nSource + Target Features \n40.1 \n23.3 \n19.0 \n22.2 \n5.6 \n36.9 \n61.1 \n43.2 \n51.3 \n16.9 \n24.2 \n21.2 \n30.4 15.4 \nSource + Target Logits \n44.2 \n13.5 \n1.5 \n28.7 \n1.5 \n28.2 \n57.9 \n40.3 \n33.0 \n9.3 \n8.5 \n8.9 \n23.0 17.6 \nTarget Features \n24.6 \n9.5 \n5.1 \n7.0 \n-0.3 \n19.4 \n40.9 \n35.2 \n28.5 \n-0.2 \n2.1 \n11.1 \n15.2 13.6 \nTarget Logits \n26.8 \n1.6 -11.2 \n8.6 \n-5.1 \n8.5 \n39.5 \n32.5 \n10.5 \n0.8 \n3.1 \n-1.0 \n9.6 14.9 \n\nDEV \n\nFeatures \n30.5 \n9.5 \n23.7 \n20.6 \n38.4 \n14.5 \n32.8 \n55.0 \n29.2 \n-5.3 \n14.2 \n21.5 \n23.7 14.7 \nLogits \n27.8 \n20.5 \n21.1 \n14.5 \n44.9 \n33.3 \n29.8 \n55.9 \n27.1 \n32.1 \n27.7 \n42.0 \n31.4 11.0 \nPreds \n42.6 \n55.1 \n51.0 \n49.6 \n46.7 \n72.5 \n68.8 \n60.3 \n73.2 \n28.7 \n68.8 \n71.4 \n57.4 13.6 \n\nDEVN \n\nFeatures, max normalization \n26.0 \n43.1 \n53.5 \n50.6 \n91.7 \n47.8 \n63.5 \n88.1 \n40.6 \n37.2 \n36.1 \n22.9 \n50.1 20.8 \nLogits, max normalization \n25.7 \n44.5 \n51.8 \n50.5 \n91.7 \n47.8 \n63.4 \n87.6 \n40.9 \n35.5 \n36.3 \n22.9 \n49.9 20.8 \nPreds, max normalization \n18.6 \n45.0 \n50.3 \n44.0 \n92.3 \n44.6 \n59.8 \n83.4 \n37.9 \n32.4 \n29.8 \n20.0 \n46.5 21.8 \n\nEntropy \n\nSource Train \n33.2 \n37.5 \n47.8 \n56.8 \n47.2 \n54.4 \n65.6 \n50.6 \n67.6 \n38.0 \n62.3 \n56.7 \n51.5 10.7 \nSource Train + Target \n20.3 \n30.6 \n48.8 \n59.6 \n40.4 \n53.3 \n68.2 \n51.5 \n70.0 \n47.1 \n53.5 \n59.9 \n50.3 13.9 \nSource Val \n22.5 \n32.3 \n49.8 \n59.5 \n53.8 \n54.6 \n67.4 \n54.6 \n66.2 \n46.5 \n48.4 \n52.7 \n50.7 12.2 \nSource Val + Target \n19.7 \n31.4 \n48.9 \n59.1 \n45.4 \n53.6 \n68.0 \n52.3 \n69.0 \n47.2 \n50.1 \n58.7 \n50.3 13.4 \nTarget \n17.9 \n28.3 \n47.4 \n58.6 \n36.1 \n52.4 \n67.5 \n50.8 \n68.9 \n47.1 \n49.2 \n57.8 \n48.5 14.4 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-95.9 -97.2 -97.0 -96.0 -96.6 -96.1 -97.1 -95.2 -95.5 -95.1 -94.1 -96.0 -96.0 \n0.9 \nFeatures, \u03c4 = 0.1 \n-93.9 -93.7 -95.6 -94.1 -93.4 -94.4 -92.2 -93.2 -90.9 -94.8 -93.4 -94.4 -93.7 \n1.2 \nFeatures, \u03c4 = 0.5 \n-74.9 -80.8 -76.9 -74.2 -70.5 -64.1 -48.6 -60.0 -53.8 -71.0 -74.7 -73.4 -68.6 \n9.5 \nLogits, \u03c4 = 0.05 \n-91.2 -92.3 -94.4 -93.2 -91.1 -93.5 -94.2 -91.7 -90.3 -92.2 -93.1 -93.6 -92.6 \n1.3 \nLogits, \u03c4 = 0.1 \n-89.9 -90.2 -93.4 -92.3 -88.2 -91.2 -91.6 -89.8 -85.0 -91.5 -92.3 -92.1 -90.6 \n2.2 \nLogits, \u03c4 = 0.5 \n-73.7 -71.6 -83.3 -82.5 -73.0 -79.8 -66.9 -65.0 -64.2 -82.4 -79.3 -73.2 -74.6 \n6.6 \nPreds, \u03c4 = 0.05 \n-90.3 -86.6 -91.9 -88.3 -87.8 -93.9 -85.3 -75.2 -88.8 -82.5 -89.4 -89.3 -87.4 \n4.7 \nPreds, \u03c4 = 0.1 \n-96.1 -97.9 -96.6 -96.3 -95.3 -95.7 -97.8 -92.6 -96.3 -93.9 -94.8 -96.4 -95.8 \n1.4 \nPreds, \u03c4 = 0.5 \n-66.2 -69.0 -60.4 -64.6 -55.5 -65.1 -55.9 -77.7 -54.4 -77.0 -80.2 -73.6 -66.6 \n8.7 \n\n\n\nTable 28 :\n28The weighted Spearman correlation of each validator/task pair for GVB.CP \nCR \nCS \nPC \nPR \nPS \nRC \nRP \nRS \nSC \nSP \nSR Mean \nStd \n\nAccuracy \nSource Train \n79.2 \n76.0 \n81.2 \n72.1 \n52.8 \n56.4 \n52.1 \n78.4 \n49.8 \n82.1 \n78.4 \n56.6 \n67.9 12.5 \nSource Val \n60.7 \n63.1 \n77.9 \n61.2 \n93.6 \n50.7 \n51.1 \n82.2 \n44.6 \n62.8 \n59.6 \n38.6 \n62.2 15.2 \n\nBNM \n\nSource Train \n68.7 \n85.2 \n69.6 \n71.6 \n51.1 \n70.1 \n58.5 \n72.6 \n54.0 \n73.8 \n63.3 \n65.6 \n67.0 \n8.9 \nSource Train + Target \n72.8 \n91.6 \n78.5 \n70.8 \n46.7 \n80.0 \n71.2 \n58.0 \n69.8 \n72.6 \n69.5 \n90.4 \n72.7 11.8 \nSource Val \n65.0 \n89.0 \n63.0 \n61.5 \n56.4 \n65.5 \n46.3 \n61.3 \n46.7 \n62.0 \n53.9 \n86.2 \n63.1 12.6 \nSource Val + Target \n69.3 \n90.7 \n72.3 \n67.7 \n51.5 \n75.7 \n67.6 \n57.2 \n67.0 \n68.1 \n63.8 \n89.2 \n70.0 10.8 \nTarget \n71.8 \n91.0 \n79.5 \n68.5 \n43.4 \n79.5 \n71.0 \n55.6 \n69.8 \n70.6 \n69.1 \n91.1 \n71.7 12.7 \n\nClassAMI \n\nSource + Target Features \n85.1 \n86.2 \n89.8 \n82.6 \n36.5 \n91.7 \n87.0 \n83.1 \n91.9 \n83.2 \n91.7 \n89.7 \n83.2 14.5 \nSource + Target Logits \n86.9 \n87.2 \n90.4 \n82.6 \n34.3 \n92.4 \n87.5 \n83.7 \n92.9 \n83.2 \n92.1 \n90.1 \n83.6 15.3 \nTarget Features \n-29.7 -39.8 -41.9 -44.3 -53.7 -57.2 -52.0 -33.6 -32.1 -38.0 -39.2 -61.6 -43.6 \n9.9 \nTarget Logits \n-28.9 -31.1 -35.1 -44.3 -49.1 -52.5 -51.5 -32.9 -24.6 -37.6 -38.9 -54.5 -40.1 \n9.7 \n\nClassSS \n\nSource + Target Features \n56.6 \n83.5 \n30.9 \n58.7 \n42.9 \n57.8 \n69.5 \n68.9 \n68.6 \n69.1 \n58.9 \n68.8 \n61.2 13.2 \nSource + Target Logits \n60.6 \n60.9 \n24.3 \n63.4 \n31.5 \n47.7 \n63.7 \n58.1 \n63.8 \n61.4 \n42.0 \n37.2 \n51.2 13.5 \nTarget Features \n45.9 \n78.6 \n13.4 \n38.2 \n40.3 \n49.6 \n59.1 \n53.1 \n56.1 \n56.5 \n50.9 \n70.3 \n51.0 15.8 \nTarget Logits \n47.0 \n55.7 \n3.0 \n40.2 \n28.8 \n32.3 \n61.5 \n56.0 \n47.3 \n59.7 \n50.2 \n34.0 \n43.0 15.9 \n\nDEV \n\nFeatures \n27.6 \n26.2 \n42.5 \n10.8 \n36.5 \n37.5 \n15.0 \n49.0 \n30.9 \n15.9 \n31.8 \n22.9 \n28.9 11.1 \nLogits \n29.8 \n36.3 \n33.0 \n14.5 \n40.5 \n44.0 \n19.6 \n55.3 \n24.2 \n39.8 \n37.1 \n35.9 \n34.2 10.6 \nPreds \n72.4 \n79.4 \n85.4 \n72.8 \n40.4 \n88.1 \n69.9 \n68.9 \n74.8 \n63.1 \n83.4 \n55.6 \n71.2 12.9 \n\nDEVN \n\nFeatures, max normalization \n35.4 \n41.6 \n63.0 \n50.7 \n90.6 \n38.6 \n43.5 \n79.2 \n35.2 \n48.3 \n43.9 \n14.5 \n48.7 19.6 \nLogits, max normalization \n36.8 \n42.3 \n63.6 \n50.1 \n90.8 \n38.6 \n43.6 \n78.9 \n35.5 \n46.9 \n43.7 \n15.1 \n48.8 19.5 \nPreds, max normalization \n40.1 \n52.3 \n57.1 \n40.8 \n83.5 \n31.7 \n33.8 \n74.9 \n29.9 \n47.0 \n37.9 \n5.3 \n44.5 20.0 \n\nEntropy \n\nSource Train \n65.1 \n83.3 \n65.5 \n69.9 \n50.3 \n68.3 \n54.0 \n68.1 \n49.8 \n71.7 \n60.5 \n73.4 \n65.0 \n9.5 \nSource Train + Target \n64.1 \n87.5 \n70.2 \n66.2 \n38.8 \n71.4 \n62.6 \n54.7 \n61.0 \n66.7 \n62.7 \n86.8 \n66.1 12.4 \nSource Val \n56.6 \n83.9 \n57.8 \n57.9 \n41.7 \n64.6 \n42.1 \n54.2 \n42.7 \n56.2 \n49.4 \n83.9 \n57.6 13.6 \nSource Val + Target \n60.9 \n86.5 \n66.6 \n62.2 \n38.5 \n69.1 \n58.4 \n52.3 \n57.7 \n61.6 \n58.7 \n85.7 \n63.2 12.6 \nTarget \n62.7 \n86.8 \n70.9 \n63.6 \n36.2 \n71.2 \n63.0 \n51.5 \n61.0 \n63.2 \n61.8 \n86.2 \n64.8 13.1 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-95.6 -95.3 -97.6 -96.5 -96.6 -96.4 -95.0 -94.9 -93.7 -96.3 -97.3 -97.3 -96.0 \n1.1 \nFeatures, \u03c4 = 0.1 \n-91.0 -88.3 -95.2 -92.7 -90.8 -93.6 -92.2 -89.4 -91.2 -93.9 -94.8 -94.0 -92.3 \n2.1 \nFeatures, \u03c4 = 0.5 \n-72.9 -67.9 -69.0 -69.0 -75.2 -69.3 -67.4 -54.5 -70.6 -66.4 -68.7 -80.7 -69.3 \n5.9 \nLogits, \u03c4 = 0.05 \n-94.0 -95.3 -96.4 -95.3 -88.5 -96.4 -95.6 -94.6 -95.6 -95.2 -95.4 -96.3 -94.9 \n2.1 \nLogits, \u03c4 = 0.1 \n-95.6 -95.8 -96.7 -95.0 -89.7 -95.9 -95.3 -94.9 -94.8 -95.2 -95.5 -96.6 -95.1 \n1.7 \nLogits, \u03c4 = 0.5 \n-74.8 -62.5 -88.1 -82.4 -72.3 -89.6 -75.8 -56.2 -81.2 -71.8 -74.1 -79.8 -75.7 \n9.2 \nPreds, \u03c4 = 0.05 \n-83.5 -82.7 -89.9 -86.4 -86.0 -94.4 -86.6 -78.4 -86.4 -82.7 -88.5 -88.8 -86.2 \n3.9 \nPreds, \u03c4 = 0.1 \n-94.5 -97.2 -96.6 -94.8 -92.2 -95.9 -94.1 -92.0 -92.9 -94.3 -95.6 -97.2 -94.8 \n1.7 \nPreds, \u03c4 = 0.5 \n-77.1 -74.7 -74.0 -70.8 -69.6 -78.6 -69.6 -74.8 -60.7 -79.5 -76.1 -81.8 -73.9 \n5.4 \n\n\n\nTable 29 :\n29The weighted Spearman correlation of each validator/task pair for IM.CP \nCR \nCS \nPC \nPR \nPS \nRC \nRP \nRS \nSC \nSP \nSR Mean \nStd \n\nAccuracy \nSource Train \n70.4 \n78.7 \n82.0 \n57.8 \n76.0 \n29.4 \n53.2 \n85.2 \n62.1 \n77.2 \n71.9 \n69.9 \n67.8 14.8 \nSource Val \n57.6 \n62.8 \n73.2 \n50.4 \n62.5 \n38.3 \n52.6 \n85.0 \n62.7 \n54.8 \n52.6 \n41.2 \n57.8 12.3 \n\nBNM \n\nSource Train \n72.3 \n78.1 \n78.7 \n76.2 \n77.3 \n48.4 \n74.0 \n76.3 \n85.0 \n69.0 \n73.6 \n67.6 \n73.0 \n8.6 \nSource Train + Target \n75.8 \n87.4 \n83.5 \n70.5 \n78.7 \n47.5 \n60.6 \n55.9 \n78.9 \n61.9 \n71.3 \n75.4 \n70.6 11.4 \nSource Val \n75.6 \n82.5 \n76.4 \n67.3 \n72.3 \n45.2 \n65.4 \n64.4 \n81.5 \n58.9 \n64.7 \n66.5 \n68.4 \n9.9 \nSource Val + Target \n76.0 \n85.6 \n80.6 \n67.9 \n73.6 \n45.6 \n60.8 \n56.5 \n79.0 \n59.8 \n67.4 \n70.7 \n68.6 11.0 \nTarget \n75.3 \n87.9 \n82.7 \n68.1 \n75.6 \n45.8 \n58.5 \n53.3 \n77.9 \n60.6 \n69.8 \n77.6 \n69.4 12.1 \n\nClassAMI \n\nSource + Target Features \n94.3 \n83.6 \n95.4 \n91.0 \n95.6 \n70.7 \n92.8 \n91.0 \n95.8 \n90.3 \n92.6 \n94.9 \n90.7 \n6.8 \nSource + Target Logits \n92.5 \n87.6 \n95.6 \n87.8 \n95.3 \n64.1 \n92.5 \n90.3 \n95.2 \n87.7 \n90.6 \n94.3 \n89.5 \n8.2 \nTarget Features \n-36.8 -61.8 -42.8 -42.0 -33.2 -45.7 -38.7 -44.3 -19.1 -29.2 -25.4 -43.7 -38.5 10.6 \nTarget Logits \n-37.9 -23.4 -19.3 -42.5 \n19.1 -18.7 -40.4 -44.7 \n7.3 -30.1 -27.7 \n28.3 -19.2 23.5 \n\nClassSS \n\nSource + Target Features \n-16.1 \n18.3 \n0.3 -16.0 \n24.2 \n5.3 -15.5 \n54.5 \n42.6 \n6.7 -16.3 -15.3 \n6.1 23.4 \nSource + Target Logits \n-13.7 -42.4 -26.5 \n-7.5 -21.6 -29.1 -24.6 -11.7 \n15.0 \n-4.3 -35.8 -48.8 -20.9 17.0 \nTarget Features \n-42.5 \n0.9 -23.8 -37.2 \n11.8 -35.0 -36.6 \n17.6 -16.3 -41.3 -40.7 -33.2 -23.0 20.7 \nTarget Logits \n-41.3 -47.8 -39.9 -32.6 -29.4 -48.1 -34.3 \n19.4 -30.3 -38.2 -40.5 -56.6 -35.0 18.1 \n\nDEV \n\nFeatures \n32.5 \n25.6 \n40.4 \n30.2 \n33.3 \n36.1 \n18.3 \n40.9 \n38.2 \n5.1 \n24.7 \n-9.8 \n26.3 14.6 \nLogits \n15.9 \n-4.2 \n17.0 \n-5.1 \n13.5 \n19.7 \n21.8 \n47.7 \n44.6 \n38.6 \n28.8 \n34.7 \n22.7 16.3 \nPreds \n43.0 \n37.2 \n65.8 \n57.5 \n68.0 \n58.9 \n43.3 \n73.8 \n70.2 \n58.9 \n66.4 \n47.3 \n57.5 11.6 \n\nDEVN \n\nFeatures, max normalization \n54.3 \n55.0 \n69.4 \n54.5 \n52.6 \n35.0 \n57.4 \n81.8 \n66.2 \n55.0 \n56.3 \n46.1 \n57.0 11.2 \nLogits, max normalization \n57.9 \n61.3 \n71.1 \n59.3 \n56.5 \n36.9 \n57.8 \n81.8 \n66.6 \n54.1 \n56.3 \n39.6 \n58.3 11.6 \nPreds, max normalization \n50.1 \n55.5 \n69.0 \n49.0 \n72.8 \n30.2 \n51.1 \n77.2 \n64.1 \n46.3 \n49.3 \n27.3 \n53.5 14.8 \n\nEntropy \n\nSource Train \n54.2 \n79.4 \n78.4 \n56.8 \n78.6 \n30.0 \n53.9 \n73.9 \n80.3 \n71.9 \n32.7 \n33.7 \n60.3 18.8 \nSource Train + Target \n38.1 \n73.1 \n66.6 \n42.9 \n74.0 \n13.6 \n36.3 \n56.8 \n65.9 \n56.5 \n5.7 \n13.6 \n45.3 23.2 \nSource Val \n39.0 \n69.8 \n65.5 \n33.6 \n67.9 \n11.7 \n38.5 \n60.4 \n71.1 \n47.9 \n-0.7 \n5.6 \n42.5 24.8 \nSource Val + Target \n31.8 \n68.5 \n60.4 \n35.3 \n68.6 \n8.5 \n33.1 \n54.9 \n62.7 \n49.7 \n-3.4 \n4.7 \n39.6 24.4 \nTarget \n20.9 \n65.1 \n51.0 \n33.8 \n68.8 \n3.2 \n29.2 \n52.3 \n51.6 \n50.3 \n-9.8 \n0.6 \n34.8 25.1 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-86.1 -94.7 -89.8 -81.1 -92.0 -79.6 -88.3 -93.1 -86.5 -88.0 -84.5 -87.0 -87.6 \n4.3 \nFeatures, \u03c4 = 0.1 \n-85.9 -94.1 -89.0 -80.8 -89.5 -79.3 -87.8 -91.2 -86.5 -87.3 -84.2 -87.0 -86.9 \n3.9 \nFeatures, \u03c4 = 0.5 \n-85.2 -92.0 -86.6 -79.9 -78.9 -78.1 -85.0 -78.4 -84.2 -83.2 -82.4 -86.1 -83.3 \n3.9 \nLogits, \u03c4 = 0.05 \n-84.7 -93.2 -89.4 -79.3 -91.5 -74.9 -85.6 -91.4 -85.6 -86.1 -83.5 -85.2 -85.9 \n5.0 \nLogits, \u03c4 = 0.1 \n-84.7 -93.1 -89.6 -79.3 -91.4 -75.0 -86.3 -91.7 -85.8 -86.1 -83.6 -85.2 -86.0 \n5.0 \nLogits, \u03c4 = 0.5 \n-79.5 -86.8 -84.7 -75.4 -74.7 -74.4 -81.8 -62.1 -79.9 -74.8 -78.4 -83.0 -78.0 \n6.2 \nPreds, \u03c4 = 0.05 \n-82.4 -91.3 -89.4 -77.6 -85.1 -75.6 -83.0 -80.1 -86.7 -79.4 -81.9 -86.7 -83.3 \n4.5 \nPreds, \u03c4 = 0.1 \n-84.3 -93.6 -89.8 -79.5 -94.0 -75.2 -86.6 -91.4 -87.3 -85.5 -83.5 -87.5 -86.5 \n5.3 \nPreds, \u03c4 = 0.5 \n-67.4 -71.1 -68.0 -68.0 -60.2 -63.9 -72.0 -70.6 -58.2 -69.5 -68.5 -69.3 -67.2 \n4.1 \n\n\n\nTable 30 :\n30The weighted Spearman correlation of each validator/task pair for MCC.CP \nCR \nCS \nPC \nPR \nPS \nRC \nRP \nRS \nSC \nSP \nSR Mean \nStd \n\nAccuracy \nSource Train \n59.3 \n76.0 \n59.6 \n43.9 \n59.1 \n58.8 \n64.0 \n77.0 \n44.7 \n66.3 \n46.8 \n53.9 \n59.1 10.4 \nSource Val \n49.0 \n67.4 \n55.0 \n36.4 \n54.6 \n53.9 \n54.1 \n71.5 \n49.8 \n53.0 \n39.1 \n38.9 \n51.9 10.2 \n\nBNM \n\nSource Train \n61.3 \n81.5 \n61.0 \n59.4 \n67.0 \n82.4 \n84.4 \n82.3 \n88.2 \n83.1 \n61.7 \n78.4 \n74.2 10.6 \nSource Train + Target \n79.4 \n89.6 \n78.8 \n60.5 \n87.3 \n83.0 \n70.7 \n66.6 \n86.8 \n84.8 \n73.6 \n90.0 \n79.3 \n9.2 \nSource Val \n78.1 \n88.4 \n70.8 \n63.1 \n90.6 \n81.3 \n82.1 \n80.7 \n90.3 \n84.6 \n70.5 \n87.6 \n80.7 \n8.3 \nSource Val + Target \n79.3 \n89.3 \n76.2 \n60.7 \n90.4 \n81.0 \n72.0 \n68.8 \n87.2 \n83.9 \n72.7 \n89.1 \n79.2 \n9.0 \nTarget \n78.9 \n88.6 \n78.9 \n58.5 \n88.3 \n80.1 \n68.9 \n63.5 \n85.5 \n81.6 \n72.2 \n90.1 \n77.9 \n9.8 \n\nClassAMI \n\nSource + Target Features \n95.1 \n88.3 \n94.4 \n86.3 \n92.7 \n85.6 \n83.6 \n86.5 \n89.8 \n95.3 \n91.7 \n91.2 \n90.0 \n3.8 \nSource + Target Logits \n94.8 \n91.3 \n94.4 \n85.1 \n92.3 \n87.6 \n83.6 \n87.6 \n90.4 \n94.8 \n90.8 \n92.8 \n90.5 \n3.6 \nTarget Features \n-40.3 -26.3 -25.3 -45.8 -32.7 -45.8 -21.5 -36.1 -42.6 -54.2 -37.4 -57.3 -38.8 10.7 \nTarget Logits \n-39.9 \n-1.3 \n-4.3 -46.4 \n5.5 -31.3 -21.8 -35.8 -32.4 -53.9 -38.2 -36.6 -28.0 17.9 \n\nClassSS \n\nSource + Target Features \n46.3 \n79.7 \n38.7 \n77.6 \n83.5 \n57.3 \n78.9 \n78.3 \n73.1 \n58.2 \n86.2 \n74.5 \n69.4 14.7 \nSource + Target Logits \n46.1 \n29.7 \n5.4 \n79.3 \n9.6 \n16.6 \n62.6 \n45.3 \n44.4 \n25.5 \n26.5 \n6.0 \n33.1 22.1 \nTarget Features \n35.2 \n73.6 \n16.4 \n47.9 \n80.3 \n36.1 \n52.6 \n71.7 \n67.0 \n50.1 \n74.1 \n67.7 \n56.1 18.8 \nTarget Logits \n32.4 \n25.5 -10.2 \n53.0 \n9.1 \n3.6 \n53.8 \n70.4 \n36.8 \n50.1 \n73.6 \n0.4 \n33.2 26.8 \n\nDEV \n\nFeatures \n7.4 \n17.4 \n26.9 \n-0.2 \n21.2 \n47.0 \n3.9 \n39.6 \n16.6 \n11.7 \n19.3 \n19.3 \n19.2 13.1 \nLogits \n14.4 \n11.1 \n23.3 \n-2.1 \n10.4 \n43.7 \n12.8 \n35.8 \n20.5 \n40.7 \n17.2 \n38.6 \n22.2 13.8 \nPreds \n71.4 \n62.3 \n74.6 \n54.2 \n73.9 \n89.3 \n22.0 \n77.4 \n79.5 \n55.0 \n72.3 \n57.6 \n65.8 16.7 \n\nDEVN \n\nFeatures, max normalization \n24.1 \n51.3 \n31.2 \n16.6 \n35.7 \n46.9 \n44.1 \n64.3 \n40.7 \n42.7 \n19.3 \n26.5 \n36.9 13.5 \nLogits, max normalization \n24.5 \n52.4 \n31.0 \n16.8 \n36.0 \n47.0 \n44.0 \n64.4 \n40.7 \n42.8 \n19.6 \n26.5 \n37.1 13.5 \nPreds, max normalization \n36.3 \n55.5 \n52.0 \n34.6 \n75.5 \n48.6 \n62.4 \n70.4 \n45.3 \n63.9 \n32.7 \n3.7 \n48.4 19.0 \n\nEntropy \n\nSource Train \n71.6 \n85.9 \n75.1 \n67.9 \n71.0 \n85.6 \n84.0 \n83.5 \n88.8 \n87.3 \n72.4 \n84.6 \n79.8 \n7.2 \nSource Train + Target \n80.8 \n85.9 \n79.3 \n68.8 \n89.2 \n75.4 \n77.2 \n76.2 \n85.2 \n86.3 \n76.1 \n89.9 \n80.9 \n6.2 \nSource Val \n77.9 \n85.8 \n75.2 \n66.9 \n89.2 \n73.1 \n78.6 \n79.8 \n86.4 \n83.3 \n71.1 \n88.6 \n79.7 \n6.9 \nSource Val + Target \n79.8 \n83.8 \n77.2 \n66.8 \n88.8 \n69.7 \n75.9 \n74.9 \n83.3 \n82.0 \n72.6 \n88.0 \n78.6 \n6.7 \nTarget \n79.7 \n82.1 \n77.4 \n66.4 \n88.2 \n66.3 \n74.0 \n73.1 \n80.1 \n80.6 \n73.2 \n87.0 \n77.4 \n6.7 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-90.7 -93.0 -93.7 -92.6 -91.8 -92.2 -91.1 -92.5 -92.1 -94.4 -92.3 -94.2 -92.5 \n1.1 \nFeatures, \u03c4 = 0.1 \n-90.8 -90.6 -90.9 -92.6 -85.1 -90.2 -90.3 -87.9 -91.0 -92.6 -88.5 -91.0 -90.1 \n2.0 \nFeatures, \u03c4 = 0.5 \n-81.5 -79.2 -70.0 -78.9 -62.0 -73.9 -65.5 -61.6 -74.7 -76.8 -70.6 -77.5 -72.7 \n6.5 \nLogits, \u03c4 = 0.05 \n-92.5 -94.4 -95.0 -92.0 -93.7 -92.9 -91.6 -92.3 -92.6 -95.1 -94.6 -94.2 -93.4 \n1.2 \nLogits, \u03c4 = 0.1 \n-94.6 -95.4 -96.4 -94.7 -95.4 -92.8 -93.4 -94.1 -94.2 -95.8 -95.9 -94.5 -94.8 \n1.0 \nLogits, \u03c4 = 0.5 \n-90.3 -83.9 -90.2 -92.1 -83.5 -87.9 -85.5 -69.6 -89.1 -86.4 -88.9 -87.7 -86.3 \n5.6 \nPreds, \u03c4 = 0.05 \n-87.4 -86.8 -90.4 -81.5 -75.2 -90.0 -75.8 -76.5 -84.5 -88.1 -89.3 -90.9 -84.7 \n5.7 \nPreds, \u03c4 = 0.1 \n-95.6 -97.7 -96.2 -93.1 -96.7 -93.3 -93.9 -93.5 -95.7 -95.0 -95.7 -96.6 -95.3 \n1.4 \nPreds, \u03c4 = 0.5 \n-65.6 -46.5 -57.4 -77.8 -60.8 -63.9 -68.3 -83.7 -59.8 -72.5 -62.5 -67.0 -65.5 \n9.3 \n\n\n\nTable 31 :\n31The weighted Spearman correlation of each validator/task pair for MCD.CP \nCR \nCS \nPC \nPR \nPS \nRC \nRP \nRS \nSC \nSP \nSR Mean \nStd \n\nAccuracy \nSource Train \n95.8 \n82.8 \n92.6 \n87.9 \n71.7 \n84.7 \n87.3 \n93.8 \n92.8 \n92.5 \n92.9 \n88.7 \n88.6 \n6.3 \nSource Val \n88.0 \n89.9 \n93.2 \n78.9 \n94.0 \n75.6 \n84.6 \n84.5 \n92.7 \n57.8 \n73.4 \n68.2 \n81.7 10.8 \n\nBNM \n\nSource Train \n96.6 \n86.7 \n92.3 \n94.7 \n79.7 \n96.3 \n96.1 \n95.0 \n96.8 \n95.9 \n97.0 \n92.4 \n93.3 \n4.9 \nSource Train + Target \n92.9 \n84.3 \n89.2 \n91.5 \n80.4 \n96.7 \n88.7 \n90.4 \n92.1 \n94.1 \n95.7 \n94.5 \n90.9 \n4.5 \nSource Val \n95.6 \n89.3 \n90.7 \n92.8 \n80.9 \n96.3 \n92.8 \n93.6 \n96.1 \n94.5 \n96.5 \n94.9 \n92.8 \n4.2 \nSource Val + Target \n93.7 \n86.7 \n89.0 \n91.2 \n80.5 \n96.6 \n89.3 \n91.1 \n92.7 \n94.0 \n95.7 \n94.7 \n91.3 \n4.3 \nTarget \n88.5 \n80.8 \n86.2 \n88.5 \n78.9 \n94.9 \n86.6 \n88.9 \n89.8 \n92.9 \n93.5 \n93.4 \n88.6 \n4.8 \n\nClassAMI \n\nSource + Target Features \n81.8 \n42.2 \n88.6 \n85.9 \n67.1 \n81.5 \n80.2 \n85.7 \n94.0 \n85.8 \n91.1 \n77.6 \n80.1 13.2 \nSource + Target Logits \n83.2 \n52.4 \n92.8 \n82.9 \n74.3 \n81.2 \n84.3 \n88.8 \n94.5 \n87.2 \n87.0 \n76.7 \n82.1 10.6 \nTarget Features \n-71.2 -78.4 -67.3 -62.6 -76.5 -75.5 -71.0 -63.7 -53.0 -76.6 -69.5 -68.7 -69.5 \n7.0 \nTarget Logits \n-71.3 -44.5 -39.8 -58.9 -53.2 -56.5 -66.7 -61.5 -32.0 -76.1 -68.3 -26.0 -54.6 15.3 \n\nClassSS \n\nSource + Target Features \n45.9 \n43.9 \n8.3 \n73.5 \n72.5 \n53.2 \n88.1 \n90.6 \n64.5 \n75.9 \n46.7 \n83.7 \n62.2 22.7 \nSource + Target Logits \n45.0 -53.1 -41.4 \n86.3 -44.3 -47.6 -20.6 -14.5 \n-5.9 -46.2 -44.3 -40.6 -18.9 41.2 \nTarget Features \n24.1 \n26.5 \n-2.4 \n26.5 \n62.0 \n25.1 \n58.8 \n83.8 \n32.9 \n18.3 \n7.9 \n61.8 \n35.4 24.4 \nTarget Logits \n20.0 -55.8 -46.4 \n29.6 -45.9 -53.4 \n57.5 \n81.7 -21.6 \n17.2 \n8.5 -44.1 \n-4.4 44.8 \n\nDEV \n\nFeatures \n58.3 \n46.4 \n62.9 \n33.3 \n45.7 \n34.0 \n39.4 \n75.8 \n62.5 \n4.5 \n33.5 \n27.4 \n43.6 18.5 \nLogits \n34.4 \n39.9 \n16.4 \n26.5 \n36.1 \n23.6 \n50.5 \n61.5 \n57.7 \n55.7 \n54.7 \n47.2 \n42.0 14.2 \nPreds \n91.4 \n66.4 \n91.9 \n77.6 \n79.0 \n96.3 \n26.7 \n88.4 \n87.6 \n38.0 \n90.6 \n73.9 \n75.6 21.2 \n\nDEVN \n\nFeatures, max normalization \n87.1 \n87.4 \n92.1 \n76.4 \n91.2 \n77.2 \n83.8 \n81.3 \n92.9 \n50.3 \n72.8 \n63.9 \n79.7 12.1 \nLogits, max normalization \n87.6 \n88.4 \n92.2 \n76.6 \n91.6 \n77.8 \n83.6 \n82.0 \n92.8 \n50.0 \n72.7 \n63.8 \n79.9 12.3 \nPreds, max normalization \n87.1 \n87.6 \n92.8 \n76.0 \n96.0 \n77.1 \n83.4 \n84.2 \n92.1 \n66.7 \n73.5 \n63.4 \n81.7 10.0 \n\nEntropy \n\nSource Train \n91.8 \n80.1 \n86.9 \n93.4 \n76.5 \n89.7 \n93.3 \n92.0 \n90.9 \n90.3 \n94.4 \n78.7 \n88.2 \n6.0 \nSource Train + Target \n86.1 \n58.2 \n80.1 \n87.5 \n73.8 \n86.5 \n85.3 \n87.0 \n84.7 \n87.8 \n88.9 \n63.2 \n80.8 \n9.9 \nSource Val \n89.4 \n72.9 \n82.5 \n88.6 \n74.7 \n86.7 \n88.1 \n89.9 \n88.4 \n88.0 \n88.7 \n64.8 \n83.6 \n7.9 \nSource Val + Target \n84.7 \n54.9 \n78.3 \n85.6 \n72.7 \n85.3 \n84.1 \n86.2 \n83.4 \n87.1 \n87.1 \n62.4 \n79.3 10.2 \nTarget \n81.5 \n48.8 \n74.9 \n83.1 \n71.0 \n83.4 \n81.2 \n84.4 \n79.4 \n86.1 \n85.4 \n61.1 \n76.7 10.9 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-92.9 -95.3 -94.5 -95.7 -95.2 -87.2 -93.9 -94.3 -91.6 -92.9 -93.2 -94.1 -93.4 \n2.2 \nFeatures, \u03c4 = 0.1 \n-91.2 -92.6 -93.4 -94.3 -92.5 -85.3 -92.2 -89.6 -90.1 -92.5 -91.1 -90.7 -91.3 \n2.2 \nFeatures, \u03c4 = 0.5 \n-88.6 -87.8 -86.9 -84.3 -84.1 -83.8 -82.8 -77.1 -83.9 -84.8 -83.0 -80.3 -83.\n\nTable 32 :\n32The weighted Spearman correlation of each validator/task pair for MMD.CP \nCR \nCS \nPC \nPR \nPS \nRC \nRP \nRS \nSC \nSP \nSR Mean \nStd \n\nAccuracy \nSource Train \n79.6 \n78.2 \n67.0 \n40.6 \n60.6 \n47.0 \n73.7 \n70.0 \n52.5 \n66.2 \n69.2 \n78.1 \n65.2 12.2 \nSource Val \n78.7 \n81.3 \n80.4 \n63.4 \n90.2 \n55.3 \n73.4 \n80.3 \n55.0 \n67.8 \n64.6 \n70.0 \n71.7 10.5 \n\nBNM \n\nSource Train \n67.9 \n68.6 \n48.6 \n34.8 \n49.3 \n44.2 \n74.5 \n65.9 \n52.1 \n56.4 \n55.8 \n68.0 \n57.2 11.5 \nSource Train + Target \n88.3 \n87.5 \n77.9 \n47.4 \n64.0 \n60.0 \n81.3 \n73.4 \n74.5 \n68.5 \n71.9 \n83.0 \n73.1 11.4 \nSource Val \n89.4 \n90.6 \n69.5 \n53.7 \n74.9 \n62.4 \n77.9 \n77.5 \n58.7 \n66.7 \n70.1 \n82.9 \n72.9 11.1 \nSource Val + Target \n90.5 \n90.5 \n77.2 \n53.2 \n73.3 \n65.0 \n81.3 \n75.4 \n72.7 \n69.0 \n73.5 \n84.7 \n75.5 10.1 \nTarget \n89.5 \n89.5 \n81.6 \n50.8 \n69.4 \n65.7 \n81.3 \n73.6 \n76.3 \n71.0 \n76.9 \n87.2 \n76.1 10.7 \n\nClassAMI \n\nSource + Target Features \n87.7 \n67.5 \n88.7 \n61.3 \n52.8 \n62.2 \n89.4 \n85.1 \n81.9 \n73.0 \n76.2 \n63.0 \n74.1 12.1 \nSource + Target Logits \n88.7 \n67.5 \n87.9 \n55.7 \n50.6 \n60.3 \n88.6 \n87.0 \n78.4 \n74.2 \n74.5 \n71.3 \n73.7 12.7 \nTarget Features \n55.7 \n53.2 \n34.8 \n2.9 \n9.4 \n18.4 \n74.7 \n71.2 \n59.3 \n42.6 \n27.7 \n35.7 \n40.5 22.3 \nTarget Logits \n54.6 \n61.3 \n73.4 \n2.8 \n51.9 \n59.2 \n77.2 \n76.0 \n86.9 \n46.1 \n28.7 \n66.1 \n57.0 22.2 \n\nClassSS \n\nSource + Target Features \n26.2 \n28.6 \n34.0 \n27.3 \n39.0 \n34.2 \n63.7 \n66.7 \n47.2 \n38.7 \n51.8 \n43.4 \n41.7 12.9 \nSource + Target Logits \n25.4 \n21.8 \n5.3 \n32.8 \n4.3 \n7.9 \n55.8 \n57.3 \n35.1 \n25.4 \n18.6 \n27.9 \n26.5 16.6 \nTarget Features \n-2.1 \n20.1 \n14.2 \n-5.8 \n27.6 \n29.9 \n49.2 \n62.3 \n31.1 \n11.4 \n36.2 \n31.1 \n25.4 18.7 \nTarget Logits \n-2.5 \n13.4 \n-6.4 \n-2.8 \n-3.0 \n5.4 \n51.8 \n65.0 \n20.1 \n10.7 \n40.0 \n14.9 \n17.2 22.3 \n\nDEV \n\nFeatures \n22.6 \n17.6 \n33.1 \n16.8 \n25.5 \n13.7 \n13.5 \n51.8 \n12.8 \n9.0 \n37.4 \n19.5 \n22.8 11.9 \nLogits \n19.7 \n21.2 \n18.7 \n-0.2 \n16.0 \n13.1 \n-0.0 \n40.9 \n13.2 \n20.7 \n33.3 \n40.9 \n19.8 12.8 \nPreds \n48.1 \n46.9 \n42.4 \n25.4 \n39.0 \n49.9 \n62.5 \n62.4 \n54.7 \n45.8 \n53.7 \n57.0 \n49.0 10.0 \n\nDEVN \n\nFeatures, max normalization \n64.4 \n65.5 \n62.4 \n48.7 \n83.4 \n41.1 \n67.0 \n71.9 \n42.2 \n53.7 \n47.1 \n55.4 \n58.6 12.2 \nLogits, max normalization \n64.2 \n65.4 \n61.9 \n48.7 \n83.1 \n41.1 \n67.0 \n71.2 \n42.2 \n53.1 \n47.0 \n55.4 \n58.4 12.1 \nPreds, max normalization \n62.5 \n65.2 \n56.9 \n41.3 \n82.2 \n38.6 \n60.8 \n66.4 \n37.4 \n48.9 \n43.6 \n53.0 \n54.7 12.9 \n\nEntropy \n\nSource Train \n68.1 \n67.6 \n52.4 \n36.9 \n51.3 \n45.3 \n74.0 \n66.4 \n51.7 \n58.0 \n59.6 \n71.2 \n58.5 10.9 \nSource Train + Target \n82.9 \n83.2 \n70.7 \n43.8 \n62.8 \n55.9 \n81.2 \n75.1 \n71.1 \n65.1 \n69.8 \n81.0 \n70.2 11.5 \nSource Val \n84.5 \n86.7 \n67.0 \n47.1 \n70.1 \n54.2 \n76.0 \n77.1 \n55.8 \n66.5 \n74.5 \n81.9 \n70.1 12.0 \nSource Val + Target \n84.0 \n84.7 \n71.2 \n45.8 \n66.9 \n57.2 \n80.7 \n76.1 \n70.1 \n66.4 \n72.5 \n82.2 \n71.5 11.0 \nTarget \n83.1 \n83.4 \n72.1 \n44.5 \n64.2 \n58.4 \n81.2 \n75.4 \n73.6 \n66.1 \n71.1 \n81.8 \n71.3 11.1 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-95.3 -93.3 -96.7 -96.0 -96.6 -95.5 -95.7 -94.6 -95.5 -97.4 -96.8 -95.0 -95.7 \n1.1 \nFeatures, \u03c4 = 0.1 \n-89.5 -85.4 -92.7 -93.9 -88.9 -92.1 -84.8 -83.6 -90.1 -94.6 -91.6 -86.7 -89.\n\nTable 33 :\n33The weighted Spearman correlation for the MNIST \u2192 MNISTM task, using the checkpoints of all algorithms. Features, \u03c4 = 0.1 -78.3 Features, \u03c4 = 0.5 -79.2 Logits, \u03c4 = 0.05 -80.2 Logits, \u03c4 = 0.1 -86.3 Logits, \u03c4 = 0.5 -87.5 Preds, \u03c4 = 0.05 -82.5 Preds, \u03c4 = 0.1 -86.3 Preds, \u03c4 = 0.5 -85.1MM \n\nAccuracy \nSource Train \n7.7 \nSource Val \n-4.3 \n\nBNM \n\nSource Train \n-18.9 \nSource Train + Target \n53.3 \nSource Val \n-23.1 \nSource Val + Target \n49.7 \nTarget \n54.7 \n\nClassAMI \n\nSource + Target Features \n-31.0 \nSource + Target Logits \n-21.2 \nTarget Features \n-3.0 \nTarget Logits \n-16.1 \n\nClassSS \n\nSource + Target Features \n-71.5 \nSource + Target Logits \n-61.6 \nTarget Features \n-59.1 \nTarget Logits \n-48.0 \n\nDEV \n\nFeatures \n-30.6 \nLogits \n-14.1 \nPreds \n-31.3 \n\nDEVN \n\nFeatures, max normalization -43.7 \nLogits, max normalization \n-44.1 \nPreds, max normalization \n-54.7 \n\nEntropy \n\nSource Train \n-31.5 \nSource Train + Target \n-34.3 \nSource Val \n-34.8 \nSource Val + Target \n-35.1 \nTarget \n-38.5 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-78.1 \n\n\nTable 34 :\n34The weighted Spearman correlation of each validator/algorithm pair, for the MNIST \u2192 MNISTM task.ATDOC BNM \nBSP CDAN DANN GVB \nIM MCC MCD MMD Mean \nStd \n\nAccuracy \nSource Train \n-13.8 \n38.0 -11.5 \n-24.1 \n-2.1 \n5.5 \n24.0 \n11.2 -16.2 \n17.2 \n2.8 18.9 \nSource Val \n5.8 \n30.7 \n5.7 \n-24.5 \n-23.6 \n2.0 \n24.9 \n2.2 \n-9.8 \n17.1 \n3.0 17.6 \n\nBNM \n\nSource Train \n-21.0 \n34.8 \n26.6 \n-34.3 \n-16.5 -19.0 \n43.8 -19.6 -24.4 \n-8.4 \n-3.8 26.4 \nSource Train + Target \n78.1 \n52.7 \n76.6 \n22.3 \n54.9 \n20.7 \n41.5 \n51.5 \n88.0 \n8.0 \n49.4 25.4 \nSource Val \n4.0 \n48.6 \n48.0 \n-36.0 \n-24.9 -22.9 \n64.0 -38.0 -26.4 \n-11.9 \n0.4 36.7 \nSource Val + Target \n77.9 \n53.2 \n77.0 \n20.4 \n53.7 \n16.4 \n45.4 \n52.8 \n88.1 \n6.3 \n49.1 26.3 \nTarget \n78.0 \n52.5 \n75.6 \n27.7 \n56.8 \n25.4 \n34.6 \n49.8 \n87.1 \n9.9 \n49.7 24.1 \n\nClassAMI \n\nSource + Target Features \n-67.0 \n30.0 -85.3 \n27.6 \n43.9 \n-8.8 \n46.5 -55.4 -70.7 \n-9.2 -14.8 48.5 \nSource + Target Logits \n-61.4 \n46.0 -83.8 \n37.9 \n44.9 \n4.6 \n64.1 -55.4 -72.2 \n-13.9 \n-8.9 53.1 \nTarget Features \n44.0 -14.9 -73.4 \n-39.3 \n13.8 -47.6 -29.0 -45.2 \n8.3 \n-4.3 -18.8 33.2 \nTarget Logits \n20.7 \n34.3 -73.8 \n-50.2 \n-63.7 -47.3 \n14.1 -43.0 -45.0 \n1.3 -25.3 36.8 \n\nClassSS \n\nSource + Target Features \n-65.3 -83.3 -79.6 \n-54.2 \n-42.2 -40.4 -80.2 -61.4 -76.0 \n-31.9 -61.5 17.7 \nSource + Target Logits \n-68.4 -70.8 -84.0 \n-28.4 \n-28.2 -27.7 -65.4 -69.0 -79.7 \n3.2 -51.8 27.6 \nTarget Features \n-52.2 -74.5 -83.1 \n-52.3 \n-37.2 -36.6 -73.2 -62.4 -74.1 \n-36.6 -58.2 16.8 \nTarget Logits \n-50.8 -68.6 -85.3 \n-28.4 \n-20.2 -34.3 -66.8 -67.8 -78.7 \n-4.2 -50.5 25.9 \n\nDEV \n\nFeatures \n-32.1 -57.2 -32.0 \n-3.8 \n-14.6 -23.5 -47.4 -20.9 -64.5 \n-17.5 -31.4 18.5 \nLogits \n-42.8 -39.7 -30.3 \n-11.8 \n-9.9 -12.7 -21.3 -24.1 -77.8 \n-16.5 -28.7 19.6 \nPreds \n-37.1 -29.1 -34.0 \n-2.3 \n16.7 \n2.0 -29.6 -33.0 -74.6 \n-6.0 -22.7 24.6 \n\nDEVN \n\nFeatures, max normalization \n5.7 -15.3 \n5.7 \n-45.5 \n-57.0 -47.9 \n-9.4 -29.0 \n-8.9 \n-30.0 -23.2 21.1 \nLogits, max normalization \n6.8 -17.0 \n7.3 \n-46.0 \n-58.1 -48.6 -10.7 -28.6 -11.6 \n-30.5 -23.7 21.5 \nPreds, max normalization \n-26.3 -14.1 -14.8 \n-44.7 \n-56.8 -44.7 -12.6 -32.9 -22.7 \n-29.0 -29.8 14.2 \n\nEntropy \n\nSource Train \n-16.8 \n35.9 \n39.7 \n-50.2 \n-42.7 -22.9 \n45.2 -21.8 -27.1 \n-9.4 \n-7.0 33.0 \nSource Train + Target \n-67.5 -23.9 \n69.6 \n-12.9 \n9.2 \n2.0 -15.4 -13.7 -81.9 \n-1.0 -13.5 39.4 \nSource Val \n5.6 \n50.7 \n52.4 \n-50.7 \n-45.9 -24.9 \n63.6 -41.4 -29.6 \n-12.5 \n-3.3 41.6 \nSource Val + Target \n-67.5 -25.0 \n69.4 \n-11.8 \n10.3 \n2.9 -17.0 -17.3 -82.9 \n-0.9 -14.0 39.7 \nTarget \n-67.8 -43.9 \n67.3 \n-8.5 \n4.5 \n8.8 -51.8 -17.3 -87.7 \n-0.1 -19.6 42.5 \n\nSND \n\nFeatures, \u03c4 = 0.05 \n-64.0 -78.8 -82.3 \n-79.0 \n-73.3 -81.2 -78.8 -65.3 -76.9 \n-9.5 -68.9 20.7 \nFeatures, \u03c4 = 0.1 \n-63.0 -74.1 -82.2 \n-89.2 \n-71.7 -82.0 -75.7 -58.0 -76.6 \n-9.6 -68.2 21.4 \nFeatures, \u03c4 = 0.5 \n-54.6 -72.5 -85.4 \n-88.8 \n-69.5 -82.1 -72.0 -52.3 -73.3 \n-10.2 -66.1 21.7 \nLogits, \u03c4 = 0.05 \n-60.4 -86.9 -91.5 \n-72.6 \n-75.6 -83.5 -86.5 -88.4 -66.9 \n-59.3 -77.1 11.3 \nLogits, \u03c4 = 0.1 \n-59.5 -89.3 -92.1 \n-91.6 \n-82.1 -88.5 -88.0 -87.5 -65.8 \n-70.7 -81.5 11.2 \nLogits, \u03c4 = 0.5 \n-57.5 -85.0 -91.7 \n-90.6 \n-81.6 -81.7 -81.1 -78.9 -62.3 \n-59.4 -77.0 12.0 \nPreds, \u03c4 = 0.05 \n-60.6 -83.9 -91.1 \n-78.4 \n-79.3 -82.7 -84.9 -85.2 -75.6 \n-41.0 -76.3 14.1 \nPreds, \u03c4 = 0.1 \n-58.7 -90.2 -91.4 \n-89.4 \n-86.0 -89.0 -88.0 -88.3 -75.4 \n-69.2 -82.6 10.5 \nPreds, \u03c4 = 0.5 \n-54.2 -76.3 -91.6 \n-89.1 \n-89.8 -85.0 -69.2 -79.8 -74.6 \n-59.2 -76.9 12.2 \n\n\nThe above model-selection workflow optimizes for a high validation score, but the goal is to have a model with high target-domain accuracy. UDA validation scores are not perfectly correlated with target-domain accuracy. Thus, the model with the highest validation score might have sub-optimal target-domain accuracy. The lower the correlation, the more likely that a sub-optimal model will be selected inadvertently.Our research shows that existing UDA validators have considerable room for improvement. For example, the model with the best validation score often has low target-domain accuracy. In other words, the model that gets chosen for deployment actually performs poorly, even though the validator indicates that it performs well. Until UDA validators are able to produce more reliable results, it will be difficult to determine which models have the highest target-domain accuracy. As long as this is the case, the full potential of UDA algorithms will be unrealized.Despite this fact, there are far more papers on UDA algorithms than on UDA validators. Yet validators have much more room for improvement. A UDA algorithm paper might improve target domain accuracy (as computed by an oracle) by a single percentage point, from 89% to 90% for example. But the checkpoint with the highest validation score might have only 70% target-domain accuracy. Thus, the validator has a much larger effect on accuracy than the choice of UDA algorithm. Hence, research into UDA validators is crucial.\u2022 Model: a function that receives some input (e.g. photographic images), and returns a label for each item in that input.\u2022 Domain adaptation: a type of machine-learning algorithm that repurposes existing models to work in new domains (a.k.a. target domains). For example, the existing model might work on photographs of food, whereas the target domain contains drawings of food.\u2022 Unsupervised domain adaptation (UDA): a type of domain adaptation where the target-domain does not have any existing class labels.\u2022 Validator: a function that evaluates how closely a model's output reflects certain attributes of the dataset, such as labels. The validator will return a quality score. Ideally, the quality score will indicate how similar the model's output is to the dataset attributes.\u2022 UDA validator: a validator that estimates target domain accuracy, without having access to target labels. An effective UDA validator is one that reliably estimates target-domain accuracy. For example, a higher score returned by an effective UDA validator will reliably indicate that the target-domain accuracy is high.\u2022 Target-domain accuracy: a model's accuracy in the target domain.\u2022 Oracle validator: a validator that has access to existing target labels and is therefore able to directly compute targetdomain accuracy. In UDA, no target labels are available, so the oracle validator cannot be used. When target labels are available, they should be used during training, as this will improve the model's target-domain accuracy. This type of training is known as semi-supervised or supervised domain adaptation. On the other hand, when target labels are not available, UDA is the only training method possible, and non-oracle validators must be used.G.2. What are validators, and why are they important?In this paper, we compare the performance of various validators. Validators are functions that are used to evaluate the accuracy of machine-learning models, or in the case of this paper, unsupervised domain-adaptation models. This kind of research is essential, for the following reasons.To date, most UDA papers have focused on improvements to the training procedure (algorithm), with the goal of maximizing target-domain accuracy. These papers tend to use the oracle validator to evaluate their models, which is useful only when target labels are available. In contrast, when target labels are not available, the oracle validator cannot be used. In that case, UDA validators are the only viable choice.Unfortunately, UDA validators produce scores that are not 100% correlated with target-domain accuracy. For example, in an extreme case, the UDA validator could return a high score for a low-accuracy model, and a low score for a high-accuracy model. Even in less extreme scenarios, the score might mislead the user into selecting a model that is not the most accurate one available. Yet achieving the highest possible accuracy is crucial in most application scenarios.G.3. How validators are usedHere is what a typical model-selection workflow looks like:1. Select a UDA algorithm that you think will train your model effectively.2. Set the hyperparameters either arbitrarily, or by using a hyperparameter optimizer. The UDA algorithm and hyperparameters will determine how your model is trained.3. Use the UDA algorithm to train your model for an arbitrary amount of time, and save a version (checkpoint) of the model at arbitrary regular intervals.4. To evaluate each checkpoint, employ whatever UDA validator you think will correlate well with target-domain accuracy.The goal is to obtain validation scores that are as accurate as possible.5. Keep the checkpoint with the highest validation score. Discard all other checkpoints.6. Repeat steps 2-5 an arbitrary number of times, or until the best validation scores start to plateau.At the end of this procedure, the model with the highest validation score will typically be deployed in some application. (For example, the model might be used on a smartphone to classify images of food.)G.4. Why validators are an important area of research\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, Masanori Koyama, Optuna, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningTakuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, Jul 2019. 4\n\nWeighted and unweighted correlation methods for large-scale educational assessment: wcorr formulas. air-naep working paper no. 2018-01. nces data r project se-ries# 02. Paul Bailey, Ahmad Emad, Ting Zhang, Qingshu Xie, Emmanuel Sikali, American Institutes for Research. 5Paul Bailey, Ahmad Emad, Ting Zhang, Qingshu Xie, and Emmanuel Sikali. Weighted and unweighted correlation meth- ods for large-scale educational assessment: wcorr formulas. air-naep working paper no. 2018-01. nces data r project se- ries# 02. American Institutes for Research, 2018. 5\n\n. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T J Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskeverand Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020. 1Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar- wal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020. 1\n\nPartial adversarial domain adaptation. Zhangjie Cao, Lijia Ma, Mingsheng Long, Jianmin Wang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)32Zhangjie Cao, Lijia Ma, Mingsheng Long, and Jianmin Wang. Partial adversarial domain adaptation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 135-150, 2018. 32\n\nDomain-specific batch normalization for unsupervised domain adaptation. Woong-Gi Chang, Tackgeun You, Seonguk Seo, Suha Kwak, Bohyung Han, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition33Woong-Gi Chang, Tackgeun You, Seonguk Seo, Suha Kwak, and Bohyung Han. Domain-specific batch normalization for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7354-7362, 2019. 33\n\nTransferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation. Xinyang Chen, Sinan Wang, Mingsheng Long, Jianmin Wang, PMLRInternational conference on machine learning. 433Xinyang Chen, Sinan Wang, Mingsheng Long, and Jianmin Wang. Transferability vs. discriminability: Batch spectral pe- nalization for adversarial domain adaptation. In International conference on machine learning, pages 1081-1090. PMLR, 2019. 4, 33\n\nEstimating generalization under distribution shifts via domaininvariant representations. International conference on machine learning. Ching-Yao Chuang, Antonio Torralba, Stefanie Jegelka, 2020Ching-Yao Chuang, Antonio Torralba, and Stefanie Jegelka. Estimating generalization under distribution shifts via domain- invariant representations. International conference on ma- chine learning, 2020. 2\n\nTowards discriminability and diversity: Batch nuclear-norm maximization under label insufficient situations. Shuhao Cui, Shuhui Wang, Junbao Zhuo, Liang Li, Qingming Huang, Qi Tian, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition333Shuhao Cui, Shuhui Wang, Junbao Zhuo, Liang Li, Qingming Huang, and Qi Tian. Towards discriminability and diversity: Batch nuclear-norm maximization under label insufficient situations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3941-3950, 2020. 3, 4, 33\n\nFast batch nuclear-norm maximization and minimization for robust domain adaptation. Shuhao Cui, Shuhui Wang, Junbao Zhuo, Liang Li, Qingming Huang, Qi Tian, abs/2107.06154ArXiv. 3Shuhao Cui, Shuhui Wang, Junbao Zhuo, Liang Li, Qingming Huang, and Qi Tian. Fast batch nuclear-norm maximiza- tion and minimization for robust domain adaptation. ArXiv, abs/2107.06154, 2021. 3\n\nGradually vanishing bridge for adversarial domain adaptation. Shuhao Cui, Shuhui Wang, Junbao Zhuo, Chi Su, Qingming Huang, Qi Tian, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition433Shuhao Cui, Shuhui Wang, Junbao Zhuo, Chi Su, Qingming Huang, and Qi Tian. Gradually vanishing bridge for adver- sarial domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12455-12464, 2020. 4, 33\n\nDomain-adversarial training of neural networks. The journal of machine learning research. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, Victor Lempitsky, 1732Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marc- hand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096-2030, 2016. 2, 4, 32\n\nDeep reconstructionclassification networks for unsupervised domain adaptation. Muhammad Ghifary, Mengjie Bastiaan Kleijn, David Zhang, Wen Balduzzi, Li, European conference on computer vision. Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, and Wen Li. Deep reconstruction- classification networks for unsupervised domain adaptation. In European conference on computer vision, pages 597-613.\n\n. Springer, 33Springer, 2016. 33\n\n. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Generative adversarial networks. Communications of the ACM. 6311Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communi- cations of the ACM, 63(11):139-144, 2020. 1\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)14Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. 1, 4\n\nDenoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. 33Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu- sion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851, 2020. 1\n\nMinimum class confusion for versatile domain adaptation. Ying Jin, Ximei Wang, Mingsheng Long, Jianmin Wang, European Conference on Computer Vision. Springer432Ying Jin, Ximei Wang, Mingsheng Long, and Jianmin Wang. Minimum class confusion for versatile domain adaptation. In European Conference on Computer Vision, pages 464-480. Springer, 2020. 4, 32\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, 110Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. 1, 10\n\nSliced wasserstein discrepancy for unsupervised domain adaptation. Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, Daniel Ulbricht, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition32Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasserstein discrepancy for unsuper- vised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10285-10295, 2019. 32\n\nDo we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. Jian Liang, Dapeng Hu, Jiashi Feng, International Conference on Machine Learning (ICML). Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning (ICML), pages 6028-6039, July 13-18 2020. 32\n\nDomain adaptation with auxiliary target domain-oriented classifier. Jian Liang, Dapeng Hu, Jiashi Feng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition433Jian Liang, Dapeng Hu, and Jiashi Feng. Domain adaptation with auxiliary target domain-oriented classifier. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16632-16642, 2021. 4, 33\n\nLearning transferable features with deep adaptation networks. Mingsheng Long, Yue Cao, Jianmin Wang, Michael Jordan, PMLRInternational conference on machine learning. 432Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jor- dan. Learning transferable features with deep adaptation networks. In International conference on machine learning, pages 97-105. PMLR, 2015. 4, 32\n\nMingsheng Long, Zhangjie Cao, Jianmin Wang, Michael I Jordan , arXiv:1705.10667Conditional adversarial domain adaptation. 432arXiv preprintMingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adapta- tion. arXiv preprint arXiv:1705.10667, 2017. 4, 32\n\nUnsupervised domain adaptation with residual transfer networks. Mingsheng Long, Han Zhu, Jianmin Wang, Michael I Jordan , arXiv:1602.0443333arXiv preprintMingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jor- dan. Unsupervised domain adaptation with residual transfer networks. arXiv preprint arXiv:1602.04433, 2016. 33\n\nDeep transfer learning with joint adaptation networks. Mingsheng Long, Han Zhu, Jianmin Wang, Michael I Jordan , PMLR, 2017. 32International conference on machine learning. Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning with joint adaptation networks. In International conference on machine learning, pages 2208- 2217. PMLR, 2017. 32\n\nIlya Loshchilov, Frank Hutter, Sgdr, Stochastic gradient descent with warm restarts. arXiv: Learning. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv: Learning, 2016. 1\n\nStochastic classifiers for unsupervised domain adaptation. Zhihe Lu, Yongxin Yang, Xiatian Zhu, Cong Liu, Yi-Zhe Song, Tao Xiang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition32Zhihe Lu, Yongxin Yang, Xiatian Zhu, Cong Liu, Yi-Zhe Song, and Tao Xiang. Stochastic classifiers for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 9111-9120, 2020. 32\n\nUnsupervised domain adaptation: A reality check. ArXiv, abs/2111.15672. Kevin Musgrave, Serge J Belongie, Ser Nam Lim, Kevin Musgrave, Serge J. Belongie, and Ser Nam Lim. Unsupervised domain adaptation: A reality check. ArXiv, abs/2111.15672, 2021. 1\n\nUnsupervised domain adaption of object detectors: A survey. Poojan Oza, A Vishwanath, Sindagi, V S Vibashan, Patel, arXiv:2105.1350232arXiv preprintPoojan Oza, Vishwanath A Sindagi, Vibashan VS, and Vishal M Patel. Unsupervised domain adaption of object detectors: A survey. arXiv preprint arXiv:2105.13502, 2021. 32\n\nOpen set domain adaptation. Panareda Pau, Juergen Busto, Gall, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision32Pau Panareda Busto and Juergen Gall. Open set domain adap- tation. In Proceedings of the IEEE International Conference on Computer Vision, pages 754-763, 2017. 32\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. GarnettCurran Associates, Inc324Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, An- dreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im- perative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024-8035. Curran Associates, Inc., 2019. 4\n\nMoment matching for multi-source domain adaptation. Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, Bo Wang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision432Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision, pages 1406-1415, 2019. 4, 32\n\nSentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. Viraj Prabhu, Shivam Khare, Deeksha Kartik, Judy Hoffman, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision32Viraj Prabhu, Shivam Khare, Deeksha Kartik, and Judy Hoff- man. Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. In Proceed- ings of the IEEE/CVF International Conference on Computer Vision, pages 8558-8567, 2021. 32\n\nNeural unsupervised domain adaptation in NLP-A survey. Alan Ramponi, Barbara Plank, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain32Alan Ramponi and Barbara Plank. Neural unsupervised do- main adaptation in NLP-A survey. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6838-6855, Barcelona, Spain (Online), Dec. 2020. In- ternational Committee on Computational Linguistics. 32\n\nAdversarial branch architecture search for unsupervised domain adaptation. Luca Robbiano, Muhammad Rameez Ur Rahman, Fabio Galasso, Barbara Caputo, Fabio Maria Carlucci, Luca Robbiano, Muhammad Rameez Ur Rahman, Fabio Galasso, Barbara Caputo, and Fabio Maria Carlucci. Ad- versarial branch architecture search for unsupervised domain adaptation, 2021. 3\n\nImagenet large scale visual recognition challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, International journal of computer vision. 1153Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211-252, 2015. 4\n\nAdapting visual category models to new domains. Kate Saenko, Brian Kulis, Mario Fritz, Trevor Darrell, European conference on computer vision. Springer4Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Dar- rell. Adapting visual category models to new domains. In European conference on computer vision, pages 213-226. Springer, 2010. 4\n\nSemi-supervised domain adaptation via minimax entropy. Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, Kate Saenko, 432Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Semi-supervised domain adaptation via minimax entropy. ICCV, 2019. 4, 32\n\nTune it the right way: Unsupervised validation of domain adaptation via soft neighborhood density. Kuniaki Saito, Donghyun Kim, Piotr Teterwak, Stan Sclaroff, Trevor Darrell, Kate Saenko, Kuniaki Saito, Donghyun Kim, Piotr Teterwak, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Tune it the right way: Unsupervised validation of domain adaptation via soft neigh- borhood density, 2021. 3\n\nAsymmetric tri-training for unsupervised domain adaptation. Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, PMLRInternational Conference on Machine Learning. 33Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. Asym- metric tri-training for unsupervised domain adaptation. In International Conference on Machine Learning, pages 2988- 2997. PMLR, 2017. 33\n\nMaximum classifier discrepancy for unsupervised domain adaptation. Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, Tatsuya Harada, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition432Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat- suya Harada. Maximum classifier discrepancy for unsuper- vised domain adaptation. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 3723-3732, 2018. 4, 32\n\nOpen set domain adaptation by backpropagation. Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, Tatsuya Harada, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, and Tat- suya Harada. Open set domain adaptation by backpropagation. In Proceedings of the European Conference on Computer Vi- sion (ECCV), September 2018. 32\n\nGenerate to adapt: Aligning domains using generative adversarial networks. Swami Sankaranarayanan, Yogesh Balaji, D Carlos, Rama Castillo, Chellappa, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition33Swami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo, and Rama Chellappa. Generate to adapt: Aligning domains using generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 8503-8512, 2018. 33\n\nInformation-theoretical learning of discriminative clusters for unsupervised domain adaptation. Yuan Shi, Fei Sha, Proceedings of the 29th International Conference on Machine Learning (ICML-12), ICML '12. John Langford and Joelle Pineauthe 29th International Conference on Machine Learning (ICML-12), ICML '12New York, NY, USAOmnipress432Yuan Shi and Fei Sha. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. In John Langford and Joelle Pineau, editors, Proceedings of the 29th International Conference on Machine Learning (ICML-12), ICML '12, pages 1079-1086, New York, NY, USA, July 2012. Omnipress. 4, 32\n\nA dirt-t approach to unsupervised domain adaptation. Rui Shu, H Hung, Hirokazu Bui, Stefano Narui, Ermon, arXiv:1802.0873532arXiv preprintRui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation. arXiv preprint arXiv:1802.08735, 2018. 32\n\nSuper-convergence: very fast training of neural networks using large learning rates. Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications. Leslie N Smith, Nicholay Topin, Leslie N. Smith and Nicholay Topin. Super-convergence: very fast training of neural networks using large learning rates. Artificial Intelligence and Machine Learning for Multi- Domain Operations Applications, May 2019. 10\n\nReturn of frustratingly easy domain adaptation. Baochen Sun, Jiashi Feng, Kate Saenko, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence3032Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frus- tratingly easy domain adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016. 32\n\nUnsupervised domain adaptation in semantic segmentation: a review. Marco Toldo, Andrea Maracani, Umberto Michieli, Pietro Zanuttigh, 2020. 32835TechnologiesMarco Toldo, Andrea Maracani, Umberto Michieli, and Pietro Zanuttigh. Unsupervised domain adaptation in semantic seg- mentation: a review. Technologies, 8(2):35, 2020. 32\n\nSimultaneous deep transfer across domains and tasks. Eric Tzeng, Judy Hoffman, Trevor Darrell, Kate Saenko, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision32Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Pro- ceedings of the IEEE international conference on computer vision, pages 4068-4076, 2015. 32\n\nAdversarial discriminative domain adaptation. Eric Tzeng, Judy Hoffman, Kate Saenko, Trevor Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition32Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 7167-7176, 2017. 32\n\n. Ashish Vaswani, Noam M Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you need. ArXiv, abs/1706.03762, 2017. 1Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. ArXiv, abs/1706.03762, 2017. 1\n\nDeep hashing network for unsupervised domain adaptation. Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, Sethuraman Panchanathan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5018-5027, 2017. 4\n\nPytorch image models. Ross Wightman, Ross Wightman. Pytorch image mod- els. https://github.com/rwightman/ pytorch-image-models, 2019. 4\n\nDual mixup regularized learning for adversarial domain adaptation. Yuan Wu, Diana Inkpen, Ahmed El-Roby, European Conference on Computer Vision. Springer33Yuan Wu, Diana Inkpen, and Ahmed El-Roby. Dual mixup regularized learning for adversarial domain adaptation. In European Conference on Computer Vision, pages 540-555. Springer, 2020. 33\n\nAdversarial domain adaptation with domain mixup. Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, Wenjun Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence3433Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, and Wenjun Zhang. Adversarial domain adaptation with domain mixup. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 6502- 6509, 2020. 33\n\nLarger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. Ruijia Xu, Guanbin Li, Jihan Yang, Liang Lin, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision33Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1426-1435, 2019. 33\n\nUniversal domain adaptation. Kaichao You, Mingsheng Long, Zhangjie Cao, Jianmin Wang, Michael I Jordan , Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition32Kaichao You, Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Universal domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2720-2729, 2019. 32\n\nTowards accurate model selection in deep unsupervised domain adaptation. Kaichao You, Ximei Wang, Mingsheng Long, Michael Jordan, PMLR, 09-15Proceedings of the 36th International Conference on Machine Learning. Kamalika Chaudhuri and Ruslan Salakhutdinovthe 36th International Conference on Machine Learning97Kaichao You, Ximei Wang, Mingsheng Long, and Michael Jordan. Towards accurate model selection in deep unsuper- vised domain adaptation. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 7124-7133. PMLR, 09-15 Jun 2019. 2\n\nDomainsymmetric networks for adversarial domain adaptation. Yabin Zhang, Hui Tang, Kui Jia, Mingkui Tan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition33Yabin Zhang, Hui Tang, Kui Jia, and Mingkui Tan. Domain- symmetric networks for adversarial domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 5031-5040, 2019. 33\n\nCross validation framework to choose amongst models and datasets for transfer learning. Erheng Zhong, Wei Fan, Qiang Yang, Olivier Verscheure, Jiangtao Ren, Jos\u00e9 Luis Balc\u00e1zar, Francesco Bonchi, Aristides Gionis, and Mich\u00e8le Sebag. Berlin, Heidelberg; Berlin HeidelbergSpringerMachine Learning and Knowledge Discovery in DatabasesErheng Zhong, Wei Fan, Qiang Yang, Olivier Verscheure, and Jiangtao Ren. Cross validation framework to choose amongst models and datasets for transfer learning. In Jos\u00e9 Luis Balc\u00e1zar, Francesco Bonchi, Aristides Gionis, and Mich\u00e8le Sebag, editors, Machine Learning and Knowledge Discov- ery in Databases, pages 547-562, Berlin, Heidelberg, 2010. Springer Berlin Heidelberg. 2\n", "annotations": {"author": "[{\"end\":137,\"start\":95},{\"end\":178,\"start\":138},{\"end\":221,\"start\":179},{\"end\":261,\"start\":222},{\"end\":297,\"start\":262}]", "publisher": null, "author_last_name": "[{\"end\":109,\"start\":101},{\"end\":150,\"start\":146},{\"end\":193,\"start\":185},{\"end\":233,\"start\":230},{\"end\":269,\"start\":267}]", "author_first_name": "[{\"end\":100,\"start\":95},{\"end\":145,\"start\":138},{\"end\":184,\"start\":179},{\"end\":229,\"start\":222},{\"end\":266,\"start\":262}]", "author_affiliation": "[{\"end\":136,\"start\":111},{\"end\":177,\"start\":152},{\"end\":220,\"start\":195},{\"end\":260,\"start\":235},{\"end\":296,\"start\":271}]", "title": "[{\"end\":92,\"start\":1},{\"end\":389,\"start\":298}]", "venue": null, "abstract": "[{\"end\":1711,\"start\":391}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1835,\"start\":1831},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":1838,\"start\":1835},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1857,\"start\":1853},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1860,\"start\":1857},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1886,\"start\":1882},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1889,\"start\":1886},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1906,\"start\":1903},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4301,\"start\":4297},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":5934,\"start\":5930},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5937,\"start\":5934},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":7323,\"start\":7319},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7940,\"start\":7937},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8124,\"start\":8120},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8758,\"start\":8754},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9334,\"start\":9330},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10093,\"start\":10090},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10860,\"start\":10857},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10876,\"start\":10872},{\"end\":12467,\"start\":12466},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12922,\"start\":12918},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12940,\"start\":12936},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":13010,\"start\":13006},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13090,\"start\":13086},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13093,\"start\":13090},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13329,\"start\":13325},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":13345,\"start\":13341},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13362,\"start\":13358},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13630,\"start\":13626},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13732,\"start\":13729},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14641,\"start\":14637},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14665,\"start\":14662},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14673,\"start\":14670},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14692,\"start\":14688},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14702,\"start\":14698},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14711,\"start\":14707},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14731,\"start\":14727},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14740,\"start\":14736},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14758,\"start\":14754},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14799,\"start\":14795},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16754,\"start\":16751},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19303,\"start\":19299},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24283,\"start\":24282},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24579,\"start\":24578},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":28156,\"start\":28152},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28180,\"start\":28176},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28215,\"start\":28211},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":28299,\"start\":28295},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28319,\"start\":28315},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28334,\"start\":28330},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28337,\"start\":28334},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28339,\"start\":28337},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":28356,\"start\":28352},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28375,\"start\":28371},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":28879,\"start\":28875},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":28890,\"start\":28886},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":28901,\"start\":28897},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":28912,\"start\":28908},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29027,\"start\":29023},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":29039,\"start\":29035},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29050,\"start\":29046},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29381,\"start\":29377},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29392,\"start\":29388},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":29509,\"start\":29505},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":29519,\"start\":29515},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29532,\"start\":29528},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29646,\"start\":29643},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":29821,\"start\":29817},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":29831,\"start\":29827},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":30039,\"start\":30035},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30051,\"start\":30047},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":30183,\"start\":30179},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":30194,\"start\":30190},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":30285,\"start\":30281},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30295,\"start\":30292},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":30309,\"start\":30305},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30319,\"start\":30315}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30579,\"start\":30321},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30702,\"start\":30580},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30993,\"start\":30703},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31407,\"start\":30994},{\"attributes\":{\"id\":\"fig_6\"},\"end\":32048,\"start\":31408},{\"attributes\":{\"id\":\"fig_7\"},\"end\":32787,\"start\":32049},{\"attributes\":{\"id\":\"fig_8\"},\"end\":32915,\"start\":32788},{\"attributes\":{\"id\":\"fig_9\"},\"end\":33032,\"start\":32916},{\"attributes\":{\"id\":\"fig_10\"},\"end\":33180,\"start\":33033},{\"attributes\":{\"id\":\"fig_11\"},\"end\":34114,\"start\":33181},{\"attributes\":{\"id\":\"fig_12\"},\"end\":34916,\"start\":34115},{\"attributes\":{\"id\":\"fig_14\"},\"end\":35717,\"start\":34917},{\"attributes\":{\"id\":\"fig_16\"},\"end\":36519,\"start\":35718},{\"attributes\":{\"id\":\"fig_18\"},\"end\":37051,\"start\":36520},{\"attributes\":{\"id\":\"fig_19\"},\"end\":37588,\"start\":37052},{\"attributes\":{\"id\":\"fig_21\"},\"end\":38171,\"start\":37589},{\"attributes\":{\"id\":\"fig_23\"},\"end\":38754,\"start\":38172},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38835,\"start\":38755},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39441,\"start\":38836},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":39904,\"start\":39442},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":44300,\"start\":39905},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":45143,\"start\":44301},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":46484,\"start\":45144},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":46757,\"start\":46485},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":47378,\"start\":46758},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":48182,\"start\":47379},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":48373,\"start\":48183},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":49101,\"start\":48374},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":49274,\"start\":49102},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":53352,\"start\":49275},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":58371,\"start\":53353},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":62113,\"start\":58372},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":66152,\"start\":62114},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":70199,\"start\":66153},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":73961,\"start\":70200},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":77713,\"start\":73962},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":81490,\"start\":77714},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":85955,\"start\":81491},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":90968,\"start\":85956},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":95408,\"start\":90969},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":98248,\"start\":95409},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":101060,\"start\":98249},{\"attributes\":{\"id\":\"tab_26\",\"type\":\"table\"},\"end\":103872,\"start\":101061},{\"attributes\":{\"id\":\"tab_27\",\"type\":\"table\"},\"end\":107618,\"start\":103873},{\"attributes\":{\"id\":\"tab_28\",\"type\":\"table\"},\"end\":111361,\"start\":107619},{\"attributes\":{\"id\":\"tab_29\",\"type\":\"table\"},\"end\":115090,\"start\":111362},{\"attributes\":{\"id\":\"tab_30\",\"type\":\"table\"},\"end\":118834,\"start\":115091},{\"attributes\":{\"id\":\"tab_31\",\"type\":\"table\"},\"end\":122570,\"start\":118835},{\"attributes\":{\"id\":\"tab_32\",\"type\":\"table\"},\"end\":126306,\"start\":122571},{\"attributes\":{\"id\":\"tab_33\",\"type\":\"table\"},\"end\":129434,\"start\":126307},{\"attributes\":{\"id\":\"tab_34\",\"type\":\"table\"},\"end\":132453,\"start\":129435},{\"attributes\":{\"id\":\"tab_35\",\"type\":\"table\"},\"end\":133482,\"start\":132454},{\"attributes\":{\"id\":\"tab_36\",\"type\":\"table\"},\"end\":136886,\"start\":133483}]", "paragraph": "[{\"end\":2102,\"start\":1727},{\"end\":2340,\"start\":2104},{\"end\":4094,\"start\":2342},{\"end\":4751,\"start\":4096},{\"end\":4833,\"start\":4753},{\"end\":4942,\"start\":4835},{\"end\":5147,\"start\":4944},{\"end\":5183,\"start\":5149},{\"end\":5297,\"start\":5185},{\"end\":5347,\"start\":5299},{\"end\":5393,\"start\":5349},{\"end\":5476,\"start\":5395},{\"end\":5573,\"start\":5516},{\"end\":5909,\"start\":5622},{\"end\":6667,\"start\":5911},{\"end\":6723,\"start\":6679},{\"end\":7286,\"start\":6789},{\"end\":7570,\"start\":7288},{\"end\":7924,\"start\":7619},{\"end\":8192,\"start\":7926},{\"end\":8715,\"start\":8228},{\"end\":9296,\"start\":8717},{\"end\":9403,\"start\":9298},{\"end\":9942,\"start\":9443},{\"end\":10050,\"start\":9961},{\"end\":10241,\"start\":10052},{\"end\":10861,\"start\":10260},{\"end\":11211,\"start\":10863},{\"end\":11254,\"start\":11213},{\"end\":11400,\"start\":11277},{\"end\":11610,\"start\":11434},{\"end\":11739,\"start\":11650},{\"end\":12009,\"start\":11766},{\"end\":12046,\"start\":12011},{\"end\":12336,\"start\":12048},{\"end\":12626,\"start\":12380},{\"end\":12721,\"start\":12628},{\"end\":12795,\"start\":12723},{\"end\":12875,\"start\":12835},{\"end\":12923,\"start\":12877},{\"end\":12991,\"start\":12925},{\"end\":13069,\"start\":12993},{\"end\":13156,\"start\":13071},{\"end\":13631,\"start\":13158},{\"end\":14265,\"start\":13633},{\"end\":14599,\"start\":14307},{\"end\":14939,\"start\":14613},{\"end\":15034,\"start\":14941},{\"end\":15225,\"start\":15036},{\"end\":15310,\"start\":15227},{\"end\":15622,\"start\":15312},{\"end\":16268,\"start\":15658},{\"end\":16658,\"start\":16270},{\"end\":16915,\"start\":16660},{\"end\":17358,\"start\":17065},{\"end\":17845,\"start\":17488},{\"end\":17981,\"start\":17976},{\"end\":18085,\"start\":17983},{\"end\":18198,\"start\":18087},{\"end\":18256,\"start\":18200},{\"end\":18509,\"start\":18258},{\"end\":18629,\"start\":18511},{\"end\":19183,\"start\":18683},{\"end\":19310,\"start\":19282},{\"end\":20015,\"start\":19312},{\"end\":20475,\"start\":20164},{\"end\":20619,\"start\":20477},{\"end\":20725,\"start\":20631},{\"end\":20819,\"start\":20727},{\"end\":20966,\"start\":20821},{\"end\":21118,\"start\":20968},{\"end\":21303,\"start\":21120},{\"end\":21783,\"start\":21305},{\"end\":22288,\"start\":21785},{\"end\":22368,\"start\":22303},{\"end\":22485,\"start\":22370},{\"end\":22820,\"start\":22487},{\"end\":23144,\"start\":22822},{\"end\":23419,\"start\":23146},{\"end\":23522,\"start\":23474},{\"end\":23572,\"start\":23533},{\"end\":23633,\"start\":23582},{\"end\":23862,\"start\":23642},{\"end\":24082,\"start\":23891},{\"end\":24183,\"start\":24084},{\"end\":24379,\"start\":24185},{\"end\":24476,\"start\":24381},{\"end\":24679,\"start\":24478},{\"end\":24973,\"start\":24681},{\"end\":27304,\"start\":25387},{\"end\":27764,\"start\":27362},{\"end\":28126,\"start\":27963},{\"end\":28275,\"start\":28128},{\"end\":28586,\"start\":28277},{\"end\":28845,\"start\":28588},{\"end\":29016,\"start\":28857},{\"end\":29361,\"start\":29018},{\"end\":29498,\"start\":29372},{\"end\":29628,\"start\":29500},{\"end\":29809,\"start\":29638},{\"end\":30027,\"start\":29811},{\"end\":30169,\"start\":30029},{\"end\":30265,\"start\":30171},{\"end\":30320,\"start\":30276}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5621,\"start\":5574},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6754,\"start\":6724},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6788,\"start\":6754},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7618,\"start\":7571},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8227,\"start\":8193},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9430,\"start\":9404},{\"attributes\":{\"id\":\"formula_6\"},\"end\":9442,\"start\":9430},{\"attributes\":{\"id\":\"formula_7\"},\"end\":10259,\"start\":10242},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11276,\"start\":11255},{\"attributes\":{\"id\":\"formula_9\"},\"end\":11649,\"start\":11611},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17064,\"start\":16916},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17487,\"start\":17359},{\"attributes\":{\"id\":\"formula_12\"},\"end\":17895,\"start\":17846},{\"attributes\":{\"id\":\"formula_13\"},\"end\":17944,\"start\":17895},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17975,\"start\":17944},{\"attributes\":{\"id\":\"formula_15\"},\"end\":18682,\"start\":18630},{\"attributes\":{\"id\":\"formula_16\"},\"end\":19281,\"start\":19184},{\"attributes\":{\"id\":\"formula_17\"},\"end\":20163,\"start\":20016},{\"attributes\":{\"id\":\"formula_18\"},\"end\":25323,\"start\":24974},{\"attributes\":{\"id\":\"formula_19\"},\"end\":25386,\"start\":25323},{\"attributes\":{\"id\":\"formula_20\"},\"end\":28856,\"start\":28846},{\"attributes\":{\"id\":\"formula_21\"},\"end\":29371,\"start\":29362},{\"attributes\":{\"id\":\"formula_22\"},\"end\":29637,\"start\":29629},{\"attributes\":{\"id\":\"formula_23\"},\"end\":30275,\"start\":30266}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":13596,\"start\":13589},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20817,\"start\":20810},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":21949,\"start\":21941},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22063,\"start\":22056},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22186,\"start\":22178},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23066,\"start\":23059},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27274,\"start\":26893}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1725,\"start\":1713},{\"attributes\":{\"n\":\"1.1.\"},\"end\":5514,\"start\":5479},{\"end\":6677,\"start\":6670},{\"attributes\":{\"n\":\"1.2.\"},\"end\":9959,\"start\":9945},{\"end\":11432,\"start\":11403},{\"attributes\":{\"n\":\"2.\"},\"end\":11764,\"start\":11742},{\"attributes\":{\"n\":\"2.\"},\"end\":12378,\"start\":12339},{\"attributes\":{\"n\":\"2.1.\"},\"end\":12833,\"start\":12798},{\"attributes\":{\"n\":\"2.2.\"},\"end\":14305,\"start\":14268},{\"end\":14611,\"start\":14602},{\"attributes\":{\"n\":\"2.3.\"},\"end\":15656,\"start\":15625},{\"attributes\":{\"n\":\"3.\"},\"end\":20629,\"start\":20622},{\"attributes\":{\"n\":\"4.\"},\"end\":22301,\"start\":22291},{\"end\":23455,\"start\":23422},{\"end\":23461,\"start\":23458},{\"end\":23472,\"start\":23464},{\"end\":23531,\"start\":23525},{\"end\":23580,\"start\":23575},{\"end\":23640,\"start\":23636},{\"end\":23889,\"start\":23865},{\"end\":27336,\"start\":27307},{\"end\":27360,\"start\":27339},{\"end\":27829,\"start\":27767},{\"end\":27883,\"start\":27832},{\"end\":27930,\"start\":27886},{\"end\":27961,\"start\":27933},{\"end\":30332,\"start\":30322},{\"end\":30591,\"start\":30581},{\"end\":30714,\"start\":30704},{\"end\":31429,\"start\":31409},{\"end\":32073,\"start\":32050},{\"end\":32799,\"start\":32789},{\"end\":33045,\"start\":33034},{\"end\":38765,\"start\":38756},{\"end\":39452,\"start\":39443},{\"end\":44311,\"start\":44302},{\"end\":45154,\"start\":45145},{\"end\":46495,\"start\":46486},{\"end\":46768,\"start\":46759},{\"end\":47389,\"start\":47380},{\"end\":48193,\"start\":48184},{\"end\":48384,\"start\":48375},{\"end\":49113,\"start\":49103},{\"end\":49286,\"start\":49276},{\"end\":53364,\"start\":53354},{\"end\":58383,\"start\":58373},{\"end\":62125,\"start\":62115},{\"end\":66164,\"start\":66154},{\"end\":70211,\"start\":70201},{\"end\":73973,\"start\":73963},{\"end\":77725,\"start\":77715},{\"end\":81502,\"start\":81492},{\"end\":85967,\"start\":85957},{\"end\":90980,\"start\":90970},{\"end\":95420,\"start\":95410},{\"end\":98260,\"start\":98250},{\"end\":101072,\"start\":101062},{\"end\":103884,\"start\":103874},{\"end\":107630,\"start\":107620},{\"end\":111373,\"start\":111363},{\"end\":115102,\"start\":115092},{\"end\":118846,\"start\":118836},{\"end\":122582,\"start\":122572},{\"end\":126318,\"start\":126308},{\"end\":129446,\"start\":129436},{\"end\":132465,\"start\":132455},{\"end\":133494,\"start\":133484}]", "table": "[{\"end\":39441,\"start\":39156},{\"end\":39904,\"start\":39454},{\"end\":44300,\"start\":39995},{\"end\":45143,\"start\":44313},{\"end\":46484,\"start\":45460},{\"end\":46757,\"start\":46546},{\"end\":47378,\"start\":47117},{\"end\":48182,\"start\":47544},{\"end\":48373,\"start\":48263},{\"end\":49101,\"start\":48450},{\"end\":49274,\"start\":49208},{\"end\":53352,\"start\":49392},{\"end\":58371,\"start\":53439},{\"end\":62113,\"start\":58456},{\"end\":66152,\"start\":62198},{\"end\":70199,\"start\":66238},{\"end\":73961,\"start\":70285},{\"end\":77713,\"start\":74046},{\"end\":81490,\"start\":77797},{\"end\":85955,\"start\":81575},{\"end\":90968,\"start\":86040},{\"end\":95408,\"start\":91053},{\"end\":98248,\"start\":95526},{\"end\":101060,\"start\":98335},{\"end\":103872,\"start\":101145},{\"end\":107618,\"start\":103957},{\"end\":111361,\"start\":107704},{\"end\":115090,\"start\":111447},{\"end\":118834,\"start\":115175},{\"end\":122570,\"start\":118918},{\"end\":126306,\"start\":122655},{\"end\":129434,\"start\":126391},{\"end\":132453,\"start\":129519},{\"end\":133482,\"start\":132750},{\"end\":136886,\"start\":133593}]", "figure_caption": "[{\"end\":30579,\"start\":30334},{\"end\":30702,\"start\":30593},{\"end\":30993,\"start\":30716},{\"end\":31407,\"start\":30996},{\"end\":32048,\"start\":31432},{\"end\":32787,\"start\":32077},{\"end\":32915,\"start\":32802},{\"end\":33032,\"start\":32918},{\"end\":33180,\"start\":33048},{\"end\":34114,\"start\":33183},{\"end\":34916,\"start\":34117},{\"end\":35717,\"start\":34919},{\"end\":36519,\"start\":35720},{\"end\":37051,\"start\":36522},{\"end\":37588,\"start\":37054},{\"end\":38171,\"start\":37591},{\"end\":38754,\"start\":38174},{\"end\":38835,\"start\":38767},{\"end\":39156,\"start\":38838},{\"end\":39995,\"start\":39907},{\"end\":45460,\"start\":45156},{\"end\":46546,\"start\":46497},{\"end\":47117,\"start\":46770},{\"end\":47544,\"start\":47391},{\"end\":48263,\"start\":48195},{\"end\":48450,\"start\":48386},{\"end\":49208,\"start\":49116},{\"end\":49392,\"start\":49289},{\"end\":53439,\"start\":53367},{\"end\":58456,\"start\":58386},{\"end\":62198,\"start\":62128},{\"end\":66238,\"start\":66167},{\"end\":70285,\"start\":70214},{\"end\":74046,\"start\":73976},{\"end\":77797,\"start\":77728},{\"end\":81575,\"start\":81505},{\"end\":86040,\"start\":85970},{\"end\":91053,\"start\":90983},{\"end\":95526,\"start\":95423},{\"end\":98335,\"start\":98263},{\"end\":101145,\"start\":101075},{\"end\":103957,\"start\":103887},{\"end\":107704,\"start\":107633},{\"end\":111447,\"start\":111376},{\"end\":115175,\"start\":115105},{\"end\":118918,\"start\":118849},{\"end\":122655,\"start\":122585},{\"end\":126391,\"start\":126321},{\"end\":129519,\"start\":129449},{\"end\":132750,\"start\":132468},{\"end\":133593,\"start\":133497}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16864,\"start\":16856},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19386,\"start\":19377},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19608,\"start\":19599},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19694,\"start\":19685},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20251,\"start\":20243},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20965,\"start\":20957},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21117,\"start\":21109},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21302,\"start\":21294},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21379,\"start\":21371},{\"end\":25596,\"start\":25588},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26387,\"start\":26378}]", "bib_author_first_name": "[{\"end\":142472,\"start\":142466},{\"end\":142487,\"start\":142480},{\"end\":142503,\"start\":142494},{\"end\":142518,\"start\":142512},{\"end\":142533,\"start\":142525},{\"end\":143101,\"start\":143097},{\"end\":143115,\"start\":143110},{\"end\":143126,\"start\":143122},{\"end\":143141,\"start\":143134},{\"end\":143155,\"start\":143147},{\"end\":143490,\"start\":143487},{\"end\":143492,\"start\":143491},{\"end\":143508,\"start\":143500},{\"end\":143519,\"start\":143515},{\"end\":143534,\"start\":143527},{\"end\":143549,\"start\":143544},{\"end\":143566,\"start\":143558},{\"end\":143583,\"start\":143577},{\"end\":143603,\"start\":143597},{\"end\":143617,\"start\":143611},{\"end\":143632,\"start\":143626},{\"end\":143649,\"start\":143641},{\"end\":143664,\"start\":143659},{\"end\":143687,\"start\":143679},{\"end\":143698,\"start\":143697},{\"end\":143700,\"start\":143699},{\"end\":143716,\"start\":143711},{\"end\":143730,\"start\":143724},{\"end\":143745,\"start\":143739},{\"end\":143747,\"start\":143746},{\"end\":143761,\"start\":143757},{\"end\":143773,\"start\":143766},{\"end\":143793,\"start\":143782},{\"end\":143805,\"start\":143801},{\"end\":143816,\"start\":143812},{\"end\":143832,\"start\":143825},{\"end\":144636,\"start\":144628},{\"end\":144647,\"start\":144642},{\"end\":144661,\"start\":144652},{\"end\":144675,\"start\":144668},{\"end\":145070,\"start\":145062},{\"end\":145086,\"start\":145078},{\"end\":145099,\"start\":145092},{\"end\":145109,\"start\":145105},{\"end\":145123,\"start\":145116},{\"end\":145644,\"start\":145637},{\"end\":145656,\"start\":145651},{\"end\":145672,\"start\":145663},{\"end\":145686,\"start\":145679},{\"end\":146138,\"start\":146129},{\"end\":146154,\"start\":146147},{\"end\":146173,\"start\":146165},{\"end\":146508,\"start\":146502},{\"end\":146520,\"start\":146514},{\"end\":146533,\"start\":146527},{\"end\":146545,\"start\":146540},{\"end\":146558,\"start\":146550},{\"end\":146568,\"start\":146566},{\"end\":147122,\"start\":147116},{\"end\":147134,\"start\":147128},{\"end\":147147,\"start\":147141},{\"end\":147159,\"start\":147154},{\"end\":147172,\"start\":147164},{\"end\":147182,\"start\":147180},{\"end\":147474,\"start\":147468},{\"end\":147486,\"start\":147480},{\"end\":147499,\"start\":147493},{\"end\":147509,\"start\":147506},{\"end\":147522,\"start\":147514},{\"end\":147532,\"start\":147530},{\"end\":148046,\"start\":148038},{\"end\":148062,\"start\":148054},{\"end\":148077,\"start\":148073},{\"end\":148092,\"start\":148086},{\"end\":148106,\"start\":148102},{\"end\":148127,\"start\":148119},{\"end\":148145,\"start\":148140},{\"end\":148162,\"start\":148156},{\"end\":148530,\"start\":148522},{\"end\":148547,\"start\":148540},{\"end\":148570,\"start\":148565},{\"end\":148581,\"start\":148578},{\"end\":148895,\"start\":148892},{\"end\":148912,\"start\":148908},{\"end\":148933,\"start\":148928},{\"end\":148945,\"start\":148941},{\"end\":148955,\"start\":148950},{\"end\":148977,\"start\":148970},{\"end\":148990,\"start\":148985},{\"end\":149008,\"start\":149002},{\"end\":149350,\"start\":149343},{\"end\":149362,\"start\":149355},{\"end\":149378,\"start\":149370},{\"end\":149388,\"start\":149384},{\"end\":149808,\"start\":149800},{\"end\":149817,\"start\":149813},{\"end\":149830,\"start\":149824},{\"end\":150114,\"start\":150110},{\"end\":150125,\"start\":150120},{\"end\":150141,\"start\":150132},{\"end\":150155,\"start\":150148},{\"end\":150452,\"start\":150451},{\"end\":150468,\"start\":150463},{\"end\":150648,\"start\":150641},{\"end\":150660,\"start\":150654},{\"end\":150676,\"start\":150668},{\"end\":150682,\"start\":150677},{\"end\":150695,\"start\":150689},{\"end\":151222,\"start\":151218},{\"end\":151236,\"start\":151230},{\"end\":151247,\"start\":151241},{\"end\":151621,\"start\":151617},{\"end\":151635,\"start\":151629},{\"end\":151646,\"start\":151640},{\"end\":152104,\"start\":152095},{\"end\":152114,\"start\":152111},{\"end\":152127,\"start\":152120},{\"end\":152141,\"start\":152134},{\"end\":152417,\"start\":152408},{\"end\":152432,\"start\":152424},{\"end\":152445,\"start\":152438},{\"end\":152468,\"start\":152452},{\"end\":152777,\"start\":152768},{\"end\":152787,\"start\":152784},{\"end\":152800,\"start\":152793},{\"end\":152823,\"start\":152807},{\"end\":153092,\"start\":153083},{\"end\":153102,\"start\":153099},{\"end\":153115,\"start\":153108},{\"end\":153138,\"start\":153122},{\"end\":153404,\"start\":153400},{\"end\":153422,\"start\":153417},{\"end\":153680,\"start\":153675},{\"end\":153692,\"start\":153685},{\"end\":153706,\"start\":153699},{\"end\":153716,\"start\":153712},{\"end\":153728,\"start\":153722},{\"end\":153738,\"start\":153735},{\"end\":154223,\"start\":154218},{\"end\":154239,\"start\":154234},{\"end\":154241,\"start\":154240},{\"end\":154259,\"start\":154252},{\"end\":154464,\"start\":154458},{\"end\":154471,\"start\":154470},{\"end\":154494,\"start\":154493},{\"end\":154496,\"start\":154495},{\"end\":154752,\"start\":154744},{\"end\":154765,\"start\":154758},{\"end\":155140,\"start\":155136},{\"end\":155152,\"start\":155149},{\"end\":155169,\"start\":155160},{\"end\":155181,\"start\":155177},{\"end\":155194,\"start\":155189},{\"end\":155212,\"start\":155205},{\"end\":155227,\"start\":155221},{\"end\":155243,\"start\":155237},{\"end\":155256,\"start\":155249},{\"end\":155273,\"start\":155269},{\"end\":155287,\"start\":155282},{\"end\":155306,\"start\":155299},{\"end\":155319,\"start\":155313},{\"end\":155333,\"start\":155326},{\"end\":155348,\"start\":155342},{\"end\":155364,\"start\":155357},{\"end\":155379,\"start\":155373},{\"end\":155400,\"start\":155394},{\"end\":155412,\"start\":155410},{\"end\":155425,\"start\":155419},{\"end\":155438,\"start\":155431},{\"end\":156257,\"start\":156249},{\"end\":156270,\"start\":156264},{\"end\":156280,\"start\":156276},{\"end\":156291,\"start\":156286},{\"end\":156303,\"start\":156299},{\"end\":156314,\"start\":156312},{\"end\":156795,\"start\":156790},{\"end\":156810,\"start\":156804},{\"end\":156825,\"start\":156818},{\"end\":156838,\"start\":156834},{\"end\":157308,\"start\":157304},{\"end\":157325,\"start\":157318},{\"end\":157857,\"start\":157853},{\"end\":157876,\"start\":157868},{\"end\":157900,\"start\":157895},{\"end\":157917,\"start\":157910},{\"end\":157931,\"start\":157926},{\"end\":157937,\"start\":157932},{\"end\":158188,\"start\":158184},{\"end\":158205,\"start\":158202},{\"end\":158215,\"start\":158212},{\"end\":158228,\"start\":158220},{\"end\":158244,\"start\":158237},{\"end\":158259,\"start\":158255},{\"end\":158271,\"start\":158264},{\"end\":158285,\"start\":158279},{\"end\":158302,\"start\":158296},{\"end\":158318,\"start\":158311},{\"end\":158701,\"start\":158697},{\"end\":158715,\"start\":158710},{\"end\":158728,\"start\":158723},{\"end\":158742,\"start\":158736},{\"end\":159049,\"start\":159042},{\"end\":159065,\"start\":159057},{\"end\":159075,\"start\":159071},{\"end\":159092,\"start\":159086},{\"end\":159106,\"start\":159102},{\"end\":159375,\"start\":159368},{\"end\":159391,\"start\":159383},{\"end\":159402,\"start\":159397},{\"end\":159417,\"start\":159413},{\"end\":159434,\"start\":159428},{\"end\":159448,\"start\":159444},{\"end\":159727,\"start\":159720},{\"end\":159744,\"start\":159735},{\"end\":159760,\"start\":159753},{\"end\":160093,\"start\":160086},{\"end\":160106,\"start\":160101},{\"end\":160126,\"start\":160117},{\"end\":160142,\"start\":160135},{\"end\":160603,\"start\":160596},{\"end\":160617,\"start\":160611},{\"end\":160637,\"start\":160628},{\"end\":160653,\"start\":160646},{\"end\":161067,\"start\":161062},{\"end\":161092,\"start\":161086},{\"end\":161102,\"start\":161101},{\"end\":161115,\"start\":161111},{\"end\":161644,\"start\":161640},{\"end\":161653,\"start\":161650},{\"end\":162256,\"start\":162253},{\"end\":162263,\"start\":162262},{\"end\":162278,\"start\":162270},{\"end\":162291,\"start\":162284},{\"end\":162668,\"start\":162662},{\"end\":162670,\"start\":162669},{\"end\":162686,\"start\":162678},{\"end\":162972,\"start\":162965},{\"end\":162984,\"start\":162978},{\"end\":162995,\"start\":162991},{\"end\":163369,\"start\":163364},{\"end\":163383,\"start\":163377},{\"end\":163401,\"start\":163394},{\"end\":163418,\"start\":163412},{\"end\":163682,\"start\":163678},{\"end\":163694,\"start\":163690},{\"end\":163710,\"start\":163704},{\"end\":163724,\"start\":163720},{\"end\":164119,\"start\":164115},{\"end\":164131,\"start\":164127},{\"end\":164145,\"start\":164141},{\"end\":164160,\"start\":164154},{\"end\":164537,\"start\":164531},{\"end\":164551,\"start\":164547},{\"end\":164553,\"start\":164552},{\"end\":164567,\"start\":164563},{\"end\":164581,\"start\":164576},{\"end\":164598,\"start\":164593},{\"end\":164611,\"start\":164606},{\"end\":164613,\"start\":164612},{\"end\":164627,\"start\":164621},{\"end\":164641,\"start\":164636},{\"end\":164963,\"start\":164956},{\"end\":164982,\"start\":164978},{\"end\":164998,\"start\":164992},{\"end\":165022,\"start\":165012},{\"end\":165454,\"start\":165450},{\"end\":165636,\"start\":165632},{\"end\":165646,\"start\":165641},{\"end\":165660,\"start\":165655},{\"end\":165963,\"start\":165956},{\"end\":165972,\"start\":165968},{\"end\":165988,\"start\":165980},{\"end\":165997,\"start\":165993},{\"end\":166010,\"start\":166002},{\"end\":166019,\"start\":166017},{\"end\":166032,\"start\":166026},{\"end\":166502,\"start\":166496},{\"end\":166514,\"start\":166507},{\"end\":166524,\"start\":166519},{\"end\":166536,\"start\":166531},{\"end\":166963,\"start\":166956},{\"end\":166978,\"start\":166969},{\"end\":166993,\"start\":166985},{\"end\":167006,\"start\":166999},{\"end\":167029,\"start\":167013},{\"end\":167484,\"start\":167477},{\"end\":167495,\"start\":167490},{\"end\":167511,\"start\":167502},{\"end\":167525,\"start\":167518},{\"end\":168138,\"start\":168133},{\"end\":168149,\"start\":168146},{\"end\":168159,\"start\":168156},{\"end\":168172,\"start\":168165},{\"end\":168649,\"start\":168643},{\"end\":168660,\"start\":168657},{\"end\":168671,\"start\":168666},{\"end\":168685,\"start\":168678},{\"end\":168706,\"start\":168698}]", "bib_author_last_name": "[{\"end\":142478,\"start\":142473},{\"end\":142492,\"start\":142488},{\"end\":142510,\"start\":142504},{\"end\":142523,\"start\":142519},{\"end\":142540,\"start\":142534},{\"end\":142548,\"start\":142542},{\"end\":143108,\"start\":143102},{\"end\":143120,\"start\":143116},{\"end\":143132,\"start\":143127},{\"end\":143145,\"start\":143142},{\"end\":143162,\"start\":143156},{\"end\":143498,\"start\":143493},{\"end\":143513,\"start\":143509},{\"end\":143525,\"start\":143520},{\"end\":143542,\"start\":143535},{\"end\":143556,\"start\":143550},{\"end\":143575,\"start\":143567},{\"end\":143595,\"start\":143584},{\"end\":143609,\"start\":143604},{\"end\":143624,\"start\":143618},{\"end\":143639,\"start\":143633},{\"end\":143657,\"start\":143650},{\"end\":143677,\"start\":143665},{\"end\":143695,\"start\":143688},{\"end\":143709,\"start\":143701},{\"end\":143722,\"start\":143717},{\"end\":143737,\"start\":143731},{\"end\":143755,\"start\":143748},{\"end\":143764,\"start\":143762},{\"end\":143780,\"start\":143774},{\"end\":143799,\"start\":143794},{\"end\":143810,\"start\":143806},{\"end\":143823,\"start\":143817},{\"end\":143839,\"start\":143833},{\"end\":144640,\"start\":144637},{\"end\":144650,\"start\":144648},{\"end\":144666,\"start\":144662},{\"end\":144680,\"start\":144676},{\"end\":145076,\"start\":145071},{\"end\":145090,\"start\":145087},{\"end\":145103,\"start\":145100},{\"end\":145114,\"start\":145110},{\"end\":145127,\"start\":145124},{\"end\":145649,\"start\":145645},{\"end\":145661,\"start\":145657},{\"end\":145677,\"start\":145673},{\"end\":145691,\"start\":145687},{\"end\":146145,\"start\":146139},{\"end\":146163,\"start\":146155},{\"end\":146181,\"start\":146174},{\"end\":146512,\"start\":146509},{\"end\":146525,\"start\":146521},{\"end\":146538,\"start\":146534},{\"end\":146548,\"start\":146546},{\"end\":146564,\"start\":146559},{\"end\":146573,\"start\":146569},{\"end\":147126,\"start\":147123},{\"end\":147139,\"start\":147135},{\"end\":147152,\"start\":147148},{\"end\":147162,\"start\":147160},{\"end\":147178,\"start\":147173},{\"end\":147187,\"start\":147183},{\"end\":147478,\"start\":147475},{\"end\":147491,\"start\":147487},{\"end\":147504,\"start\":147500},{\"end\":147512,\"start\":147510},{\"end\":147528,\"start\":147523},{\"end\":147537,\"start\":147533},{\"end\":148052,\"start\":148047},{\"end\":148071,\"start\":148063},{\"end\":148084,\"start\":148078},{\"end\":148100,\"start\":148093},{\"end\":148117,\"start\":148107},{\"end\":148138,\"start\":148128},{\"end\":148154,\"start\":148146},{\"end\":148172,\"start\":148163},{\"end\":148538,\"start\":148531},{\"end\":148563,\"start\":148548},{\"end\":148576,\"start\":148571},{\"end\":148590,\"start\":148582},{\"end\":148594,\"start\":148592},{\"end\":148866,\"start\":148858},{\"end\":148906,\"start\":148896},{\"end\":148926,\"start\":148913},{\"end\":148939,\"start\":148934},{\"end\":148948,\"start\":148946},{\"end\":148968,\"start\":148956},{\"end\":148983,\"start\":148978},{\"end\":149000,\"start\":148991},{\"end\":149015,\"start\":149009},{\"end\":149353,\"start\":149351},{\"end\":149368,\"start\":149363},{\"end\":149382,\"start\":149379},{\"end\":149392,\"start\":149389},{\"end\":149811,\"start\":149809},{\"end\":149822,\"start\":149818},{\"end\":149837,\"start\":149831},{\"end\":150118,\"start\":150115},{\"end\":150130,\"start\":150126},{\"end\":150146,\"start\":150142},{\"end\":150160,\"start\":150156},{\"end\":150461,\"start\":150453},{\"end\":150475,\"start\":150469},{\"end\":150479,\"start\":150477},{\"end\":150652,\"start\":150649},{\"end\":150666,\"start\":150661},{\"end\":150687,\"start\":150683},{\"end\":150704,\"start\":150696},{\"end\":151228,\"start\":151223},{\"end\":151239,\"start\":151237},{\"end\":151252,\"start\":151248},{\"end\":151627,\"start\":151622},{\"end\":151638,\"start\":151636},{\"end\":151651,\"start\":151647},{\"end\":152109,\"start\":152105},{\"end\":152118,\"start\":152115},{\"end\":152132,\"start\":152128},{\"end\":152148,\"start\":152142},{\"end\":152422,\"start\":152418},{\"end\":152436,\"start\":152433},{\"end\":152450,\"start\":152446},{\"end\":152782,\"start\":152778},{\"end\":152791,\"start\":152788},{\"end\":152805,\"start\":152801},{\"end\":153097,\"start\":153093},{\"end\":153106,\"start\":153103},{\"end\":153120,\"start\":153116},{\"end\":153415,\"start\":153405},{\"end\":153429,\"start\":153423},{\"end\":153435,\"start\":153431},{\"end\":153683,\"start\":153681},{\"end\":153697,\"start\":153693},{\"end\":153710,\"start\":153707},{\"end\":153720,\"start\":153717},{\"end\":153733,\"start\":153729},{\"end\":153744,\"start\":153739},{\"end\":154232,\"start\":154224},{\"end\":154250,\"start\":154242},{\"end\":154263,\"start\":154260},{\"end\":154468,\"start\":154465},{\"end\":154482,\"start\":154472},{\"end\":154491,\"start\":154484},{\"end\":154505,\"start\":154497},{\"end\":154512,\"start\":154507},{\"end\":154756,\"start\":154753},{\"end\":154771,\"start\":154766},{\"end\":154777,\"start\":154773},{\"end\":155147,\"start\":155141},{\"end\":155158,\"start\":155153},{\"end\":155175,\"start\":155170},{\"end\":155187,\"start\":155182},{\"end\":155203,\"start\":155195},{\"end\":155219,\"start\":155213},{\"end\":155235,\"start\":155228},{\"end\":155247,\"start\":155244},{\"end\":155267,\"start\":155257},{\"end\":155280,\"start\":155274},{\"end\":155297,\"start\":155288},{\"end\":155311,\"start\":155307},{\"end\":155324,\"start\":155320},{\"end\":155340,\"start\":155334},{\"end\":155355,\"start\":155349},{\"end\":155371,\"start\":155365},{\"end\":155392,\"start\":155380},{\"end\":155408,\"start\":155401},{\"end\":155417,\"start\":155413},{\"end\":155429,\"start\":155426},{\"end\":155447,\"start\":155439},{\"end\":156262,\"start\":156258},{\"end\":156274,\"start\":156271},{\"end\":156284,\"start\":156281},{\"end\":156297,\"start\":156292},{\"end\":156310,\"start\":156304},{\"end\":156319,\"start\":156315},{\"end\":156802,\"start\":156796},{\"end\":156816,\"start\":156811},{\"end\":156832,\"start\":156826},{\"end\":156846,\"start\":156839},{\"end\":157316,\"start\":157309},{\"end\":157331,\"start\":157326},{\"end\":157866,\"start\":157858},{\"end\":157893,\"start\":157877},{\"end\":157908,\"start\":157901},{\"end\":157924,\"start\":157918},{\"end\":157946,\"start\":157938},{\"end\":158200,\"start\":158189},{\"end\":158210,\"start\":158206},{\"end\":158218,\"start\":158216},{\"end\":158235,\"start\":158229},{\"end\":158253,\"start\":158245},{\"end\":158262,\"start\":158260},{\"end\":158277,\"start\":158272},{\"end\":158294,\"start\":158286},{\"end\":158309,\"start\":158303},{\"end\":158328,\"start\":158319},{\"end\":158708,\"start\":158702},{\"end\":158721,\"start\":158716},{\"end\":158734,\"start\":158729},{\"end\":158750,\"start\":158743},{\"end\":159055,\"start\":159050},{\"end\":159069,\"start\":159066},{\"end\":159084,\"start\":159076},{\"end\":159100,\"start\":159093},{\"end\":159113,\"start\":159107},{\"end\":159381,\"start\":159376},{\"end\":159395,\"start\":159392},{\"end\":159411,\"start\":159403},{\"end\":159426,\"start\":159418},{\"end\":159442,\"start\":159435},{\"end\":159455,\"start\":159449},{\"end\":159733,\"start\":159728},{\"end\":159751,\"start\":159745},{\"end\":159767,\"start\":159761},{\"end\":160099,\"start\":160094},{\"end\":160115,\"start\":160107},{\"end\":160133,\"start\":160127},{\"end\":160149,\"start\":160143},{\"end\":160609,\"start\":160604},{\"end\":160626,\"start\":160618},{\"end\":160644,\"start\":160638},{\"end\":160660,\"start\":160654},{\"end\":161084,\"start\":161068},{\"end\":161099,\"start\":161093},{\"end\":161109,\"start\":161103},{\"end\":161124,\"start\":161116},{\"end\":161135,\"start\":161126},{\"end\":161648,\"start\":161645},{\"end\":161657,\"start\":161654},{\"end\":162260,\"start\":162257},{\"end\":162268,\"start\":162264},{\"end\":162282,\"start\":162279},{\"end\":162297,\"start\":162292},{\"end\":162304,\"start\":162299},{\"end\":162676,\"start\":162671},{\"end\":162692,\"start\":162687},{\"end\":162976,\"start\":162973},{\"end\":162989,\"start\":162985},{\"end\":163002,\"start\":162996},{\"end\":163375,\"start\":163370},{\"end\":163392,\"start\":163384},{\"end\":163410,\"start\":163402},{\"end\":163428,\"start\":163419},{\"end\":163688,\"start\":163683},{\"end\":163702,\"start\":163695},{\"end\":163718,\"start\":163711},{\"end\":163731,\"start\":163725},{\"end\":164125,\"start\":164120},{\"end\":164139,\"start\":164132},{\"end\":164152,\"start\":164146},{\"end\":164168,\"start\":164161},{\"end\":164545,\"start\":164538},{\"end\":164561,\"start\":164554},{\"end\":164574,\"start\":164568},{\"end\":164591,\"start\":164582},{\"end\":164604,\"start\":164599},{\"end\":164619,\"start\":164614},{\"end\":164634,\"start\":164628},{\"end\":164652,\"start\":164642},{\"end\":164976,\"start\":164964},{\"end\":164990,\"start\":164983},{\"end\":165010,\"start\":164999},{\"end\":165035,\"start\":165023},{\"end\":165463,\"start\":165455},{\"end\":165639,\"start\":165637},{\"end\":165653,\"start\":165647},{\"end\":165668,\"start\":165661},{\"end\":165966,\"start\":165964},{\"end\":165978,\"start\":165973},{\"end\":165991,\"start\":165989},{\"end\":166000,\"start\":165998},{\"end\":166015,\"start\":166011},{\"end\":166024,\"start\":166020},{\"end\":166038,\"start\":166033},{\"end\":166505,\"start\":166503},{\"end\":166517,\"start\":166515},{\"end\":166529,\"start\":166525},{\"end\":166540,\"start\":166537},{\"end\":166967,\"start\":166964},{\"end\":166983,\"start\":166979},{\"end\":166997,\"start\":166994},{\"end\":167011,\"start\":167007},{\"end\":167488,\"start\":167485},{\"end\":167500,\"start\":167496},{\"end\":167516,\"start\":167512},{\"end\":167532,\"start\":167526},{\"end\":168144,\"start\":168139},{\"end\":168154,\"start\":168150},{\"end\":168163,\"start\":168160},{\"end\":168176,\"start\":168173},{\"end\":168655,\"start\":168650},{\"end\":168664,\"start\":168661},{\"end\":168676,\"start\":168672},{\"end\":168696,\"start\":168686},{\"end\":168710,\"start\":168707}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":142926,\"start\":142466},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":127534943},\"end\":143483,\"start\":142928},{\"attributes\":{\"id\":\"b2\"},\"end\":144587,\"start\":143485},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":51980671},\"end\":144988,\"start\":144589},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":150369467},\"end\":145534,\"start\":144990},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b5\",\"matched_paper_id\":174799862},\"end\":145992,\"start\":145536},{\"attributes\":{\"id\":\"b6\"},\"end\":146391,\"start\":145994},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":214693204},\"end\":147030,\"start\":146393},{\"attributes\":{\"doi\":\"abs/2107.06154\",\"id\":\"b8\",\"matched_paper_id\":235829479},\"end\":147404,\"start\":147032},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":214714367},\"end\":147946,\"start\":147406},{\"attributes\":{\"id\":\"b10\"},\"end\":148441,\"start\":147948},{\"attributes\":{\"id\":\"b11\"},\"end\":148854,\"start\":148443},{\"attributes\":{\"id\":\"b12\"},\"end\":148888,\"start\":148856},{\"attributes\":{\"id\":\"b13\"},\"end\":149295,\"start\":148890},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206594692},\"end\":149756,\"start\":149297},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":219955663},\"end\":150051,\"start\":149758},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":216540785},\"end\":150405,\"start\":150053},{\"attributes\":{\"id\":\"b17\"},\"end\":150572,\"start\":150407},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":73728950},\"end\":151108,\"start\":150574},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":211205159},\"end\":151547,\"start\":151110},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":229171234},\"end\":152031,\"start\":151549},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b21\",\"matched_paper_id\":556999},\"end\":152406,\"start\":152033},{\"attributes\":{\"doi\":\"arXiv:1705.10667\",\"id\":\"b22\"},\"end\":152702,\"start\":152408},{\"attributes\":{\"doi\":\"arXiv:1602.04433\",\"id\":\"b23\"},\"end\":153026,\"start\":152704},{\"attributes\":{\"doi\":\"PMLR, 2017. 32\",\"id\":\"b24\",\"matched_paper_id\":12757870},\"end\":153398,\"start\":153028},{\"attributes\":{\"id\":\"b25\"},\"end\":153614,\"start\":153400},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":219630841},\"end\":154144,\"start\":153616},{\"attributes\":{\"id\":\"b27\"},\"end\":154396,\"start\":154146},{\"attributes\":{\"doi\":\"arXiv:2105.13502\",\"id\":\"b28\"},\"end\":154714,\"start\":154398},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":19598463},\"end\":155064,\"start\":154716},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":202786778},\"end\":156195,\"start\":155066},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":54458071},\"end\":156687,\"start\":156197},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":229340471},\"end\":157247,\"start\":156689},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":219176513},\"end\":157776,\"start\":157249},{\"attributes\":{\"id\":\"b34\"},\"end\":158131,\"start\":157778},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2930547},\"end\":158647,\"start\":158133},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":7534823},\"end\":158985,\"start\":158649},{\"attributes\":{\"id\":\"b37\"},\"end\":159267,\"start\":158987},{\"attributes\":{\"id\":\"b38\"},\"end\":159658,\"start\":159269},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b39\",\"matched_paper_id\":12570770},\"end\":160017,\"start\":159660},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":4619542},\"end\":160547,\"start\":160019},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":13752992},\"end\":160985,\"start\":160549},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4547917},\"end\":161542,\"start\":160987},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":2346915},\"end\":162198,\"start\":161544},{\"attributes\":{\"doi\":\"arXiv:1802.08735\",\"id\":\"b44\"},\"end\":162488,\"start\":162200},{\"attributes\":{\"id\":\"b45\"},\"end\":162915,\"start\":162490},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":16439870},\"end\":163295,\"start\":162917},{\"attributes\":{\"doi\":\"2020. 32\",\"id\":\"b47\"},\"end\":163623,\"start\":163297},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":2655115},\"end\":164067,\"start\":163625},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":4357800},\"end\":164527,\"start\":164069},{\"attributes\":{\"id\":\"b50\"},\"end\":164897,\"start\":164529},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":2928248},\"end\":165426,\"start\":164899},{\"attributes\":{\"id\":\"b52\"},\"end\":165563,\"start\":165428},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":220381573},\"end\":165905,\"start\":165565},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":208617520},\"end\":166393,\"start\":165907},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":199405326},\"end\":166925,\"start\":166395},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":157060790},\"end\":167402,\"start\":166927},{\"attributes\":{\"doi\":\"PMLR, 09-15\",\"id\":\"b57\",\"matched_paper_id\":174800461},\"end\":168071,\"start\":167404},{\"attributes\":{\"id\":\"b58\"},\"end\":168553,\"start\":168073},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":13322062},\"end\":169260,\"start\":168555}]", "bib_title": "[{\"end\":143095,\"start\":142928},{\"end\":144626,\"start\":144589},{\"end\":145060,\"start\":144990},{\"end\":145635,\"start\":145536},{\"end\":146500,\"start\":146393},{\"end\":147114,\"start\":147032},{\"end\":147466,\"start\":147406},{\"end\":148520,\"start\":148443},{\"end\":149341,\"start\":149297},{\"end\":149798,\"start\":149758},{\"end\":150108,\"start\":150053},{\"end\":150639,\"start\":150574},{\"end\":151216,\"start\":151110},{\"end\":151615,\"start\":151549},{\"end\":152093,\"start\":152033},{\"end\":153081,\"start\":153028},{\"end\":153673,\"start\":153616},{\"end\":154742,\"start\":154716},{\"end\":155134,\"start\":155066},{\"end\":156247,\"start\":156197},{\"end\":156788,\"start\":156689},{\"end\":157302,\"start\":157249},{\"end\":158182,\"start\":158133},{\"end\":158695,\"start\":158649},{\"end\":159718,\"start\":159660},{\"end\":160084,\"start\":160019},{\"end\":160594,\"start\":160549},{\"end\":161060,\"start\":160987},{\"end\":161638,\"start\":161544},{\"end\":162963,\"start\":162917},{\"end\":163676,\"start\":163625},{\"end\":164113,\"start\":164069},{\"end\":164954,\"start\":164899},{\"end\":165630,\"start\":165565},{\"end\":165954,\"start\":165907},{\"end\":166494,\"start\":166395},{\"end\":166954,\"start\":166927},{\"end\":167475,\"start\":167404},{\"end\":168131,\"start\":168073},{\"end\":168641,\"start\":168555}]", "bib_author": "[{\"end\":142480,\"start\":142466},{\"end\":142494,\"start\":142480},{\"end\":142512,\"start\":142494},{\"end\":142525,\"start\":142512},{\"end\":142542,\"start\":142525},{\"end\":142550,\"start\":142542},{\"end\":143110,\"start\":143097},{\"end\":143122,\"start\":143110},{\"end\":143134,\"start\":143122},{\"end\":143147,\"start\":143134},{\"end\":143164,\"start\":143147},{\"end\":143500,\"start\":143487},{\"end\":143515,\"start\":143500},{\"end\":143527,\"start\":143515},{\"end\":143544,\"start\":143527},{\"end\":143558,\"start\":143544},{\"end\":143577,\"start\":143558},{\"end\":143597,\"start\":143577},{\"end\":143611,\"start\":143597},{\"end\":143626,\"start\":143611},{\"end\":143641,\"start\":143626},{\"end\":143659,\"start\":143641},{\"end\":143679,\"start\":143659},{\"end\":143697,\"start\":143679},{\"end\":143711,\"start\":143697},{\"end\":143724,\"start\":143711},{\"end\":143739,\"start\":143724},{\"end\":143757,\"start\":143739},{\"end\":143766,\"start\":143757},{\"end\":143782,\"start\":143766},{\"end\":143801,\"start\":143782},{\"end\":143812,\"start\":143801},{\"end\":143825,\"start\":143812},{\"end\":143841,\"start\":143825},{\"end\":144642,\"start\":144628},{\"end\":144652,\"start\":144642},{\"end\":144668,\"start\":144652},{\"end\":144682,\"start\":144668},{\"end\":145078,\"start\":145062},{\"end\":145092,\"start\":145078},{\"end\":145105,\"start\":145092},{\"end\":145116,\"start\":145105},{\"end\":145129,\"start\":145116},{\"end\":145651,\"start\":145637},{\"end\":145663,\"start\":145651},{\"end\":145679,\"start\":145663},{\"end\":145693,\"start\":145679},{\"end\":146147,\"start\":146129},{\"end\":146165,\"start\":146147},{\"end\":146183,\"start\":146165},{\"end\":146514,\"start\":146502},{\"end\":146527,\"start\":146514},{\"end\":146540,\"start\":146527},{\"end\":146550,\"start\":146540},{\"end\":146566,\"start\":146550},{\"end\":146575,\"start\":146566},{\"end\":147128,\"start\":147116},{\"end\":147141,\"start\":147128},{\"end\":147154,\"start\":147141},{\"end\":147164,\"start\":147154},{\"end\":147180,\"start\":147164},{\"end\":147189,\"start\":147180},{\"end\":147480,\"start\":147468},{\"end\":147493,\"start\":147480},{\"end\":147506,\"start\":147493},{\"end\":147514,\"start\":147506},{\"end\":147530,\"start\":147514},{\"end\":147539,\"start\":147530},{\"end\":148054,\"start\":148038},{\"end\":148073,\"start\":148054},{\"end\":148086,\"start\":148073},{\"end\":148102,\"start\":148086},{\"end\":148119,\"start\":148102},{\"end\":148140,\"start\":148119},{\"end\":148156,\"start\":148140},{\"end\":148174,\"start\":148156},{\"end\":148540,\"start\":148522},{\"end\":148565,\"start\":148540},{\"end\":148578,\"start\":148565},{\"end\":148592,\"start\":148578},{\"end\":148596,\"start\":148592},{\"end\":148868,\"start\":148858},{\"end\":148908,\"start\":148892},{\"end\":148928,\"start\":148908},{\"end\":148941,\"start\":148928},{\"end\":148950,\"start\":148941},{\"end\":148970,\"start\":148950},{\"end\":148985,\"start\":148970},{\"end\":149002,\"start\":148985},{\"end\":149017,\"start\":149002},{\"end\":149355,\"start\":149343},{\"end\":149370,\"start\":149355},{\"end\":149384,\"start\":149370},{\"end\":149394,\"start\":149384},{\"end\":149813,\"start\":149800},{\"end\":149824,\"start\":149813},{\"end\":149839,\"start\":149824},{\"end\":150120,\"start\":150110},{\"end\":150132,\"start\":150120},{\"end\":150148,\"start\":150132},{\"end\":150162,\"start\":150148},{\"end\":150463,\"start\":150451},{\"end\":150477,\"start\":150463},{\"end\":150481,\"start\":150477},{\"end\":150654,\"start\":150641},{\"end\":150668,\"start\":150654},{\"end\":150689,\"start\":150668},{\"end\":150706,\"start\":150689},{\"end\":151230,\"start\":151218},{\"end\":151241,\"start\":151230},{\"end\":151254,\"start\":151241},{\"end\":151629,\"start\":151617},{\"end\":151640,\"start\":151629},{\"end\":151653,\"start\":151640},{\"end\":152111,\"start\":152095},{\"end\":152120,\"start\":152111},{\"end\":152134,\"start\":152120},{\"end\":152150,\"start\":152134},{\"end\":152424,\"start\":152408},{\"end\":152438,\"start\":152424},{\"end\":152452,\"start\":152438},{\"end\":152471,\"start\":152452},{\"end\":152784,\"start\":152768},{\"end\":152793,\"start\":152784},{\"end\":152807,\"start\":152793},{\"end\":152826,\"start\":152807},{\"end\":153099,\"start\":153083},{\"end\":153108,\"start\":153099},{\"end\":153122,\"start\":153108},{\"end\":153141,\"start\":153122},{\"end\":153417,\"start\":153400},{\"end\":153431,\"start\":153417},{\"end\":153437,\"start\":153431},{\"end\":153685,\"start\":153675},{\"end\":153699,\"start\":153685},{\"end\":153712,\"start\":153699},{\"end\":153722,\"start\":153712},{\"end\":153735,\"start\":153722},{\"end\":153746,\"start\":153735},{\"end\":154234,\"start\":154218},{\"end\":154252,\"start\":154234},{\"end\":154265,\"start\":154252},{\"end\":154470,\"start\":154458},{\"end\":154484,\"start\":154470},{\"end\":154493,\"start\":154484},{\"end\":154507,\"start\":154493},{\"end\":154514,\"start\":154507},{\"end\":154758,\"start\":154744},{\"end\":154773,\"start\":154758},{\"end\":154779,\"start\":154773},{\"end\":155149,\"start\":155136},{\"end\":155160,\"start\":155149},{\"end\":155177,\"start\":155160},{\"end\":155189,\"start\":155177},{\"end\":155205,\"start\":155189},{\"end\":155221,\"start\":155205},{\"end\":155237,\"start\":155221},{\"end\":155249,\"start\":155237},{\"end\":155269,\"start\":155249},{\"end\":155282,\"start\":155269},{\"end\":155299,\"start\":155282},{\"end\":155313,\"start\":155299},{\"end\":155326,\"start\":155313},{\"end\":155342,\"start\":155326},{\"end\":155357,\"start\":155342},{\"end\":155373,\"start\":155357},{\"end\":155394,\"start\":155373},{\"end\":155410,\"start\":155394},{\"end\":155419,\"start\":155410},{\"end\":155431,\"start\":155419},{\"end\":155449,\"start\":155431},{\"end\":156264,\"start\":156249},{\"end\":156276,\"start\":156264},{\"end\":156286,\"start\":156276},{\"end\":156299,\"start\":156286},{\"end\":156312,\"start\":156299},{\"end\":156321,\"start\":156312},{\"end\":156804,\"start\":156790},{\"end\":156818,\"start\":156804},{\"end\":156834,\"start\":156818},{\"end\":156848,\"start\":156834},{\"end\":157318,\"start\":157304},{\"end\":157333,\"start\":157318},{\"end\":157868,\"start\":157853},{\"end\":157895,\"start\":157868},{\"end\":157910,\"start\":157895},{\"end\":157926,\"start\":157910},{\"end\":157948,\"start\":157926},{\"end\":158202,\"start\":158184},{\"end\":158212,\"start\":158202},{\"end\":158220,\"start\":158212},{\"end\":158237,\"start\":158220},{\"end\":158255,\"start\":158237},{\"end\":158264,\"start\":158255},{\"end\":158279,\"start\":158264},{\"end\":158296,\"start\":158279},{\"end\":158311,\"start\":158296},{\"end\":158330,\"start\":158311},{\"end\":158710,\"start\":158697},{\"end\":158723,\"start\":158710},{\"end\":158736,\"start\":158723},{\"end\":158752,\"start\":158736},{\"end\":159057,\"start\":159042},{\"end\":159071,\"start\":159057},{\"end\":159086,\"start\":159071},{\"end\":159102,\"start\":159086},{\"end\":159115,\"start\":159102},{\"end\":159383,\"start\":159368},{\"end\":159397,\"start\":159383},{\"end\":159413,\"start\":159397},{\"end\":159428,\"start\":159413},{\"end\":159444,\"start\":159428},{\"end\":159457,\"start\":159444},{\"end\":159735,\"start\":159720},{\"end\":159753,\"start\":159735},{\"end\":159769,\"start\":159753},{\"end\":160101,\"start\":160086},{\"end\":160117,\"start\":160101},{\"end\":160135,\"start\":160117},{\"end\":160151,\"start\":160135},{\"end\":160611,\"start\":160596},{\"end\":160628,\"start\":160611},{\"end\":160646,\"start\":160628},{\"end\":160662,\"start\":160646},{\"end\":161086,\"start\":161062},{\"end\":161101,\"start\":161086},{\"end\":161111,\"start\":161101},{\"end\":161126,\"start\":161111},{\"end\":161137,\"start\":161126},{\"end\":161650,\"start\":161640},{\"end\":161659,\"start\":161650},{\"end\":162262,\"start\":162253},{\"end\":162270,\"start\":162262},{\"end\":162284,\"start\":162270},{\"end\":162299,\"start\":162284},{\"end\":162306,\"start\":162299},{\"end\":162678,\"start\":162662},{\"end\":162694,\"start\":162678},{\"end\":162978,\"start\":162965},{\"end\":162991,\"start\":162978},{\"end\":163004,\"start\":162991},{\"end\":163377,\"start\":163364},{\"end\":163394,\"start\":163377},{\"end\":163412,\"start\":163394},{\"end\":163430,\"start\":163412},{\"end\":163690,\"start\":163678},{\"end\":163704,\"start\":163690},{\"end\":163720,\"start\":163704},{\"end\":163733,\"start\":163720},{\"end\":164127,\"start\":164115},{\"end\":164141,\"start\":164127},{\"end\":164154,\"start\":164141},{\"end\":164170,\"start\":164154},{\"end\":164547,\"start\":164531},{\"end\":164563,\"start\":164547},{\"end\":164576,\"start\":164563},{\"end\":164593,\"start\":164576},{\"end\":164606,\"start\":164593},{\"end\":164621,\"start\":164606},{\"end\":164636,\"start\":164621},{\"end\":164654,\"start\":164636},{\"end\":164978,\"start\":164956},{\"end\":164992,\"start\":164978},{\"end\":165012,\"start\":164992},{\"end\":165037,\"start\":165012},{\"end\":165465,\"start\":165450},{\"end\":165641,\"start\":165632},{\"end\":165655,\"start\":165641},{\"end\":165670,\"start\":165655},{\"end\":165968,\"start\":165956},{\"end\":165980,\"start\":165968},{\"end\":165993,\"start\":165980},{\"end\":166002,\"start\":165993},{\"end\":166017,\"start\":166002},{\"end\":166026,\"start\":166017},{\"end\":166040,\"start\":166026},{\"end\":166507,\"start\":166496},{\"end\":166519,\"start\":166507},{\"end\":166531,\"start\":166519},{\"end\":166542,\"start\":166531},{\"end\":166969,\"start\":166956},{\"end\":166985,\"start\":166969},{\"end\":166999,\"start\":166985},{\"end\":167013,\"start\":166999},{\"end\":167032,\"start\":167013},{\"end\":167490,\"start\":167477},{\"end\":167502,\"start\":167490},{\"end\":167518,\"start\":167502},{\"end\":167534,\"start\":167518},{\"end\":168146,\"start\":168133},{\"end\":168156,\"start\":168146},{\"end\":168165,\"start\":168156},{\"end\":168178,\"start\":168165},{\"end\":168657,\"start\":168643},{\"end\":168666,\"start\":168657},{\"end\":168678,\"start\":168666},{\"end\":168698,\"start\":168678},{\"end\":168712,\"start\":168698}]", "bib_venue": "[{\"end\":142729,\"start\":142648},{\"end\":144797,\"start\":144748},{\"end\":145278,\"start\":145212},{\"end\":146724,\"start\":146658},{\"end\":147688,\"start\":147622},{\"end\":149549,\"start\":149480},{\"end\":150855,\"start\":150789},{\"end\":151802,\"start\":151736},{\"end\":153895,\"start\":153829},{\"end\":154900,\"start\":154848},{\"end\":156450,\"start\":156394},{\"end\":156977,\"start\":156921},{\"end\":157490,\"start\":157412},{\"end\":160292,\"start\":160230},{\"end\":160777,\"start\":160728},{\"end\":161278,\"start\":161216},{\"end\":161870,\"start\":161780},{\"end\":163113,\"start\":163067},{\"end\":163854,\"start\":163802},{\"end\":164311,\"start\":164249},{\"end\":165178,\"start\":165116},{\"end\":166149,\"start\":166103},{\"end\":166671,\"start\":166615},{\"end\":167181,\"start\":167115},{\"end\":167711,\"start\":167658},{\"end\":168327,\"start\":168261},{\"end\":168824,\"start\":168787},{\"end\":142646,\"start\":142550},{\"end\":143196,\"start\":143164},{\"end\":144746,\"start\":144682},{\"end\":145210,\"start\":145129},{\"end\":145741,\"start\":145697},{\"end\":146127,\"start\":145994},{\"end\":146656,\"start\":146575},{\"end\":147208,\"start\":147203},{\"end\":147620,\"start\":147539},{\"end\":148036,\"start\":147948},{\"end\":148634,\"start\":148596},{\"end\":149075,\"start\":149017},{\"end\":149478,\"start\":149394},{\"end\":149888,\"start\":149839},{\"end\":150200,\"start\":150162},{\"end\":150449,\"start\":150407},{\"end\":150787,\"start\":150706},{\"end\":151305,\"start\":151254},{\"end\":151734,\"start\":151653},{\"end\":152198,\"start\":152154},{\"end\":152528,\"start\":152487},{\"end\":152766,\"start\":152704},{\"end\":153199,\"start\":153155},{\"end\":153500,\"start\":153437},{\"end\":153827,\"start\":153746},{\"end\":154216,\"start\":154146},{\"end\":154456,\"start\":154398},{\"end\":154846,\"start\":154779},{\"end\":155498,\"start\":155449},{\"end\":156392,\"start\":156321},{\"end\":156919,\"start\":156848},{\"end\":157410,\"start\":157333},{\"end\":157851,\"start\":157778},{\"end\":158370,\"start\":158330},{\"end\":158790,\"start\":158752},{\"end\":159040,\"start\":158987},{\"end\":159366,\"start\":159269},{\"end\":159817,\"start\":159773},{\"end\":160228,\"start\":160151},{\"end\":160726,\"start\":160662},{\"end\":161214,\"start\":161137},{\"end\":161747,\"start\":161659},{\"end\":162251,\"start\":162200},{\"end\":162660,\"start\":162490},{\"end\":163065,\"start\":163004},{\"end\":163362,\"start\":163297},{\"end\":163800,\"start\":163733},{\"end\":164247,\"start\":164170},{\"end\":165114,\"start\":165037},{\"end\":165448,\"start\":165428},{\"end\":165708,\"start\":165670},{\"end\":166101,\"start\":166040},{\"end\":166613,\"start\":166542},{\"end\":167113,\"start\":167032},{\"end\":167613,\"start\":167545},{\"end\":168259,\"start\":168178},{\"end\":168785,\"start\":168712}]"}}}, "year": 2023, "month": 12, "day": 17}