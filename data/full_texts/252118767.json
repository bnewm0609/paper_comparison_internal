{"id": 252118767, "updated": "2023-10-05 10:46:00.797", "metadata": {"title": "Self-Supervised Multimodal Fusion Transformer for Passive Activity Recognition", "authors": "[{\"first\":\"Armand\",\"last\":\"Koupai\",\"middle\":[\"K.\"]},{\"first\":\"Mohammud\",\"last\":\"Bocus\",\"middle\":[\"J.\"]},{\"first\":\"Raul\",\"last\":\"Santos-Rodriguez\",\"middle\":[]},{\"first\":\"Robert\",\"last\":\"Piechocki\",\"middle\":[\"J.\"]},{\"first\":\"Ryan\",\"last\":\"McConville\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "The pervasiveness of Wi-Fi signals provides significant opportunities for human sensing and activity recognition in fields such as healthcare. The sensors most commonly used for passive Wi-Fi sensing are based on passive Wi-Fi radar (PWR) and channel state information (CSI) data, however current systems do not effectively exploit the information acquired through multiple sensors to recognise the different activities. In this paper, we explore new properties of the Transformer architecture for multimodal sensor fusion. We study different signal processing techniques to extract multiple image-based features from PWR and CSI data such as spectrograms, scalograms and Markov transition field (MTF). We first propose the Fusion Transformer, an attention-based model for multimodal and multi-sensor fusion. Experimental results show that our Fusion Transformer approach can achieve competitive results compared to a ResNet architecture but with much fewer resources. To further improve our model, we propose a simple and effective framework for multimodal and multi-sensor self-supervised learning (SSL). The self-supervised Fusion Transformer outperforms the baselines, achieving a F1-score of 95.9%. Finally, we show how this approach significantly outperforms the others when trained with as little as 1% (2 minutes) of labelled training data to 20% (40 minutes) of labelled training data.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2209.03765", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/iet-wss/KoupaiBSPM22", "doi": "10.1049/wss2.12044"}}, "content": {"source": {"pdf_hash": "d6f410e6ce2c7254ea19197ea81bd6b895dd754c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2209.03765v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5961d515c50665250deb92a5387fb5c3fb5efed4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d6f410e6ce2c7254ea19197ea81bd6b895dd754c.txt", "contents": "\nSelf-Supervised Multimodal Fusion Transformer for Passive Activity Recognition\n\n\nArmand K Koupai \nSchool of Computer Science\nElectrical and Electronic Engineering, and Engineering Maths\nUniversity of Bristol\nUK\n\nMohammud J Bocus \nSchool of Computer Science\nElectrical and Electronic Engineering, and Engineering Maths\nUniversity of Bristol\nUK\n\nRaul Santos-Rodriguez \nSchool of Computer Science\nElectrical and Electronic Engineering, and Engineering Maths\nUniversity of Bristol\nUK\n\nRobert J Piechocki \nSchool of Computer Science\nElectrical and Electronic Engineering, and Engineering Maths\nUniversity of Bristol\nUK\n\nRyan Mcconville ryan.mcconville@bristol.ac.uk. \nSchool of Computer Science\nElectrical and Electronic Engineering, and Engineering Maths\nUniversity of Bristol\nUK\n\nSelf-Supervised Multimodal Fusion Transformer for Passive Activity Recognition\n1Index Terms-Passive WiFi-based HARmulti-modal/sensor fusionDeep LearningVision Transformer (ViT)self-supervised learning\nThe pervasiveness of Wi-Fi signals provides significant opportunities for human sensing and activity recognition in fields such as healthcare. The sensors most commonly used for passive Wi-Fi sensing are based on passive Wi-Fi radar (PWR) and channel state information (CSI) data, however current systems do not effectively exploit the information acquired through multiple sensors to recognise the different activities. In this paper, we explore new properties of the Transformer architecture for multimodal sensor fusion. We study different signal processing techniques to extract multiple image-based features from PWR and CSI data such as spectrograms, scalograms and Markov transition field (MTF). We first propose the Fusion Transformer, an attention-based model for multimodal and multi-sensor fusion. Experimental results show that our Fusion Transformer approach can achieve competitive results compared to a ResNet architecture but with much fewer resources. To further improve our model, we propose a simple and effective framework for multimodal and multi-sensor self-supervised learning (SSL). The self-supervised Fusion Transformer outperforms the baselines, achieving a F1-score of 95.9%. Finally, we show how this approach significantly outperforms the others when trained with as little as 1% (2 minutes) of labelled training data to 20% (40 minutes) of labelled training data.\n\nI. INTRODUCTION\n\nIn recent years, there has been growing research interest in healthcare applications to diagnose and prevent mental and physical diseases, often within the home, and often with the objective to relieve the burden on healthcare services. Many systems have been developed to collect and provide information about a person's health condition in this way [1]. A wide array of sensors have been deployed, from wearables, to cameras, to more recently passive sensing systems using radio frequency (RF) signals. Sensors such as Wi-Fi are particularly promising for in-home healthcare applications, as they 1) perform sensing passively, 2) avoid any discomfort for the user (as no sensors need to be worn), 3) are ubiquitous, 4) and they are more privacy-friendly than alternatives such as cameras. Wi-Fi based sensing systems have been studied for tasks such as language recognition [2] and fall detection [3]. These systems can also be used for human activity recognition (HAR) [4], [5] as human activities cause changes in the wireless signal transmitted by the passive WiFi sensors in terms of frequency shifts, multipath propagation and signal attenuation [6]. Two Wi-Fi sensors are commonly used in HAR, namely passive Wi-Fi radar (PWR) and channel state information (CSI). CSI represents how a wireless signal propagates from the transmitter to its receiver at particular carrier frequencies along multiple paths. The CSI data, which can be extracted from specific network interface cards (NICs) such as Intel 5300 [7] or Atheros [8], can be viewed as a 3D time-series matrix of complex values representing both the amplitude attenuation and the phase shift of multiple propagation paths. It captures how wireless signals travel through surrounding objects or humans in time, frequency and spatial domains. Despite that both CSI and PWR sensors use the same signal source and have a similar function, PWR works differently. A PWR system correlates the transmitted signal from a WiFi access point and the reflected signal from the surveillance area and calculate the distance between the antenna and the object or human [9].\n\nResearch in radio-based human sensing and activity recognition has moved towards deep learning, principally because deep learning models can learn complex representations from raw and noisy data. However, most passive HAR deep learning based systems are uni-modal, i.e., they use information from only one type of sensor. These systems usually use two different architectures: convolutional neural networks (CNN) which have been principally used with RF sensors' raw data transformed into image-like representation such as spectrograms [10], or recurrent neural networks (RNN) which work directly on the raw Wi-Fi data [11]. In this work, we study and propose to use multiple synchronised sensors, or views, in an indoor environment to improve the performance of a passive HAR system. More specifically, we propose to build a system that collects raw data from synchronized sensors, and after some signal processing, all modalities can be fused effectively; in this case using Transformers.\n\nRecent work has demonstrated that the Vision Transformer (ViT) [12] architecture is capable of competitive or superior performance on image classification tasks at a large scale. Instead of convolutions, it uses a self-attention mechanism to aggregate information across locations. Thus, we investigate the potential of the ViT for sensor and feature fusion. For this purpose, we need to address the possible challenges: firstly, how the self-attention mechanism present in the ViT can be used for sensor feature fusion? Secondly, does ViT benefit from sensor fusion and lead to better predictions for HAR. In this paper, we study these questions and compare our findings with a traditional ResNet model. The main contributions of this work are the following:\n\n\u2022 We propose a model, the Sensor Fusion Vision Transformer (SF-ViT), based on the Vision Transformer (ViT) architecture [12], that can fuse multiple image features and views. \u2022 We extend it to a more general framework which we called the Fusion Transformer, that can effectively fuse multiple features from different types of sensors and we assess the effectiveness of our transformer-based model for multi-modal and multi-sensor fusion. \u2022 We evaluate the effectiveness of the Fusion Transformer on a human activity recognition (HAR) dataset (collected using Wi-Fi sensors) in a purely supervised fashion, and compare its performance against ResNet. \u2022 We also propose a new method for multi-modal and multi-sensor self-supervised learning (SSL) that outperforms the baselines using multiple image features and views for passive HAR. This paper is organised as follows: Related works on multimodal sensor fusion are presented in Section II. The methodology and system design of our multimodal sensor fusion transformer models are described in Section III, including details on the signal processing of WiFi-based signals. Section IV provides detailed information on the experimental setup. Section V presents the results obtained using our fully supervised Fusion Transformer model on a human activity recognition dataset. Section VI describes our self-supervised multimodal sensor fusion transformer architecture, along with details on the experiment setup and results. Finally, conclusions are drawn in Section VII.\n\n\nII. RELATED WORK\n\nMost works on multi-modal or multi-sensor fusion for human action recognition using RF, inertial and/or vision sensors, have considered either decision-level fusion or feature-level fusion. For example, the authors of [13] perform multimodal fusion at the decision level to combine the advantages of Wi-Fi and vision-based sensors using a hybrid deep neural network (DNN) model to achieve a 97.5% cross-validation accuracy on average for 3 activities: sitting, standing and walking. The model essentially consists of a Wi-Fi sensing module (CNN architecture) and a vision sensing module (based on the convolutional 3D model) for processing Wi-Fi and video frames for unimodal inference, followed by a multimodal fusion module. Multimodal fusion is performed at the decision level (after both the Wi-Fi and vision modules have made a classification) because this framework is stated to be more flexible and robust to unimodal failure compared to feature level fusion. The authors of [14] present a method for HAR, which leverages four sensor modalities, namely, skeleton sequences, inertial and motion capture measurements and Wi-Fi fingerprints. The fusion of signals is formulated as a matrix concatenation. The individual signals of different sensor modalities are transformed and represented as an image. The resulting images are then fed to a 2D CNN (EfficientNet B2) for classification. The authors evaluated their approach on four different datasets; the NTU RGB+D 120 dataset for skeleton data, the UTD-MHAD dataset for skeleton and inertial data, the ARIL dataset for Wi-Fi data and the Simitate dataset for motion capture data. Good experimental results were achieved across the different sensor modalities. The authors of [15] proposed a multimodal HAR system that leverages Wi-Fi and wearable sensor modalities to jointly infer human activities. They collected CSI data from a standard Wi-Fi NIC, alongside the user's local body movements via a wearable inertial measurement unit (IMU) consisting of an accelerometer, gyroscope, and magnetometer sensors. They calculated the time-variant mean Doppler shift (MDS) from the processed CSI data and magnitude from the inertial data for each sensor of the IMU. Then, various time and frequency domain features were separately extracted from the magnitude data and the MDS. The authors applied a feature-level fusion method which sequentially concatenates feature vectors that belong to the same activity sample. Finally, supervised machine learning techniques were used to classify four activities, such as walking, falling, sitting, and picking up an object from the floor. The authors of [9] conducted a comprehensive study on the comparison of two RF sensing devices for the purpose of HAR, namely, CSI and PWR systems. Two pipelines were proposed for filtering and processing the raw signals from the two sensors into Doppler spectrograms, which were then used to train a simple supervised CNN to evaluate the HAR performance. They considered the combined activity data from 3 different layouts. In the first layout, the transmitter and receiver were facing each other (in a line-of-sight configuration) while in the second layout, the transmitter and receiver were at 90 \u2022 to each other. Finally, in the third layout, the transmitter and receiver were co-located (placed next to each other). The CSI system achieved an overall accuracy of 67,3% while the PWR system had an accuracy of 66,7%. Although this work presents a simple system which combines CSI and PWR spectrograms by merging probabilities from two networks (decision-level fusion), current state-of-the-art models are not specifically designed for the fusion of multiple passive Wi-Fi devices.\n\nWhile CNN architecture was the de-facto standard for computer vision tasks, gradually, ViT showed very promising results when pre-trained on large amounts of data and then fine-tuned to mid-sized or small-sized image recognition benchmarks while requiring fewer computational resources for training [16], [17]. However, most of ViT models have been trained on natural images of very large size, together with pre-training and very strong data augmentation techniques. A similar work which also trained a ViT with spectrograms is the audio spectrogram transformer (AST) [18], which presents a new method for audio classification with a ViT using spectrogram data. Recent works showed that ViTs could outperform ResNets without pre-training or strong data augmentations [19], notably by using sharpness-aware minimisation technique [20], which simultaneously minimises the loss value and loss sharpness by seeking parameters that lie in neighbourhoods and having uniformly low loss. However, this technique requires the computation of two forward-backward propagations to estimate the 'sharpness-aware' gradient, and thus, leads to an increased training time.\n\nIn this paper, we evaluate the performance of our Transformer-based sensor fusion model for HAR using image data generated from multiple sensors. We evaluate its potential for sensor fusion and propose a method for multi-modal and multi-sensor self-supervised learning (SSL). The CSI system and PWR system deployment [21].\n\nThe CSI system consisted of 2 receivers (denoted as NUC1 and NUC2) while the PWR system consisted of 3 receivers (denoted as rx2, rx3, and rx4). For more details on the dataset, the interested reader is referred to [21].\n\n\nIII. METHODOLOGY AND SYSTEM DESIGN\n\n\nA. Signal Processing of RF Sensors\n\nInspired by other work in this area [9], which has explored two pipelines for extracting image features from RF sensors using signal processing techniques, we apply the same principles. In this work, we use the OPERAnet dataset [21], which includes publicly available data from both CSI and PWR systems. The dataset was collected with the intention to evaluate HAR and localisation techniques with measurements obtained from synchronized RF devices and vision-based sensors. The experimental setup established to collect both CSI and PWR data is shown in Fig. 1. Fig. 2a and 2b show some examples of the generated spectrograms with these two pipelines, for each of the six activities, namely, sitting down on a chair ('sit'), standing from chair ('stand'), laying down on the floor ('laydown'), standing from floor ('standff '), body rotation ('bodyrotate'), and walking ('walk'). The pipelines are as follows:\n\n\u2022 In Fig. 2a: we denoise the CSI signal using discrete wavelet transform (DWT) and median filtering, then reduce the dimensionality using principal component analysis (PCA) and generate a spectrogram using short time Fourier transform (STFT). \u2022 In Fig. 2b: we apply cross ambiguity function to the raw PWR data, use the CLEAN algorithm and constant false alarm rate (CFAR) for direct signal cancellation and noise reduction, generating as output a Doppler spectrogram [9]. These two pipelines are necessary to extract informative data from CSI and PWR sensors. The raw CSI data is very noisy in nature, and thus the DWT technique helps to filter out high frequency components and remove noises, while preserving most of the information and avoiding the distortion of the signal [22]. Afterwards, we perform median filtering to remove any undesired transients in the CSI mesurements which have not been performed by human motion. Despite that the CSI data is highly informative, it consists of a lot of complex values per second, depending on the number of transmit and receive antennas, orthogonal frequency-division multiplexing (OFDM) subcarriers and packet rate (for example, the Intel 5300 chipset captures complex CSI data over 3 transmit antennas, 3 receive antennas and 30 subcarriers). Therefore, we use PCA to reduce the computational complexity of such data, while preserving as much information as possible. Finally, we convert the PCA signal into spectrograms using STFT [9].\n\nFor the PWR signal, we first apply the cross ambiguity function (CAF) to extract target range and Doppler information. However, we also capture an interference source which is the strong direct signal emitted from the Wi-Fi access point and which is captured by the PWR surveillance channel. Thus, to remove this signal, we employ the CLEAN algorithm [23]. The last step consists of reducing the noise on the CAF surface. We use CFAR to estimate the background noise and apply it to the CAF surface. PWR's Doppler spectrogram is generated by selecting the maximum Doppler pulse from each Doppler bin within the CAF surface [9].\n\nIt should be noted that all the devices were synchronised to the same network time protocol (NTP) server and were labelled in sync. Thus, the raw data could be segmented as per the ground truth activity labels and processed accordingly.\n\nUsing the CSI data, we also generate other features such as scalograms and Markov transition fields (MTF). Each feature captures particular information about the activity. Given all of these different features, we aim to build a network that can fuse all these images together effectively to improve the overall system performance. In this work, we have extracted 15 different features (see Fig. 3):\n\n\u2022 PWR spectrogram data collected from the three receiver surveillance channels, rx2, rx3, and rx4 in Fig. 1 (a) Amplitude CSI spectrograms from view 1 (NUC1) for each activity.  Each channel and receiver can be seen as another view of the human activity performed in the room. Previously, we presented the spectrograms of CSI and PWR data, which give a visual representation of the spectrum of frequencies of a signal varying through time. Spectrograms are generated through STFT by applying a sliding window to obtain equallysized segments of the signal and then FFT is performed on the samples in each segment, which converts the signal from the time domain to the frequency domain. Similar to STFT, the scalogram is a time-frequency representation of a signal and it is obtained from the absolute value of the CWT of a signal. Finally, we also introduced another type of representation called the Markov transition field (MTF), which is an image generated from time series data, representing a field of transition probabilities for a discretized time series.  Fig. 4 illustrates an overview of the SF-ViT, where the shape of the input image has been changed for convenience. The SF-ViT trains a transformer to recognise human activities by assigning a high attention weight to relevant features (i.e., our patches of different features), and a low attention weight to less pertinent image features. The SF-ViT's inspiration is that the more unique the image features that are used with the ViT are, the more effective is the model for recognising human activities, as each image feature will represent or capture different information about the activity, and thus combining them effectively should lead to better performance.\n\n\nB. Multimodal Sensor Fusion\n\n2) The Fusion Transformer: One of the potential issues with this approach is that we do a linear projection of patches of size 224 \u00d7 224 \u00d7 1 into a feature space of size 512, which is computationally expensive and results in a very large number of trainable parameters and potential over-fitting, as each input pixel is connected to the linear layer. To remedy this, we instead first encode each image-based feature using a CNN encoder, which transforms the raw image feature of size 224 \u00d7 224 \u00d7 1 into an image of size 16 \u00d7 16 \u00d7 64. This new architecture can be considered as a multi-modal model, where each modality is first passed into an encoder that transforms the raw modality into a smaller feature space.\n\nThe corresponding model architecture, which we call Fusion Transformer, is presented in Fig. 5.\n\n\nIV. EXPERIMENTAL SETUP\n\nWe evaluate the capabilities of our Fusion Transformer on human activity recognition and compare its performance to ResNet and show that the Fusion Transformer is successful in achieving competitive results while requiring less computational resources than ResNet. In this section, the experimental setup used throughout the findings of the paper is presented. The system was developed in PyTorch and all models have been trained on a single GPU (Nvidia 2080Ti).\n\n\nA. Dataset and Metrics\n\nAs mentioned previously, for our experimentation, we will use the OPERAnet dataset [21], which includes publicly available data from both CSI and PWR systems. The RF sensors captured the changes in the wireless signals while six daily activities were being performed by six participants, namely, sitting down on a chair ('sit'), standing from chair ('stand'), laying down on the floor ('laydown'), standing from floor ('standff '), body rotation ('bodyrotate'), and walking ('walk'). Applying the signal processing pipelines described earlier, led to a dataset composed of 2,897 data samples (non-overlapping windows each representing 4 seconds of an activity) for the six activities. Worth noting however, as is the case in reality, the distribution of the different activities a human engages in is highly imbalanced. In this case, we have an imbalanced dataset where the two most represented classes are body rotating and   walking, representing respectively 30% and 33% of the total observations. The two classes which are less represented are 'standing from floor' and 'laying down', each representing 7% of the dataset. For training and validation purposes, we randomly split the dataset into a train set and a validation set, respectively composed of 80% and 20% of the total dataset samples. For this dataset, we use the accuracy and macro F1score as our main metrics.\n\n\nB. Models\n\nIn this section, we will initially focus on the Fusion Transformer. All experiments have been performed with the configuration presented in Table I. The width of the image is N \u00d7 224, where N is the number of different image features generated. We compare and train the model with different numbers of features to analyse how the model's performance scales.\n\nIn the Fusion Transformer shown in Fig. 5, we add a CNN encoder for our images to extract more relevant features from the raw images and to reduce the size of the images. The CNN encoder is composed of 4 blocks, where each block consists of a convolution, ReLU and pooling layers. Each image feature of size 224 \u00d7 224 \u00d7 1 is embedded in a new image representation of dimension 16 \u00d7 16 \u00d7 64.\n\nTo compare the performance of the Fusion Transformer with a baseline, we also train a ResNet model to evaluate whether it achieves better performance when trained with multiple image features, by considering each feature as a new channel. We trained two models: ResNet18 and ResNet34.\n\n\nC. Training\n\nAll models, including ResNet, have been trained using the AdamW optimizer, with \u03b2 1 = 0.90 and \u03b2 2 = 0.999, with a weight decay settled at 0.01 and a batch size of 64. The learning rate has been initialised at 1e-4 and reduced during training using a learning step scheduler with a unitary step size and \u03b3 = 0.5. The loss function used for these experiments is cross-entropy.\n\nDespite that recent works train ViTs using pre-training or transfer learning on large datasets, we decided to train our model from scratch, to more closely study the benefit of our sensor fusion model for activity recognition. Furthermore, when training on smaller datasets, ViT-based models have a weaker inductive bias compared to CNNs and thus leads to an increased reliance on model regularisation or data augmentation [25]. In the case of CSI and PWR data, using similar data augmentation techniques as those used on natural images is not possible. Thus, throughout all our experiments, we did not use data augmentation. We used a simple dropout strategy as mentioned in section IV-B and a weight decay equal to 0.01. \n\n\nV. RESULTS\n\n\nA. Fully Supervised Fusion Transformer Results\n\nOur experiments showed that when training both our Fusion Transformer and ResNet from scratch, the Fusion Transformer obtained competitive results without any pre-training on a small amount of images. In Table 2, we present the results of SF-ViT, ResNet and Fusion Transformer performance on the validation set when varying the number of image-based features used for training. With our Fusion Transformer architecture, we obtained our best results when using only PWR and CSI spectrograms, reaching a macro F1-Score of 94.3%. With ResNet34, we also obtained the best results when using PWR and CSI spectrograms, reaching a F1-score of 94.9%. The two confusion matrices are shown in Fig. 6. Thus, ResNet34 seems to achieve slightly better performance for HAR. However, the Fusion Transformer can achieve competitive performance with less parameters when trained from scratch, without pre-training. The Fusion Transformer has 11.7M trainable parameters against 12.4M for the ResNet34. One benefit of the Fusion Transformer is that the number of trainable parameters is invariant to the addition of new image-based representations. Unlike the Fusion Transformer, the number of trainable parameters increases with ResNet when doing so.\n\n\nVI. TOWARDS SELF-SUPERVISION\n\nTransformers outperform many state-of-the-art models when trained on large scale datasets. In this work, we succeeded in achieving competitive results compared to ResNet while training our model from scratch. However, we believe that with self-supervision, the Fusion Transformer can outperform ResNet34 for HAR. Instead of training a model from scratch with weights initialised arbitrarily, the model can be pre-trained via different self-supervised learning (SSL) methods.\n\nWe propose a self-supervised method based on image masking as in [26]. However, instead of masking some parts of a natural image, in our approach, we mask multiple image features and we pre-train our model to predict the masked image features. The architecture used during the pre-training phase is presented in Fig. 7.\n\n\nA. Pre-training Phase: Experimental Setup\n\nWe pre-train our model with both PWR and CSI sepctrograms, using all different views and image-based features. We masked 60% of the image-based features and pre-train our  model for 500 epochs. We use an AdamW optimizer and a multi-step learning rate scheduler. The batch size is fixed as 64, the base learning rate as 5e-4, weight decay as 0.05, \u03b2 1 = 0.9, \u03b2 2 = 0.999 and a warm-up for 10 epochs.   Next, we fine-tune the pre-trained model in a supervised way. The strength of self-supervised learning is that we can fine-tune the pre-trained model on a smaller training set. This is particularly useful when labelling the data is time consuming and expensive. We train the model on different number of training samples: 1 sample per class, 5% (10 minutes), 10% (20 minutes), 15% (30 minutes), 20% (40 minutes) of the train set and also the full train set. We fine-tuned our model using the following hyper-parameters settings: an AdamW optimizer, a base learning rate fixed as 1e-3, \u03b2 1 = 0.9, \u03b2 2 = 0.999 and a warm-up for 10 epochs. We added multiple regularisation methods: a weight decay of 0.05 and a stochastic depth [27] ratio of 0.1. We report the results of our self-supervised method in Table III. ResNet as an architecture is not well-defined for SSL when having multi-modal and multi-sensor data. Existing approaches involve contrastive learning methods [28], [29], which require multiple views for each modality, data augmentation techniques and pairs of negative/positive samples. A similar work has proposed a self-supervised contrastive pretraining method for passive Wi-Fi activity recognition [30]. Two different approaches have been explored in [30]: (1) pre-training the model using two views of CSI data or (2) pre-training the model with one view of CSI data and one view of PWR data. Although we can simply pre-train a model using a contrastive method with two views on many different CNN benchmark models, this framework is not well defined for multi-view and multi-modal pre-training and led to worse results than those presented by our non pre-trained ResNet34. Thus, in the context of multi-sensor fusion with multi-views, we cannot rely on a ResNet architecture.\n\nIn this work, we have proposed a simple but yet very effective method for multi-modal and multi-sensor self-supervised learning with a Fusion Transformer which outperforms the results obtained with a non-pre-trained ResNet34 and a nonpre-trained Fusion Transformer, regardless of the training set size. The strength of the Fusion Transformer is that it can be easily pre-trained with multiple views, sensors and modalities, thanks to the transformer architecture.\n\n\nVII. CONCLUSION\n\nWe proposed a new architecture for multi-modal, multisensor passive Wi-Fi based human activity recognition (HAR). Using signal processing, we extracted 15 image-based features from multiple sensors. With our Fusion Transformer architecture, we first embed each modality via an encoder and then pass it into our transformer network. The Fusion Transformer can fuse multiple image-based features and train a classifier to predict six daily activities performed by six participants. The best results of this model were achieved with PWR and CSI spectrograms, achieving competitive performance with ResNet34, but with less trainable parameters. We next demonstrated that with our proposed self-supervision technique, our pre-trained model outperformed non pre-trained ResNet34, achieving a F1-score of 95.9% when fine-tuned on the full training set. Furthermore, it outperformed the other models when fine-tuned with as little as 1% (2 minutes) of labelled training data with an F1-score of 56.3%, while the F1-score achieved with 20% (40 minutes) of training data was 91.2%. These results are promising given the need to collect training data for each new indoor environment.\n\n\nFig. 1: The CSI system and PWR system deployment [21]. The CSI system consisted of 2 receivers (denoted as NUC1 and NUC2) while the PWR system consisted of 3 receivers (denoted as rx2, rx3, and rx4). For more details on the dataset, the interested reader is referred to [21].\n\n\n) PWR spectrograms from first channel for each activity.\n\nFig. 2 : N2 Fig. 3 :\n2N23Visualisation of PWR and CSI spectrograms for each activity. (a) PWR channel 1 (b) PWR channel 2 (c) PWR channel 3 (d) Amp. spec. N1 (e) Amp. spec. N2 (f) Ph. diff. N1 (g) Ph. diff. N2 (h) MTF ph. diff. N1 (i) MTF ph. diff. N2 (j) MTF amp. N1 (k) MTF amp. N2 (l) Amp. scal. N1 (m) Amp. scal. N2 (n) Ph. diff. scal. N1 (o) Ph. diff. scal. Fifteen image features extracted via signal processing techniques representing a person walking in the monitoring area for a duration of four seconds.\n\nFig. 5 :\n5The Fusion Transformer model overview. Each modality is encoded into a new feature space and then linearly embedded. We add position embeddings and feed the output embedding to the transformer encoder. We add an extra learnable classification token.\n\nFig. 6 :\n6Visualisation of the confusion matrices of the models with highest scores.\n\nMasked\n\n\n\nTransformer 1) A first approach: Sensor Fusion Vision Transformer: We will first present the Sensor-Fusion Vision Transformer (SF-ViT), which uses a similar architecture to the conventional Vision Transformer (ViT). Nevertheless, in most applications where ViT is used, the model is trained with 'natural' images of size 224 \u00d7 224 \u00d7 3 (height, width, channels) that areFig. 4: Sensor-Fusion Vision Transformer (SF-ViT) model overview. We split our concatenated image into patches of fixed size, 224 \u00d7 224, where each patch corresponds to one of the image features. We linearly embed each of them, add position embeddings, and feed the output embedding to the transformer encoder. In order to perform classification, we add an extra learnable 'classification token'. The network model is inspired from the original ViT architecture [12]. divided into small patches of size 16 \u00d7 16 or 32 \u00d7 32. Here instead, we concatenate all image features and obtain an image of size 224 \u00d7 ( 224 \u00d7 N ) \u00d7 1 where N is the number of different image features concatenated. Instead of dividing our image into small patches of size 16 \u00d7 16, we patch the image so that each patch represents a different image-based feature.Patch + Position embedding \n\n*extra learnable [class] embedding \n\nLinear projection of flattened patches \n\n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n1 \n0 \n\n1 \n1 \n\n1 \n2 \n\n1 \n3 \n\n1 \n4 \n\n1 \n5 \n0 * \n\nTransformer encoder \n\nMLP \nhead \n\nClass \nStand \nSit \nWalk \n\u2026 \n\nEmbedded \npatches \n\nNorm \n\nMulti-head \nattention \n\nNorm \n\nMLP \n\nStack of L transformer \nencoders \n\nTransformer encoder \n\n\n\nTABLE I :\nIViT parameters.image size \npatch size \nchannels emb. dim. \ndepth qkv bias \ndrop out \nMLP ratio \n224, N \u00d7 224 \n224, 224 \n1 \n512 \n3 \nFalse \n0.1 \n1.0 \n\n\n\nTABLE II :\nIIPerformance comparison of the Fusion Transformer with ResNet.SF-ViT \nResNet18 \nResNet34 \nFusion Transformer \nAccuracy F1-score \nAccuracy F1-score \nAccuracy F1-score \nAccuracy F1-score \nCSI amplitude spectrogram (view 1 = 1 feature) \n80.3% \n71.7% \n92.8% \n89.7% \n73.3% \n71.0% \n88.3% \n83.5% \nCSI amplitude spectrogram (2 views = 2 features) \n85.9% \n78.6% \n93.1% \n90.0% \n35.6% \n43.4% \n92.1% \n87.0% \nCSI ph. diff + amp. spectrograms (4 features) \n84.3% \n77.4% \n94.5% \n91.4% \n95.7% \n93.9% \n92.2% \n88.4% \nCSI (amp. + ph. diff.) & PWR spectrograms (7 \nfeatures) \n\n91.6% \n88.2% \n95.0% \n92.4% \n96.6% \n94.9% \n95.9% \n94.3% \n\nCSI (amp. + ph. diff.) spectrograms + PWR spectro-\ngrams + CSI (amp. + ph. diff.) MFT (11 features) \n\n91.9% \n88.6% \n93.8% \n91.1% \n91.9% \n88.2% \n94.3% \n91.9% \n\nAll 15 image features \n92.8% \n89.5% \n91.0% \n86.5% \n93.3% \n90.4% \n93.6% \n91.1% \n\n\n\n\nFig. 7: Self-supervised Fusion Transformer method. We randomly mask 60% of our modalities and we train a model to predict the masked modalities.modality1 \n\nModality 2 \n\nMasked \nmodality n \n\n. \n. \n. \n\nEncoder 1 \n\nEncoder 2 \n\nEncoder n \n\n. \n. \n. \n\n. \n. \n. \n\nFeature space \n\nLinear projection \n\n1 \n\n2 \n\nn \n\nTransformer encoder \n\n. \n. \n. \n\nModality 1 \n\nModality 2 \n\nModality n \n\n. \n. \n. \n\nModality 1 \n\nModality 2 \n\nModality n \n\n. \n. \n. \n\nPredicted \nReal data \n\nL1 loss \n\n\n\nTABLE III :\nIIIComparison of pre-trained and supervised Fusion Transformer depending on the size of training set D. B. Fine-tuning Phase: Experimental Setup and Results1 sample 2.5% of \n5% of \n10% of \n15% of \n20% of \nFull \nper class \nD \nD \nD \nD \nD \nD \nFusion Transformer (with SSL) \n56.3% \n77.0% \n84.5% \n89.7% \n90.4% \n91.2% \n95.9% \nF1-Score \nFusion Transformer (no SSL) \n32.8% \n60.0% \n67% \n83.1% \n84.4% \n84.4% \n94.2% \nF1-Score \nResNet34 \n32.6% \n43.4% \n56.9% \n62.7% \n62.2% \n73.8% \n94.9% \nF1-Score \n\n\nACKNOWLEDGEMENTSThis work was performed as a part of the OPERA Project, funded by the UK Engineering and Physical Sciences Research Council (EPSRC), Grant EP/R018677/1.\nVesta: A digital health analytics platform for a smart home in a box. R Mcconville, G Archer, I Craddock, M Koz\u0142owski, R Piechocki, J Pope, R Santos-Rodriguez, Future Generation Computer Systems. 114R. McConville, G. Archer, I. Craddock, M. Koz\u0142owski, R. Piechocki, J. Pope, and R. Santos-Rodriguez, \"Vesta: A digital health analytics platform for a smart home in a box,\" Future Generation Computer Systems, vol. 114, pp. 106 -119, 2021.\n\nSignFi: Sign language recognition using WiFi. Y Ma, G Zhou, S Wang, H Zhao, W Jung, Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 21Y. Ma, G. Zhou, S. Wang, H. Zhao, and W. Jung, \"SignFi: Sign language recognition using WiFi,\" Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., vol. 2, no. 1, mar 2018.\n\nFallDeFi: Ubiquitous fall detection using commodity Wi-Fi devices. S Palipana, D Rojas, P Agrawal, D Pesch, Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 14S. Palipana, D. Rojas, P. Agrawal, and D. Pesch, \"FallDeFi: Ubiquitous fall detection using commodity Wi-Fi devices,\" Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., vol. 1, no. 4, jan 2018.\n\nA dataset for Wi-Fi-based human-to-human interaction recognition. R Alazrai, A Awad, B Alsaify, M Hababeh, M I Daoud, Data in Brief. 31105668R. Alazrai, A. Awad, B. Alsaify, M. Hababeh, and M. I. Daoud, \"A dataset for Wi-Fi-based human-to-human interaction recognition,\" Data in Brief, vol. 31, p. 105668, 2020.\n\nA dataset for Wi-Fi-based human activity recognition in line-of-sight and nonline-of-sight indoor environments. B A Alsaify, M M Almazari, R Alazrai, M I Daoud, Data in Brief. 33106534B. A. Alsaify, M. M. Almazari, R. Alazrai, and M. I. Daoud, \"A dataset for Wi-Fi-based human activity recognition in line-of-sight and non- line-of-sight indoor environments,\" Data in Brief, vol. 33, p. 106534, 2020.\n\nWiFi sensing with channel state information: A survey. Y Ma, G Zhou, S Wang, ACM Comput. Surv. 523Y. Ma, G. Zhou, and S. Wang, \"WiFi sensing with channel state information: A survey,\" ACM Comput. Surv., vol. 52, no. 3, jun 2019.\n\nTool release: Gathering 802.11n traces with channel state information. D Halperin, W Hu, A Sheth, D Wetherall, SIGCOMM Comput. Commun. Rev. 41153D. Halperin, W. Hu, A. Sheth, and D. Wetherall, \"Tool release: Gathering 802.11n traces with channel state information,\" SIGCOMM Comput. Commun. Rev., vol. 41, no. 1, p. 53, Jan. 2011. [Online].\n\n. 10.1145/1925861.1925870Available: https://doi.org/10.1145/1925861.1925870\n\nPrecise power delay profiling with commodity WiFi. Y Xie, Z Li, M Li, http:/doi.acm.org/10.1145/2789168.2790124Proceedings of the 21st Annual International Conference on Mobile Computing and Networking, ser. MobiCom '15. the 21st Annual International Conference on Mobile Computing and Networking, ser. MobiCom '15New York, NY, USAACMY. Xie, Z. Li, and M. Li, \"Precise power delay profiling with commodity WiFi,\" in Proceedings of the 21st Annual International Conference on Mobile Computing and Networking, ser. MobiCom '15. New York, NY, USA: ACM, 2015, p. 53-64. [Online]. Available: http://doi.acm.org/10.1145/2789168.2790124\n\nOn CSI and passive Wi-Fi radar for opportunistic physical activity recognition. W Li, M J Bocus, C Tang, R J Piechocki, K Woodbridge, K Chetty, IEEE Transactions on Wireless Communications. 211W. Li, M. J. Bocus, C. Tang, R. J. Piechocki, K. Woodbridge, and K. Chetty, \"On CSI and passive Wi-Fi radar for opportunistic physical activity recognition,\" IEEE Transactions on Wireless Communications, vol. 21, no. 1, pp. 607-620, 2022.\n\nPassive activity classification using just WiFi probe response signals. F Shi, K Chetty, S Julier, 2019 IEEE Radar Conference (RadarConf). F. Shi, K. Chetty, and S. Julier, \"Passive activity classification using just WiFi probe response signals,\" in 2019 IEEE Radar Conference (RadarConf), 2019, pp. 1-6.\n\nWiFi CSI based passive human activity recognition using attention based BLSTM. Z Chen, L Zhang, C Jiang, Z Cao, W Cui, IEEE Transactions on Mobile Computing. 1811Z. Chen, L. Zhang, C. Jiang, Z. Cao, and W. Cui, \"WiFi CSI based passive human activity recognition using attention based BLSTM,\" IEEE Transactions on Mobile Computing, vol. 18, no. 11, pp. 2714-2724, 2019.\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, J Uszkoreit, N Houlsby, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \"An image is worth 16x16 words: Trans- formers for image recognition at scale,\" in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.\n\nWiFi and vision multimodal learning for accurate and robust device-free human activity recognition. H Zou, J Yang, H P Das, H Liu, Y Zhou, C J Spanos, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. CVPRWH. Zou, J. Yang, H. P. Das, H. Liu, Y. Zhou, and C. J. Spanos, \"WiFi and vision multimodal learning for accurate and robust device-free human activity recognition,\" in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019, pp. 426-433.\n\nGimme signals: Discriminative signal encoding for multimodal activity recognition. R Memmesheimer, N Theisen, D Paulus, 2020R. Memmesheimer, N. Theisen, and D. Paulus, \"Gimme signals: Dis- criminative signal encoding for multimodal activity recognition,\" in 2020\n\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 10IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 10 394-10 401.\n\nWiWeHAR: Multimodal human activity recognition using Wi-Fi and wearable sensing modalities. M Muaaz, A Chelli, A A Abdelgawwad, A C Mallofr\u00e9, M P\u00e4tzold, IEEE Access. 8470M. Muaaz, A. Chelli, A. A. Abdelgawwad, A. C. Mallofr\u00e9, and M. P\u00e4tzold, \"WiWeHAR: Multimodal human activity recognition using Wi-Fi and wearable sensing modalities,\" IEEE Access, vol. 8, pp. 164 453-164 470, 2020.\n\nEmerging Properties in Self-Supervised Vision Transformers. M Caron, H Touvron, I Misra, H Jegou, J Mairal, P Bojanowski, A Joulin, ICCV 2021 -International Conference on Computer Vision. Virtual, FranceM. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin, \"Emerging Properties in Self-Supervised Vision Transformers,\" in ICCV 2021 -International Conference on Computer Vision, Virtual, France, Oct. 2021, pp. 1-21. [Online]. Available: https://hal.archives-ouvertes.fr/hal-03323359\n\nBEit: BERT pre-training of image transformers. H Bao, L Dong, S Piao, F Wei, International Conference on Learning Representations. H. Bao, L. Dong, S. Piao, and F. Wei, \"BEit: BERT pre-training of image transformers,\" in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/ forum?id=p-BhZSz59o4\n\nAST: Audio spectrogram transformer. Y Gong, Y.-A Chung, J Glass, Proc. Interspeech. InterspeechY. Gong, Y.-A. Chung, and J. Glass, \"AST: Audio spectrogram trans- former,\" in Proc. Interspeech 2021, 2021, pp. 571-575.\n\nWhen vision transformers outperform ResNets without pre-training or strong data augmentations. X Chen, C.-J Hsieh, B Gong, International Conference on Learning Representations. X. Chen, C.-J. Hsieh, and B. Gong, \"When vision transformers outperform ResNets without pre-training or strong data augmentations,\" in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/forum?id=LtKcMgGOeLt\n\nSharpness-aware minimization for efficiently improving generalization. P Foret, A Kleiner, H Mobahi, B Neyshabur, International Conference on Learning Representations. P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur, \"Sharpness-aware minimization for efficiently improving generalization,\" in International Conference on Learning Representations, 2021. [Online]. Available: https://openreview.net/forum?id=6Tm1mposlrM\n\nOPERAnet, a multimodal activity recognition dataset acquired from radio frequency and vision-based sensors. M J Bocus, W Li, S Vishwakarma, R Kou, C Tang, K Woodbridge, I Craddock, R Mcconville, R Santos-Rodriguez, K Chetty, R Piechocki, 10.1038/s41597-022-01573-2Scientific Data. 91474M. J. Bocus, W. Li, S. Vishwakarma, R. Kou, C. Tang, K. Woodbridge, I. Craddock, R. McConville, R. Santos-Rodriguez, K. Chetty, and R. Piechocki, \"OPERAnet, a multimodal activity recognition dataset acquired from radio frequency and vision-based sensors,\" Scientific Data, vol. 9, no. 1, p. 474, 2022. [Online]. Available: https://doi.org/10.1038/s41597-022-01573-2\n\nTranslation resilient opportunistic WiFi sensing. M J Bocus, W Li, J Paulavicius, R Mcconville, R Santos-Rodriguez, K Chetty, R Piechocki, 2020 25th International Conference on Pattern Recognition (ICPR). M. J. Bocus, W. Li, J. Paulavicius, R. McConville, R. Santos-Rodriguez, K. Chetty, and R. Piechocki, \"Translation resilient opportunistic WiFi sensing,\" in 2020 25th International Conference on Pattern Recognition (ICPR), 2021, pp. 5627-5633.\n\nThrough-the-wall sensing of personnel using passive bistatic WiFi radar at standoff distances. K Chetty, G E Smith, K Woodbridge, IEEE Transactions on Geoscience and Remote Sensing. 504K. Chetty, G. E. Smith, and K. Woodbridge, \"Through-the-wall sensing of personnel using passive bistatic WiFi radar at standoff distances,\" IEEE Transactions on Geoscience and Remote Sensing, vol. 50, no. 4, pp. 1218-1226, 2012.\n\nDeep learning for time-series analysis. J C B Gamboa, J. C. B. Gamboa, \"Deep learning for time-series analysis,\" 2017. [Online]. Available: https://arxiv.org/abs/1701.01887\n\nHow to train your ViT? data, augmentation, and regularization in vision transformers. A P Steiner, A Kolesnikov, X Zhai, R Wightman, J Uszkoreit, L Beyer, Transactions on Machine Learning Research. A. P. Steiner, A. Kolesnikov, X. Zhai, R. Wightman, J. Uszkoreit, and L. Beyer, \"How to train your ViT? data, augmentation, and regularization in vision transformers,\" Transactions on Machine Learning Research, 2022. [Online]. Available: https://openreview.net/forum?id=4nPswr1KcP\n\nSimMIM: A simple framework for masked image modeling. Z Xie, Z Zhang, Y Cao, Y Lin, J Bao, Z Yao, Q Dai, H Hu, Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu, \"SimMIM: A simple framework for masked image modeling,\" 2021.\n\nDeep networks with stochastic depth. G Huang, Y Sun, Z Liu, D Sedra, K Q Weinberger, Computer Vision -ECCV 2016. B. Leibe, J. Matas, N. Sebe, and M. WellingSpringer International PublishingG. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, \"Deep networks with stochastic depth,\" in Computer Vision -ECCV 2016, B. Leibe, J. Matas, N. Sebe, and M. Welling, Eds. Cham: Springer International Publishing, 2016, pp. 646-661.\n\nMulti-label image classification with contrastive learning. S D Dao, E Zhao, D Phung, J Cai, S. D. Dao, E. Zhao, D. Phung, and J. Cai, \"Multi-label image classification with contrastive learning,\" 2021. [Online]. Available: https://arxiv.org/abs/2107.11626\n\nA survey on contrastive self-supervised learning. A Jaiswal, A R Babu, M Z Zadeh, D Banerjee, F Makedon, 9TechnologiesA. Jaiswal, A. R. Babu, M. Z. Zadeh, D. Banerjee, and F. Makedon, \"A survey on contrastive self-supervised learning,\" Technologies, vol. 9, no. 1, 2021.\n\nSelf-supervised wifi-based activity recognition. H.-S Lau, R Mcconville, M J Bocus, R J Piechocki, R Santos-Rodriguez, H.-S. Lau, R. McConville, M. J. Bocus, R. J. Piechocki, and R. Santos-Rodriguez, \"Self-supervised wifi-based activity recognition,\" 2021. [Online]. Available: https://arxiv.org/abs/2104.09072\n", "annotations": {"author": "[{\"end\":212,\"start\":82},{\"end\":344,\"start\":213},{\"end\":481,\"start\":345},{\"end\":615,\"start\":482},{\"end\":777,\"start\":616}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":91},{\"end\":229,\"start\":224},{\"end\":366,\"start\":350},{\"end\":500,\"start\":491},{\"end\":631,\"start\":621}]", "author_first_name": "[{\"end\":88,\"start\":82},{\"end\":90,\"start\":89},{\"end\":221,\"start\":213},{\"end\":223,\"start\":222},{\"end\":349,\"start\":345},{\"end\":488,\"start\":482},{\"end\":490,\"start\":489},{\"end\":620,\"start\":616}]", "author_affiliation": "[{\"end\":211,\"start\":99},{\"end\":343,\"start\":231},{\"end\":480,\"start\":368},{\"end\":614,\"start\":502},{\"end\":776,\"start\":664}]", "title": "[{\"end\":79,\"start\":1},{\"end\":856,\"start\":778}]", "venue": null, "abstract": "[{\"end\":2373,\"start\":979}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2746,\"start\":2743},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3271,\"start\":3268},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3294,\"start\":3291},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3367,\"start\":3364},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3372,\"start\":3369},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3548,\"start\":3545},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3909,\"start\":3906},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3924,\"start\":3921},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4513,\"start\":4510},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5056,\"start\":5052},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5139,\"start\":5135},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5575,\"start\":5571},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6393,\"start\":6389},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8028,\"start\":8024},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8792,\"start\":8788},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9542,\"start\":9538},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10455,\"start\":10452},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11827,\"start\":11823},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11833,\"start\":11829},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12097,\"start\":12093},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12296,\"start\":12292},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12358,\"start\":12354},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13004,\"start\":13000},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13226,\"start\":13222},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13342,\"start\":13339},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13535,\"start\":13531},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14686,\"start\":14683},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14997,\"start\":14993},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15701,\"start\":15698},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16059,\"start\":16055},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16330,\"start\":16327},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20144,\"start\":20140},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23302,\"start\":23298},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25472,\"start\":25468},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26898,\"start\":26894},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27141,\"start\":27137},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":27147,\"start\":27143},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27386,\"start\":27382},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27439,\"start\":27435},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27444,\"start\":27441},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27502,\"start\":27499}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29896,\"start\":29619},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29955,\"start\":29897},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30470,\"start\":29956},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30731,\"start\":30471},{\"attributes\":{\"id\":\"fig_6\"},\"end\":30817,\"start\":30732},{\"attributes\":{\"id\":\"fig_7\"},\"end\":30826,\"start\":30818},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32395,\"start\":30827},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32557,\"start\":32396},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33424,\"start\":32558},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":33894,\"start\":33425},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34394,\"start\":33895}]", "paragraph": "[{\"end\":4514,\"start\":2392},{\"end\":5506,\"start\":4516},{\"end\":6267,\"start\":5508},{\"end\":7785,\"start\":6269},{\"end\":11522,\"start\":7806},{\"end\":12681,\"start\":11524},{\"end\":13005,\"start\":12683},{\"end\":13227,\"start\":13007},{\"end\":14213,\"start\":13303},{\"end\":15702,\"start\":14215},{\"end\":16331,\"start\":15704},{\"end\":16569,\"start\":16333},{\"end\":16970,\"start\":16571},{\"end\":18700,\"start\":16972},{\"end\":19444,\"start\":18732},{\"end\":19541,\"start\":19446},{\"end\":20030,\"start\":19568},{\"end\":21433,\"start\":20057},{\"end\":21804,\"start\":21447},{\"end\":22196,\"start\":21806},{\"end\":22482,\"start\":22198},{\"end\":22873,\"start\":22498},{\"end\":23598,\"start\":22875},{\"end\":24894,\"start\":23662},{\"end\":25401,\"start\":24927},{\"end\":25722,\"start\":25403},{\"end\":27961,\"start\":25768},{\"end\":28426,\"start\":27963},{\"end\":29618,\"start\":28446}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21594,\"start\":21587},{\"end\":23873,\"start\":23866},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26978,\"start\":26968}]", "section_header": "[{\"end\":2390,\"start\":2375},{\"end\":7804,\"start\":7788},{\"end\":13264,\"start\":13230},{\"end\":13301,\"start\":13267},{\"end\":18730,\"start\":18703},{\"end\":19566,\"start\":19544},{\"end\":20055,\"start\":20033},{\"end\":21445,\"start\":21436},{\"end\":22496,\"start\":22485},{\"end\":23611,\"start\":23601},{\"end\":23660,\"start\":23614},{\"end\":24925,\"start\":24897},{\"end\":25766,\"start\":25725},{\"end\":28444,\"start\":28429},{\"end\":29977,\"start\":29957},{\"end\":30480,\"start\":30472},{\"end\":30741,\"start\":30733},{\"end\":30825,\"start\":30819},{\"end\":32406,\"start\":32397},{\"end\":32569,\"start\":32559},{\"end\":33907,\"start\":33896}]", "table": "[{\"end\":32395,\"start\":32030},{\"end\":32557,\"start\":32423},{\"end\":33424,\"start\":32633},{\"end\":33894,\"start\":33571},{\"end\":34394,\"start\":34064}]", "figure_caption": "[{\"end\":29896,\"start\":29621},{\"end\":29955,\"start\":29899},{\"end\":30470,\"start\":29982},{\"end\":30731,\"start\":30482},{\"end\":30817,\"start\":30743},{\"end\":32030,\"start\":30829},{\"end\":32423,\"start\":32408},{\"end\":32633,\"start\":32572},{\"end\":33571,\"start\":33427},{\"end\":34064,\"start\":33911}]", "figure_ref": "[{\"end\":13880,\"start\":13858},{\"end\":14227,\"start\":14220},{\"end\":14471,\"start\":14463},{\"end\":16968,\"start\":16962},{\"end\":17079,\"start\":17073},{\"end\":18041,\"start\":18035},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19540,\"start\":19534},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21847,\"start\":21841},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24351,\"start\":24345},{\"end\":25721,\"start\":25715}]", "bib_author_first_name": "[{\"end\":34635,\"start\":34634},{\"end\":34649,\"start\":34648},{\"end\":34659,\"start\":34658},{\"end\":34671,\"start\":34670},{\"end\":34684,\"start\":34683},{\"end\":34697,\"start\":34696},{\"end\":34705,\"start\":34704},{\"end\":35050,\"start\":35049},{\"end\":35056,\"start\":35055},{\"end\":35064,\"start\":35063},{\"end\":35072,\"start\":35071},{\"end\":35080,\"start\":35079},{\"end\":35387,\"start\":35386},{\"end\":35399,\"start\":35398},{\"end\":35408,\"start\":35407},{\"end\":35419,\"start\":35418},{\"end\":35749,\"start\":35748},{\"end\":35760,\"start\":35759},{\"end\":35768,\"start\":35767},{\"end\":35779,\"start\":35778},{\"end\":35790,\"start\":35789},{\"end\":35792,\"start\":35791},{\"end\":36108,\"start\":36107},{\"end\":36110,\"start\":36109},{\"end\":36121,\"start\":36120},{\"end\":36123,\"start\":36122},{\"end\":36135,\"start\":36134},{\"end\":36146,\"start\":36145},{\"end\":36148,\"start\":36147},{\"end\":36453,\"start\":36452},{\"end\":36459,\"start\":36458},{\"end\":36467,\"start\":36466},{\"end\":36699,\"start\":36698},{\"end\":36711,\"start\":36710},{\"end\":36717,\"start\":36716},{\"end\":36726,\"start\":36725},{\"end\":37097,\"start\":37096},{\"end\":37104,\"start\":37103},{\"end\":37110,\"start\":37109},{\"end\":37757,\"start\":37756},{\"end\":37763,\"start\":37762},{\"end\":37765,\"start\":37764},{\"end\":37774,\"start\":37773},{\"end\":37782,\"start\":37781},{\"end\":37784,\"start\":37783},{\"end\":37797,\"start\":37796},{\"end\":37811,\"start\":37810},{\"end\":38182,\"start\":38181},{\"end\":38189,\"start\":38188},{\"end\":38199,\"start\":38198},{\"end\":38495,\"start\":38494},{\"end\":38503,\"start\":38502},{\"end\":38512,\"start\":38511},{\"end\":38521,\"start\":38520},{\"end\":38528,\"start\":38527},{\"end\":38862,\"start\":38861},{\"end\":38877,\"start\":38876},{\"end\":38886,\"start\":38885},{\"end\":38900,\"start\":38899},{\"end\":38915,\"start\":38914},{\"end\":38923,\"start\":38922},{\"end\":38938,\"start\":38937},{\"end\":38950,\"start\":38949},{\"end\":38962,\"start\":38961},{\"end\":38973,\"start\":38972},{\"end\":38982,\"start\":38981},{\"end\":38995,\"start\":38994},{\"end\":39555,\"start\":39554},{\"end\":39562,\"start\":39561},{\"end\":39570,\"start\":39569},{\"end\":39572,\"start\":39571},{\"end\":39579,\"start\":39578},{\"end\":39586,\"start\":39585},{\"end\":39594,\"start\":39593},{\"end\":39596,\"start\":39595},{\"end\":40048,\"start\":40047},{\"end\":40064,\"start\":40063},{\"end\":40075,\"start\":40074},{\"end\":40501,\"start\":40500},{\"end\":40510,\"start\":40509},{\"end\":40520,\"start\":40519},{\"end\":40522,\"start\":40521},{\"end\":40537,\"start\":40536},{\"end\":40539,\"start\":40538},{\"end\":40551,\"start\":40550},{\"end\":40854,\"start\":40853},{\"end\":40863,\"start\":40862},{\"end\":40874,\"start\":40873},{\"end\":40883,\"start\":40882},{\"end\":40892,\"start\":40891},{\"end\":40902,\"start\":40901},{\"end\":40916,\"start\":40915},{\"end\":41354,\"start\":41353},{\"end\":41361,\"start\":41360},{\"end\":41369,\"start\":41368},{\"end\":41377,\"start\":41376},{\"end\":41691,\"start\":41690},{\"end\":41702,\"start\":41698},{\"end\":41711,\"start\":41710},{\"end\":41968,\"start\":41967},{\"end\":41979,\"start\":41975},{\"end\":41988,\"start\":41987},{\"end\":42382,\"start\":42381},{\"end\":42391,\"start\":42390},{\"end\":42402,\"start\":42401},{\"end\":42412,\"start\":42411},{\"end\":42840,\"start\":42839},{\"end\":42842,\"start\":42841},{\"end\":42851,\"start\":42850},{\"end\":42857,\"start\":42856},{\"end\":42872,\"start\":42871},{\"end\":42879,\"start\":42878},{\"end\":42887,\"start\":42886},{\"end\":42901,\"start\":42900},{\"end\":42913,\"start\":42912},{\"end\":42927,\"start\":42926},{\"end\":42947,\"start\":42946},{\"end\":42957,\"start\":42956},{\"end\":43435,\"start\":43434},{\"end\":43437,\"start\":43436},{\"end\":43446,\"start\":43445},{\"end\":43452,\"start\":43451},{\"end\":43467,\"start\":43466},{\"end\":43481,\"start\":43480},{\"end\":43501,\"start\":43500},{\"end\":43511,\"start\":43510},{\"end\":43929,\"start\":43928},{\"end\":43939,\"start\":43938},{\"end\":43941,\"start\":43940},{\"end\":43950,\"start\":43949},{\"end\":44289,\"start\":44288},{\"end\":44293,\"start\":44290},{\"end\":44509,\"start\":44508},{\"end\":44511,\"start\":44510},{\"end\":44522,\"start\":44521},{\"end\":44536,\"start\":44535},{\"end\":44544,\"start\":44543},{\"end\":44556,\"start\":44555},{\"end\":44569,\"start\":44568},{\"end\":44957,\"start\":44956},{\"end\":44964,\"start\":44963},{\"end\":44973,\"start\":44972},{\"end\":44980,\"start\":44979},{\"end\":44987,\"start\":44986},{\"end\":44994,\"start\":44993},{\"end\":45001,\"start\":45000},{\"end\":45008,\"start\":45007},{\"end\":45183,\"start\":45182},{\"end\":45192,\"start\":45191},{\"end\":45199,\"start\":45198},{\"end\":45206,\"start\":45205},{\"end\":45215,\"start\":45214},{\"end\":45217,\"start\":45216},{\"end\":45634,\"start\":45633},{\"end\":45636,\"start\":45635},{\"end\":45643,\"start\":45642},{\"end\":45651,\"start\":45650},{\"end\":45660,\"start\":45659},{\"end\":45882,\"start\":45881},{\"end\":45893,\"start\":45892},{\"end\":45895,\"start\":45894},{\"end\":45903,\"start\":45902},{\"end\":45905,\"start\":45904},{\"end\":45914,\"start\":45913},{\"end\":45926,\"start\":45925},{\"end\":46156,\"start\":46152},{\"end\":46163,\"start\":46162},{\"end\":46177,\"start\":46176},{\"end\":46179,\"start\":46178},{\"end\":46188,\"start\":46187},{\"end\":46190,\"start\":46189},{\"end\":46203,\"start\":46202}]", "bib_author_last_name": "[{\"end\":34646,\"start\":34636},{\"end\":34656,\"start\":34650},{\"end\":34668,\"start\":34660},{\"end\":34681,\"start\":34672},{\"end\":34694,\"start\":34685},{\"end\":34702,\"start\":34698},{\"end\":34722,\"start\":34706},{\"end\":35053,\"start\":35051},{\"end\":35061,\"start\":35057},{\"end\":35069,\"start\":35065},{\"end\":35077,\"start\":35073},{\"end\":35085,\"start\":35081},{\"end\":35396,\"start\":35388},{\"end\":35405,\"start\":35400},{\"end\":35416,\"start\":35409},{\"end\":35425,\"start\":35420},{\"end\":35757,\"start\":35750},{\"end\":35765,\"start\":35761},{\"end\":35776,\"start\":35769},{\"end\":35787,\"start\":35780},{\"end\":35798,\"start\":35793},{\"end\":36118,\"start\":36111},{\"end\":36132,\"start\":36124},{\"end\":36143,\"start\":36136},{\"end\":36154,\"start\":36149},{\"end\":36456,\"start\":36454},{\"end\":36464,\"start\":36460},{\"end\":36472,\"start\":36468},{\"end\":36708,\"start\":36700},{\"end\":36714,\"start\":36712},{\"end\":36723,\"start\":36718},{\"end\":36736,\"start\":36727},{\"end\":37101,\"start\":37098},{\"end\":37107,\"start\":37105},{\"end\":37113,\"start\":37111},{\"end\":37760,\"start\":37758},{\"end\":37771,\"start\":37766},{\"end\":37779,\"start\":37775},{\"end\":37794,\"start\":37785},{\"end\":37808,\"start\":37798},{\"end\":37818,\"start\":37812},{\"end\":38186,\"start\":38183},{\"end\":38196,\"start\":38190},{\"end\":38206,\"start\":38200},{\"end\":38500,\"start\":38496},{\"end\":38509,\"start\":38504},{\"end\":38518,\"start\":38513},{\"end\":38525,\"start\":38522},{\"end\":38532,\"start\":38529},{\"end\":38874,\"start\":38863},{\"end\":38883,\"start\":38878},{\"end\":38897,\"start\":38887},{\"end\":38912,\"start\":38901},{\"end\":38920,\"start\":38916},{\"end\":38935,\"start\":38924},{\"end\":38947,\"start\":38939},{\"end\":38959,\"start\":38951},{\"end\":38970,\"start\":38963},{\"end\":38979,\"start\":38974},{\"end\":38992,\"start\":38983},{\"end\":39003,\"start\":38996},{\"end\":39559,\"start\":39556},{\"end\":39567,\"start\":39563},{\"end\":39576,\"start\":39573},{\"end\":39583,\"start\":39580},{\"end\":39591,\"start\":39587},{\"end\":39603,\"start\":39597},{\"end\":40061,\"start\":40049},{\"end\":40072,\"start\":40065},{\"end\":40082,\"start\":40076},{\"end\":40507,\"start\":40502},{\"end\":40517,\"start\":40511},{\"end\":40534,\"start\":40523},{\"end\":40548,\"start\":40540},{\"end\":40559,\"start\":40552},{\"end\":40860,\"start\":40855},{\"end\":40871,\"start\":40864},{\"end\":40880,\"start\":40875},{\"end\":40889,\"start\":40884},{\"end\":40899,\"start\":40893},{\"end\":40913,\"start\":40903},{\"end\":40923,\"start\":40917},{\"end\":41358,\"start\":41355},{\"end\":41366,\"start\":41362},{\"end\":41374,\"start\":41370},{\"end\":41381,\"start\":41378},{\"end\":41696,\"start\":41692},{\"end\":41708,\"start\":41703},{\"end\":41717,\"start\":41712},{\"end\":41973,\"start\":41969},{\"end\":41985,\"start\":41980},{\"end\":41993,\"start\":41989},{\"end\":42388,\"start\":42383},{\"end\":42399,\"start\":42392},{\"end\":42409,\"start\":42403},{\"end\":42422,\"start\":42413},{\"end\":42848,\"start\":42843},{\"end\":42854,\"start\":42852},{\"end\":42869,\"start\":42858},{\"end\":42876,\"start\":42873},{\"end\":42884,\"start\":42880},{\"end\":42898,\"start\":42888},{\"end\":42910,\"start\":42902},{\"end\":42924,\"start\":42914},{\"end\":42944,\"start\":42928},{\"end\":42954,\"start\":42948},{\"end\":42967,\"start\":42958},{\"end\":43443,\"start\":43438},{\"end\":43449,\"start\":43447},{\"end\":43464,\"start\":43453},{\"end\":43478,\"start\":43468},{\"end\":43498,\"start\":43482},{\"end\":43508,\"start\":43502},{\"end\":43521,\"start\":43512},{\"end\":43936,\"start\":43930},{\"end\":43947,\"start\":43942},{\"end\":43961,\"start\":43951},{\"end\":44300,\"start\":44294},{\"end\":44519,\"start\":44512},{\"end\":44533,\"start\":44523},{\"end\":44541,\"start\":44537},{\"end\":44553,\"start\":44545},{\"end\":44566,\"start\":44557},{\"end\":44575,\"start\":44570},{\"end\":44961,\"start\":44958},{\"end\":44970,\"start\":44965},{\"end\":44977,\"start\":44974},{\"end\":44984,\"start\":44981},{\"end\":44991,\"start\":44988},{\"end\":44998,\"start\":44995},{\"end\":45005,\"start\":45002},{\"end\":45011,\"start\":45009},{\"end\":45189,\"start\":45184},{\"end\":45196,\"start\":45193},{\"end\":45203,\"start\":45200},{\"end\":45212,\"start\":45207},{\"end\":45228,\"start\":45218},{\"end\":45640,\"start\":45637},{\"end\":45648,\"start\":45644},{\"end\":45657,\"start\":45652},{\"end\":45664,\"start\":45661},{\"end\":45890,\"start\":45883},{\"end\":45900,\"start\":45896},{\"end\":45911,\"start\":45906},{\"end\":45923,\"start\":45915},{\"end\":45934,\"start\":45927},{\"end\":46160,\"start\":46157},{\"end\":46174,\"start\":46164},{\"end\":46185,\"start\":46180},{\"end\":46200,\"start\":46191},{\"end\":46220,\"start\":46204}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":225025209},\"end\":35001,\"start\":34564},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4362611},\"end\":35317,\"start\":35003},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4063197},\"end\":35680,\"start\":35319},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218890043},\"end\":35993,\"start\":35682},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":227762724},\"end\":36395,\"start\":35995},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":85547734},\"end\":36625,\"start\":36397},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":13561174},\"end\":36966,\"start\":36627},{\"attributes\":{\"doi\":\"10.1145/1925861.1925870\",\"id\":\"b7\"},\"end\":37043,\"start\":36968},{\"attributes\":{\"doi\":\"http:/doi.acm.org/10.1145/2789168.2790124\",\"id\":\"b8\",\"matched_paper_id\":9786871},\"end\":37674,\"start\":37045},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":237731033},\"end\":38107,\"start\":37676},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":202689524},\"end\":38413,\"start\":38109},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":69473552},\"end\":38783,\"start\":38415},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":225039882},\"end\":39452,\"start\":38785},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":198918103},\"end\":39962,\"start\":39454},{\"attributes\":{\"id\":\"b14\"},\"end\":40226,\"start\":39964},{\"attributes\":{\"id\":\"b15\"},\"end\":40406,\"start\":40228},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":221845955},\"end\":40791,\"start\":40408},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":233444273},\"end\":41304,\"start\":40793},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":235436185},\"end\":41652,\"start\":41306},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":233024831},\"end\":41870,\"start\":41654},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":235313572},\"end\":42308,\"start\":41872},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":222134093},\"end\":42729,\"start\":42310},{\"attributes\":{\"doi\":\"10.1038/s41597-022-01573-2\",\"id\":\"b22\",\"matched_paper_id\":238531437},\"end\":43382,\"start\":42731},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":225742649},\"end\":43831,\"start\":43384},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6190854},\"end\":44246,\"start\":43833},{\"attributes\":{\"id\":\"b25\"},\"end\":44420,\"start\":44248},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":235485156},\"end\":44900,\"start\":44422},{\"attributes\":{\"id\":\"b27\"},\"end\":45143,\"start\":44902},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6773885},\"end\":45571,\"start\":45145},{\"attributes\":{\"id\":\"b29\"},\"end\":45829,\"start\":45573},{\"attributes\":{\"id\":\"b30\"},\"end\":46101,\"start\":45831},{\"attributes\":{\"id\":\"b31\"},\"end\":46413,\"start\":46103}]", "bib_title": "[{\"end\":34632,\"start\":34564},{\"end\":35047,\"start\":35003},{\"end\":35384,\"start\":35319},{\"end\":35746,\"start\":35682},{\"end\":36105,\"start\":35995},{\"end\":36450,\"start\":36397},{\"end\":36696,\"start\":36627},{\"end\":37094,\"start\":37045},{\"end\":37754,\"start\":37676},{\"end\":38179,\"start\":38109},{\"end\":38492,\"start\":38415},{\"end\":38859,\"start\":38785},{\"end\":39552,\"start\":39454},{\"end\":40498,\"start\":40408},{\"end\":40851,\"start\":40793},{\"end\":41351,\"start\":41306},{\"end\":41688,\"start\":41654},{\"end\":41965,\"start\":41872},{\"end\":42379,\"start\":42310},{\"end\":42837,\"start\":42731},{\"end\":43432,\"start\":43384},{\"end\":43926,\"start\":43833},{\"end\":44506,\"start\":44422},{\"end\":45180,\"start\":45145}]", "bib_author": "[{\"end\":34648,\"start\":34634},{\"end\":34658,\"start\":34648},{\"end\":34670,\"start\":34658},{\"end\":34683,\"start\":34670},{\"end\":34696,\"start\":34683},{\"end\":34704,\"start\":34696},{\"end\":34724,\"start\":34704},{\"end\":35055,\"start\":35049},{\"end\":35063,\"start\":35055},{\"end\":35071,\"start\":35063},{\"end\":35079,\"start\":35071},{\"end\":35087,\"start\":35079},{\"end\":35398,\"start\":35386},{\"end\":35407,\"start\":35398},{\"end\":35418,\"start\":35407},{\"end\":35427,\"start\":35418},{\"end\":35759,\"start\":35748},{\"end\":35767,\"start\":35759},{\"end\":35778,\"start\":35767},{\"end\":35789,\"start\":35778},{\"end\":35800,\"start\":35789},{\"end\":36120,\"start\":36107},{\"end\":36134,\"start\":36120},{\"end\":36145,\"start\":36134},{\"end\":36156,\"start\":36145},{\"end\":36458,\"start\":36452},{\"end\":36466,\"start\":36458},{\"end\":36474,\"start\":36466},{\"end\":36710,\"start\":36698},{\"end\":36716,\"start\":36710},{\"end\":36725,\"start\":36716},{\"end\":36738,\"start\":36725},{\"end\":37103,\"start\":37096},{\"end\":37109,\"start\":37103},{\"end\":37115,\"start\":37109},{\"end\":37762,\"start\":37756},{\"end\":37773,\"start\":37762},{\"end\":37781,\"start\":37773},{\"end\":37796,\"start\":37781},{\"end\":37810,\"start\":37796},{\"end\":37820,\"start\":37810},{\"end\":38188,\"start\":38181},{\"end\":38198,\"start\":38188},{\"end\":38208,\"start\":38198},{\"end\":38502,\"start\":38494},{\"end\":38511,\"start\":38502},{\"end\":38520,\"start\":38511},{\"end\":38527,\"start\":38520},{\"end\":38534,\"start\":38527},{\"end\":38876,\"start\":38861},{\"end\":38885,\"start\":38876},{\"end\":38899,\"start\":38885},{\"end\":38914,\"start\":38899},{\"end\":38922,\"start\":38914},{\"end\":38937,\"start\":38922},{\"end\":38949,\"start\":38937},{\"end\":38961,\"start\":38949},{\"end\":38972,\"start\":38961},{\"end\":38981,\"start\":38972},{\"end\":38994,\"start\":38981},{\"end\":39005,\"start\":38994},{\"end\":39561,\"start\":39554},{\"end\":39569,\"start\":39561},{\"end\":39578,\"start\":39569},{\"end\":39585,\"start\":39578},{\"end\":39593,\"start\":39585},{\"end\":39605,\"start\":39593},{\"end\":40063,\"start\":40047},{\"end\":40074,\"start\":40063},{\"end\":40084,\"start\":40074},{\"end\":40509,\"start\":40500},{\"end\":40519,\"start\":40509},{\"end\":40536,\"start\":40519},{\"end\":40550,\"start\":40536},{\"end\":40561,\"start\":40550},{\"end\":40862,\"start\":40853},{\"end\":40873,\"start\":40862},{\"end\":40882,\"start\":40873},{\"end\":40891,\"start\":40882},{\"end\":40901,\"start\":40891},{\"end\":40915,\"start\":40901},{\"end\":40925,\"start\":40915},{\"end\":41360,\"start\":41353},{\"end\":41368,\"start\":41360},{\"end\":41376,\"start\":41368},{\"end\":41383,\"start\":41376},{\"end\":41698,\"start\":41690},{\"end\":41710,\"start\":41698},{\"end\":41719,\"start\":41710},{\"end\":41975,\"start\":41967},{\"end\":41987,\"start\":41975},{\"end\":41995,\"start\":41987},{\"end\":42390,\"start\":42381},{\"end\":42401,\"start\":42390},{\"end\":42411,\"start\":42401},{\"end\":42424,\"start\":42411},{\"end\":42850,\"start\":42839},{\"end\":42856,\"start\":42850},{\"end\":42871,\"start\":42856},{\"end\":42878,\"start\":42871},{\"end\":42886,\"start\":42878},{\"end\":42900,\"start\":42886},{\"end\":42912,\"start\":42900},{\"end\":42926,\"start\":42912},{\"end\":42946,\"start\":42926},{\"end\":42956,\"start\":42946},{\"end\":42969,\"start\":42956},{\"end\":43445,\"start\":43434},{\"end\":43451,\"start\":43445},{\"end\":43466,\"start\":43451},{\"end\":43480,\"start\":43466},{\"end\":43500,\"start\":43480},{\"end\":43510,\"start\":43500},{\"end\":43523,\"start\":43510},{\"end\":43938,\"start\":43928},{\"end\":43949,\"start\":43938},{\"end\":43963,\"start\":43949},{\"end\":44302,\"start\":44288},{\"end\":44521,\"start\":44508},{\"end\":44535,\"start\":44521},{\"end\":44543,\"start\":44535},{\"end\":44555,\"start\":44543},{\"end\":44568,\"start\":44555},{\"end\":44577,\"start\":44568},{\"end\":44963,\"start\":44956},{\"end\":44972,\"start\":44963},{\"end\":44979,\"start\":44972},{\"end\":44986,\"start\":44979},{\"end\":44993,\"start\":44986},{\"end\":45000,\"start\":44993},{\"end\":45007,\"start\":45000},{\"end\":45013,\"start\":45007},{\"end\":45191,\"start\":45182},{\"end\":45198,\"start\":45191},{\"end\":45205,\"start\":45198},{\"end\":45214,\"start\":45205},{\"end\":45230,\"start\":45214},{\"end\":45642,\"start\":45633},{\"end\":45650,\"start\":45642},{\"end\":45659,\"start\":45650},{\"end\":45666,\"start\":45659},{\"end\":45892,\"start\":45881},{\"end\":45902,\"start\":45892},{\"end\":45913,\"start\":45902},{\"end\":45925,\"start\":45913},{\"end\":45936,\"start\":45925},{\"end\":46162,\"start\":46152},{\"end\":46176,\"start\":46162},{\"end\":46187,\"start\":46176},{\"end\":46202,\"start\":46187},{\"end\":46222,\"start\":46202}]", "bib_venue": "[{\"end\":34758,\"start\":34724},{\"end\":35139,\"start\":35087},{\"end\":35479,\"start\":35427},{\"end\":35813,\"start\":35800},{\"end\":36169,\"start\":36156},{\"end\":36490,\"start\":36474},{\"end\":36765,\"start\":36738},{\"end\":37264,\"start\":37156},{\"end\":37864,\"start\":37820},{\"end\":38246,\"start\":38208},{\"end\":38571,\"start\":38534},{\"end\":39087,\"start\":39005},{\"end\":39682,\"start\":39605},{\"end\":40045,\"start\":39964},{\"end\":40302,\"start\":40228},{\"end\":40572,\"start\":40561},{\"end\":40979,\"start\":40925},{\"end\":41435,\"start\":41383},{\"end\":41736,\"start\":41719},{\"end\":42047,\"start\":41995},{\"end\":42476,\"start\":42424},{\"end\":43010,\"start\":42995},{\"end\":43587,\"start\":43523},{\"end\":44013,\"start\":43963},{\"end\":44286,\"start\":44248},{\"end\":44618,\"start\":44577},{\"end\":44954,\"start\":44902},{\"end\":45256,\"start\":45230},{\"end\":45631,\"start\":45573},{\"end\":45879,\"start\":45831},{\"end\":46150,\"start\":46103},{\"end\":37376,\"start\":37266},{\"end\":39096,\"start\":39089},{\"end\":40996,\"start\":40981},{\"end\":41749,\"start\":41738}]"}}}, "year": 2023, "month": 12, "day": 17}