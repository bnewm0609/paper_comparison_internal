{"id": 253531392, "updated": "2022-11-17 17:51:22.714", "metadata": {"title": "Indian Peak Power demand Forecasting: Transformer Based Implementation of Temporal Architecture", "authors": "[{\"first\":\"Shashwat\",\"last\":\"Jha\",\"middle\":[]},{\"first\":\"Vishvaditya\",\"last\":\"Luhach\",\"middle\":[]}]", "venue": "2022 IEEE Global Conference on Computing, Power and Communication Technologies (GlobConPT)", "journal": "2022 IEEE Global Conference on Computing, Power and Communication Technologies (GlobConPT)", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "The long-term forecast of electricity demand has been a recurrent research topic, due to its economic and strategic relevance. Several machine learning as well as deep learning techniques have evolved in parallel with the complexity of the peak demand, planning for future generation facilities and transmission augmentation. Most of these proposed techniques work on short-term forecasting as long-term forecasting is considerably more challenging due to unpredictable and unforeseeable variables that may arise in the future. This paper proposes a Temporal Fusion Transformer based deep learning approach for long term forecasting of peak power demand. The dataset used in this paper consists of peak power demand in India for a period of 6 years and the prediction was done for a period of 1 year. Our proposed model was compared with other popular forecasting models and it performed considerably better in benchmarks and was also more accurate in modelling the variance in the power demand.", "fields_of_study": null, "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.1109/globconpt57482.2022.9938155"}}, "content": {"source": {"pdf_hash": "7166e0e68292829bdf7633bb9c72fa39b85c1167", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "06acdeca87a7d83aab4bd53cef93171ab8add255", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7166e0e68292829bdf7633bb9c72fa39b85c1167.txt", "contents": "\nIndian Peak Power demand Forecasting : Transformer Based Implementation of Temporal Architecture\n\n\nShashwat Jha shashwat6jha@gmail.com \nDepartment of Electronics and Electrical Engineering\nDepartment of Computer Science and Engineering Maharaja Surajmal Institute of Technology New Delhi\nBirla Institute of Technology Mesra Ranchi\nIndia, India\n\nVishvaditya Luhach vishvadityaluhach@gmail.co \nDepartment of Electronics and Electrical Engineering\nDepartment of Computer Science and Engineering Maharaja Surajmal Institute of Technology New Delhi\nBirla Institute of Technology Mesra Ranchi\nIndia, India\n\nIndian Peak Power demand Forecasting : Transformer Based Implementation of Temporal Architecture\n10.1109/GLOBCONPT57482.2022.9938155\u2020 Equal ContributionArtificial Intelligence (AI)Machine Learning (ML)Deep Learning (DL)Time series forecastingTemporal Fusion Transformer (TFT)Peak power demand forecasting\nThe long-term forecast of electricity demand has been a recurrent research topic, due to its economic and strategic relevance. Several machine learning as well as deep learning techniques have evolved in parallel with the complexity of the peak demand, planning for future generation facilities and transmission augmentation. Most of these proposed techniques work on short-term forecasting as long-term forecasting is considerably more challenging due to unpredictable and unforeseeable variables that may arise in the future. This paper proposes a Temporal Fusion Transformer based deep learning approach for long term forecasting of peak power demand. The dataset used in this paper consists of peak power demand in India for a period of 6 years and the prediction was done for a period of 1 year. Our proposed model was compared with other popular forecasting models and it performed considerably better in benchmarks and was also more accurate in modelling the variance in the power demand.\n\nINTRODUCTION\n\nLong term power demand forecasting is one of the most vigorously researched domains pertaining to its relevance in the socio-economic functioning of various countries. Peak electricity demand of a given time frame is subject to a range of uncertainties, including underlying growth in population, the development of technology, various aspects of the of the economy, varying weather conditions (and the timing of those conditions), as well as the general randomness inherent in individual usage of power. It is also subject to some known calendar effects due to the time of day, day of week, time of year, and public holidays [1]. Accurate forecasting methods can be of immense value in developing power supply strategy and development plan, especially for developing countries like India where the demand is increased with dynamic and high growth rate [2]. With the consistently increasing power demand, Judicious utilization of all relevant resources becomes of extreme importance hence contributions in the development of better forecasting techniques is pivotal.\n\n\nII. LITERATURE REVIEW\n\nUpon exploration it's clear that power demand forecasting has been addressed by using various approaches based on machine learning as well as deep learning. Some of the early approaches during late 1990s for power forecasting include applications of regression analysis comprising of various techniques like composite multi-regression decompositionbased model [3] and multiple linear regression [4].\n\nFurther developments led to applications of stochastic time series models like ARIMA [5] and SARIMA [6] for short term as well as long term demand forecasting.\n\nIn terms of approaches based on deep neural networks, prior research shows applications of cascaded artificial neural network models [7], ANNs with quasi-newton based back propagation and utilization of principal component analysis (PCA) have also been proposed [8]. Further developments also lead to applications of recurrent neural networks (RNN) with dynamic time warping [9]. Probabilistic density-based forecasting techniques [10] as well as Relevance vector machine (RVM) based approaches [11] have also been presented especially for long term forecasting.\n\nPrior works which have been presented do not address the extreme outliers and variations in peak power demand data of a country like India. Moreover, the trade-off between the accurate prediction, computational cost and the time taken is significant. Therefore, making it extremely important to investigate new and more efficient methods for long term peak power demand forecasting.\n\nWith this paper we intend to propose the application of transformer based TFT model and investigate its performance while comparing it to other deep learning and statistical models in accurately predicting the long-term peak power demand in India.\n\n\nIII. DATASET\n\nThe Indian Power Sector Evening Hour Peak Demand data from January 2014 to the last day of 2019 within which the daily overall peak demand data has been utilized for our experiment [12] with 2191 datapoints in tabular format. The data was scaled between 0 and 1 before feeding it to the models.\n\n\nIV. PROPOSED METHODOLOGY\n\n\nA. Temporal Fusion Transformer (TFT)\n\nIntroduced by Lim et al [14] temporal fusion transformer pertaining to its intricate architecture became one of the most powerful tools in forecasting. TFT inputs static metadata, time-varying past inputs and time varying future inputs.\n\nVariable Selection is used for judicious selection of the most salient features based on the input. Gated Residual Network blocks [15] enable efficient flow of information with skip connections as well as gating layers. Time-dependent processing is done by utilizing LSTMs [19] for local processing of information, and the integration of information from any time step is done by multi-head attention [16]. Figure 1 depicts the complete in-depth architecture of TFT. The major constituents of TFT and their functions are described below:\n\n1) Gating mechanisms: These are utilised to reduce extra computational cost by skipping components of the architecture which are not utilised for the task, providing adaptive depth and network complexity to be tolerant towards a variety of datasets and scenarios.\n\n2) Variable selection networks: These selection networks assist in selecting relevant input variables for every time step.\n\n3) Static covariate encoders: Covariate encoders are utilised for the integration of fratures which are static into the model by encoding of context vectors for the temporal dynamic health of the process. 4) Temporal processing: It is utilised to learn both shortterm as well as long-term temporal relationships from both known as well as observed inputs that vary with time. The local processing of information is done by a sequence-tosequence layer, whereas a multi-head attention block is used for capturing long-term dependencies .The multihead attention bloock represented in Figure 2 is responsible for the integration of the attention outputs linearly. The first thing that happens is a sequence of embeddings is fed into the encoder, the embeddings are then computed upon to generate three distinct linear transformations resulting in three vectors namely 'query', 'key' and 'value'. Now these 3 vectors are then used to calculate the attention outputs for each embedding which are basically dot products of the 3 vectors for each separate embedding. Calculation of self-attention occurs multiple times independently and in parallel. It is therefore referred to as Multi-head Attention. The attention measures the strength of the relationship between the patches and then further helps in the prediction.\n\n\n5) Prediction intervals via quantile forecasts:\n\nThese intervals are utilised specifically to realise the range of high probabiliy destination values. \n\n\nB. Temporal Convoltional Network (TCN)\n\nFirst introduced in 2016 by Lea et al [17] temporal convolutional networks were initially proposed for video-based action segmentation. Later post identifying its high-level capability in capturing temporal relationships similar to RNNs and the training time being a fraction of what it would be with RNNs it was utilised for time series prediction as well. A great example of which was presented by Yan et al [18] where he utilised TCN as a part of his ensemble method to predict El Ni\u00f1o-Southern Oscillation (ENSO) in advance. TCNs contain two kinds of convolutions one is called Causal whereas the other Dilated which contribute to TCNs having no information \"leakage\" from future to past and the ability of the architecture to take a sequence of any length and map it to an output sequence of the same length. TCNs also have very long effective history using a combination of dilated convolutions and very deep networks. Temporal Fusion Transformer Architecture\n\n\nC. Stacked LSTMs\n\nLSTM (Long Short-Term Memory) was first introduced in 1996 by Hochreiter et al [19] whereas the full backpropagation and bi directional LSTMs were introduced by Graves.A. and Schmidhuber. J. in 2005 [20] Since then LSTMs have been utilized on multiple fronts in the domain of artificial intelligence [21][22][23]. It is a variety of Recurrent Neural Networks (RNNs) that are capable of learning longterm dependencies, especially in sequence prediction problems. LSTMs have feedback connections implying they are capable of processing the entire sequence of data, apart from single data points such as images. The central role of an LSTM model is held by a memory cell known as a 'cell state' that maintains its state over time. Information can be added to or removed from the cell state in LSTM and is regulated by gates. These gates optionally let the information flow in and out of the cell. It contains a pointwise multiplication operation and a sigmoid neural net layer that assist the mechanism. Stacking LSTMs together has proven to be an exceptional forecasting technique since its inception and has been widely utilized for applications in various time series problems. Figure 3 depicts the architecture of a stacked LSTM model.\n\n\nD. Na\u00efve Forecasting (Seasonal + Drift)\n\nThis method utilizes two of the models based on the Na\u00efve forecasting methods. The seasonal forecasting model always predicts the value of K time steps ago. When K=1, this model predicts the last value of the training set. When K>1, it repeats the last K values of the training set whereas the Na\u00efve drift model fits a line between the first and last point of the training series, and extends it in the future.\n\n\nV. EXPERIMENTATION\n\nThe experimentation has been implemented using TensorFlow v2 while using the Keras API as well as Darts API. All deep learning models were trained for 200 epochs and then performance was analyzed for the prediction of Indian peak power demand. The hardware specifications of the system on which the models were trained comprised of AMD Ryzen 5 3600 CPU, 16 GB of system memory and Nvidia 1660ti GPU with 6GB of VRAM was utilized for acceleration.\n\nThe experiment was conducted such that the dataset after pre-processing was fed into 4 different models for forecasting which are explained in this section and their performance was compared on the basis of MAPE (mean absolute percentage error). Future covariates [13] were utilised for this time series prediction task. Future covariates are time series whose future values are available at prediction time. More precisely, for a prediction made at time t for a forecast horizon n and lookback window k, the values at times (t+1, \u2026, t+n) and past values for times (t-k, t-k+1, \u2026, t) of future covariates are also known.\n\nThe input chunk length for the deep learning models -TFT, LSTM, and TCN -was set at 30 and dropout rate was set at 0.1. Mean squared error was used as the loss function and a batch size of 24 was taken for all the models. For the TFT model, the output chunk length (forecast horizon) was set as 36. The number of various layers in the TFT model were -64 hidden layer, 4 LSTM layers, and 2 Attention heads. The LSTM model had 25 hidden layers and 3 RNN layers. For the TCN model, the output chunk length was set at 28 with kernel size-3 and 6 filters with a total of 4 layers. \n\n\nVI. RESULTS\n\nUpon application of all the mentioned models the performance was analysed on the basis of MAPE (mean absolute percentage error). Cross model comparison shows that Na\u00efve Forecasting predicted the power demand with MAPE 5.06% while Temporal Convolutional Network performed the task with MAPE 7.94%. Stacked LSTMs based technique attained MAPE 4.71% whereas the attention based TFT (Temporal Fusion Transformer) attained an MAPE of 4.15%. The cross-model performance comparison is exhibited in Table  1 whereas the figures [4][5][6] depict the prediction performance in the graphical format. \n\n\nVII. CONCLUSION\n\nAfter analysis of all models, it is certain that transformerbased models have proven to be effective in predicting the peak power demand in India considering the nature of the dataset. The temporal fusion transformer (TFT) had the best performance amongst all followed by the stacked LSTM model. The Temporal Convolutional Network performed the poorest in this experiment whereas Na\u00efve Forecasting performed well but wasn't at par with TFT and LSTMs.\n\nSince it is evident that transformer-based models incorporate great caliber when it comes to time series prediction, future research is encouraged as one can fine tune these architectures further as well as improve the quality and size of dataset for better performance and analysis.\n\n\nFig. 1. Temporal Fusion Transformer (TFT) Architecture\n\nFig. 2 .Fig. 6 .\n26MultiTemporal Convolutional Network Predicted Output 4\n\nTABLE I .\nICROSS MODEL PERFORMANCE COMPARISON ON THE BAISS OF M.A.P.E.\n\nDensity Forecasting for Long-Term Peak Electricity Demand. R J Hyndman, S Fan, IEEE Transactions on Power Systems. 25R. J. Hyndman and S. Fan, \"Density Forecasting for Long-Term Peak Electricity Demand,\" in IEEE Transactions on Power Systems, vol. 25, no. 2, pp. 1142-1153, May 2010.\n\nLong-term electricity demand forecasting using relevance vector learning mechanism. Z Du, L Niu, J Zhao, International Symposium on Neural Networks. Berlin, HeidelbergSpringerZ. Du, L. Niu, and J. Zhao, \"Long-term electricity demand forecasting using relevance vector learning mechanism,\" in International Symposium on Neural Networks, pp. 465-472, Springer, Berlin, Heidelberg, 2007.\n\nForecasting monthly peak demand in fast growing electric utility using a composite multiregressiondecomposition model. E H Barakat, M A M Eissa, IEE Proceedings C (Generation, Transmission and Distribution). 136IET Digital LibraryE. H. Barakat, and M. A. M. Eissa, \"Forecasting monthly peak demand in fast growing electric utility using a composite multiregression- decomposition model.\" In IEE Proceedings C (Generation, Transmission and Distribution), vol. 136, no. 1, pp. 35-41, IET Digital Library, 1989.\n\nShort-run forecasts of electricity loads and peaks. R Ramanathan, R Engle, C Granger, F Vahid-Araghi, C Brace, International journal of forecasting. 132R. Ramanathan, R. Engle, C. WJ Granger, F. Vahid-Araghi, and C. Brace, \"Short-run forecasts of electricity loads and peaks,\" International journal of forecasting, vol. 13, no. 2, pp. 161-174, 1997.\n\nShort-term hourly load forecasting using time-series modeling with peak load estimation capability. N Amjady, IEEE Transactions on Power Systems. 16N. Amjady, \"Short-term hourly load forecasting using time-series modeling with peak load estimation capability,\" in IEEE Transactions on Power Systems, vol. 16, no. 3, pp. 498-505, August 2001.\n\nFinding the best ARIMA model to forecast daily peak electricity demand. Mohamad As&apos; Ad, Proceedings of the Fifth Annual ASEARC Conference. the Fifth Annual ASEARC ConferenceAs' ad, Mohamad, \"Finding the best ARIMA model to forecast daily peak electricity demand,\" in Proceedings of the Fifth Annual ASEARC Conference, pp. 1-4, 2012.\n\nCascaded artificial neural networks for short-term load forecasting. A S Alfuhaid, M A El-Sayed, M S Mahmoud, IEEE Transactions on Power Systems. 12A. S. AlFuhaid, M. A. El-Sayed and M. S. Mahmoud, \"Cascaded artificial neural networks for short-term load forecasting,\" in IEEE Transactions on Power Systems, vol. 12, no. 4, pp. 1524-1529, Nov. 1997.\n\nArtificial neural network-based peak load forecasting using conjugate gradient methods. L , Mohan Saini, M. Kumar Soni, IEEE Transactions on Power Systems. 17L. Mohan Saini and M. Kumar Soni, \"Artificial neural network-based peak load forecasting using conjugate gradient methods,\" in IEEE Transactions on Power Systems, vol. 17, no. 3, pp. 907-912, Aug. 2002.\n\nDeep Learning for Daily Peak Load Forecasting-A Novel Gated Recurrent Neural Network Combining Dynamic Time Warping. Z Yu, Z Niu, W Tang, Q Wu, IEEE Access. 7Z. Yu, Z. Niu, W. Tang and Q. Wu, \"Deep Learning for Daily Peak Load Forecasting-A Novel Gated Recurrent Neural Network Combining Dynamic Time Warping,\" IEEE Access, vol. 7, pp. 17184-17194, 2019.\n\nDensity Forecasting for Long-Term Peak Electricity Demand. R J Hyndman, S Fan, IEEE Transactions on Power Systems. 25R. J. Hyndman and S. Fan, \"Density Forecasting for Long-Term Peak Electricity Demand,\" in IEEE Transactions on Power Systems, vol. 25, no. 2, pp. 1142-1153, May 2010.\n\nS Fei, Z Hou, C Sun, Huaguang Zhang, Advances in Neural Networks: ISNN 2007. Derong LiuSpringerS. Fei, Z. Hou, C. Sun, Huaguang Zhang, and Derong Liu, eds. Advances in Neural Networks: ISNN 2007. Springer, 2007.\n\nIndian Power Sector Peak Power Demand Time Series. Nirjhar Basu, 10.21227/jcsm-hj03IEEE Dataport. Nirjhar Basu, March 2, 2022, \"Indian Power Sector Peak Power Demand Time Series\", IEEE Dataport, doi: https://dx.doi.org/10.21227/jcsm-hj03.\n\nJ Herzen, F L\u00e4ssig, arXiv:2110.03224Darts: User Friendly Modern Machine Learning for Time Series. arXiv preprintJ. Herzen, F. L\u00e4ssig et al., \"Darts: User Friendly Modern Machine Learning for Time Series,\" arXiv preprint arXiv: 2110.03224, 2021.\n\nTemporal fusion transformers for interpretable multi-horizon time series forecasting. B Lim, S \u00d6 Ar\u0131k, N Loeff, T Pfister, International Journal of Forecasting. 374B. Lim, S. \u00d6. Ar\u0131k, N. Loeff, and T. Pfister, \"Temporal fusion transformers for interpretable multi-horizon time series forecasting,\" International Journal of Forecasting, vol. 37, no. 4, pp. 1748-1764, 2021.\n\nOn the properties of neural machine translation: Encoder-decoder approaches. K Cho, B V Merri\u00ebnboer, D Bahdanau, Y Bengio, arXiv:1409.1259arXiv preprintK. Cho, B. V. Merri\u00ebnboer, D. Bahdanau, and Y. Bengio, \"On the properties of neural machine translation: Encoder-decoder approaches,\" arXiv preprint arXiv:1409.1259, 2014.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Advances in neural information processing systems. 30A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \"Attention is all you need,\" Advances in neural information processing systems, vol. 30, 2017.\n\nTemporal convolutional networks: A unified approach to action segmentation. C Lea, R Vidal, A Reiter, G D Hager, European conference on computer vision. C. Lea, R. Vidal, A, Reiter, and G. D. Hager, \"Temporal convolutional networks: A unified approach to action segmentation,\" in European conference on computer vision, pp. 47-54, 2016.\n\nTemporal convolutional networks for the advance prediction of ENSO. J Yan, L Mu, L Wang, R Ranjan, A Y Zomaya, Scientific reports. 101J. Yan, L. Mu, L. Wang, R. Ranjan, and A. Y. Zomaya, \"Temporal convolutional networks for the advance prediction of ENSO,\" Scientific reports, vol. 10, no. 1, pp. 1-15, 2020.\n\nLSTM can solve hard long time lag problems. S Hochreiter, J Schmidhuber, Advances in neural information processing systems. 9S. Hochreiter, and J. Schmidhuber, \"LSTM can solve hard long time lag problems,\" Advances in neural information processing systems, vol. 9, 1996.\n\nFramewise phoneme classification with bidirectional LSTM networks. A Graves, J Schmidhuber, 10.1109/IJCNN.2005.1556215Proceedings. 2005 IEEE International Joint Conference on Neural Networks. 2005 IEEE International Joint Conference on Neural Networks4A. Graves and J. Schmidhuber, \"Framewise phoneme classification with bidirectional LSTM networks,\" Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., 2005, pp. 2047-2052 vol. 4, doi: 10.1109/IJCNN.2005.1556215.\n\nA Novel Connectionist System for Unconstrained Handwriting Recognition. A Graves, M Liwicki, S Fern\u00e1ndez, R Bertolami, H Bunke, J Schmidhuber, 10.1109/TPAMI.2008.137IEEE Transactions on Pattern Analysis and Machine Intelligence. 31A. Graves, M. Liwicki, S. Fern\u00e1ndez, R. Bertolami, H. Bunke and J. Schmidhuber, \"A Novel Connectionist System for Unconstrained Handwriting Recognition,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, pp. 855-868, May 2009, doi: 10.1109/TPAMI.2008.137.\n\nConstructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition. X Li, X Wu, 10.1109/ICASSP.2015.71788262015 IEEE International Conference on Acoustics, Speech and Signal Processing. X. Li and X. Wu, \"Constructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition,\" 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 4520-4524, doi: 10.1109/ICASSP.2015.7178826.\n\nJoint Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation. X Duan, 10.1109/ICIP.2018.845169225th IEEE International Conference on Image Processing. X. Duan et al., \"Joint Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation,\" 2018 25th IEEE International Conference on Image Processing (ICIP), 2018, pp. 918- 922, doi: 10.1109/ICIP.2018.8451692.\n", "annotations": {"author": "[{\"end\":345,\"start\":100},{\"end\":601,\"start\":346}]", "publisher": null, "author_last_name": "[{\"end\":112,\"start\":109},{\"end\":364,\"start\":358}]", "author_first_name": "[{\"end\":108,\"start\":100},{\"end\":357,\"start\":346}]", "author_affiliation": "[{\"end\":344,\"start\":137},{\"end\":600,\"start\":393}]", "title": "[{\"end\":97,\"start\":1},{\"end\":698,\"start\":602}]", "venue": null, "abstract": "[{\"end\":1902,\"start\":907}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2547,\"start\":2544},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2774,\"start\":2771},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3373,\"start\":3370},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3408,\"start\":3405},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3499,\"start\":3496},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3514,\"start\":3511},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3708,\"start\":3705},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3837,\"start\":3834},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3950,\"start\":3947},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4007,\"start\":4003},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4071,\"start\":4067},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4969,\"start\":4965},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5174,\"start\":5170},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5518,\"start\":5514},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5661,\"start\":5657},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5789,\"start\":5785},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7863,\"start\":7859},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8235,\"start\":8231},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8890,\"start\":8886},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9010,\"start\":9006},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9111,\"start\":9107},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9115,\"start\":9111},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9119,\"start\":9115},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11236,\"start\":11232},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12705,\"start\":12702},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12708,\"start\":12705},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12711,\"start\":12708}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":13583,\"start\":13527},{\"attributes\":{\"id\":\"fig_1\"},\"end\":13658,\"start\":13584},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":13730,\"start\":13659}]", "paragraph": "[{\"end\":2984,\"start\":1918},{\"end\":3409,\"start\":3010},{\"end\":3570,\"start\":3411},{\"end\":4134,\"start\":3572},{\"end\":4518,\"start\":4136},{\"end\":4767,\"start\":4520},{\"end\":5078,\"start\":4784},{\"end\":5382,\"start\":5146},{\"end\":5921,\"start\":5384},{\"end\":6186,\"start\":5923},{\"end\":6310,\"start\":6188},{\"end\":7624,\"start\":6312},{\"end\":7778,\"start\":7676},{\"end\":8786,\"start\":7821},{\"end\":10043,\"start\":8807},{\"end\":10497,\"start\":10087},{\"end\":10966,\"start\":10520},{\"end\":11588,\"start\":10968},{\"end\":12166,\"start\":11590},{\"end\":12771,\"start\":12182},{\"end\":13241,\"start\":12791},{\"end\":13526,\"start\":13243}]", "formula": null, "table_ref": "[{\"end\":12681,\"start\":12673}]", "section_header": "[{\"end\":1916,\"start\":1904},{\"end\":3008,\"start\":2987},{\"end\":4782,\"start\":4770},{\"end\":5105,\"start\":5081},{\"end\":5144,\"start\":5108},{\"end\":7674,\"start\":7627},{\"end\":7819,\"start\":7781},{\"end\":8805,\"start\":8789},{\"end\":10085,\"start\":10046},{\"end\":10518,\"start\":10500},{\"end\":12180,\"start\":12169},{\"end\":12789,\"start\":12774},{\"end\":13601,\"start\":13585},{\"end\":13669,\"start\":13660}]", "table": null, "figure_caption": "[{\"end\":13583,\"start\":13529},{\"end\":13658,\"start\":13604},{\"end\":13730,\"start\":13671}]", "figure_ref": "[{\"end\":5799,\"start\":5791},{\"end\":6901,\"start\":6893},{\"end\":9993,\"start\":9985}]", "bib_author_first_name": "[{\"end\":13792,\"start\":13791},{\"end\":13794,\"start\":13793},{\"end\":13805,\"start\":13804},{\"end\":14102,\"start\":14101},{\"end\":14108,\"start\":14107},{\"end\":14115,\"start\":14114},{\"end\":14523,\"start\":14522},{\"end\":14525,\"start\":14524},{\"end\":14536,\"start\":14535},{\"end\":14540,\"start\":14537},{\"end\":14966,\"start\":14965},{\"end\":14980,\"start\":14979},{\"end\":14989,\"start\":14988},{\"end\":15000,\"start\":14999},{\"end\":15016,\"start\":15015},{\"end\":15365,\"start\":15364},{\"end\":15686,\"start\":15679},{\"end\":16016,\"start\":16015},{\"end\":16018,\"start\":16017},{\"end\":16030,\"start\":16029},{\"end\":16032,\"start\":16031},{\"end\":16044,\"start\":16043},{\"end\":16046,\"start\":16045},{\"end\":16386,\"start\":16385},{\"end\":16394,\"start\":16389},{\"end\":16410,\"start\":16402},{\"end\":16777,\"start\":16776},{\"end\":16783,\"start\":16782},{\"end\":16790,\"start\":16789},{\"end\":16798,\"start\":16797},{\"end\":17075,\"start\":17074},{\"end\":17077,\"start\":17076},{\"end\":17088,\"start\":17087},{\"end\":17301,\"start\":17300},{\"end\":17308,\"start\":17307},{\"end\":17315,\"start\":17314},{\"end\":17329,\"start\":17321},{\"end\":17571,\"start\":17564},{\"end\":17754,\"start\":17753},{\"end\":17764,\"start\":17763},{\"end\":18086,\"start\":18085},{\"end\":18093,\"start\":18092},{\"end\":18095,\"start\":18094},{\"end\":18103,\"start\":18102},{\"end\":18112,\"start\":18111},{\"end\":18451,\"start\":18450},{\"end\":18458,\"start\":18457},{\"end\":18460,\"start\":18459},{\"end\":18475,\"start\":18474},{\"end\":18487,\"start\":18486},{\"end\":18726,\"start\":18725},{\"end\":18737,\"start\":18736},{\"end\":18748,\"start\":18747},{\"end\":18758,\"start\":18757},{\"end\":18771,\"start\":18770},{\"end\":18780,\"start\":18779},{\"end\":18782,\"start\":18781},{\"end\":18791,\"start\":18790},{\"end\":18801,\"start\":18800},{\"end\":19142,\"start\":19141},{\"end\":19149,\"start\":19148},{\"end\":19158,\"start\":19157},{\"end\":19168,\"start\":19167},{\"end\":19170,\"start\":19169},{\"end\":19472,\"start\":19471},{\"end\":19479,\"start\":19478},{\"end\":19485,\"start\":19484},{\"end\":19493,\"start\":19492},{\"end\":19503,\"start\":19502},{\"end\":19505,\"start\":19504},{\"end\":19758,\"start\":19757},{\"end\":19772,\"start\":19771},{\"end\":20053,\"start\":20052},{\"end\":20063,\"start\":20062},{\"end\":20552,\"start\":20551},{\"end\":20562,\"start\":20561},{\"end\":20573,\"start\":20572},{\"end\":20586,\"start\":20585},{\"end\":20599,\"start\":20598},{\"end\":20608,\"start\":20607},{\"end\":21115,\"start\":21114},{\"end\":21121,\"start\":21120},{\"end\":21601,\"start\":21600}]", "bib_author_last_name": "[{\"end\":13802,\"start\":13795},{\"end\":13809,\"start\":13806},{\"end\":14105,\"start\":14103},{\"end\":14112,\"start\":14109},{\"end\":14120,\"start\":14116},{\"end\":14533,\"start\":14526},{\"end\":14546,\"start\":14541},{\"end\":14977,\"start\":14967},{\"end\":14986,\"start\":14981},{\"end\":14997,\"start\":14990},{\"end\":15013,\"start\":15001},{\"end\":15022,\"start\":15017},{\"end\":15372,\"start\":15366},{\"end\":15698,\"start\":15687},{\"end\":16027,\"start\":16019},{\"end\":16041,\"start\":16033},{\"end\":16054,\"start\":16047},{\"end\":16400,\"start\":16395},{\"end\":16415,\"start\":16411},{\"end\":16780,\"start\":16778},{\"end\":16787,\"start\":16784},{\"end\":16795,\"start\":16791},{\"end\":16801,\"start\":16799},{\"end\":17085,\"start\":17078},{\"end\":17092,\"start\":17089},{\"end\":17305,\"start\":17302},{\"end\":17312,\"start\":17309},{\"end\":17319,\"start\":17316},{\"end\":17335,\"start\":17330},{\"end\":17576,\"start\":17572},{\"end\":17761,\"start\":17755},{\"end\":17771,\"start\":17765},{\"end\":18090,\"start\":18087},{\"end\":18100,\"start\":18096},{\"end\":18109,\"start\":18104},{\"end\":18120,\"start\":18113},{\"end\":18455,\"start\":18452},{\"end\":18472,\"start\":18461},{\"end\":18484,\"start\":18476},{\"end\":18494,\"start\":18488},{\"end\":18734,\"start\":18727},{\"end\":18745,\"start\":18738},{\"end\":18755,\"start\":18749},{\"end\":18768,\"start\":18759},{\"end\":18777,\"start\":18772},{\"end\":18788,\"start\":18783},{\"end\":18798,\"start\":18792},{\"end\":18812,\"start\":18802},{\"end\":19146,\"start\":19143},{\"end\":19155,\"start\":19150},{\"end\":19165,\"start\":19159},{\"end\":19176,\"start\":19171},{\"end\":19476,\"start\":19473},{\"end\":19482,\"start\":19480},{\"end\":19490,\"start\":19486},{\"end\":19500,\"start\":19494},{\"end\":19512,\"start\":19506},{\"end\":19769,\"start\":19759},{\"end\":19784,\"start\":19773},{\"end\":20060,\"start\":20054},{\"end\":20075,\"start\":20064},{\"end\":20559,\"start\":20553},{\"end\":20570,\"start\":20563},{\"end\":20583,\"start\":20574},{\"end\":20596,\"start\":20587},{\"end\":20605,\"start\":20600},{\"end\":20620,\"start\":20609},{\"end\":21118,\"start\":21116},{\"end\":21124,\"start\":21122},{\"end\":21606,\"start\":21602}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2311616},\"end\":14015,\"start\":13732},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":33566603},\"end\":14401,\"start\":14017},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":140614555},\"end\":14911,\"start\":14403},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":108213354},\"end\":15262,\"start\":14913},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":111239312},\"end\":15605,\"start\":15264},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":117486489},\"end\":15944,\"start\":15607},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":109989743},\"end\":16295,\"start\":15946},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":38607946},\"end\":16657,\"start\":16297},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":61805121},\"end\":17013,\"start\":16659},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2311616},\"end\":17298,\"start\":17015},{\"attributes\":{\"id\":\"b10\"},\"end\":17511,\"start\":17300},{\"attributes\":{\"doi\":\"10.21227/jcsm-hj03\",\"id\":\"b11\"},\"end\":17751,\"start\":17513},{\"attributes\":{\"doi\":\"arXiv:2110.03224\",\"id\":\"b12\"},\"end\":17997,\"start\":17753},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":209414891},\"end\":18371,\"start\":17999},{\"attributes\":{\"doi\":\"arXiv:1409.1259\",\"id\":\"b14\"},\"end\":18696,\"start\":18373},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":13756489},\"end\":19063,\"start\":18698},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":12414640},\"end\":19401,\"start\":19065},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":218652460},\"end\":19711,\"start\":19403},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7452865},\"end\":19983,\"start\":19713},{\"attributes\":{\"doi\":\"10.1109/IJCNN.2005.1556215\",\"id\":\"b19\",\"matched_paper_id\":9594328},\"end\":20477,\"start\":19985},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2008.137\",\"id\":\"b20\",\"matched_paper_id\":14635907},\"end\":20998,\"start\":20479},{\"attributes\":{\"doi\":\"10.1109/ICASSP.2015.7178826\",\"id\":\"b21\",\"matched_paper_id\":13930403},\"end\":21507,\"start\":21000},{\"attributes\":{\"doi\":\"10.1109/ICIP.2018.8451692\",\"id\":\"b22\",\"matched_paper_id\":49231787},\"end\":21917,\"start\":21509}]", "bib_title": "[{\"end\":13789,\"start\":13732},{\"end\":14099,\"start\":14017},{\"end\":14520,\"start\":14403},{\"end\":14963,\"start\":14913},{\"end\":15362,\"start\":15264},{\"end\":15677,\"start\":15607},{\"end\":16013,\"start\":15946},{\"end\":16383,\"start\":16297},{\"end\":16774,\"start\":16659},{\"end\":17072,\"start\":17015},{\"end\":17562,\"start\":17513},{\"end\":18083,\"start\":17999},{\"end\":18723,\"start\":18698},{\"end\":19139,\"start\":19065},{\"end\":19469,\"start\":19403},{\"end\":19755,\"start\":19713},{\"end\":20050,\"start\":19985},{\"end\":20549,\"start\":20479},{\"end\":21112,\"start\":21000},{\"end\":21598,\"start\":21509}]", "bib_author": "[{\"end\":13804,\"start\":13791},{\"end\":13811,\"start\":13804},{\"end\":14107,\"start\":14101},{\"end\":14114,\"start\":14107},{\"end\":14122,\"start\":14114},{\"end\":14535,\"start\":14522},{\"end\":14548,\"start\":14535},{\"end\":14979,\"start\":14965},{\"end\":14988,\"start\":14979},{\"end\":14999,\"start\":14988},{\"end\":15015,\"start\":14999},{\"end\":15024,\"start\":15015},{\"end\":15374,\"start\":15364},{\"end\":15700,\"start\":15679},{\"end\":16029,\"start\":16015},{\"end\":16043,\"start\":16029},{\"end\":16056,\"start\":16043},{\"end\":16389,\"start\":16385},{\"end\":16402,\"start\":16389},{\"end\":16417,\"start\":16402},{\"end\":16782,\"start\":16776},{\"end\":16789,\"start\":16782},{\"end\":16797,\"start\":16789},{\"end\":16803,\"start\":16797},{\"end\":17087,\"start\":17074},{\"end\":17094,\"start\":17087},{\"end\":17307,\"start\":17300},{\"end\":17314,\"start\":17307},{\"end\":17321,\"start\":17314},{\"end\":17337,\"start\":17321},{\"end\":17578,\"start\":17564},{\"end\":17763,\"start\":17753},{\"end\":17773,\"start\":17763},{\"end\":18092,\"start\":18085},{\"end\":18102,\"start\":18092},{\"end\":18111,\"start\":18102},{\"end\":18122,\"start\":18111},{\"end\":18457,\"start\":18450},{\"end\":18474,\"start\":18457},{\"end\":18486,\"start\":18474},{\"end\":18496,\"start\":18486},{\"end\":18736,\"start\":18725},{\"end\":18747,\"start\":18736},{\"end\":18757,\"start\":18747},{\"end\":18770,\"start\":18757},{\"end\":18779,\"start\":18770},{\"end\":18790,\"start\":18779},{\"end\":18800,\"start\":18790},{\"end\":18814,\"start\":18800},{\"end\":19148,\"start\":19141},{\"end\":19157,\"start\":19148},{\"end\":19167,\"start\":19157},{\"end\":19178,\"start\":19167},{\"end\":19478,\"start\":19471},{\"end\":19484,\"start\":19478},{\"end\":19492,\"start\":19484},{\"end\":19502,\"start\":19492},{\"end\":19514,\"start\":19502},{\"end\":19771,\"start\":19757},{\"end\":19786,\"start\":19771},{\"end\":20062,\"start\":20052},{\"end\":20077,\"start\":20062},{\"end\":20561,\"start\":20551},{\"end\":20572,\"start\":20561},{\"end\":20585,\"start\":20572},{\"end\":20598,\"start\":20585},{\"end\":20607,\"start\":20598},{\"end\":20622,\"start\":20607},{\"end\":21120,\"start\":21114},{\"end\":21126,\"start\":21120},{\"end\":21608,\"start\":21600}]", "bib_venue": "[{\"end\":14184,\"start\":14166},{\"end\":15785,\"start\":15751},{\"end\":20236,\"start\":20177},{\"end\":13845,\"start\":13811},{\"end\":14164,\"start\":14122},{\"end\":14609,\"start\":14548},{\"end\":15060,\"start\":15024},{\"end\":15408,\"start\":15374},{\"end\":15749,\"start\":15700},{\"end\":16090,\"start\":16056},{\"end\":16451,\"start\":16417},{\"end\":16814,\"start\":16803},{\"end\":17128,\"start\":17094},{\"end\":17375,\"start\":17337},{\"end\":17609,\"start\":17596},{\"end\":17849,\"start\":17789},{\"end\":18158,\"start\":18122},{\"end\":18448,\"start\":18373},{\"end\":18863,\"start\":18814},{\"end\":19216,\"start\":19178},{\"end\":19532,\"start\":19514},{\"end\":19835,\"start\":19786},{\"end\":20175,\"start\":20103},{\"end\":20706,\"start\":20644},{\"end\":21230,\"start\":21153},{\"end\":21687,\"start\":21633}]"}}}, "year": 2023, "month": 12, "day": 17}