{"id": 246863488, "updated": "2023-10-05 16:54:27.332", "metadata": {"title": "Transformer Memory as a Differentiable Search Index", "authors": "[{\"first\":\"Yi\",\"last\":\"Tay\",\"middle\":[]},{\"first\":\"Vinh\",\"last\":\"Tran\",\"middle\":[\"Q.\"]},{\"first\":\"Mostafa\",\"last\":\"Dehghani\",\"middle\":[]},{\"first\":\"Jianmo\",\"last\":\"Ni\",\"middle\":[]},{\"first\":\"Dara\",\"last\":\"Bahri\",\"middle\":[]},{\"first\":\"Harsh\",\"last\":\"Mehta\",\"middle\":[]},{\"first\":\"Zhen\",\"last\":\"Qin\",\"middle\":[]},{\"first\":\"Kai\",\"last\":\"Hui\",\"middle\":[]},{\"first\":\"Zhe\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Jai\",\"last\":\"Gupta\",\"middle\":[]},{\"first\":\"Tal\",\"last\":\"Schuster\",\"middle\":[]},{\"first\":\"William\",\"last\":\"Cohen\",\"middle\":[\"W.\"]},{\"first\":\"Donald\",\"last\":\"Metzler\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2202.06991", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/Tay00NBM000GSCM22", "doi": null}}, "content": {"source": {"pdf_hash": "9d40837175577bb0009b138269b422f6d5820d00", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2202.06991v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "9e92ce73aa246dbd7c5fb35b1e10468eca938138", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9d40837175577bb0009b138269b422f6d5820d00.txt", "contents": "\nTransformer Memory as a Differentiable Search Index\n\n\nYi Tay yitay@google.com \nVinh Q Tran vqtran@google.com \nMostafa Dehghani \nJianmo Ni \nDara Bahri \nHarsh Mehta \nZhen Qin \nKai Hui \nZhe Zhao \nJai Gupta \nTal Schuster \nWilliam W Cohen \nDonald Metzler metzler@google.com \nGoogle Research \nTransformer Memory as a Differentiable Search Index\n\nIn this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.\n\nIntroduction\n\nInformation retrieval (IR) systems map a user query q \u2208 Q to a ranked list of relevant documents {d 1 , . . . , d n } \u2286 D, typically represented by integers or short strings called document identifiers (docids). The most widely used IR approaches are based on pipelined retrieve-then-rank strategies. For retrieval, approaches based on inverted indexes or nearest neighbor search are common where contrastive learning based dual encoders (DEs) (Gillick et al., 2018;Karpukhin et al., 2020;Ni et al., 2021) are the present state-of-the-art. This paper proposes an alternative architecture, wherein a sequence-to-sequence (seq2seq) learning system (Sutskever et al., 2014) is used to directly map a query q to a relevant docid j \u2208 Y. This proposal is shown in the bottom half of Figure 1, for a sequence-to-sequence encoder-decoder architecture.\n\nWe call this proposed architecture a differentiable search index (DSI), and implement it with a large pre-trained Transformer (Vaswani et al., 2017) model, building on the recent success of large generative language models (LMs) Raffel et al., 2019;Devlin et al., 2018;Thoppilan et al., 2022;Du et al., 2021). In this proposed architecture, all information of the corpus is encoded within the parameters of the Transformer language model. At inference time, the trained model takes as input a text query q and outputs a docid j. If desired, beam search can be used to produce a ranked list of potentially-relevant docids. As we show, this process can work surprisingly well when trained properly. In our experiments it can consistently outperform DE baselines, sometimes drastically: for a base-sized T5 model, Hits@1 on the smallest corpus is improved by more than 20 points, from 12.4% for a DE to 33.9% for DSI; and on a corpus   \neach term t \u2192 {dj 1 , . . . , dj k } each docvec v d j \u2192 j to map dj \u2192 j retrieval approximate sparse matmul approximate MIPS run trained model (top-1) to find argmax j v T q v d j to find argmax j v T q v d j to find argmax j Pr(j|q)\n30\u00d7 larger, performance is improved by nearly 7 points. These gains increase when larger models are used: for an 11B-parameter T5 model, Hits@1 performance improves by more than 25 points over DE on the small corpus, and more than 15 points on the large corpus. DSI also performs extremely well in a zero-shot setting, e.g., improving Hits@1 by 14 points over BM25.\n\nIn addition to these quantitative gains, the DSI architecture is much simpler than a DE (see Table 1).\n\nA DE system fixes a search procedure (MIPS) and learns internal representations that optimize performance for that search procedure; in contrast, a DSI system contains no special-purpose fixed search procedure, instead using standard model inference to map from encodings to docids.\n\nOf particular interest to the machine learning community, as Table 1 shows, in DSI all aspects of retrieval are mapped into well-understood ML tasks. This may lead to new potential approaches to solving long-standing IR problems. As one example, since indexing is now a special case of model training, incrementally updating an index becomes a special case of model updating .\n\nIn this paper, DSI is applied to moderate-sized corpora (from 10k to 320k documents), all of which are derived from one challenging retrieval task, and we leave the important question of the scaling DSI to larger corpora to future work. The task considered is retrieving supporting passages given questions from the Natural Questions (NQ) dataset, a challenging task for lexical models.\n\nWhile the idea of DSI is simple, there are a number of ways it can be realized, some of which work surprisingly well, and some of which work surprisingly poorly. Below we explore a number of variations of the DSI architecture.\n\nDocument representation. We explore several approaches to representing documents, including a \"naive\" approach of using the document's full text, as well as variants of the bag-of-words representation used by traditional IR engines.\n\nDocid representation. We look at several ways to represent docids. In addition to naively representing integers as text strings, we also consider unstructured atomic docids, where each document is assigned a unique token, and some simple baselines for constructing structured semantic docids that describe how to navigate to a document through a hierarchical clustering of the corpus. Structured docidseither semantically structured via clustering, or naively structured as tokenized integers-scale better to large corpora, since the size of the vocabulary used in the decoder is made larger.\n\nIndexing. A trainable IR system traditionally has two phases: indexing a corpus (i.e., memorizing information about each document), and learning how to effectively retrieve from the index. In DSI, the index is stored in the model parameters, and indexing is simply another kind of model training. Figure 1 suggests one approach to indexing a corpus: namely, to train on (1) (2) alone do not provide enough information for a system to generalize to novel retrievals, there are many alternatives to examples of type (1) that might plausibly \"teach\" a model about the associations between documents and docids. We explore a number of these below, and show that some plausible-seeming techniques perform very poorly. We also explore a number of alternative multi-task optimization and curriculum learning schemes for combining these types of examples.\n\nEffects of model and corpus size. Since recent results suggest that some properties of large LMs emerge only for very large model sizes , we explore the performance of DSI for a range of model sizes and corpus sizes of 10k, 100k, and 320k documents.\n\nSummary. We show that even naive representations for documents and docids, coupled with appropriate training procedures to fine-tune modern large LMs, can perform surprisingly well; we present two improved docid representations, unstructured docids and semantically-structured docids, which improve the naive representation choice. We show that there is substantial variation in performance among indexing/training strategies and we show that performance of DSI significantly and consistently improves with model scale. To our knowledge this is the first case of generative indexing improving performance over strong baselines for a well-studied document retrieval task.\n\n\nRelated Work\n\nDe Cao et al. (2020) describe a related sequence-to-sequence system called autoregressive entity linking, in which documents mentioning an entity-perhaps implicitly, e.g., by posing a question to which that entity is an answer-are mapped to a canonical name of that entity. In the case of Wikipedia, canonical entity names correspond to page titles, so this could be viewed as a sort of document retrieval. This approach has been adapted to other purposes, such as generating knowledge base triples in canonical form (Josifoski et al., 2021). The task we consider is different from those considered in autoregressive entity linking: our goal is to retrieve a document containing the answer, rather than a document whose title is the answer. More importantly, in autoregressive entity linking the generation target is a semantically meaningful name, whereas we allow targets to be arbitrary docids. This makes our approach applicable to general retrieval tasks, but raises new questions about docid representation and indexing strategies.\n\nIn autoregressive entity linking, generation is constrained to return an output from a fixed set. It would be feasible to constrain DSI generation outputs to be valid docids. Although we do not use this technique, the degree to which this might improve performance is a worthwhile question.\n\nThere is a large body of work on retrieval augmented generation, i.e., retrieving auxiliary documents to enhance language models (Borgeaud et al., 2021;Guu et al., 2020). These techniques are useful for many tasks including question-answering, but rely on traditional retrieval methods such as DEs. Here we use generation to replace a retrieval process, rather than using retrieval to augment a generation process.\n\nDual encoders (Dehghani et al., 2017;Gillick et al., 2018;Gao et al., 2021;Ni et al., 2021;Karpukhin et al., 2020) are a well-established paradigm for retrieval. The key idea is produce query and document embeddings independently and perform a similarity retrieval in vector space across all embedding pairs. Query and candidate documents are produced by a sequence encoder and training is performed using a form of contrastive loss.\n\nThe interpretation of a large Transformer model as a memory store have been investigated in prior work. (Roberts et al., 2020) demonstrated success on a closed-book QA task whereby they train T5 models to retrieve facts that are encoded within the parameters of the model during pretraining. However, different from CBQA, the presented problem here in this paper is to retrieve full documents based on docids instead of generating direct answers. Meanwhile, (Petroni et al., 2019) also investigated language models as knowledge bases and found that pretrained LMs may already contain relational knowledge. (Geva et al., 2020) analyzes the knowledge encoded within Transformer feedforward layers. There have been also works that demonstrate the relation of Transformers to associative memory and Hopfield networks (Ramsauer et al., 2020), which reinforce the notion that Transformers should intuitively serve as a good associative memory store or search index.\n\n\nDifferentiable Search Index\n\nThe core idea behind the proposed Differentiable Search Index (DSI) is to fully parameterize traditionally multi-stage retrieve-then-rank pipelines within a single neural model. To do so, DSI models must support two basic modes of operation:\n\n\u2022 Indexing: a DSI model should learn to associate the content of each document d j with its corresponding docid j. This paper utilizes a straightforward sequence-to-sequence (seq2seq) approach that takes document tokens as input and generates identifiers as output.\n\n\u2022 Retrieval: Given an input query, a DSI model should return a ranked list of candidate docids.\n\nHere, this is achieved with autoregressive generation.\n\nFollowing these two operations, a DSI model can be trained to index a corpus of documents and optionally fine-tune on an available set of labeled data (queries and labeled documents), and thereafter used to retrieve relevant documents-all within a single, unified model. As opposed to retrieve-thenrank approaches, this type of model allows for simple end-to-end training and can easily be used as a differentiable sub-component of a larger, more complex neural model.\n\n\nIndexing Strategies\n\nWe investigate various indexing strategies that are meant to learn associations between documents and their identifiers. We train our model to predict docids given a sequence of document tokens. This allows our model to learn which identifier belongs to which document and can be thought of as a differentiable take on traditional search indexes. We consider various alternatives and ablate these settings in subsequent sections. The final strategy employed was Inputs2Targets with direct indexing.\n\n\nIndexing Method\n\nThis section discusses the indexing task variants that we consider.\n\n\nInputs2Target\n\nWe frame this as a seq2seq task of doc_tokens \u2192 docid. As its name suggests, this binds the docids to the document tokens in a straightforward inputs-to-targets fashion. The advantage here is that the identifier is the denoising target, which puts it in closer proximity to the loss function. Since the retrieval task is also concerned with predicting identifiers, this formulation allows the network to follow a similar input-target balance in terms of sequence length. A potential weakness is that the document tokens are not denoising targets and therefore there is no opportunity for general pre-training on document tokens.\n\nTargets2Inputs This formulation considers the opposite of the above, i.e., generating document tokens from identifiers, i.e., docid \u2192 doc_tokens. Intuitively, this is equivalent to training an autoregressive language model that is conditioned on the docid.\n\nBidirectional This formulation trains both Inputs2Targets and Targets2Inputs within the same co-training setup. A prefix token is prepended to allow the model to know which direction the task is being performed in.\n\n\nSpan Corruption\n\nWe also explored a setup that performs span corruption-based denoising (Raffel et al., 2019) with the inclusion of docid tokens. In this approach, we concatenate the identifier to the document tokens as a prefix that can be randomly masked as spans in the span corruption objective.\n\nThis method has the advantage of (1) also performing general pre-training during indexing and (2) achieving a good balance of docids as denoising targets and inputs.\n\n\nDocument Representation Strategies\n\nIn the previous section, we explored \"how to index\". This section investigates \"what to index?\", i.e., how to best represent doc_tokens. We state our options here and carefully ablate them in our experiments later. The best option in the end was the direct indexing method.\n\nDirect Indexing This strategy represents a document exactly. We take the first L tokens of a document, with sequential order preserved, and associate them with the docid.\n\nSet Indexing Documents may contain repeated terms and/or non-informative words (e.g., stopwords). This strategy de-duplicates repeated terms using the default Python set operation and removes stopwords from the document. The rest of the document after filtering is passed into the model in similar fashion to the direct index.\n\nInverted Index This strategy maps chunked documents (contiguous blocks of tokens) instead of entire documents directly to the docid. We randomly subsample a single contiguous chunk of k tokens and associate them with the docid. The key advantage of this approach is to allow looking beyond the first k tokens.\n\n\nRepresenting Docids for Retrieval\n\nRetrieval within seq2seq-based DSI models is accomplished by decoding docids given an input query. How to do this decoding in an effective way largely depends on how docids are represented in the model. The remainder of this section explores a number of possible ways for representing docids and how to handle decoding for each.\n\n\nUnstructured Atomic Identifiers\n\nThe most naive way to represent documents is assign each an arbitrary (and possibly random) unique integer identifier. We refer to these as unstructured atomic identifiers.\n\nWith these identifiers, an obvious decoding formulation is to learn a probability distribution over the identifiers. In this case, models are trained to emit one logit for each unique docid (|N documents |). This is analogous to the output layer in standard language models, but extended to include docids.\n\nTo accommodate this, we extend the output vocabulary of a standard language model as follows:\nO = Softmax([W tokens ; W docs ] T h last )\nwhere [; ] is the row-wise concatenation operator, W tokens \u2208 R d model \u00d7|N tokens | and W docs \u2208 R d model \u00d7|N documents | . h last is the last layer's hidden state (\u2208 R d model ) of the decoder stack. To retrieve the top-k documents for a given query, we simply sort the output logits and return the corresponding indices. This is also reminiscent of standard listwise learning to rank where all documents are considered at once.\n\n\nNaively Structured String Identifiers\n\nWe also consider an ostensibly absurd approach that treats unstructured identifiers, i.e., arbitrary unique integers, as tokenizable strings. We refer to these as naively structured identifiers.\n\nIn this formulation, retrieval is accomplished by decoding a docid string sequentially one token at a time. This eliminates the need for the large softmax output space that comes with unstructured atomic identifiers. It also eliminates the need to learn embeddings for each individual docid.\n\nWhen decoding, beam search is used to obtain the predicted best docid. With this strategy, it is less straightforward to obtain a top-k ranking. One could exhaustively comb through the entire docid space and obtain the likelihood of each docid given the query. Instead, we use the partial beam search tree to construct top-k retrieval scores. We find this approximation to be quite efficient and effective in practice.\n\nSemantically Structured Identifiers All of the approaches for representing docids thus far assumed that the identifiers are assigned in an arbitrary manner. While exploring the limits of arbitrary identifiers is quite interesting, it is only intuitive that imbuing the docid space with semantic structure can lead to better indexing and retrieval capabilities. As such, this section explores semantically structured identifiers.\n\nSpecifically, we aim to automatically create identifiers that satisfy the following properties:\n\n(1) the docid should capture some information about the semantics of its associated document, (2) the docid should be structured in a way that the search space is effectively reduced after each decoding step. This results in identifiers where semantically similar documents share identifier prefixes.\n\nIn this work, we treat this as a fully unsupervised pre-processing step. However, as part of future work it may be possible to integrate and automatically learn semantic identifiers in a fully end-to-end manner.\n\nTo construct identifiers with this property, we employ a simple hierarchical clustering process over document embeddings to induce a decimal tree (or more generally, a trie).\n\nGiven a corpus to be indexed, all documents are clustered into 10 clusters. Each document is assigned an identifier with the number of their cluster from 0-9. For every cluster containing more than c documents, the algorithm is applied recursively, with the next level's result (the remaining suffix of the identifier) appended to the existing identifier. For clusters with c documents or less, each element is assigned an arbitrary number from 0 to at most c \u2212 1 and likewise its digits are appended to the existing identifier. Although this specific process induces a decimal tree, it is possible to induce similar types of tries using any number of other reasonable strategies.In practice, we simply apply k-means over embeddings generated by a small 8-layer BERT model, with c = 100. We include pseudo-code for this process in Algorithm 1.\n\n\nTraining and Optimization\n\nThe DSI models that we train are optimized for seq2seq cross entropy loss and are trained with teacher forcing. We explored two main strategies for training DSI models. The first and more straightforward strategy is to first train a model to perform indexing (memorization), followed by a fine-tuning stage where the trained model is used to map queries to docids (e.g., retrieval). The second strategy is to train them together in a multi-task setup. To this end, we frame co-training tasks in similar fashion to T5-style co-training (e.g., using task prompts to differentiate them). The latter performed significantly better, especially when the proportion of indexing to retrieval task examples is high. Hence, we adopted multi-task learning as the default strategy.\n\nHere, we make the observation that our setup is unique and unlike traditional multi-task learning or transfer learning. In typical multi-task setups, two tasks have shared commonalities that could improve the performance of both tasks if they were learned together. However, in our setup, the retrieval task is completely dependent on the indexing task. In particular, without the indexing task, the identifiers leveraged by the retrieval task would be completely meaningless. Hence, in order to solve task B (retrieval), the model needs to learn task A (indexing) well enough. This problem setup presents unique and largely unexplored research challenges that might be of interest to the ML community.\n\n\nExperiments\n\nIn this section, we discuss our experimental setup, datasets used and baselines compared. We also discuss experimental results, findings and effect of various strategies discussed in earlier sections of the paper. Since this is fairly new concept, this work aims to put forth a proof-of-concept and seeks to answer research questions instead of making a 'sotaeesque' comparison. We leave extensive comparisons on other setups and baselines to future work.\n\nDataset We conduct our experiments on the challenging Natural Questions (NQ) (Kwiatkowski et al., 2019) dataset. NQ consists of 307K query-document training pairs and 8K validation pairs, where the queries are natural language questions and the documents are Wikipedia articles. Given a question, the retrieval task is to identify the Wikipedia article that answers it. For evaluating how DSI models perform at different scales, we construct three sets from NQ to form our testbed, namely NQ10K, NQ100K, and NQ320K denoting different numbers of total query-document pairs in the combined train and validation splits. NQ320K is the full NQ set and uses its predetermined training and validation split for evaluation purposes. Unlike NQ320K, NQ10K and NQ100K constructs randomly sampled validation sets. For all datasets, we use the same docid space/budget of 320K tokens for all unstructured atomic and naively structured identifier experiments. Semantically structured identifiers are generated separately for each dataset so as to prevent leakage of semantic information from larger splits into smaller ones. Text is lowercased. Note that there exists fewer unique documents than query-document pairs in these datasets. Please refer to Table 4 (Appendix) which reports the statistics of these datasets.\n\nMetrics We evaluate our models on Hits@N where N={1, 10}. This metric reports the proportion of correct documents ranked in the top N predictions.\n\nImplementation Details All DSI models are initialized using standard pretrained T5 (Raffel et al., 2019) model configurations. The configurations names and corresponding number of model parameters are: Base (0.2B), Large (0.8B), XL (3B) and XXL (11B). For unstructured atomic identifiers runs, we initialize the identifiers randomly as new parameters and only finetune the weights during the indexing stage. We use the Jax/T5X 2 implementation for our experiments. The DSI models are trained for a maximum of 1M steps using a batch size of 128. We pick the best checkpoint based on retrieval validation performance. Our training hardware consists of 128-256 TPUv4 chips for models above 1B parameters and 64-128 TPUv3 or TPUv4 chips otherwise. As an estimate, models above 1B parameters typically take about at least a full day for convergence for NQ320K. We tune the learning rate amongst {0.001, 0.0005} and linear warmup amongst {10K, 100K, 200K, 300K} and/or none. Semantically structured identifiers are generated using an 8-layer BERT (Devlin et al., 2018) model 3 , and the default k-means clustering in scikit-learn. Based on our early ablation experiments of various DSI setting, the main results presented use direct indexing (L = 32) and the Inputs2Targets indexing strategy. We present results for all the docid representation methods. Following the main results, we present our ablation studies.\n\n\nBaselines\n\nFor baselines, we use T5-based dual encoders implemented by (Ni et al., 2021). We use the gensim 4 package for computing BM25 scores. For the T5-based dual encoders, we train with contrastive learning on the NQ pairs until convergence (\u2248 10K steps) and obtain top-k nearest neighbors with a system similar to ScaNN (Guo et al., 2020). For zero-shot retrieval, we also compare with a state-ofthe-art unsupervised baseline, Sentence T5 (Ni et al., 2021) which have been specially pre-trained with a similarity learning task. There two reasons why we consider (Ni et al., 2021) the relevant dual encoder baseline for this work rather than other dense retrieval works such as DPR (Karpukhin et al.,   2020). Firstly, we employ the exact identical pretrained model, which allows systematic ablation of the proposed approach without conflating other factors. Scientifically, we believe this comparison against fine-tuned T5 is the best apples to apples comparison that we provide. Secondly, fine-tuned T5 dual encoders are considered to be architecturally and methodologically very identical to DPR (with some minor differences such as parameter sharing but use the same concept of in-batch negatives). Table 2 reports retrieval results for NQ10K, NQ100K, and NQ320K with finetuning and Table 3 reports zero-shot retrieval results. For zero-shot retrieval, the model is only trained on the indexing task and not the retrieval task, so the model sees no labeled query \u2192 docid data points. Section 7.2 of the Appendix reports extended results regarding the indexing performance and training dynamics of DSI.\n\n\nExperimental Results\n\n\nSupervised Finetuning Results\n\nOur results show that DSI outperforms DE across all dataset sizes. On the small dataset (NQ10K), the performance gap between DSI and DE is large, e.g., the best DSI variant outperforms DE by 2 times. On NQ100K, the gap becomes less prominent with the best DSI model (unstructured atomic identifiers) outperforming DE by +5% Hits@1 and Hits@10. On the large dataset (NQ320K), the best DSI model (structured semantic identifiers) outperform the best DE model by +66% relative Hits@1 and +4.5% Hits@10. Table 3 reports results on zeros-shot retrieval. Recall that zero-shot retrieval is performed by only performing indexing and not the retrieval task. In other words, the model does not see any annotated query or document pairs. Generally, the best result is obtained by DSI with   unstructured atomic identifiers on both NQ100K and NQ320K. The best performance on all NQ datasets outperform well-established unsupervised retrieval baselines such as BM25. Moreover, DSI outperforms unsupervised representation learning methods such as SentenceT5 (Ni et al., 2021), which is trained to learn similarity-aware representations via contrastive learning. We also note that raw T5 embeddings perform extremely poorly and do not produce reasonable results on the task of unsupervised retrieval. Given that it is generally difficult for an unsupervised neural method to outperform BM25, we find these early results very encouraging.\n\n\nZero-Shot Results\n\n\nDocument Identifiers\n\nOne key research question in this paper is the crucial choice of how to represent docids. Generally, we find that structured semantic identifiers are helpful and improve over unstructured identifiers. When comparing naive versus semantic string identifiers, it seems imperative to use semantic identifiers if possible. This is intuitive, since imbuing the target space with semantic structure can facilitate greater ease of optimization and additional unsupervised representation learning methods as external knowledge. The competitiveness of unstructured atomic identifiers is somewhat mixed and we had some difficulty optimizing such models. We hypothesize that this could possibly be because of the the newly initialized softmax layer and that training such a system from scratch would mitigate these issues. However, we defer this line of investigation to future work. In lieu of the instability and high variance of the unstructured atomic identifiers, the performance is not consistent across the different datasets. Moreover, these docids might also run into intermittent non-convergence which we trace back to an optimization related quirk. However, we also note that unstructured atomic identifiers perform the best, by a wide margin, on the zero-shot retrieval setup and achieve performance often more than double than that of beam decoding methods.\n\nIndexing Strategies In this section, we explore the effect of different indexing methods (Section 3.1.1). We run experiments on NQ100K with the different indexing strategies described earlier.\n\nModels are trained using the Naive Docid method. Without indexing, the model achieves 0% Hits@1. This is intuitive, since the Docids are not meaningful without the indexing task. Secondly, the Inputs2Targets and Bidirectional formulation performs the best, with the bidirectional method performing slightly worse (13.5 vs 13.2) compared to the former. Finally, the accuracy with Targets2Inputs and Span Corrpution with Docids yield no meaningful results (0% accuracy). This goes to show that there can be huge variance across indexing strategies whereby some strategies work reasonably well and some completely do not work at all.\n\n\nDocument Representations\n\nIn this section, we explore the performance of the different document representation strategies described in Section 3.1.2. Figure 5 reports the results on NQ320K. Overall, we find that the direct indexing approach works the best. We also find that it is difficult to train the inverted index method since the docid is repeatedly exposed to different tokens. We also find that shorter document lengths seem to work well where performance seems to substantially dip beyond 64 tokens suggesting that it might be harder to optimize or efficiently memorize when there are a larger number of document tokens. Finally, we also find that there was no additional advantage in applying set processing or stopwords preprocessing to the document tokens.\n\nScaling Laws Another interesting insight is how the scaling law of DSI differs from Dual Encoders.\n\nUnderstanding the scaling behaviour of Transformers have garnered significant interest in recent years Tay et al., 2021;. We find that the gain in retrieval performance obtained from increasing model parameterization in DE seems to be relatively small. Conversely, the scaling properties of DSI seems to be more optimistic. Figure 3 plots the scaling behaviour (log scale) of three methods (DE and DSI with naive and semantic IDs). DSI (naive) strongly benefits from scale going from base to XXL and seems to still have headroom for improvement. Meanwhile, DSI (semantic) starts off equally competitive as DE base but performs much better with scale. DE models, unfortunately are more or less plateaued at smaller parameterization.\n\n\nInterplay Between Indexing and Retrieval\n\nOur early experiments showed that first learning the indexing task and then learning the retrieval task in a sequential manner results in mediocre performance. There, we focused on exploring good ratios r for co-training the indexing and retrieval tasks together using multi-task learning. Figure 4 shows the effect of modifying the ratio of indexing to retrieval samples. We find the optimization process is significantly influenced by the interplay between the indexing and retrieval tasks. Setting r too high or low generally resulted in poor performance. We find that a rate of 32 generally performed well.\n\n\nConclusion\n\nThis paper proposed the Differentiable Search Index (DSI), a new paradigm for learning an end-to-end search system in a unified manner, paving the way for next generation search (Metzler et al., 2021). We define novel indexing and retrieval tasks that encode the relationship between terms and docids completely within the parameters of a Transformer model. The paper proposed a number of different ways to represent documents and docids, and explored different model architectures and model training strategies. Experiments conducted on the Natural Questions data set show that DSI performs favorably against common baselines such as BM25 and dual encoders, both in a standard fine-tuning setup as well as in a zero-shot setup.\n\nAlthough the models and results presented here are promising, there is a great deal of potential future research that can be explored based on this work to improve this approach. For example, it would be interesting to explore alternative strategies for representing documents and docids, as well as to investigate mixture-of-expert models (Du et al., 2021;Lepikhin et al., 2020) for scaling the memory capacity of DSI. One important direction will also be to explore how such models can be updated for dynamic corpora, where documents may be added or removed from the system. Finally it may also be interesting to further investigate DSI as an unsupervised representation learning method and/or memory store for other language models to leverage.\n\n\nAppendix\n\nHere we include figures containing additional details about our experiments. \n\n\nDataset Statistics\n\n\nExtended Results\n\nWe report additional results and observations here. We can observe that indexing performance is relatively strong on NQ across methods and model sizes.\n\n\nIndexing/Memorization Performance\n\nIt is clear though that increasing model size improves indexing performance.\n\n\nDiscussion of DSI Training Dynamics\n\nIn this paper, all indexing tasks are trained on the union of documents in both the train and validation splits of Natural Questions (NQ). This aligns with traditional definitions of indices, where a document must be in the index in order for the index to retrieve it. Retrieval then is trained on trained only on the NQ train split, with retrieval performance evaluated on NQ validation, based on the best checkpoint.\n\nAnalysis following this original work showed that, when training, a DSI model experiences forgetting of previously indexed batches as it indexes new batches, until it loops around again to the next epoch, and processes the same examples again. The indexing task we use in this paper was constructed by concatenating validation documents after the train documents, then applying a buffered shuffle while training the model (sampling the next training batch from a buffer every step). We used a shuffle buffer of size 5000, which is smaller than the size of the validation split for NQ100K and NQ320K.\n\nFigure 1 :\n1Comparison of dual encoders (top) to differentiable search index (bottom).\n\nFigure 2 :\n2Visual example of a hierarchical clustering process used to assign semantically structured identifiers. During inference, beam search navigates this trie to decode the correct docid.Algorithm 1 Generating semantically structured identifiers. (Referenced in Section 3.2.) Input: Document embeddings X 1:N , where Xi \u2208 R d Output: Corresponding docid strings J 1:N function GENERATESEMANTICIDS(X 1:N ) C1:10 \u2190 Cluster(X 1:N , k = 10) J \u2190 empty list for i = 0 to 9 do Jcurrent \u2190 [i] * |Ci+1| if |Ci+1| > c then Jrest \u2190GENERATESEMANTICIDS(Ci+1) else Jrest \u2190 [0, . . . , |Ci+1| \u2212 1] end if J cluster \u2190elementwiseStrConcat(Jcurrent, Jrest) J \u2190 J.appendElements(J cluster ) end for J \u2190 reorderToOriginal(J, X 1:N , C1:10) return J end function\n\nFigure 3 :\n3Scaling plots for DSI vs. DE across model sizes. Performance refers to the Hits@1 metric.\n\nFigure 4 :\n4Effect of multi-task ratio of indexing to retrieval examples.\n\nFigure 5 :\n5Performance of different document representations. (Referenced in Section 4.2.)\n\n\nWho is the author of tipping the velvet? \u2026 As part of her research \u2026 Sarah Waters came across the title of her first book, Tipping the Velvet. \u2026 \u2026. Keeley Hawes is known for her roleas Kitty Butler in Tipping the Velvet, \u2026query123 \n\ndoc456 \n\ndoc137 \n\nEncoder \n\nLearning to encode \nRetrieve \n\nEncoder \n\n\u2026 \n\nWho is the author of tipping the velvet? \n\n\u2026 As part of her research \u2026 Sarah Waters came across \nthe title of her first book, Tipping the Velvet. \u2026 \n\n\u2026. Keeley Hawes is known for her roleas Kitty Butler in \nTipping the Velvet, \u2026 \n\nquery123 \n\ndoc456 \n\ndoc137 \n\nEncoder \n\nLearning to encode and retrieve \n\n1. \ndoc456 \n2. \ndoc283 \n\n\u2026 \n\nRank List \n\ndoc456 \n\ndoc456 \n\ndoc137 \n\nDecoder \n\n1. \ndoc456 \n2. \ndoc283 \n\n\u2026 \n\nRank List \nBeam \nSearch \n\n!\"# \n\n$%& \n\n$'% \n\n$'% \n\n'(% \n\n!\"# \n\n$%& \n\n$'% \n\n!\"# \n\nContrastive loss \nduring training \n\nquery123 \n\nMaximal Inner \nProduct Search \n\n\n\nTable 1 :\n1Information retrieval requires a series of decisions, associated with the subproblems of document representation, indexing, and retrieval. Structured-document variants of DSI are also sensitive to a fourth decision, namely how docids are represented.BM25 or TFIDF \nDual Encoder (DE) \nDifferentiable Search Index (DSI) \n\ndoc/query rep. \nsparse v d j vector in R |V | \ndense v d j vector in R d \nVarious (see Section 3.1.2) \n\ndocid rep. \n\u2212 \n\u2212 \nVarious (see Section 3.2) \n\nindexing \nbuild inverted index mapping \nbuild table mapping \ntrain model (see Section 3.1.1) \n\n\nTable 2 :\n2Experimental results on NQ document retrieval. DSI outperforms BM25 and Dual Encoder baselines. Among all the Docid representation methods, Semantic String Docids perform the best.NQ10K \nNQ100K \nNQ320K \n\n\nTable 3 :\n3Experimental results on Zero-Shot NQ document retrieval. DSI outperforms BM25, T5 \nembeddings and SentenceT5, the state-of-the-art for unsupervised similarity modeling. Among Docid \nrepresentation method, the Atomic Docid performs the best on zero-shot learning. \n\nNQ10K \nNQ100K \nNQ320K \nModel \nSize \nMethod \nHits@1 Hits@10 Hits@1 Hits@10 Hits@1 Hits@10 \n\nBM25 \n-\n-\n12.4 \n33.5 \n20.9 \n46.4 \n11.6 \n34.4 \nT5 \nXXL \nDual Encoder \n0.3 \n1.3 \n1.9 \n8.0 \n1.1 \n5.9 \nSentenceT5 Large Dual Encoder \n17.6 \n50.7 \n17.4 \n50.8 \n16.9 \n51.0 \n\nDSI \nXXL \nAtomic Docid \n25.7 \n60.1 \n23.0 \n57.3 \n25.1 \n56.6 \nDSI \nXXL \nNaive String Docid \n43.4 \n67.4 \n17.4 \n41.5 \n9.2 \n22.6 \nDSI \nXXL \nSemantic String Docid \n43.9 \n68.8 \n11.4 \n26.6 \n13.9 \n31.1 \n\n\n\nTable 4 :\n4Statistics of NQ datasets used in our experiments. (Referenced in Section 4.) The number in the dataset name corresponds to the total number of document-query pairs in the dataset, while |D| corresponds to the number of unique documents, based on the first 4000 UTF-8 characters of the document.Dataset \n|D| \nTrain Pairs Val Pairs V doc_out \n\nNQ10K \n10K \n8K \n2K \n320K \nNQ100K \n86K \n80K \n20K \n320K \nNQ320K 228K \n307K \n8K \n320K \n\n\n\nTable 5 :\n5Indexing performance (memorization) on NQ documents via the Inputs2Targets indexing objective. All models were indexed on all NQ documents (train and validation), with memorization evaluated on only the documents in the validation set.Size \nParams Method \nIndexing Hits@1 \n\nBase \n250M \nAtomic Docid \n85.4 \nLarge 800M \nAtomic Docid \n84.9 \nXL \n3B \nAtomic Docid \n88.4 \nXXL \n11B \nAtomic Docid \n92.7 \n\nBase \n250M \nNaive String Docid \n76.3 \nLarge 800M \nNaive String Docid \n92.1 \nXL \n3B \nNaive String Docid \n92.2 \nXXL \n11B \nNaive String Docid \n91.9 \n\nBase \n250M \nSemantic String Docid \n87.6 \nLarge 800M \nSemantic String Docid \n91.5 \nXL \n3B \nSemantic String Docid \n92.6 \nXXL \n11B \nSemantic String Docid \n92.0 \n\n\nhttps://github.com/google-research/t5x 3 https://tfhub.dev/google/collections/bert 4 https://pypi.org/project/gensim\nAcknowledgementsThe authors would like to thank you Fernando Pereira, Huaixiu Steven Zheng, Sebastian Ruder, Adam D. Lelkes, Ian Wetherbee and Dani Yogatama for their valuable feedback and discussions. We would also like to extend a special thanks to Sanket Vaibhav Mehta for additional experimental contributions.As a result, DSI experiments in this paper experienced cycles of minimum and maximum forgetting, i.e. higher and lower validation scores, based on whether the model had just indexed the validation documents or indexed them one epoch ago, causing regular peaks and valleys in the validation performance. When picking a checkpoint with maximum validation performance, as we do in the main experiments of this paper, we are implicitly then picking the checkpoint with minimum forgetting.InTable 6, we aim to provide more context to this phenomenon by providing retrieval validation scores for minimum forgetting checkpoints (highest peak), maximum forgetting checkpoints (highest trough), as well as their average score representing if the validation documents were uniformly distributed across the entire indexing split. Results. We see that for the best configuration of DSI (semantic docids), even when experiencing maximum forgetting DSI is still competitive with BM25, and in the average case DSI still outperforms the Dual Encoder baseline.\nExploring the limits of large scale pre-training. Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, Hanie Sedghi, arXiv:2110.02095arXiv preprintSamira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the limits of large scale pre-training. arXiv preprint arXiv:2110.02095, 2021.\n\nImproving language models by retrieving from trillions of tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, arXiv:2112.04426arXiv preprintSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. arXiv preprint arXiv:2112.04426, 2021.\n\nLanguage models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:2005.14165arXiv preprintTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n\nAutoregressive entity retrieval. Nicola De Cao, Gautier Izacard, Sebastian Riedel, Fabio Petroni, arXiv:2010.00904arXiv preprintNicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autoregressive entity retrieval. arXiv preprint arXiv:2010.00904, 2020.\n\nNeural ranking models with weak supervision. Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, W Bruce Croft, Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 40th International ACM SIGIR Conference on Research and Development in Information RetrievalMostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W Bruce Croft. Neural ranking models with weak supervision. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 65-74, 2017.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nNan Du, Yanping Huang, M Andrew, Simon Dai, Dmitry Tong, Yuanzhong Lepikhin, Maxim Xu, Yanqi Krikun, Adams Wei Zhou, Orhan Yu, Firat, arXiv:2112.06905Efficient scaling of language models with mixture-of-experts. arXiv preprintNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. arXiv preprint arXiv:2112.06905, 2021.\n\nSwitch transformers: Scaling to trillion parameter models with simple and efficient sparsity. William Fedus, Barret Zoph, Noam Shazeer, arXiv:2101.03961arXiv preprintWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.\n\nSimCSE: Simple contrastive learning of sentence embeddings. Tianyu Gao, Xingcheng Yao, Danqi Chen, 10.18653/v1/2021.emnlp-main.552Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingOnline and Punta Cana, Dominican RepublicAssociation for Computational LinguisticsTianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894-6910, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552. URL https: //aclanthology.org/2021.emnlp-main.552.\n\nTransformer feed-forward layers are key-value memories. Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy, arXiv:2012.14913arXiv preprintMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020.\n\nEnd-to-end retrieval in continuous space. Daniel Gillick, Alessandro Presta, Gaurav Singh Tomar, arXiv:1811.08008arXiv preprintDaniel Gillick, Alessandro Presta, and Gaurav Singh Tomar. End-to-end retrieval in continuous space. arXiv preprint arXiv:1811.08008, 2018.\n\nAccelerating large-scale inference with anisotropic vector quantization. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, Sanjiv Kumar, International Conference on Machine Learning. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, 2020. URL https://arxiv.org/abs/1908.10396.\n\nREALM: Retrieval-Augmented Language Model Pre-Training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Proceedings of ICML 2020. ICML 2020Kelvin Guu, Kenton Lee, Zora Tung, and Panupong Pasupat. REALM: Retrieval-Augmented Language Model Pre-Training. In Proceedings of ICML 2020, 2020.\n\nMartin Josifoski, Nicola De Cao, Maxime Peyrard, Robert West Genie, arXiv:2112.08340Generative information extraction. arXiv preprintMartin Josifoski, Nicola De Cao, Maxime Peyrard, and Robert West. Genie: Generative information extraction. arXiv preprint arXiv:2112.08340, 2021.\n\nJared Kaplan, Sam Mccandlish, Tom Henighan, B Tom, Benjamin Brown, Rewon Chess, Scott Child, Alec Gray, Jeffrey Radford, Dario Wu, Amodei, arXiv:2001.08361Scaling laws for neural language models. arXiv preprintJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, arXiv:2004.04906Dense passage retrieval for open-domain question answering. arXiv preprintVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.\n\nNatural Questions: a Benchmark for Question Answering Research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Transactions of the ACL. Illia Polosukhin, Jacob Devlin Kenton Lee, Kristina Toutanova, Llion Jones Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, and Slav PetrovTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin Kenton Lee, Kristina Toutanova, Llion Jones Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. In Transactions of the ACL, 2019.\n\nGshard: Scaling giant models with conditional computation and automatic sharding. Dmitry Lepikhin, Hyoukjoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen, arXiv:2006.16668arXiv preprintDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\n\nRethinking search: making domain experts out of dilettantes. Donald Metzler, Yi Tay, Dara Bahri, Marc Najork, ACM SIGIR Forum. New York, NY, USAACM552021Donald Metzler, Yi Tay, Dara Bahri, and Marc Najork. Rethinking search: making domain experts out of dilettantes. In ACM SIGIR Forum, volume 55, pages 1-27. ACM New York, NY, USA, 2021.\n\nSentence-t5: Scalable sentence encoders from pre-trained text-to-text models. Jianmo Ni, Gustavo Hern\u00e1ndez \u00c1brego, Noah Constant, Ji Ma, B Keith, Daniel Hall, Yinfei Cer, Yang, arXiv:2108.08877arXiv preprintJianmo Ni, Gustavo Hern\u00e1ndez \u00c1brego, Noah Constant, Ji Ma, Keith B Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. arXiv preprint arXiv:2108.08877, 2021.\n\nLanguage models as knowledge bases?. Fabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, Sebastian Riedel, arXiv:1909.01066arXiv preprintFabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.10683arXiv preprintColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n\nHubert Ramsauer, Bernhard Sch\u00e4fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovi\u0107, arXiv:2008.02217Geir Kjetil Sandve, et al. Hopfield networks is all you need. arXiv preprintHubert Ramsauer, Bernhard Sch\u00e4fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovi\u0107, Geir Kjetil Sandve, et al. Hopfield networks is all you need. arXiv preprint arXiv:2008.02217, 2020.\n\nHow much knowledge can you pack into the parameters of a language model. Adam Roberts, Colin Raffel, Noam Shazeer, arXiv:2002.08910arXiv preprintAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:2002.08910, 2020.\n\nTest-time training with self-supervision for generalization under distribution shifts. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, Moritz Hardt, PMLRProceedings of the 37th International Conference on Machine Learning. Hal Daum\u00e9 III and Aarti Singhthe 37th International Conference on Machine Learning119Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 9229-9248. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/sun20b.html.\n\nIlya Sutskever, Oriol Vinyals, Quoc V Le, arXiv:1409.3215Sequence to sequence learning with neural networks. arXiv preprintIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. arXiv preprint arXiv:1409.3215, 2014.\n\nScale efficiently: Insights from pre-training and fine-tuning transformers. Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won, Sharan Chung, Dani Narang, Ashish Yogatama, Donald Vaswani, Metzler, arXiv:2109.10686arXiv preprintYi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale efficiently: Insights from pre-training and fine-tuning transformers. arXiv preprint arXiv:2109.10686, 2021.\n\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Language models for dialog applications. arXiv preprintRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.\n", "annotations": {"author": "[{\"end\":79,\"start\":55},{\"end\":110,\"start\":80},{\"end\":128,\"start\":111},{\"end\":139,\"start\":129},{\"end\":151,\"start\":140},{\"end\":164,\"start\":152},{\"end\":174,\"start\":165},{\"end\":183,\"start\":175},{\"end\":193,\"start\":184},{\"end\":204,\"start\":194},{\"end\":218,\"start\":205},{\"end\":235,\"start\":219},{\"end\":270,\"start\":236},{\"end\":287,\"start\":271}]", "publisher": null, "author_last_name": "[{\"end\":61,\"start\":58},{\"end\":91,\"start\":87},{\"end\":127,\"start\":119},{\"end\":138,\"start\":136},{\"end\":150,\"start\":145},{\"end\":163,\"start\":158},{\"end\":173,\"start\":170},{\"end\":182,\"start\":179},{\"end\":192,\"start\":188},{\"end\":203,\"start\":198},{\"end\":217,\"start\":209},{\"end\":234,\"start\":229},{\"end\":250,\"start\":243},{\"end\":286,\"start\":278}]", "author_first_name": "[{\"end\":57,\"start\":55},{\"end\":84,\"start\":80},{\"end\":86,\"start\":85},{\"end\":118,\"start\":111},{\"end\":135,\"start\":129},{\"end\":144,\"start\":140},{\"end\":157,\"start\":152},{\"end\":169,\"start\":165},{\"end\":178,\"start\":175},{\"end\":187,\"start\":184},{\"end\":197,\"start\":194},{\"end\":208,\"start\":205},{\"end\":226,\"start\":219},{\"end\":228,\"start\":227},{\"end\":242,\"start\":236},{\"end\":277,\"start\":271}]", "author_affiliation": null, "title": "[{\"end\":52,\"start\":1},{\"end\":339,\"start\":288}]", "venue": null, "abstract": "[{\"end\":1246,\"start\":341}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1728,\"start\":1706},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1751,\"start\":1728},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1767,\"start\":1751},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1931,\"start\":1908},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2255,\"start\":2233},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2356,\"start\":2336},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2376,\"start\":2356},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2399,\"start\":2376},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2415,\"start\":2399},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8181,\"start\":8157},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9123,\"start\":9100},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9140,\"start\":9123},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9424,\"start\":9401},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9445,\"start\":9424},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9462,\"start\":9445},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9478,\"start\":9462},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9500,\"start\":9478},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9948,\"start\":9926},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10302,\"start\":10280},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10447,\"start\":10428},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10658,\"start\":10635},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13785,\"start\":13764},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21809,\"start\":21783},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23263,\"start\":23242},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24221,\"start\":24200},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24658,\"start\":24641},{\"end\":24914,\"start\":24890},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25032,\"start\":25015},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25155,\"start\":25138},{\"end\":25275,\"start\":25257},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":27299,\"start\":27282},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30883,\"start\":30866},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32364,\"start\":32342},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33251,\"start\":33234},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":33273,\"start\":33251}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35185,\"start\":35098},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35935,\"start\":35186},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36038,\"start\":35936},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36113,\"start\":36039},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36206,\"start\":36114},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37085,\"start\":36207},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37662,\"start\":37086},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":37878,\"start\":37663},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38609,\"start\":37879},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":39050,\"start\":38610},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":39766,\"start\":39051}]", "paragraph": "[{\"end\":2105,\"start\":1262},{\"end\":3040,\"start\":2107},{\"end\":3641,\"start\":3276},{\"end\":3745,\"start\":3643},{\"end\":4029,\"start\":3747},{\"end\":4407,\"start\":4031},{\"end\":4795,\"start\":4409},{\"end\":5023,\"start\":4797},{\"end\":5257,\"start\":5025},{\"end\":5851,\"start\":5259},{\"end\":6700,\"start\":5853},{\"end\":6951,\"start\":6702},{\"end\":7623,\"start\":6953},{\"end\":8677,\"start\":7640},{\"end\":8969,\"start\":8679},{\"end\":9385,\"start\":8971},{\"end\":9820,\"start\":9387},{\"end\":10781,\"start\":9822},{\"end\":11054,\"start\":10813},{\"end\":11321,\"start\":11056},{\"end\":11418,\"start\":11323},{\"end\":11474,\"start\":11420},{\"end\":11944,\"start\":11476},{\"end\":12466,\"start\":11968},{\"end\":12553,\"start\":12486},{\"end\":13199,\"start\":12571},{\"end\":13457,\"start\":13201},{\"end\":13673,\"start\":13459},{\"end\":13975,\"start\":13693},{\"end\":14142,\"start\":13977},{\"end\":14454,\"start\":14181},{\"end\":14626,\"start\":14456},{\"end\":14954,\"start\":14628},{\"end\":15265,\"start\":14956},{\"end\":15631,\"start\":15303},{\"end\":15839,\"start\":15667},{\"end\":16147,\"start\":15841},{\"end\":16242,\"start\":16149},{\"end\":16718,\"start\":16287},{\"end\":16954,\"start\":16760},{\"end\":17247,\"start\":16956},{\"end\":17667,\"start\":17249},{\"end\":18097,\"start\":17669},{\"end\":18194,\"start\":18099},{\"end\":18496,\"start\":18196},{\"end\":18709,\"start\":18498},{\"end\":18885,\"start\":18711},{\"end\":19730,\"start\":18887},{\"end\":20529,\"start\":19760},{\"end\":21233,\"start\":20531},{\"end\":21704,\"start\":21249},{\"end\":23009,\"start\":21706},{\"end\":23157,\"start\":23011},{\"end\":24567,\"start\":23159},{\"end\":26180,\"start\":24581},{\"end\":27660,\"start\":26237},{\"end\":29064,\"start\":27705},{\"end\":29258,\"start\":29066},{\"end\":29890,\"start\":29260},{\"end\":30661,\"start\":29919},{\"end\":30761,\"start\":30663},{\"end\":31494,\"start\":30763},{\"end\":32149,\"start\":31539},{\"end\":32892,\"start\":32164},{\"end\":33641,\"start\":32894},{\"end\":33731,\"start\":33654},{\"end\":33924,\"start\":33773},{\"end\":34038,\"start\":33962},{\"end\":34496,\"start\":34078},{\"end\":35097,\"start\":34498}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3275,\"start\":3041},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16286,\"start\":16243}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":3743,\"start\":3736},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":4099,\"start\":4092},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25785,\"start\":25778},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25869,\"start\":25862},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26744,\"start\":26737}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1260,\"start\":1248},{\"attributes\":{\"n\":\"2\"},\"end\":7638,\"start\":7626},{\"attributes\":{\"n\":\"3\"},\"end\":10811,\"start\":10784},{\"attributes\":{\"n\":\"3.1\"},\"end\":11966,\"start\":11947},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":12484,\"start\":12469},{\"end\":12569,\"start\":12556},{\"end\":13691,\"start\":13676},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":14179,\"start\":14145},{\"attributes\":{\"n\":\"3.2\"},\"end\":15301,\"start\":15268},{\"end\":15665,\"start\":15634},{\"end\":16758,\"start\":16721},{\"attributes\":{\"n\":\"3.3\"},\"end\":19758,\"start\":19733},{\"attributes\":{\"n\":\"4\"},\"end\":21247,\"start\":21236},{\"attributes\":{\"n\":\"4.1\"},\"end\":24579,\"start\":24570},{\"attributes\":{\"n\":\"4.2\"},\"end\":26203,\"start\":26183},{\"end\":26235,\"start\":26206},{\"end\":27680,\"start\":27663},{\"end\":27703,\"start\":27683},{\"end\":29917,\"start\":29893},{\"end\":31537,\"start\":31497},{\"attributes\":{\"n\":\"5\"},\"end\":32162,\"start\":32152},{\"attributes\":{\"n\":\"7\"},\"end\":33652,\"start\":33644},{\"attributes\":{\"n\":\"7.1\"},\"end\":33752,\"start\":33734},{\"attributes\":{\"n\":\"7.2\"},\"end\":33771,\"start\":33755},{\"attributes\":{\"n\":\"7.2.1\"},\"end\":33960,\"start\":33927},{\"attributes\":{\"n\":\"7.2.2\"},\"end\":34076,\"start\":34041},{\"end\":35109,\"start\":35099},{\"end\":35197,\"start\":35187},{\"end\":35947,\"start\":35937},{\"end\":36050,\"start\":36040},{\"end\":36125,\"start\":36115},{\"end\":37096,\"start\":37087},{\"end\":37673,\"start\":37664},{\"end\":37889,\"start\":37880},{\"end\":38620,\"start\":38611},{\"end\":39061,\"start\":39052}]", "table": "[{\"end\":37085,\"start\":36431},{\"end\":37662,\"start\":37348},{\"end\":37878,\"start\":37855},{\"end\":38609,\"start\":37891},{\"end\":39050,\"start\":38917},{\"end\":39766,\"start\":39298}]", "figure_caption": "[{\"end\":35185,\"start\":35111},{\"end\":35935,\"start\":35199},{\"end\":36038,\"start\":35949},{\"end\":36113,\"start\":36052},{\"end\":36206,\"start\":36127},{\"end\":36431,\"start\":36209},{\"end\":37348,\"start\":37098},{\"end\":37855,\"start\":37675},{\"end\":38917,\"start\":38622},{\"end\":39298,\"start\":39063}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2047,\"start\":2039},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6158,\"start\":6150},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30051,\"start\":30043},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":31095,\"start\":31087},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31837,\"start\":31829}]", "bib_author_first_name": "[{\"end\":41298,\"start\":41292},{\"end\":41313,\"start\":41306},{\"end\":41330,\"start\":41324},{\"end\":41347,\"start\":41342},{\"end\":41619,\"start\":41610},{\"end\":41636,\"start\":41630},{\"end\":41651,\"start\":41645},{\"end\":41668,\"start\":41662},{\"end\":41679,\"start\":41674},{\"end\":41697,\"start\":41692},{\"end\":41714,\"start\":41708},{\"end\":41747,\"start\":41734},{\"end\":41763,\"start\":41757},{\"end\":41776,\"start\":41771},{\"end\":42148,\"start\":42140},{\"end\":42166,\"start\":42162},{\"end\":42180,\"start\":42173},{\"end\":42193,\"start\":42188},{\"end\":42211,\"start\":42203},{\"end\":42226,\"start\":42220},{\"end\":42243,\"start\":42237},{\"end\":42263,\"start\":42257},{\"end\":42277,\"start\":42271},{\"end\":42603,\"start\":42597},{\"end\":42619,\"start\":42612},{\"end\":42638,\"start\":42629},{\"end\":42652,\"start\":42647},{\"end\":42886,\"start\":42879},{\"end\":42902,\"start\":42897},{\"end\":42919,\"start\":42911},{\"end\":42933,\"start\":42929},{\"end\":42948,\"start\":42941},{\"end\":43432,\"start\":43427},{\"end\":43449,\"start\":43441},{\"end\":43463,\"start\":43457},{\"end\":43477,\"start\":43469},{\"end\":43487,\"start\":43478},{\"end\":43791,\"start\":43788},{\"end\":43803,\"start\":43796},{\"end\":43812,\"start\":43811},{\"end\":43826,\"start\":43821},{\"end\":43838,\"start\":43832},{\"end\":43854,\"start\":43845},{\"end\":43870,\"start\":43865},{\"end\":43880,\"start\":43875},{\"end\":43894,\"start\":43889},{\"end\":43898,\"start\":43895},{\"end\":43910,\"start\":43905},{\"end\":44363,\"start\":44356},{\"end\":44377,\"start\":44371},{\"end\":44388,\"start\":44384},{\"end\":44674,\"start\":44668},{\"end\":44689,\"start\":44680},{\"end\":44700,\"start\":44695},{\"end\":45440,\"start\":45437},{\"end\":45451,\"start\":45447},{\"end\":45470,\"start\":45462},{\"end\":45483,\"start\":45479},{\"end\":45721,\"start\":45715},{\"end\":45741,\"start\":45731},{\"end\":45762,\"start\":45750},{\"end\":46019,\"start\":46014},{\"end\":46031,\"start\":46025},{\"end\":46041,\"start\":46037},{\"end\":46056,\"start\":46052},{\"end\":46068,\"start\":46063},{\"end\":46082,\"start\":46077},{\"end\":46096,\"start\":46090},{\"end\":46473,\"start\":46467},{\"end\":46485,\"start\":46479},{\"end\":46495,\"start\":46491},{\"end\":46510,\"start\":46502},{\"end\":46710,\"start\":46704},{\"end\":46728,\"start\":46722},{\"end\":46731,\"start\":46729},{\"end\":46743,\"start\":46737},{\"end\":46759,\"start\":46753},{\"end\":46764,\"start\":46760},{\"end\":46990,\"start\":46985},{\"end\":47002,\"start\":46999},{\"end\":47018,\"start\":47015},{\"end\":47030,\"start\":47029},{\"end\":47044,\"start\":47036},{\"end\":47057,\"start\":47052},{\"end\":47070,\"start\":47065},{\"end\":47082,\"start\":47078},{\"end\":47096,\"start\":47089},{\"end\":47111,\"start\":47106},{\"end\":47426,\"start\":47418},{\"end\":47444,\"start\":47438},{\"end\":47456,\"start\":47451},{\"end\":47469,\"start\":47462},{\"end\":47483,\"start\":47477},{\"end\":47494,\"start\":47488},{\"end\":47508,\"start\":47503},{\"end\":47522,\"start\":47515},{\"end\":47899,\"start\":47896},{\"end\":47923,\"start\":47913},{\"end\":47940,\"start\":47934},{\"end\":47958,\"start\":47951},{\"end\":47973,\"start\":47968},{\"end\":47987,\"start\":47982},{\"end\":48005,\"start\":47997},{\"end\":48672,\"start\":48666},{\"end\":48693,\"start\":48683},{\"end\":48708,\"start\":48699},{\"end\":48718,\"start\":48713},{\"end\":48730,\"start\":48725},{\"end\":48745,\"start\":48738},{\"end\":48758,\"start\":48753},{\"end\":48771,\"start\":48767},{\"end\":48788,\"start\":48781},{\"end\":49147,\"start\":49141},{\"end\":49159,\"start\":49157},{\"end\":49169,\"start\":49165},{\"end\":49181,\"start\":49177},{\"end\":49504,\"start\":49498},{\"end\":49516,\"start\":49509},{\"end\":49526,\"start\":49517},{\"end\":49539,\"start\":49535},{\"end\":49552,\"start\":49550},{\"end\":49558,\"start\":49557},{\"end\":49572,\"start\":49566},{\"end\":49585,\"start\":49579},{\"end\":49889,\"start\":49884},{\"end\":49902,\"start\":49899},{\"end\":49923,\"start\":49916},{\"end\":49936,\"start\":49931},{\"end\":49953,\"start\":49946},{\"end\":49967,\"start\":49958},{\"end\":49969,\"start\":49968},{\"end\":49987,\"start\":49978},{\"end\":50306,\"start\":50301},{\"end\":50319,\"start\":50315},{\"end\":50333,\"start\":50329},{\"end\":50352,\"start\":50343},{\"end\":50364,\"start\":50358},{\"end\":50380,\"start\":50373},{\"end\":50394,\"start\":50389},{\"end\":50404,\"start\":50401},{\"end\":50416,\"start\":50409},{\"end\":50706,\"start\":50700},{\"end\":50725,\"start\":50717},{\"end\":50742,\"start\":50734},{\"end\":50758,\"start\":50751},{\"end\":50773,\"start\":50766},{\"end\":50789,\"start\":50783},{\"end\":50802,\"start\":50797},{\"end\":50817,\"start\":50811},{\"end\":50837,\"start\":50831},{\"end\":51267,\"start\":51263},{\"end\":51282,\"start\":51277},{\"end\":51295,\"start\":51291},{\"end\":51583,\"start\":51581},{\"end\":51597,\"start\":51589},{\"end\":51610,\"start\":51604},{\"end\":51620,\"start\":51616},{\"end\":51635,\"start\":51629},{\"end\":51649,\"start\":51643},{\"end\":52248,\"start\":52244},{\"end\":52265,\"start\":52260},{\"end\":52281,\"start\":52275},{\"end\":52582,\"start\":52580},{\"end\":52595,\"start\":52588},{\"end\":52613,\"start\":52606},{\"end\":52626,\"start\":52619},{\"end\":52640,\"start\":52634},{\"end\":52665,\"start\":52659},{\"end\":52677,\"start\":52673},{\"end\":52692,\"start\":52686},{\"end\":52709,\"start\":52703},{\"end\":53030,\"start\":53025},{\"end\":53048,\"start\":53042},{\"end\":53051,\"start\":53049},{\"end\":53066,\"start\":53061},{\"end\":53077,\"start\":53073},{\"end\":53093,\"start\":53087},{\"end\":53124,\"start\":53118},{\"end\":53138,\"start\":53132},{\"end\":53150,\"start\":53144},{\"end\":53158,\"start\":53156},{\"end\":53513,\"start\":53507},{\"end\":53527,\"start\":53523},{\"end\":53541,\"start\":53537},{\"end\":53555,\"start\":53550},{\"end\":53572,\"start\":53567},{\"end\":53585,\"start\":53580},{\"end\":53587,\"start\":53586},{\"end\":53601,\"start\":53595},{\"end\":53615,\"start\":53610}]", "bib_author_last_name": "[{\"end\":41304,\"start\":41299},{\"end\":41322,\"start\":41314},{\"end\":41340,\"start\":41331},{\"end\":41354,\"start\":41348},{\"end\":41628,\"start\":41620},{\"end\":41643,\"start\":41637},{\"end\":41660,\"start\":41652},{\"end\":41672,\"start\":41669},{\"end\":41690,\"start\":41680},{\"end\":41706,\"start\":41698},{\"end\":41732,\"start\":41715},{\"end\":41755,\"start\":41748},{\"end\":41769,\"start\":41764},{\"end\":41782,\"start\":41777},{\"end\":42160,\"start\":42149},{\"end\":42171,\"start\":42167},{\"end\":42186,\"start\":42181},{\"end\":42201,\"start\":42194},{\"end\":42218,\"start\":42212},{\"end\":42235,\"start\":42227},{\"end\":42255,\"start\":42244},{\"end\":42269,\"start\":42264},{\"end\":42284,\"start\":42278},{\"end\":42292,\"start\":42286},{\"end\":42610,\"start\":42604},{\"end\":42627,\"start\":42620},{\"end\":42645,\"start\":42639},{\"end\":42660,\"start\":42653},{\"end\":42895,\"start\":42887},{\"end\":42909,\"start\":42903},{\"end\":42927,\"start\":42920},{\"end\":42939,\"start\":42934},{\"end\":42954,\"start\":42949},{\"end\":43439,\"start\":43433},{\"end\":43455,\"start\":43450},{\"end\":43467,\"start\":43464},{\"end\":43492,\"start\":43488},{\"end\":43794,\"start\":43792},{\"end\":43809,\"start\":43804},{\"end\":43819,\"start\":43813},{\"end\":43830,\"start\":43827},{\"end\":43843,\"start\":43839},{\"end\":43863,\"start\":43855},{\"end\":43873,\"start\":43871},{\"end\":43887,\"start\":43881},{\"end\":43903,\"start\":43899},{\"end\":43913,\"start\":43911},{\"end\":43920,\"start\":43915},{\"end\":44369,\"start\":44364},{\"end\":44382,\"start\":44378},{\"end\":44396,\"start\":44389},{\"end\":44678,\"start\":44675},{\"end\":44693,\"start\":44690},{\"end\":44705,\"start\":44701},{\"end\":45445,\"start\":45441},{\"end\":45460,\"start\":45452},{\"end\":45477,\"start\":45471},{\"end\":45488,\"start\":45484},{\"end\":45729,\"start\":45722},{\"end\":45748,\"start\":45742},{\"end\":45768,\"start\":45763},{\"end\":46023,\"start\":46020},{\"end\":46035,\"start\":46032},{\"end\":46050,\"start\":46042},{\"end\":46061,\"start\":46057},{\"end\":46075,\"start\":46069},{\"end\":46088,\"start\":46083},{\"end\":46102,\"start\":46097},{\"end\":46477,\"start\":46474},{\"end\":46489,\"start\":46486},{\"end\":46500,\"start\":46496},{\"end\":46518,\"start\":46511},{\"end\":46720,\"start\":46711},{\"end\":46735,\"start\":46732},{\"end\":46751,\"start\":46744},{\"end\":46770,\"start\":46765},{\"end\":46997,\"start\":46991},{\"end\":47013,\"start\":47003},{\"end\":47027,\"start\":47019},{\"end\":47034,\"start\":47031},{\"end\":47050,\"start\":47045},{\"end\":47063,\"start\":47058},{\"end\":47076,\"start\":47071},{\"end\":47087,\"start\":47083},{\"end\":47104,\"start\":47097},{\"end\":47114,\"start\":47112},{\"end\":47122,\"start\":47116},{\"end\":47436,\"start\":47427},{\"end\":47449,\"start\":47445},{\"end\":47460,\"start\":47457},{\"end\":47475,\"start\":47470},{\"end\":47486,\"start\":47484},{\"end\":47501,\"start\":47495},{\"end\":47513,\"start\":47509},{\"end\":47526,\"start\":47523},{\"end\":47911,\"start\":47900},{\"end\":47932,\"start\":47924},{\"end\":47949,\"start\":47941},{\"end\":47966,\"start\":47959},{\"end\":47980,\"start\":47974},{\"end\":47995,\"start\":47988},{\"end\":48013,\"start\":48006},{\"end\":48681,\"start\":48673},{\"end\":48697,\"start\":48694},{\"end\":48711,\"start\":48709},{\"end\":48723,\"start\":48719},{\"end\":48736,\"start\":48731},{\"end\":48751,\"start\":48746},{\"end\":48765,\"start\":48759},{\"end\":48779,\"start\":48772},{\"end\":48793,\"start\":48789},{\"end\":49155,\"start\":49148},{\"end\":49163,\"start\":49160},{\"end\":49175,\"start\":49170},{\"end\":49188,\"start\":49182},{\"end\":49507,\"start\":49505},{\"end\":49533,\"start\":49527},{\"end\":49548,\"start\":49540},{\"end\":49555,\"start\":49553},{\"end\":49564,\"start\":49559},{\"end\":49577,\"start\":49573},{\"end\":49589,\"start\":49586},{\"end\":49595,\"start\":49591},{\"end\":49897,\"start\":49890},{\"end\":49914,\"start\":49903},{\"end\":49929,\"start\":49924},{\"end\":49944,\"start\":49937},{\"end\":49956,\"start\":49954},{\"end\":49976,\"start\":49970},{\"end\":49994,\"start\":49988},{\"end\":50313,\"start\":50307},{\"end\":50327,\"start\":50320},{\"end\":50341,\"start\":50334},{\"end\":50356,\"start\":50353},{\"end\":50371,\"start\":50365},{\"end\":50387,\"start\":50381},{\"end\":50399,\"start\":50395},{\"end\":50407,\"start\":50405},{\"end\":50420,\"start\":50417},{\"end\":50715,\"start\":50707},{\"end\":50732,\"start\":50726},{\"end\":50749,\"start\":50743},{\"end\":50764,\"start\":50759},{\"end\":50781,\"start\":50774},{\"end\":50795,\"start\":50790},{\"end\":50809,\"start\":50803},{\"end\":50829,\"start\":50818},{\"end\":50846,\"start\":50838},{\"end\":51275,\"start\":51268},{\"end\":51289,\"start\":51283},{\"end\":51303,\"start\":51296},{\"end\":51587,\"start\":51584},{\"end\":51602,\"start\":51598},{\"end\":51614,\"start\":51611},{\"end\":51627,\"start\":51621},{\"end\":51641,\"start\":51636},{\"end\":51655,\"start\":51650},{\"end\":52258,\"start\":52249},{\"end\":52273,\"start\":52266},{\"end\":52284,\"start\":52282},{\"end\":52586,\"start\":52583},{\"end\":52604,\"start\":52596},{\"end\":52617,\"start\":52614},{\"end\":52632,\"start\":52627},{\"end\":52646,\"start\":52641},{\"end\":52657,\"start\":52648},{\"end\":52671,\"start\":52666},{\"end\":52684,\"start\":52678},{\"end\":52701,\"start\":52693},{\"end\":52717,\"start\":52710},{\"end\":52726,\"start\":52719},{\"end\":53040,\"start\":53031},{\"end\":53059,\"start\":53052},{\"end\":53071,\"start\":53067},{\"end\":53085,\"start\":53078},{\"end\":53106,\"start\":53094},{\"end\":53116,\"start\":53108},{\"end\":53130,\"start\":53125},{\"end\":53142,\"start\":53139},{\"end\":53154,\"start\":53151},{\"end\":53164,\"start\":53159},{\"end\":53168,\"start\":53166},{\"end\":53521,\"start\":53514},{\"end\":53535,\"start\":53528},{\"end\":53548,\"start\":53542},{\"end\":53565,\"start\":53556},{\"end\":53578,\"start\":53573},{\"end\":53593,\"start\":53588},{\"end\":53608,\"start\":53602},{\"end\":53626,\"start\":53616}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2110.02095\",\"id\":\"b0\"},\"end\":41542,\"start\":41242},{\"attributes\":{\"doi\":\"arXiv:2112.04426\",\"id\":\"b1\"},\"end\":42099,\"start\":41544},{\"attributes\":{\"doi\":\"arXiv:2005.14165\",\"id\":\"b2\"},\"end\":42562,\"start\":42101},{\"attributes\":{\"doi\":\"arXiv:2010.00904\",\"id\":\"b3\"},\"end\":42832,\"start\":42564},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3666085},\"end\":43425,\"start\":42834},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b5\"},\"end\":43786,\"start\":43427},{\"attributes\":{\"doi\":\"arXiv:2112.06905\",\"id\":\"b6\"},\"end\":44260,\"start\":43788},{\"attributes\":{\"doi\":\"arXiv:2101.03961\",\"id\":\"b7\"},\"end\":44606,\"start\":44262},{\"attributes\":{\"doi\":\"10.18653/v1/2021.emnlp-main.552\",\"id\":\"b8\",\"matched_paper_id\":233296292},\"end\":45379,\"start\":44608},{\"attributes\":{\"doi\":\"arXiv:2012.14913\",\"id\":\"b9\"},\"end\":45671,\"start\":45381},{\"attributes\":{\"doi\":\"arXiv:1811.08008\",\"id\":\"b10\"},\"end\":45939,\"start\":45673},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":218614141},\"end\":46409,\"start\":45941},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":211204736},\"end\":46702,\"start\":46411},{\"attributes\":{\"doi\":\"arXiv:2112.08340\",\"id\":\"b13\"},\"end\":46983,\"start\":46704},{\"attributes\":{\"doi\":\"arXiv:2001.08361\",\"id\":\"b14\"},\"end\":47416,\"start\":46985},{\"attributes\":{\"doi\":\"arXiv:2004.04906\",\"id\":\"b15\"},\"end\":47830,\"start\":47418},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":86611921},\"end\":48582,\"start\":47832},{\"attributes\":{\"doi\":\"arXiv:2006.16668\",\"id\":\"b17\"},\"end\":49078,\"start\":48584},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":237106778},\"end\":49418,\"start\":49080},{\"attributes\":{\"doi\":\"arXiv:2108.08877\",\"id\":\"b19\"},\"end\":49845,\"start\":49420},{\"attributes\":{\"doi\":\"arXiv:1909.01066\",\"id\":\"b20\"},\"end\":50216,\"start\":49847},{\"attributes\":{\"doi\":\"arXiv:1910.10683\",\"id\":\"b21\"},\"end\":50698,\"start\":50218},{\"attributes\":{\"doi\":\"arXiv:2008.02217\",\"id\":\"b22\"},\"end\":51188,\"start\":50700},{\"attributes\":{\"doi\":\"arXiv:2002.08910\",\"id\":\"b23\"},\"end\":51492,\"start\":51190},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b24\",\"matched_paper_id\":220301705},\"end\":52242,\"start\":51494},{\"attributes\":{\"doi\":\"arXiv:1409.3215\",\"id\":\"b25\"},\"end\":52502,\"start\":52244},{\"attributes\":{\"doi\":\"arXiv:2109.10686\",\"id\":\"b26\"},\"end\":53023,\"start\":52504},{\"attributes\":{\"doi\":\"arXiv:2201.08239\",\"id\":\"b27\"},\"end\":53478,\"start\":53025},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13756489},\"end\":53907,\"start\":53480}]", "bib_title": "[{\"end\":42877,\"start\":42834},{\"end\":44666,\"start\":44608},{\"end\":46012,\"start\":45941},{\"end\":46465,\"start\":46411},{\"end\":47894,\"start\":47832},{\"end\":49139,\"start\":49080},{\"end\":51579,\"start\":51494},{\"end\":53505,\"start\":53480}]", "bib_author": "[{\"end\":41306,\"start\":41292},{\"end\":41324,\"start\":41306},{\"end\":41342,\"start\":41324},{\"end\":41356,\"start\":41342},{\"end\":41630,\"start\":41610},{\"end\":41645,\"start\":41630},{\"end\":41662,\"start\":41645},{\"end\":41674,\"start\":41662},{\"end\":41692,\"start\":41674},{\"end\":41708,\"start\":41692},{\"end\":41734,\"start\":41708},{\"end\":41757,\"start\":41734},{\"end\":41771,\"start\":41757},{\"end\":41784,\"start\":41771},{\"end\":42162,\"start\":42140},{\"end\":42173,\"start\":42162},{\"end\":42188,\"start\":42173},{\"end\":42203,\"start\":42188},{\"end\":42220,\"start\":42203},{\"end\":42237,\"start\":42220},{\"end\":42257,\"start\":42237},{\"end\":42271,\"start\":42257},{\"end\":42286,\"start\":42271},{\"end\":42294,\"start\":42286},{\"end\":42612,\"start\":42597},{\"end\":42629,\"start\":42612},{\"end\":42647,\"start\":42629},{\"end\":42662,\"start\":42647},{\"end\":42897,\"start\":42879},{\"end\":42911,\"start\":42897},{\"end\":42929,\"start\":42911},{\"end\":42941,\"start\":42929},{\"end\":42956,\"start\":42941},{\"end\":43441,\"start\":43427},{\"end\":43457,\"start\":43441},{\"end\":43469,\"start\":43457},{\"end\":43494,\"start\":43469},{\"end\":43796,\"start\":43788},{\"end\":43811,\"start\":43796},{\"end\":43821,\"start\":43811},{\"end\":43832,\"start\":43821},{\"end\":43845,\"start\":43832},{\"end\":43865,\"start\":43845},{\"end\":43875,\"start\":43865},{\"end\":43889,\"start\":43875},{\"end\":43905,\"start\":43889},{\"end\":43915,\"start\":43905},{\"end\":43922,\"start\":43915},{\"end\":44371,\"start\":44356},{\"end\":44384,\"start\":44371},{\"end\":44398,\"start\":44384},{\"end\":44680,\"start\":44668},{\"end\":44695,\"start\":44680},{\"end\":44707,\"start\":44695},{\"end\":45447,\"start\":45437},{\"end\":45462,\"start\":45447},{\"end\":45479,\"start\":45462},{\"end\":45490,\"start\":45479},{\"end\":45731,\"start\":45715},{\"end\":45750,\"start\":45731},{\"end\":45770,\"start\":45750},{\"end\":46025,\"start\":46014},{\"end\":46037,\"start\":46025},{\"end\":46052,\"start\":46037},{\"end\":46063,\"start\":46052},{\"end\":46077,\"start\":46063},{\"end\":46090,\"start\":46077},{\"end\":46104,\"start\":46090},{\"end\":46479,\"start\":46467},{\"end\":46491,\"start\":46479},{\"end\":46502,\"start\":46491},{\"end\":46520,\"start\":46502},{\"end\":46722,\"start\":46704},{\"end\":46737,\"start\":46722},{\"end\":46753,\"start\":46737},{\"end\":46772,\"start\":46753},{\"end\":46999,\"start\":46985},{\"end\":47015,\"start\":46999},{\"end\":47029,\"start\":47015},{\"end\":47036,\"start\":47029},{\"end\":47052,\"start\":47036},{\"end\":47065,\"start\":47052},{\"end\":47078,\"start\":47065},{\"end\":47089,\"start\":47078},{\"end\":47106,\"start\":47089},{\"end\":47116,\"start\":47106},{\"end\":47124,\"start\":47116},{\"end\":47438,\"start\":47418},{\"end\":47451,\"start\":47438},{\"end\":47462,\"start\":47451},{\"end\":47477,\"start\":47462},{\"end\":47488,\"start\":47477},{\"end\":47503,\"start\":47488},{\"end\":47515,\"start\":47503},{\"end\":47528,\"start\":47515},{\"end\":47913,\"start\":47896},{\"end\":47934,\"start\":47913},{\"end\":47951,\"start\":47934},{\"end\":47968,\"start\":47951},{\"end\":47982,\"start\":47968},{\"end\":47997,\"start\":47982},{\"end\":48015,\"start\":47997},{\"end\":48683,\"start\":48666},{\"end\":48699,\"start\":48683},{\"end\":48713,\"start\":48699},{\"end\":48725,\"start\":48713},{\"end\":48738,\"start\":48725},{\"end\":48753,\"start\":48738},{\"end\":48767,\"start\":48753},{\"end\":48781,\"start\":48767},{\"end\":48795,\"start\":48781},{\"end\":49157,\"start\":49141},{\"end\":49165,\"start\":49157},{\"end\":49177,\"start\":49165},{\"end\":49190,\"start\":49177},{\"end\":49509,\"start\":49498},{\"end\":49535,\"start\":49509},{\"end\":49550,\"start\":49535},{\"end\":49557,\"start\":49550},{\"end\":49566,\"start\":49557},{\"end\":49579,\"start\":49566},{\"end\":49591,\"start\":49579},{\"end\":49597,\"start\":49591},{\"end\":49899,\"start\":49884},{\"end\":49916,\"start\":49899},{\"end\":49931,\"start\":49916},{\"end\":49946,\"start\":49931},{\"end\":49958,\"start\":49946},{\"end\":49978,\"start\":49958},{\"end\":49996,\"start\":49978},{\"end\":50315,\"start\":50301},{\"end\":50329,\"start\":50315},{\"end\":50343,\"start\":50329},{\"end\":50358,\"start\":50343},{\"end\":50373,\"start\":50358},{\"end\":50389,\"start\":50373},{\"end\":50401,\"start\":50389},{\"end\":50409,\"start\":50401},{\"end\":50422,\"start\":50409},{\"end\":50717,\"start\":50700},{\"end\":50734,\"start\":50717},{\"end\":50751,\"start\":50734},{\"end\":50766,\"start\":50751},{\"end\":50783,\"start\":50766},{\"end\":50797,\"start\":50783},{\"end\":50811,\"start\":50797},{\"end\":50831,\"start\":50811},{\"end\":50848,\"start\":50831},{\"end\":51277,\"start\":51263},{\"end\":51291,\"start\":51277},{\"end\":51305,\"start\":51291},{\"end\":51589,\"start\":51581},{\"end\":51604,\"start\":51589},{\"end\":51616,\"start\":51604},{\"end\":51629,\"start\":51616},{\"end\":51643,\"start\":51629},{\"end\":51657,\"start\":51643},{\"end\":52260,\"start\":52244},{\"end\":52275,\"start\":52260},{\"end\":52286,\"start\":52275},{\"end\":52588,\"start\":52580},{\"end\":52606,\"start\":52588},{\"end\":52619,\"start\":52606},{\"end\":52634,\"start\":52619},{\"end\":52648,\"start\":52634},{\"end\":52659,\"start\":52648},{\"end\":52673,\"start\":52659},{\"end\":52686,\"start\":52673},{\"end\":52703,\"start\":52686},{\"end\":52719,\"start\":52703},{\"end\":52728,\"start\":52719},{\"end\":53042,\"start\":53025},{\"end\":53061,\"start\":53042},{\"end\":53073,\"start\":53061},{\"end\":53087,\"start\":53073},{\"end\":53108,\"start\":53087},{\"end\":53118,\"start\":53108},{\"end\":53132,\"start\":53118},{\"end\":53144,\"start\":53132},{\"end\":53156,\"start\":53144},{\"end\":53166,\"start\":53156},{\"end\":53170,\"start\":53166},{\"end\":53523,\"start\":53507},{\"end\":53537,\"start\":53523},{\"end\":53550,\"start\":53537},{\"end\":53567,\"start\":53550},{\"end\":53580,\"start\":53567},{\"end\":53595,\"start\":53580},{\"end\":53610,\"start\":53595},{\"end\":53628,\"start\":53610}]", "bib_venue": "[{\"end\":41290,\"start\":41242},{\"end\":41608,\"start\":41544},{\"end\":42138,\"start\":42101},{\"end\":42595,\"start\":42564},{\"end\":43067,\"start\":42956},{\"end\":43584,\"start\":43510},{\"end\":43998,\"start\":43938},{\"end\":44354,\"start\":44262},{\"end\":44824,\"start\":44738},{\"end\":45435,\"start\":45381},{\"end\":45713,\"start\":45673},{\"end\":46148,\"start\":46104},{\"end\":46544,\"start\":46520},{\"end\":46821,\"start\":46788},{\"end\":47179,\"start\":47140},{\"end\":47602,\"start\":47544},{\"end\":48038,\"start\":48015},{\"end\":48664,\"start\":48584},{\"end\":49205,\"start\":49190},{\"end\":49496,\"start\":49420},{\"end\":49882,\"start\":49847},{\"end\":50299,\"start\":50218},{\"end\":50924,\"start\":50864},{\"end\":51261,\"start\":51190},{\"end\":51729,\"start\":51661},{\"end\":52351,\"start\":52301},{\"end\":52578,\"start\":52504},{\"end\":53225,\"start\":53186},{\"end\":53677,\"start\":53628},{\"end\":43165,\"start\":43069},{\"end\":44938,\"start\":44826},{\"end\":46555,\"start\":46546},{\"end\":49224,\"start\":49207},{\"end\":51813,\"start\":51760}]"}}}, "year": 2023, "month": 12, "day": 17}