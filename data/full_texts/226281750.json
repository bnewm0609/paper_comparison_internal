{"id": 226281750, "updated": "2023-10-06 09:38:53.227", "metadata": {"title": "Quantum-Inspired Algorithms from Randomized Numerical Linear Algebra", "authors": "[{\"first\":\"Nadiia\",\"last\":\"Chepurko\",\"middle\":[]},{\"first\":\"Kenneth\",\"last\":\"Clarkson\",\"middle\":[\"L.\"]},{\"first\":\"Lior\",\"last\":\"Horesh\",\"middle\":[]},{\"first\":\"Honghao\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Woodruff\",\"middle\":[\"P.\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "We create classical (non-quantum) dynamic data structures supporting queries for recommender systems and least-squares regression that are comparable to their quantum analogues. De-quantizing such algorithms has received a flurry of attention in recent years; we obtain sharper bounds for these problems. More significantly, we achieve these improvements by arguing that the previous quantum-inspired algorithms for these problems are doing leverage or ridge-leverage score sampling in disguise; these are powerful and standard techniques in randomized numerical linear algebra. With this recognition, we are able to employ the large body of work in numerical linear algebra to obtain algorithms for these problems that are simpler or faster (or both) than existing approaches. Our experiments demonstrate that the proposed data structures also work well on real-world datasets.", "fields_of_study": "[\"Computer Science\",\"Physics\"]", "external_ids": {"arxiv": "2011.04125", "mag": "3102366407", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/ChepurkoCHLW22", "doi": null}}, "content": {"source": {"pdf_hash": "3a5af79aadeef874c4afdea3221676db027e4445", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2011.04125v7.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2b9e58637533c81f10b64b7d45ce7849efd000a3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3a5af79aadeef874c4afdea3221676db027e4445.txt", "contents": "\nQuantum-Inspired Algorithms from Randomized Numerical Linear Algebra\n28 Jun 2022\n\nNadiia Chepurko nadiia@mit.edu \nIBM Research\nIBM Research\n\n\nMit \nIBM Research\nIBM Research\n\n\nKenneth L Clarkson klclarks@us.ibm.com \nIBM Research\nIBM Research\n\n\nLior Horesh lhoresh@us.ibm.com \nIBM Research\nIBM Research\n\n\nHonghao Lin cmuhonghaol@andrew.cmu.edu \nIBM Research\nIBM Research\n\n\nDavid P Woodruff dwoodruf@cs.cmu.edu \nIBM Research\nIBM Research\n\n\nCmu \nIBM Research\nIBM Research\n\n\nQuantum-Inspired Algorithms from Randomized Numerical Linear Algebra\n28 Jun 2022\nWe create classical (non-quantum) dynamic data structures supporting queries for recommender systems and least-squares regression that are comparable to their quantum analogues. De-quantizing such algorithms has received a flurry of attention in recent years; we obtain sharper bounds for these problems. More significantly, we achieve these improvements by arguing that the previous quantum-inspired algorithms for these problems are doing leverage or ridge-leverage score sampling in disguise; these are powerful and standard techniques in randomized numerical linear algebra. With this recognition, we are able to employ the large body of work in numerical linear algebra to obtain algorithms for these problems that are simpler or faster (or both) than existing approaches. Our experiments demonstrate that the proposed data structures also work well on real-world datasets.\n\nIntroduction\n\nIn recent years, quantum algorithms for various problems in numerical linear algebra have been proposed, with applications including least-squares regression and recommender systems [HHL09,\n\nLGZ16, RML14, GSLW19, ZFF19, BKL + 19, vAG19, LMR14, CD16, BCK15]. Some of these algorithms have the striking property that their running times do not depend on the input size. That is, for a given matrix A \u2208 R n\u00d7d with nnz(A) nonzero entries, the running times for these proposed quantum algorithms are at most polylogarithmic in n and d, and polynomial in other parameters of A, such as rank(A), the condition number \u03ba(A), or Frobenius norm A F . However, as observed by Tang [Tan19] and others, there is a catch: these quantum algorithms depend on a particular input representation of A, which is a simple data structure that allows A to be employed for quick preparation of a quantum state suitable for further quantum computations. This data structure, which is a collection of weighted complete binary trees, also supports rapid weighted random sampling of A, for example, sampling the rows of A with probability proportional to their squared Euclidean lengths. So, if an \"apples to apples\" comparison of quantum to classical computation is to be made, it is reasonable to ask what can be accomplished in the classical realm using the sampling that the given data structure supports.\n\nA recent line of work analyzes the speedups of these quantum algorithms by developing classical counterparts that exploit these restrictive input and output assumptions, and shows that previous quantum algorithms do not give an exponential speedup. In this setting, it has recently been shown that sublinear time is sufficient for least-squares regression using a low-rank design matrix A [GLT18, CLW18], for computing a low-rank approximation to input matrix A [Tan19], and for solving ridge regression problems [GST20a], using classical (non-quantum) methods, assuming the data structure of trees has already been constructed. Further, the results obtained in [Tan19, GLT18, GST20a] serve as appropriate comparisons of the power of quantum to classical computing, due to their novel input-output model: data structures are input, then sublinear-time computations are done, yielding data structures as output.\n\nThe simple weighted-sampling data structure used in these works to represent the input can be efficiently constructed and stored: it uses O(nnz(A)) space, with a small constant overhead, and requires O(nnz(A)) time to construct, in the static case where the matrix A is given in its entirety, and can support updates and queries to individual entries of A in O(log(nd)) time. However, the existing reported sublinear bounds are high-degree polynomials in the parameters involved: for instance, the sublinear term in the running time for low-rank least-squares regression is\u00d5(rank(A) 6 A 6 F \u03ba(A) 16 /\u03b5 6 ); see also more recent work for ridge regression [GST20a]. This combination of features raises the following question:\n\nQuestion 1: Can the sublinear terms in the running time be reduced significantly while preserving the leading order dependence of O(nnz(A)) and O(log(nd)) per update (dynamic)? Perhaps a question of greater importance is the connection between quantum-inspired algorithms and the vast body of work in randomized numerical linear algebra: see the surveys [KV09,Mah11,Woo14]. There are a large number of randomized algorithms based on sampling and sketching techniques for problems in linear algebra. Yet prior to our work, none of the quantuminspired algorithms, which are sampling-based, have discussed the connection to leverage scores, for example, which are a powerful and standard tool.\n\nQuestion 2: Can the large body of work in randomized numerical linear algebra be applied effectively in the setting of quantum-inspired algorithms?\n\n\nOur Results\n\nWe answer both of the questions above affirmatively. In fact, we answer Question 1 by answering Question 2. Namely, we obtain significant improvements in the sublinear terms, and our analysis relies on simulating leverage score sampling and ridge leverage score sampling, using the aforementioned data structure to sample rows proportional to squared Euclidean norms. Additionally, we empirically demonstrate the speedup we achieve on real-world and synthetic datasets (see Section 6).\n\nConnection to Classical Linear Algebra and Dynamic Data Structures. The work on quantum-inspired algorithms builds data structures for sampling according to the squared row and column lengths of a matrix. This is also a common technique in randomized numerical linear algebra -see the recent survey on length-squared sampling by Kannan and Vempala [KV17]. However, it is well-known that leverage score sampling often gives stronger guarantees than length-squared sampling; leverage score sampling was pioneered in the algorithms community in [DMM06], and made efficient in [DMIMW12] (see also analogous prior work in the \u2113 1 setting, starting with [Cla05]). Given an n \u00d7 d matrix A, with n \u2265 d, its (row) leverage scores are the squared row norms of U , where U is an orthonormal basis with the same column span as A. One can show that any choice of basis gives the same scores. Writing A = U \u03a3V T in its thin singular value decomposition (SVD), and letting A i, * and U i, * denote the i-th rows of A and U respectively, we see that A i, * = U i, * \u03a3 . Consequently, letting k = rank(A), and with \u03c3 1 and \u03c3 k denoting the maximum and minimum non-zero singular values of A, we have A i, * \u2265 U i, * \u03c3 k (A), and A i, * \u2264 U i, * \u03c3 1 (A).\n\nThus, sampling according to the squared row norms of A is equivalent to sampling from a distribution with ratio distance at most \u03ba 2 (A) = \u03c3 1 (A) 2 \u03c3 k (A) 2 from the leverage score distribution, that is, sampling a row with probability proportional to its leverage score. This is crucial, as it implies using standard arguments (see, e.g., [Woo14] for a survey) that if we oversample by a factor of \u03ba 2 (A), then we obtain the same guarantees for various problems that leverage score sampling achieves. Notice that the running times of quantum-inspired algorithms, e.g., the aforementioned O(rank(A) 6 A 6 F \u03ba(A) 16 /\u03b5 6 ) time for regression of [GLT18] and the\u00d5( A 8 F \u03ba(A) 2 /(\u03c3 min (A) 6 \u03b5 4 )) time for regression of [GST20a], both take a number of squared-length samples of A depending on \u03ba(A), and thus are implicitly doing leverage score sampling, or in the case of ridge regression, ridge leverage score sampling.\n\nGiven the connection above, we focus on two central problems in machine learning and numerical linear algebra, ridge regression (Problem 1) and low rank approximation (Problem 2). We show how to obtain simpler algorithms and analysis than those in the quantum-inspired literature by using existing approximate matrix product and subspace embedding guarantees of leverage score sampling. In addition to improved bounds, our analysis de-mystifies what the rather involved \u2113 2 -sampling arguments of quantum-inspired work are doing, and decreases the gap between quantum and classical algorithms for machine learning problems. We begin by formally defining ridge regression and low-rank approximation, and the dynamic data structure model we focus on.\n\nProblem 1 (Ridge Regression). Given an n \u00d7 d matrix A, n \u00d7 d \u2032 matrix B and a ridge parameter \u03bb \u2265 0, the ridge regression problem is defined as follows:\nmin X\u2208R d\u00d7d \u2032 AX \u2212 B 2 F + \u03bb X 2 F ,\nwhere \u00b7 2 F denotes the sum-of-squares of entries.\n\nProblem 2 (Low-Rank Approximation). Given an n \u00d7 d matrix A and a rank parameter k \u2208 [d], the low-rank approximation problem is defined as follows:\nmin X\u2208R n\u00d7d :rank(X)=k A \u2212 X 2 F .\nDefinition 3 (Dynamic Data Structure Model). Given an n \u00d7 d matrix A, the dynamic data structure supports the following operations in O(log(nd)) time: (a) sample row A i, * with probability\nA i, * 2 2 / A 2 F , (b) sample entry j in row i with probability A 2 i,j / A i, * 2 2 and (c) output the (i, j)-th entry of A.\nWe note that in this input model, reading the entire matrix would be prohibitive and the algorithms can only access the matrix through the weighted sampling data structure.\n\nWe now describe our concrete results in more detail. At a high level, our algorithm for ridge regression, Algorithm 2, does the following: sample a subset of the rows of A via length-squared sampling, and take a length-squared sample of the columns of that subset. Then, solve a linear system on the resulting small submatrix using the conjugate gradient method. Our analysis of this algorithm results in the following theorem. (Some of the (standard) matrix notation used is given in Section 3.)\n\nThe residual error is bounded in the theorem using 1\n\u221a 2\u03bb U \u03bb,\u22a5 B F , where U \u03bb,\u22a5 denotes the bottom m S \u2212 p left singular vectors of a sketch SA of A,\nwhere p is such that \u03bb is between \u03c3 2 p+1 (SA) and \u03c3 2 p (SA). (Here we use p = rankA when \u03bb \u2264 \u03c3 2 rankA .) We could write this roughly as\nA \u2212p A + \u2212p B / A \u2212p ,\nwhere A \u2212p denotes A minus its best rank p approximation. It is the part of B we are \"giving up\" by including a ridge term. Proofs for this section are deferred to Appendix B.\n\nTheorem 4 (Dynamic Ridge Regression). Given an n \u00d7 d matrix A of rank k, an n \u00d7 d \u2032 matrix B, error parameter \u03b5 > 0, and ridge parameter \u03bb, let \u03ba 2 \u03bb = (\u03bb + \u03c3 2 1 (A))/(\u03bb + \u03c3 2 k (A)) be the ridge condition number of A, and let \u03c8 \u03bb = A 2 F /(\u03bb + \u03c3 2 k (A)). Further, let X * be the optimal ridge regression solution, i.e., X * = argmin X AX \u2212 B 2 F + \u03bb X 2 F . Then there is a data structure supporting turnstile updates of the entries of A in O(log(nd)) time, and an algorithm using that data structure that computes a sample SA of m = O \u03ba 2 \u03bb \u03c8 \u03bb log(nd)/\u03b5 2 rows of A, where S \u2208 R m\u00d7n is a sampling matrix, and outputsX \u2208 R m\u00d7d \u2032 , such that with probability 99/100,\nA \u22a4 S \u22a4X \u2212 X * F \u2264 \u03b5 (1 + 2\u03b3) X * F + \u03b5 \u221a \u03bb U k,\u22a5 B F ,\nwhere U \u03bb,\u22a5 B is the projection of B onto the subspace corresponding to the singular values of SA less than \u221a \u03bb; and \u03b3 2 =\nB 2 F AA + X * 2 F is a problem dependent parameter.\nFurther, the running time of the algorithm is\u00d5\nd \u2032 \u03b5 \u22124 \u03c8 2 \u03bb \u03ba 2 \u03bb log(d) . Finally, for all i \u2208 [d], and j \u2208 [d \u2032 ], an entry (A \u22a4 S \u22a4X ) i,j can be computed in O(m log(nd)) time.\nWe note that the \"numerical\" quantities \u03ba \u03bb and \u03c8 \u03bb are decreasing in \u03bb. When \u03bb is within a constant factor of A 2 , \u03c8 \u03bb is within a constant factor of the stable rank A 2 F / A 2 , where the stable rank is always at most rank(A). We also note that in the theorem, and the remainder of the paper, a row sampling matrix S has rows that are multiples of natural basis vectors, so that SA is a (weighted) sample of the rows of A. A column sampling matrix is defined similarly.\n\nConcurrent Work on Ridge Regression. In an independent and concurrent work, Gily\u00e9n, Song and Tang [GST20a] obtain a roughly comparable classical algorithm for regression, assuming access to the tree data structure, which runs in time\u00d5\nA 6 F A 2 2\nA + 8 2 \u03b5 4 , or in the notation above, O(\u03b5 \u22124 \u03c8 3 \u03bb \u03ba 2 ), for the special case of d \u2032 = 1. Their algorithm is based on Stochastic Gradient Descent.\n\nNext, we describe our results for low-rank approximation. We obtain a dynamic algorithm (Algorithm 3) for approximating A with a rank-k matrix, for a given k, and a data structure for sampling from it, in the vein of [Tan19]. At a high-level, as with our ridge regression algorithm, we first sample rows proportional to their squared Euclidean norm (length-squared sampling) and then sample a subset of columns resulting in a small submatrix with\u00d5(\u03b5 \u22122 k) rows and columns. We then compute the SVD of this matrix, and then work back up to A with more sampling and a QR factorization. The key component in our algorithm and analysis is using Projection-Cost Preserving sketches (see Definition 22). These enable us to preserve the Frobenius cost of projections onto all rank-k subspaces simultaneously. As a result, we obtain the following theorem:\n\nTheorem 5 (Sampling from a low-rank approximation). Given an n \u00d7 d matrix A for which a sampling data structure has been maintained, target rank k \u2208 [d] and error parameter \u03b5 > 0, we can find sampling matrices S and R, and rank-k matrix W , such that ARW SA\n\u2212 A F \u2264 (1 + O(\u03b5)) A \u2212 A k F . Further, the running time is\u00d5(\u03b5 \u22126 k 3 + \u03b5 \u22124 \u03c8 \u03bb (\u03c8 \u03bb + k 2 + k\u03c8 k )), where \u03c8 \u03bb is as in Theorem 4, and \u03c8 k = A 2 F \u03c3 k (A) 2 .Given j \u2208 [d], a random index i \u2208 [n] with probability distribution (ARW SA) 2 ij / ARW SA) * ,j 2 can be generated in expected time\u00d5(\u03c8 k +k 2 \u03b5 \u22122 \u03ba 2 ), where \u03ba = \u03c3 1 (A) \u00b7 \u03c3 rank(A) (A).\nHere if the assumption A k 2 F \u2265 \u03b5 A 2 F does not hold, the trivial solution 0 satisfies the relative error target and we assume the resulting approximation is not worth sampling:\nA \u2212 0 2 F \u2264 1 1 \u2212 \u03b5 ( A 2 F \u2212 A k 2 F ) = 1 1 \u2212 \u03b5 A \u2212 A k 2 F .\nThis result is directly comparable to Tang's algorithm [Tan19] for recommender systems which again needs query time that is a large polynomial in k, \u03ba and \u03b5 \u22121 . Our algorithm returns a relative error approximation, a rank-k approximation within 1 + \u03b5 of the best rank-k approximation; Tang's algorithm has additive error, with a bound more like A \u2212 A k F + \u03b5 A F . Finally, we note that \u03c8 k \u2264 \u03ba 2 and for several settings of k can be significantly smaller.\n\nFor ease of comparison we summarize our results in Table 1.\n\n\nRelated Work\n\nMatrix Sketching. The sketch and solve paradigm [CW15, Woo14] was designed to reduce the dimensionality of a problem, while maintaining enough structure such that a solution to the smaller problem remains an approximate solution the original one. This approach has been pivotal in speeding up basic linear algebra primitives such as least-squares regression [Sar06, RT08, CW15], \u2113 p regression [CP15, WW19], low-rank approximation [NN13, CMM17, LW20], linear and semi-definite programming [CLS19, JSWZ20, JKL + 20] and solving non-convex optimization problems such as \u2113 p low-rank approximation [SWZ17, SWZ19, BBB + 19] and training neural networks [BJW19, BPSW20]. For a comprehensive overview we refer the reader to the aforementioned papers and citations therein. Several applications use rank computation, finding a full rank subset of rows/columns, leverage score sampling, and computing subspace embeddings as key algorithmic primitives. Table 1: Comparison of our results and prior work. Let the target error be \u03b5, target rank be k and let \u03c8 k = A 2 F /\u03c3 k (A) 2 , where \u03c3 k is the k-th singular value of the input matrix. Also,\u03c3 k \u2264 1/ A + , \u03c3 1 \u2265 A , d \u2032 is the number of columns of B for multiple-response, and \u03b7 denotes some numerical properties of A. To avoid numerous parameters, we state our results by setting \u03bb = \u0398( A 2 2 ) in the corresponding theorems.\n\n\nProblem\n\nTime\nPrior Work Update Query Update Query Ridge Regression O(log(n))\u00d5 d \u2032 \u03ba 3 A 2 F log(d) \u03b5 4 A 2 2 O(log(n))\u00d5 k 6 A 6 F \u03ba 16 \u03b5 6 Thm. 4 [GLT18] O A 8 F \u03ba(A) 2 (\u03c3 6 min \u03b5 4 ) [GST20a] Low Rank Sampling O(log(n))\u00d5 \uf8eb \uf8ed A 2 F A 2 F A 2 2 +k 2 +k\u03c8 k \u03b5 4 A 2 2 + k 3 \u03b5 6 \uf8f6 \uf8f8 O(log(n)) \u2126(poly(\u03bak\u03b5 \u22121 \u03b7))\n\nThm. 5 [Tan19]\n\nSublinear Algorithms and Quantum Linear Algebra. Recently, there has been a flurry of work on sublinear time algorithms for structured linear algebra problems [MW17, SW19, BLWZ19, BCJ20] and quantum linear algebra [HHL09, GSLW19, LMR14, KP16, DW20]. The unifying goal of these works is to avoid reading the entire input to solve tasks such as linear system solving, regression and low-rank approximation. The work on sublinear algorithms assumes the input is drawn from special classes of matrices, such as positive semi-definite matrices [MW17, BCW19], distance matrices [BW18, IVWW19] and Toeplitz matrices [LLMM20], whereas the quantum algorithms (and their de-quantized analogues) assume access to data structures that admit efficient sampling [Tan19, GLT18, CGL + 20]. The work of Gily\u00e9n, Lloyd and Tang [GLT18] on low-rank least squares produces a data structure as output: given index i \u2208 [d] = {1, . . . , d}, the data structure returns entry x \u2032 i of x \u2032 \u2208 R d , which is an approximation to the solution x * of min\nx\u2208R d Ax \u2212 b , where b \u2208 R n .\nThe error bound is\n\nx \u2032 \u2212 x * \u2264 \u03b5 x * , for given \u03b5 > 0. This requires the condition that Ax * \u2212 b / Ax * is bounded above by a constant. Subsequent work [CGL + 20] removes this requirement, and both results obtain data structures that need space polynomial in rank(A), \u03b5, \u03ba(A), 1 and other parameters.\n\nThe work [Tan19] also produces a data structure, that supports sampling relevant to the setting of recommender systems: the nonzero entries of the input matrix A are a subset of the entries of a matrix P of, for example, user preferences. An entry A ij \u2208 [0, 1] is one if user j strongly prefers product i, and zero if user j definitely does not like product i. It is assumed that P is wellapproximated by a matrix of some small rank k. The goal is to estimate P using A; one way to make that estimate effective, without simply returning all entries of P , is to create a data structure so that given j, a random index i is returned, where i is returned with probability\u00e2 2 ij / \u00c2 * ,j 2 .\n\nHere\u00c2 * ,j is the j'th column of\u00c2 (and\u00e2 ij an entry), where\u00c2 is a good rank-k approximation to A, and therefore, under appropriate assumptions, to P . The estimate\u00c2 is regarded as a good\napproximation if \u00c2 \u2212 A F \u2264 (1 + \u03b5) A \u2212 [A] k F , where [A]\nk is the matrix of rank k closest to A in Frobenius norm. Here \u03b5 is a given error parameter. As shown in [Tan19], this condition (or indeed, a weaker one) implies that the described sampler is useful in the context of recommender systems.\n\n\nOutline\n\nThe next section gives some notation and mathematical preliminaries, in particular regarding leverage-score and length-squared sampling. This is followed by descriptions of our data structures and algorithms, and then by our computational experiments. The appendices give some extensive descriptions, proofs of theorems, and in Appendix D, some additional experiments.\n\n\nPreliminaries\n\nLet X + denote the Moore-Penrose pseudo-inverse of matrix X, equal to V \u03a3 \u22121 U \u22a4 when X has thin SVD X = U \u03a3V \u22a4 , so that \u03a3 is a square invertible matrix. We note that X + = (X \u22a4 X) + X \u22a4 = X \u22a4 (XX \u22a4 ) + and X + XX \u22a4 = X \u22a4 , which is provable using the SVDs of X and X + . Also, if X has full column rank, so that V is square, then X + is a left inverse of X, that is, X + X = I d , where d is the number of columns of X. Let X denote the spectral (operator) norm of X. Let \u03ba(X) = X + X denote the condition number of X. We write a \u00b1 b to denote the set {c | |c \u2212 a| \u2264 |b|}, and c = a \u00b1 b to denote the condition that c is in the set a \u00b1 b. Let [m] = {1, 2, . . . , m} for an integer m. As mentioned, nnz(A) is the number of nonzero entries of A, and we assume nnz(A) \u2265 n, which can be ensured by removing any rows of A that only contain zeros. We let [A] k or sometimes A k denote the best rank-k approximation to A. Let 0 a\u00d7b \u2208 R a\u00d7b have all entries equal to zero, and similarly 0 a \u2208 R a denotes the zero vector. Further, for an n \u00d7 d matrix A and a subset S of [n], we use the notation A |S to denote the restriction of the rows of A to the subset indexed by S. As mentioned, n \u03c9 is the time needed to multiply two n \u00d7 n matrices.\n\nLemma 6 (Oblivious Subspace Embedding Theorem 7.4 [CCKW22]). For given matrix A \u2208 R n\u00d7d with k = rank(A), there exists an oblivious sketching matrix S that samples m = O(\u03b5 \u22122 0 k log k) rows of A such that with probability at least 99/100, for all x \u2208 R d , S is an \u03b5 0 -subspace embedding, that is, SAx = (1\u00b1\u03b5 0 ) Ax . Further, the matrix SA can be computed in O(nnz(A)+k \u03c9 poly(log log(k))+ poly(1/\u03b5 0 )k 2+o(1) ) time.\n\nWe obtain the following data structure for leverage-score sampling. We provide a statement of its properties below, but defer the description of the algorithm and proof to the supplementary material. While leverage-score sampling is well-known, we give an algorithm for completeness; also, our algorithm removes a log factor in some terms in the runtime, due to our use of the sketch of Lemma 6.\n\nTheorem 7 (Leverage Score Data Structure). Let k = rank(A), and choose \u00b5 s \u2265 1. Then, Algorithm 5 (LevSample(A, \u00b5 s , v)) uses space O(n + k \u03c9 log log(nd)), not counting the space to store A, and runs in time O(\u00b5 s nnz(A) + k \u03c9 poly(log log(k)) + k 2+o(1) + vkn 1/\u00b5s ), and outputs a leverage score sketching matrix L, which samples v rows of A with probability proportional to their leverage scores. (It also outputs a column selector \u039b, selecting an orthogonal basis of the column space of A.)\nDefinition 8 (Ridge Leverage-score Sample, Statistical Dimension). Let A be such that k = rank(A), and suppose A has thin SVD A = U \u03a3V \u22a4 , implying \u03a3 \u2208 R k\u00d7k . For \u03bb > 0, let A (\u03bb) = A \u221a \u03bbV V \u22a4 and A (\u03bb) has SVD A (\u03bb) = U \u03a3D \u221a \u03bbV D D \u22121 V \u22a4 , where D = (\u03a3 2 + \u03bbI k ) \u22121/2 . Call S \u2282 [n] a ridge leverage-score sample of A if each i \u2208 S is chosen independently with probability at least U i, * \u03a3D 2 /sd \u03bb (A), where the statistical dimension sd \u03bb (A) = U \u03a3D 2 F = i\u2208[d] \u03c3 2 i /(\u03bb + \u03c3 2 i )\n, recalling that U \u03a3D comprises the top n rows of the left singular matrix of A (\u03bb) .\n\nWe can also use length-squared sampling to obtain subspace embeddings. In Section 4 we will give a data structure and algorithm that implements length-squared sampling. We defer the analysis to Appendix A.\n\nDefinition 9 (Length-squared sample). Let A \u2208 R n\u00d7d , \u03bb \u2265 0, and A (\u03bb) be as in Lemma 18. For given m, let matrix L \u2208 R m\u00d7n be chosen by picking each row of L to be e\n\u22a4 i / \u221a p i m, where e i \u2208 R n\nis the i'th standard basis vector, and picking i \u2208 [n] with probability p i \u2190 A i, * 2 / A 2 F . We obtain the corresponding lemma for length-squared sampling and defer the proof to Appendix A.\n\nLemma 10 (Length-squared sketch). Given a matrix A \u2208 R n\u00d7d and a sample size parameter\nv \u2208 [n], let m = O v A + (\u03bb) 2 A 2 F /sd \u03bb (A)\n. Then, with probability at least 99/100, the set of m length-squared samples contains a ridge leverage-score sample of A of size v.\n\n\nDynamic Data Structures for Ridge Regression\n\nIn this section, we describe our dynamic data structures, and then our algorithm for solving Ridge Regression problems. Given an input matrix A \u2208 R n\u00d7d , our data structure can be maintained under insertions and deletions (and changes) in O(log(nd)) time, such that sampling a row or column with probability proportional to its squared length can be done in O(log(nd)) time. The data structure is used for solving both ridge regression and LRA (Low-Rank Approximation) problems.\n\nFirst, we start with a simple folklore data structure.\n\nLemma 11. Given \u2113 real values {u i } i\u2208 [\u2113] , there is a data structure using storage O(\u2113), so that L = i\u2208[\u2113] u 2 i can be maintained, and such that a random i can be chosen with probability u 2 i /L in time O(log \u2113). Values can be inserted, deleted, or changed in the data structure in time O(log \u2113).\n\nThe implementation of this data structure is discussed in Appendix B. We use it in our data structure DynSamp(A), given below, which is used in LenSqSample, Alg. 1, to sample rows and columns of A.\n\nDefinition 12. DynSamp(A) is a data structure that, for A \u2208 R n\u00d7d , comprises:\n\n\u2022 For each row of A, the data structure of Lemma 11 for the nonzero entries of the row or column.\n\n\u2022 For the rows of A, the data structure of Lemma 11 for their lengths.\n\n\u2022 For given i, j, a data structure supporting access to the value of entry a ij of A in O(1) time.\n\nLemma 13. DynSamp(A) can be maintained under turnstile updates of A in O(log(nd)) time.\n\nUsing DynSamp(A), rows can be chosen at random with row i \u2208 [n] chosen with probability\nA i, * 2 / A 2 F in O(log(nd))\ntime. If S \u2208 R m\u00d7n is a sampling matrix, so that SA has rows that are each a multiple of a row of A, then c columns can be sampled from SA using DynSamp(A) in O((c + m) log(nd)) time, with the column j \u2208 [d] chosen with probability (SA) * ,j 2 / SA 2 F .\nAlgorithm 1 LenSqSample(DS, S = null, m S , m R ) Input: DS = DynSamp(A) (Def. 12) for A \u2208 R n\u00d7d , sample sizes m S , m R Output: Sampling matrices S \u2208 R m S \u00d7n , R \u2208 R d\u00d7m R 1: if S == null Use DS to build row sampler S \u2208 R m S \u00d7n of A 2: Use DS and S to build column sampler R d\u00d7m R of SA {cf. Lemma 13} 3: return S, R Algorithm 2 RidgeRegDyn(DS, B,\u03c3 k ,\u03c3 1 , \u03b5, \u03bb) Input: DS = DynSamp(A), B \u2208 R n\u00d7d \u2032 ,\u03c3 k \u2264 1/ A + ,\u03c3 1 \u2265 A , \u03b5 an error parameter, \u03bb a ridge weight Output: Data for approximate ridge regression solution A \u22a4 S \u22a4X where S is a sampling matrix 1: Z \u03bb \u2190 1/ \u03bb +\u03c3 2 k ,\u03ba \u2190 Z \u03bb \u03bb +\u03c3 2 1 2: Choose m S = O(\u03b5 \u22122\u03ba2 Z 2 \u03bb A 2 F log(d)), m R = O(m R Z 2 \u03bb A 2 F ), wherem R = O(\u03b5 \u22122 log m S ) 3: S, R \u2190 LenSqSample(DS, null, m S , m R ) {cf. Alg. 1;} 4:X \u2190 (SARR \u22a4 A \u22a4 S \u22a4 + \u03bbI mS ) \u22121 SB {Solve using conjugate gradient} 5: returnX, S {approximate ridge regression solution is A \u22a4 S \u22a4X }\nWe designate the algorithm of Lemma 10 as LenSqSample, as given at a high level in Algorithm 1, and in more detail in the proof of Lemma 13 in Appendix B.\n\nThis simple data structure and sampling scheme will be used to solve ridge regression problems, via Algorithm 2. Its analysis, which proves Theorem 4, is given in Appendix B.\n\n\nSampling from a Low-Rank Approximation\n\nOur algorithm for low-rank approximation is BuildLowRankFactors, Algorithm 3, given below. As discussed in the introduction, it uses LenSqSample, Algorithm 1, to reduce to a matrix whose size is independent of the input size, beyond log factors, as well as Projection-Cost Preserving sketches, QR factorization, and leverage-score sampling. Its analysis, proving Theorem 5, is given in Appendix C.\n\n\nExperiments\n\nWe evaluate the empirical performance of our algorithm on both synthetic and real-world datasets. All of our experiments were done in Python and conducted on a laptop with a 1.90GHz CPU and 16GB RAM. Prior work [ADBL20] suggests the tree data structure is only faster than the built-in sampling function when the matrix size max{n, d} is larger than 10 6 . Hence we follow the implementation in [ADBL20] that directly uses the built-in function. For a fair comparison, we also modified the code in [ADBL20], which reduces the time to maintain the data structure by roughly 30x. For each experiment, we took an average over 10 independent trials.\n\nWe note that we do not compare with classical sketching algorithms for several reasons. First,\nAlgorithm 3 BuildLowRankFactors (DynSampler, k,\u03c3 k ,\u03c3 k , \u03b5, \u03c4 ) Input: DynSampler = DynSamp(A) (Def. 12) for A \u2208 R n\u00d7d , k target rank,\u03c3 k \u2264 1/ A + ,\u03c3 k \u2264 \u03c3 k (A), \u03b5 an error parameter, \u03c4 estimate of A \u2212 A k 2 F ,\nwhere A k is the best rank-k approximation to A Output: Small matrix W and sampling matrices S, R so that rank(ARW SA) = k and\nARW SA \u2212 A \u2264 (1 + \u03b5) A \u2212 A k 1: \u03bb \u2190 \u03c4 /k, Z \u03bb \u2190 1/ \u03bb +\u03c3 2 k , Z k \u2190 1/\u03c3 k 2: Choose m R = m S = O(m S Z 2 \u03bb A 2 F ), wherem S = O(\u03b5 \u22122 log k) 3: S, R 1 \u2190 LenSqSample(DynSampler, null, m S , m R ) 4: Apply Alg. 1 and Thm. 1 of [CMM17] to SAR 1 , get col. sampler R 2 { m R2 = O(\u03b5 \u22122 k log k)} 5: Apply Alg. 1 and Thm. 1 of [CMM17] to SAR 1 R 2 , get row sampler S 2 { m S2 = O(\u03b5 \u22122 k log k)} 6: V \u2190 top-k right singular matrix of S 2 SAR 1 R 2 7: U, \u2190 QR(SAR 1 R 2 V ) {U has orthonormal cols, SAR 1 R 2 V = U C for matrix C} 8: Choose m R3 = O(m R3 \u03b5 \u22121 Z 2 k A 2 F ), wherem R3 = O(\u03b5 \u22122 0 log k + \u03b5 \u22121 ), \u03b5 0 a small constant 9: R 3 \u2190 LenSqSample(DynSampler, S, m S , m R3 ) 10: Let f (k, C) be the function returning the value m R4 = O(\u03b5 \u22122 0 k log k + \u03b5 \u22121 k) 11: R \u22a4 4 , \u2190 LevSample((U \u22a4 SAR 3 ) \u22a4 , log(m R3 ), f ()) {Alg. 5} 12: R \u2190 R 3 R 4 13: W \u2190 (U \u22a4 SAR) + U \u22a4 14: return W , S, R\nthere is no classical contender with the same functionality as ours. This is because our dynamic algorithms support operations not seen elsewhere: sublinear work for regression and low-rank approximation, using simple fast data structures that allow, as special cases, row-wise or columnwise updates. Second, unlike dynamic algorithms where a sketch is maintained, our algorithms are not vulnerable to updates based on prior outputs, whether adversarially, or due to use in the inner loop of an optimization problem. This is because our algorithms are based on independent sampling from the exact input matrix.\n\n\nLow-Rank Approximation\n\nWe conduct experiments on the following datasets:\n\n\u2022 KOS data. 2 A word frequency dataset. The matrix represents word frequencies in blogs and has dimensions 3430 \u00d7 6906 with 353160 non-zero entries.\n\n\u2022 MovieLens 100K.\n\n[HK16] A movie ratings dataset, which consists of a preference matrix with 100,000 ratings from 611 users across 9,724 movies. We compare our algorithms with the implementations in [ADBL20], which are based on the algorithms in [FKV04] and [Tan19]. We refer to this algorithm as ADBL henceforth. For the KOS dataset, we set the number of sampled rows and columns to be (r, c) = (500, 700) for both algorithms. For the MovieLens dataset we set (r, c) = (300, 500). We define the error \u03b5 = A \u2212 \nY F / A \u2212 A k F \u2212 1,\nwhere Y is the algorithm's output and A k is the best k-rank approximation.\n\nSince the regime of interest is k \u226a n, we vary k among {10, 15, 20}.\n\nThe results are shown in Table 2. We first report the total runtime, which includes the time to maintain the data structure and then compute the low-rank approximation. We also report the query time, which excludes the time to maintain the data structure. From the table we see that both algorithms can achieve \u03b5 \u2248 0.05 in all cases. The query time of ours is about 6x-faster than the ADBL algorithm in [ADBL20], and even for the total time, our algorithm is much faster than the SVD. Although the accuracy of ours is slightly worse, in Appendix D.1 we show by increasing the sample size slightly, our algorithm achieves the same accuracy as ADBL [ADBL20], but still has a faster runtime.\n\nWe remark that the reason our algorithm only needs half of the time to compute the sampling probabilities is that we only need to sample rows or columns according to their squared length, but the algorithm in [ADBL20] also needs to sample entries for each sampled row according to the squared values of the entries.\n\n\nRidge Regression\n\nIn this section, we consider the problem\nX * := min X\u2208R d\u00d7d \u2032 AX \u2212 B 2 F + \u03bb X 2 F , where A \u2208 R n\u00d7d , B \u2208 R n\u00d7d \u2032 .\nWe do experiments on the following dataset with \u03bb = 1:\n\n\u2022 Synthetic data. We generate the rank-k matrix A as [ADBL20] do. Particularly, suppose the SVD of A is A = U \u03a3V \u22a4 . We first sample an n \u00d7 k Gaussian matrix, then we perform a QR-decomposition G = QR, where Q is an n \u00d7 k orthogonal matrix. We then simply set U = Q and then use a similar way to generate V . We set A \u2208 R 7000\u00d79000 , B \u2208 R 7000\u00d71 .\n\n\u2022 YearPrediction. 3 A dataset that collects 515345 songs and each song has 90 attributes. The task here is to predict the release year of the song. A \u2208 R 515345\u00d790 , B \u2208 R 515345\u00d71 . \u2022 PEMS data. 4 The data describes the occupancy rate of different car lanes of San Francisco bay area freeways. Each row is the time series for a single day. The task on this dataset 3 YearPredictionMSD Data Set 4 PEMS-SF Data Set is to classify each observed day as the correct day of the week, from Monday to Sunday.\nA \u2208 R 440\u00d7138672 , B \u2208 R 440\u00d71 .\nWe define the error \u03b5 = X \u2212 X * F / X * F , given the algorithm output X. For synthetic data, we set the number of sampled rows and columns to be r and c. For the YearPrediction data, the number of columns is small, and hence we only do row sampling, and likewise, for the PEMS data, we only do column sampling. We did not find an implementation for the ridge regression problem in the previous related work. Therefore, here we list the time to compute the closed-form optimal solution X * =\n(A \u22a4 A + \u03bbI) \u22121 A \u22a4 B or X * = A \u22a4 (AA \u22a4 + \u03bbI) \u22121 B, as a reference.\nThe results are shown in Table 3 and 4. From the tables we can see that for synthetic data, the algorithm can achieve an error \u03b5 < 0.1 when only sampling less than 10% of the rows and columns. Also, the total runtime is about 40x-faster than computing the exact solution. For the YearPrediction and PEMS data, the bottleneck of the algorithm becomes the time to compute the sample probabilities, but the query time is still very fast and we can achieve an error \u03b5 < 0.1 when only sampling a small fraction of the rows or columns. \n\n\nA Preliminaries\n\nIn, this section, we provide proofs for our theorems in Section 3. We first provide the description of the leverage score sampling data structure (Algorithm 5). We note that the advantage of the current version compared to the standard leverage score sampling (see, e.g., Section 2.4 in the survey in [Woo14]) is that it saves an O(log n) factor. We need the following data structure.\n\nDefinition 14 (Sampling Data Structure). Given a matrix A \u2208 R n\u00d7d , a column selection matrix \u039b such that k = rank(A\u039b) = rank(A), and \u03bb s \u2265 1, the data structure Samp(A, \u039b, \u03bb s ) consists of the following:\n\n\u2022 SA, where S \u2208 R m S \u00d7n is a sketching matrix as in Lemma 6, with m S = O(k log(k)/\u03b5 2 0 ), chosen to be an \u03b5 0 -embedding with failure probability 1/100, for fixed \u03b5 0 ;\n\n\u2022 C, where [Q, C] \u2190 QR(SA\u039b), the QR decomposition of SA\u039b, i.e., SA\u039b = QC, Q has orthonormal columns and C is triangular and invertible (since A\u039b has full column rank);\n\u2022 C 0 , where [Q 0 , C 0 ] \u2190 QR(SA);\n\u2022 The data structure of Lemma 11, built to enable sampling i \u2208 [n] with probability p i \u2190\nZ i, * 2 / Z 2 F in O(log n) time, where Z \u2190 A\u039b(C \u22121 G), with G \u2208 R k\u00d7m G having independent N (0, 1/m G ) entries, and m G = \u0398(\u03bb s ).\nLemma 15 (Sampling Data structure). The data structure Samp(A, \u039b, \u03bb s ), from Definition 14, can be constructed in O(\u03bb s (nnz(A) + k 2 ) + d \u03c9 ) time.\n\n\nAlgorithm 4 MatVecSampler(A, Sampler, W, v, \u03bd)\n\nInput: A \u2208 R n\u00d7d , data structure Sampler (Def. 14), W \u2208 R d\u00d7mW , desired number of samples v, normalizer \u03bd, where \u03bd = 1 6kn 1/\u03bbs by default if unspecified Output: L \u2208 R v\u00d7d , encoding v draws from i \u2208 [n] chosen with approx. probability q i\ndef = A i, * W 2 / AW 2 F 1: N \u2190 C 0 W 2 F , where C 0 is from Sampler 2: if N == 0: 3:\nreturn Uniform(v, n) {Alternatively, raise an exception here} 4: L \u2190 0 v\u00d7n , z \u2190 0 5: while z < v:\n\n\n6:\n\nChoose i \u2208 [n] with probability p i using Sampler 7:q i \u2190 A i, * W 2 /N 8:\n\nWith probability \u03bdq i pi , accept i: set L z,i = 1/ \u221a vq i ; z \u2190 z + 1 9: return L Proof. The time needed to compute SA is O(nnz(A)+ k \u03c9 poly(log log(k))+ k 2+o(1) \u03b5 \u22122 0 ). Computing the QR factorization of SA takes O(d \u03c9 ) time, by first computing (SA) \u22a4 (SA) for the m S \u00d7 d matrix SA, and then its Cholesky composition, using \"fast matrix\" methods for both, and using m S \u2264 d. This dominates the time for the similar factorization of SA\u039b.\n\nThe Z matrix can be computed in O(\u03bb s (nnz(A) + k 2 )) time, by appropriate order of multiplication, and this dominates the time needed for building the data structure of Lemma 11. Adding these terms, and using m \u2264 nnz(A), the result follows.\n\nLemma 16 (MatVecSampler Analysis). Given constant c 0 > 1 and small enough constant \u03b5 0 > 0, and Sampler for A, there is an event E holding with failure probability at most 1/k c 0 , so that if E holds, then when called with \u03bd \u2190 1 6kn 1/\u03bbs , the probability is (1 \u00b1 \u03b5 0 )q i that the accepted index in Step 8 of\nMatVecSampler is i \u2208 [n], where q i def = A i, * W 2 / AW 2 F . The time taken is O(m W d(d + vkn 1/\u03bbs ), where k = rank(A).\nProof. We need to verify that the quantity in question is a probability, that is, that \u03bdq i p i \u2208 (0, 1), when \u03bd = 1 6kn 1/\u03bbs . From Lemma 6, if m S = O(\u03b5 \u22122 0 k) for \u03b5 0 > 0, then with failure probability 1/k c 0 +1 , S will be a subspace \u03b5 0 -embedding for im(A), that is, for A\u039b and for A, using rank(A) = k. The event E includes the condition that S is indeed an \u03b5 0 -embedding. If this holds for S, then from standard arguments, A\u039bC \u22121 has singular values all in 1 \u00b1 \u03b5 0 , and A i,\n* \u039bC \u22121 2 = (1 \u00b1 O(\u03b5 0 ))\u03c4 i , where again \u03c4 i is the i'th leverage score. (We have A\u039bC \u22121 x = (1 \u00b1 \u03b5 0 ) SA\u039bC \u22121 x = (1 \u00b1 \u03b5 0 )\nx , for all x, since SA = QC.) This implies that for Z, G in the construction of Samp(A, \u039b, \u03bb s ),\nZ 2 F = A\u039bC \u22121 G 2 F \u2264 (1 + \u03b5 0 ) G 2 F .\nSince m G G 2 F is \u03c7 2 with km G degrees of freedom, with failure probability at most exp(\u2212 \u221a k\u03bb s /2) (using m G = \u0398(\u03bb s ), it is at most 3km G ([LM00], Lemma 1), so G 2 F \u2264 3k with that probability. Our event E also includes the condition that this bound holds. Thus under this condition, Z 2 F \u2264 3(1 + \u03b5 0 )k.\n\nFrom Lemma 21 and the above characterization of \u03c4 i , for the Z of Samp(A, \u039b, \u03bb s ), Z i, * 2 =\nA i, * \u039bC \u22121 G 2 \u2265 (1 \u2212 O(\u03b5 0 ))\u03c4 i /n 1/\u03bbs .\nPutting these together, we have\np i = Z i, * 2 F Z 2 \u2265 (1 \u2212 O(\u03b5 0 )) \u03c4 i /n 1/\u03bbs 3k .\n(1)\n\nUsing the \u03b5 0 -embedding property of S,\nC 0 W 2 F = Q 0 C 0 W 2 F = SAW 2 F = (1 \u00b1 2\u03b5 0 ) AW 2 F ,(2)\nand so, letting A = U C 1 for U with orthonormal columns, we have, for small enough \u03b5 0 ,\n(1 \u2212 2\u03b5 0 )q i \u2264 A i, * W 2 AW 2 F = U i, * C 1 W 2 U C 1 W 2 F = U i, * C 1 W 2 C 1 W 2 F \u2264 U i, * 2 C 1 W 2 C 1 W 2 F \u2264 \u03c4 i .\nPutting this bound with (1) we hav\u1ebd\nq i p i \u2264 \u03c4 i /(1 \u2212 2\u03b5 0 ) (1 \u2212 O(\u03b5 0 ))\u03c4 i /n 1/\u03bbs 3(1 + \u03b5 0 )k \u2264 3(1 + O(\u03b5 0 ))kn 1/\u03bbs . so that \u03bdq i p i = 1 6kn 1/\u03bbsq i p i \u2264 (1+ O(\u03b5 0 ))/2 \u2264 1 for small enough \u03b5 0 . Using (2) we haveq i = (1\u00b1 2\u03b5 0 )q i .\nThus the correctness condition of the lemma follows, for small enough \u03b5 0 .\n\nTurning to time: the time to compute C 0 W is O(d 2 m W ). Each iteration takes O(log n + dm W ), for choosing i and computingq i , and these steps dominate the time. As usual for rejection sampling, the expected number of iterations is O(vkn 1/\u03bbs ). Adding these expressions yields the expected time bound, folding a factor of log n in by adjusting \u03bb s slightly.\n\nAlgorithm 5 LevSample(A, \u00b5 s , f ()) Input: A \u2208 R n\u00d7d , \u00b5 s \u2265 1 specifying runtime tradeoff, function f (\u00b7) \u2192 Z + returns a target sample size (may be just a constant) Output: Leverage score sketching matrix L, column selector \u039b 1: Run an algorithm to compute k = rank(A) and obtain \u039b \u2208 R d\u00d7k , a subset of k lin. indep. columns of A {for example Theorem 1.5 in [CCKW22]} 2: Construct Sampler \u2190 Samp(A\u039b, I, \u03bb s ), use C from it; {Definition 14} 3:\nW \u2190 C \u22121 G \u2032 , where G \u2032 \u2208 R k\u00d7m G \u2032 with ind. N (0, 1/m G \u2032 ) entries {m G \u2032 = \u0398(log n)} 4: L \u2190 MatVecSampler(A\u039b, Sampler, W, f (k, C), \u03bd = 1/6n 1/\u03bbs )\n{Algorithm 4, sample size f (k, C), normalizer \u03bd} 5: return L, \u039b Theorem 17 (Leverage Score Data Structure, Theorem 7 restated). Let k = rank(A), and choose \u00b5 s \u2265 1. Algorithm 5 (LevSample(A, \u00b5 s , f (\u00b7))) uses space O(n + k \u03c9 log log(nd)), not counting the space to store A, and runs in time O(\u00b5 s nnz(A) + k \u03c9 poly(log log(k)) + k 2+o(1) + vkn 1/\u00b5s ),\n\nwhere v is the sample size. For v = \u03b5 \u22122 k log k, this bound can be expressed as O(\u00b5 e nnz(A) + k \u03c9 poly(log log(k)) + \u03b5 \u22122\u22121/\u00b5e k 2 ) time, for \u00b5 e \u2265 1. Note: we can get a slightly smaller running time by including more rounds of rejection sampling: the first round of sampling needs an estimate with failure probability totaled over for all n rows, while another round would only need such a bound for vn 1/\u03bbs rows; this would make the bound v 1+1/\u03bbs kn 1/\u03bb 2 s , which would be smaller when v \u226a n. However, in the latter case, the term vkn 1/\u03bbs is dominated by the other terms anyway, for relevant values of the parameters. For example if v \u2264 n and vk \u2264 nnz(A) does not hold, then sampling is not likely to be helpful. Iterating log log n times, a bound with leading term O(nnz(A)(log log n + log v)) is possible, but does not seem interesting.\n\n\nProof.\n\nStep 2, building Samp(A\u039b, \u03bb s ), take O(\u03bb s (nnz(A) + k 2 ) + k \u03c9 ) time, with d in Lemma 15 equal to k here.\n\nFrom Lemma 16, the running time of MatVecSampler is O(k 2 log n + vk 2 (log n)n 1/\u03bbs ), mapping d of the lemma to k, m W to m G \u2032 = O(log n). However, since the normalizer \u03bd is a factor of k smaller than assumed in Lemma 16, the runtime in sampling is better by that factor. Also, we subsume the second log n factor by adjusting \u03bb s .\n\nThe cost of computing C \u22121 G \u2032 is O(k 2 log n); we have a runtime of O(nnz(A) + k \u03c9 poly(log log(k)) + k 2+o (1) ) + O(\u03bb s (nnz(A) + k 2 ) + k \u03c9 ) + O(k 2 log n + vkn 1/\u03bbs ) = O(\u03bb s nnz(A) + k \u03c9 poly(log log(k)) + k 2+o(1) + vkn 1/\u03bbs ), as claimed. Finally, suppose v = \u03b5 \u22122 k log k, as suffices for an \u03b5-embedding. If vkn 1/\u03bbs \u2264 nnz(A) + k \u03c9 , then the bound follows. Suppose not. If n \u2265 k \u03c9 , then \u03b5 \u22122 \u2265 n 1\u22121/\u03bbs /k 2 log(k) \u2265 n 1\u22121/\u03bbs\u22122/\u03c9 / log(n) and so \u03b5 \u22122 \u2265 n \u03b3 , for constant \u03b3 > 0, implying \u03b5 \u22122/\u03bbs\u03b3 \u2032 \u2265 n 1/\u03bbs log n, for constant \u03b3 \u2032 < \u03b3. When k \u03c9 \u2265 n, \u03b5 \u22122 \u2265 k \u03c9\u22122\u22121/\u03c9\u03bbs / log(k) \u2265 k \u03b3 , for a constant \u03b3 > 0, so that \u03b5 \u2212\u03c9/\u03bbs\u03b3 \u2032 \u2265 n 1/\u03bbs log k, for a constant \u03b3 \u2032 < \u03b3. Using \u03bb e , a constant multiple of \u03bb s , to account for constants, the result follows.\n\nWe can also use length-squared sampling to obtain subspace embeddings. In Section 4 we have given a data structure and algorithm that implements length-squared sampling. To analyze lengthsquared sampling in the context of ridge regression, we show the following structural observations about ridge regression.\n\n\nLemma 18 (Block SVD). Let A be such that k = rank(A), and suppose A has thin SVD\nA = U \u03a3V \u22a4 , implying \u03a3 \u2208 R k\u00d7k . For \u03bb > 0, let A (\u03bb) = A \u221a \u03bbV V \u22a4 . For b \u2208 R n , letb = b 0 d .\nThen for all x \u2208 im(V ), the ridge regression loss\nAx \u2212 b 2 + \u03bb x 2 = A (\u03bb) x \u2212b 2 ,\nand ridge regression optimum\nx * = argmin x\u2208R d Ax \u2212 b 2 + \u03bb x 2 = argmin x\u2208R d A (\u03bb) x \u2212b 2 .\nThe matrix A (\u03bb) has SVD A (\u03bb) = U \u03a3D \u221a \u03bbV D D \u22121 V \u22a4 , where D = (\u03a3 2 + \u03bbI k ) \u22121/2 , and A + (\u03bb)\n2 = 1/(\u03bb + 1/ A + 2 ). We have A i, * 2 A + (\u03bb) 2 \u2265 U i, * \u03a3D 2 for i \u2208 [n].\nProof. Since x \u2208 im(V ) has x = V z for some z \u2208 R k , and since V \u22a4 V = I k , it follows that V V \u22a4 x = V z = x, and so\nA (\u03bb) x \u2212b 2 = Ax \u2212 b 2 + \u221a \u03bbV V \u22a4 x \u2212 0 2 = Ax \u2212 b 2 + \u03bb x 2 , as claimed. The SVD of A (\u03bb) is A (\u03bb) = U \u03a3D \u221a \u03bbV D D \u22121 V \u22a4 ,\nwhere D is defined as in the lemma statement, since the equality holds, and both U \u03a3D \u221a \u03bbV D and V have orthonormal columns, and D \u22121 has nonincreasing nonnegative entries. Therefore\nA + (\u03bb) = V D U \u03a3D \u221a \u03bbV D \u22a4 . We have A + (\u03bb)b = V D 2 \u03a3U \u22a4 b = V \u03a3D 2 U \u22a4 b,(3)\nusing that \u03a3 and D are diagonal matrices. By the well-known expression x * = A \u22a4 (AA \u22a4 +\u03bbI n ) \u22121 b, and using the not-thin SVD A =\u00db\u03a3V \u22a4 , with\u03a3 \u2208 R n\u00d7d and\u00db andV orthogonal matrices,\nx * =V\u03a3\u00db \u22a4 (\u00db\u03a3\u03a3 \u22a4\u00db \u22a4 + \u03bb\u00db\u00db \u22a4 ) \u22121 b =V\u03a3\u00db \u22a4\u00db (\u03a3\u03a3 \u22a4 + \u03bbI n ) \u22121\u00db \u22a4 b =V\u03a3(\u03a3\u03a3 \u22a4 + \u03bbI n ) \u22121\u00db \u22a4 b = V \u03a3(\u03a3 2 + \u03bbI k ) \u22121 U \u22a4 b = V \u03a3D 2 U \u22a4 b,\nwhere the next-to-last step uses that\u03a3 is zero except for the top k diagonal entries of\u03a3. Comparing (3) and (4), we have A + (\u03bb)b = x * . Using the expression for A + (\u03bb) , A + (\u03bb) 2 = D 2 1,1 = 1/(\u03bb + 1/ A + 2 ). Finally, since (A (\u03bb) ) i, * = A i, * for i \u2208 [n], and letting\u00db = U \u03a3D \u221a \u03bbV D ,\nA i, * 2 A + (\u03bb) 2 = (A (\u03bb) ) i, * 2 A + (\u03bb) 2 \u2265 (A (\u03bb) ) i, * A + (\u03bb) 2 = \u00db i, * D \u22121 V \u22a4 V D\u00db \u22a4 2 = \u00db i, * 2 = U i, * \u03a3D 2 ,\nas claimed.\n\nDefinition 19 (Ridge Leverage-score Sample, Statistical Dimension). Let A, \u03bb, A (\u03bb) , and D be as in Lemma 18. Call S \u2282 [n] a ridge leverage-score sample of A if each i \u2208 S is chosen independently with probability at least U i, * \u03a3D 2 /sd \u03bb (A), where the statistical dimension sd \u03bb (A) = U \u03a3D 2 F = i\u2208[d] \u03c3 2 i /(\u03bb + \u03c3 2 i ), recalling that U \u03a3D comprises the top n rows of the left singular matrix of A (\u03bb) .\n\nLemma 20 (Length-squared sketch, Lemma 10 restated). Given a matrix A \u2208 R n\u00d7d and a sample size parameter v \u2208 [n], let m = O v A + (\u03bb) 2 A 2 F /sd \u03bb (A) . Then, with probability at least 99/100, the set of m length-squared samples contains a ridge leverage-score sample of A of size v.\n\nNote that when \u03bb = 0, A + (\u03bb) = A + , sd 0 (A) = rank(A), and the ridge leverage-score samples are leverage-score samples.\n\nProof. We will show thatL contains within it a leverage-score sketching matrix; since oversampling does no harm, this implies the result using the above lemma.\n\nUsing the thin SVD A = U \u03a3V \u22a4 , and A + = V \u03a3 + U \u22a4 , we have\nA i, * A + \u2265 A i, * A + = U i, * \u03a3V \u22a4 V \u03a3 + U \u22a4 = U i, * ,\nThe expected number of times index i \u2208 [n] is chosen among mL length-squared samples, p i mL, is within a constant factor of\nA i, * 2 A 2 F v A + (\u03bb) 2 A 2 F /sd \u03bb (A) \u2265 U i, * 2\nsd \u03bb (A) v, using Lemma 18, an expectation at least as large as for a ridge leverage-score sample of size v.\n\nFinally, recall the Johnson-Lindenstraus Lemma, for sketching with a dense Gaussian matrix.\n\nLemma 21 (Johnson-Lindenstraus Lemma). For given \u03b5 > 0, if P \u2282 R c is a set of m \u2265 c vectors, and G \u2208 R m\u00d7c has entries that are independent Gaussians with mean zero and variance 1/m, then there is m = O(\u03b5 \u22122 log(m/\u03b4)) such that with failure probability \u03b4, Gx = (1 \u00b1 \u03b5) x for all x \u2208 P . Moreover, there is m G = O(\u00b5) so that Gx \u2265 x /n 1/\u00b5 , with failure probability at most 1/n 2 .\n\nDefinition 22 (Projection-Cost Preserving Sketch). Given a matrix A \u2208 R n\u00d7d , \u03b5 > 0 and an integer k \u2208 [d], a sketch SA \u2208 R s\u00d7d is an (\u03b5, k)-column projection-cost preserving sketch of A if for all rank-k projection matrices P ,\n(1 \u2212 \u03b5) A(I \u2212 P ) 2 F \u2264 SA(I \u2212 P ) 2 F \u2264 (1 + \u03b5) A(I \u2212 P ) 2 F .\nThere are several constructions of projection-cost preserving sketches known in the literature, starting with the work of Cohen et. al. [CEM + 15,CMM17]. For our purposes, it suffices to use Theorem 1 from [CMM17].\n\nWe can use the following lemma to translate from prediction error to solution error for regression problems.\nLemma 23. Let \u03b3 A,b = b AA + b . Recall that \u03ba(A) = A A + . Supposex \u2208 im(A \u22a4 ), and for some \u03b5 p \u2208 (0, 1), Ax \u2212 b 2 \u2264 (1 + \u03b5 p ) \u03be * 2 holds, where \u03be * = Ax * \u2212 b. Then x \u2212 x * \u2264 2 \u221a \u03b5 p A + \u03be * \u2264 2 \u221a \u03b5 p \u03b3 2 A,b \u2212 1\u03ba(A) x * .(4)\nThis extends to multiple response regression using \u03b3 2\nA,B def = B 2 F AA + B 2 F\n, by applying column by column to B, and extends to ridge regression, that is,\nA (\u03bb) withB = B 0 d\u00d7d \u2032 , as well. Note that x \u2208 im(A \u22a4 ) = im(V ) is no loss of generality, because the projection V V \u22a4 x of x onto im(A \u22a4 ) has AV V \u22a4 x = Ax and V V \u22a4 x \u2264 x . So argmin x Ax \u2212 b 2 + \u03bb x must be in im(A \u22a4 )\nfor \u03bb arbitrarily close to zero, and A + b \u2208 im(A \u22a4 ).\n\nFor the ridge problem min x A (\u03bb) x \u2212b , we have \u03be * 2 = A (\u03bb) x * \u2212b = Ax * \u2212 b 2 + \u03bb x * 2 , and recalling from Lemma 18 that, when A has SVD A = U \u03a3V \u22a4 , A (\u03bb) has singular value matrix D \u22121 , where D = (\u03a3 2 + \u03bbI k ) \u22121/2 , so that \u03ba(A (\u03bb) ) 2 = (\u03bb + \u03c3 2 1 )/(\u03bb + \u03c3 2 k ), where A has singular values \u03c3 1 , . . . , \u03c3 k , with k = rank(A).\nProof. Since x * = A + b = A \u22a4 (AA \u22a4 ) + b \u2208 im A \u22a4 , we havex \u2212 x * = A \u22a4 z \u2208 im A \u22a4 , for some z. Since A + AA \u22a4 = A \u22a4 , we havex \u2212 x * = A \u22a4 z = A + AA \u22a4 z = A + A(x \u2212 x * )\n. From the normal equations for regression and the Pythagorean theorem,\nA(x \u2212 x * ) 2 = Ax \u2212 b 2 \u2212 Ax * \u2212 b 2 \u2264 4\u03b5 p \u03be * 2 ,\nusing Ax \u2212 b \u2264 (1 + \u03b5 p ) \u03be * and \u03b5 p < 1. Therefore, using also submultiplicativity of the spectral norm,\nx \u2212 x * 2 = A + A(x \u2212 x * ) 2 \u2264 A + 2 A(x \u2212 x * ) 2 \u2264 A + 2 4\u03b5 p \u03be * 2 ,(5)\nand the first inequality of (4) follows. For the second, we bound\n\u03be * 2 x * 2 = b 2 \u2212 AA + b 2 A + b 2 = (\u03b3 2 A,b \u2212 1) AA + b 2 A + b 2 \u2264 (\u03b3 2 A,b \u2212 1) A 2 so from (5), we have x \u2212 x * 2 \u2264 A + 2 4\u03b5 p \u03be * 2 \u2264 A + 2 4\u03b5 p x * 2 (\u03b3 2 A,b \u2212 1) A 2 ,\nand the second inequality of (4) follows, using the definition of \u03ba(A).\n\n\nB Dynamic Data Structures for Ridge Regression\n\nThe data structure of Lemma 11 is simply a complete binary tree with \u2113 leaves, each leaf with weight u i , and each internal node with weight equal to the sum of the weights of its children. Sampling is done by walking down from the root, choosing left or right children with probability proportional to its weight. Insertion and deletion are done by inserting or deleting the leaf z that preserves the complete binary tree property, and updating the weights of its ancestors; in the case of deletion, first the leaf weight to be deleted is swapped with that of z, updating weights of ancestors. We also refer the reader to a more detailed description in [Tan19, GST20b].\n\nProof of Lemma 13. Use Lemma 11 for the first part. For the second, with a matrix S, construct the data structure of Lemma 11 for the row lengths of SA, in O(m log(nd)) time. To sample, pick i * \u2208 [m] with probability (SA) i, * 2 / SA 2 F , using the newly constructed data structure. Then pick j \u2208 [d] with probability (SA) 2 i * j / (SA) i * , * 2 . Adding the probabilities across the choices of i * , the probability of choosing index j is (SA) * ,j 2 / SA 2 F , as claimed. Once a column is chosen, the time to determine the corresponding column length (SA) * ,j is O(m log(nd)), finding each (SA) ij for i \u2208 [m] in O(log(nd)) time.\n\nWe first re-state our theorem for dynamic ridge regression, before giving its proof.\n\nTheorem 24 (Theorem 4 restated). Given matrices A \u2208 R n\u00d7d , B \u2208 R n\u00d7d \u2032 and \u03bb > 0, let X * = argmin X AX \u2212 B 2 F + \u03bb X 2 F . Let \u03c8 \u03bb = A 2 F /(\u03bb + \u03c3 2 k ), \u03ba \u03bb = (\u03bb + \u03c3 1 (A) 2 )/(\u03bb + \u03c3 k (A) 2 ) and \u03ba = (\u03bb +\u03c3 2 1 )/(\u03bb +\u03c3 2 k ), where\u03c3 1 and\u03c3 k are over and under estimates of \u03c3 1 and \u03c3 k respectively. Then, there exists a data structure that maintains Y \u2208 R d\u00d7d \u2032 such that with probability at least 99/100,\nY \u2212 X * F \u2264 (\u03b5 + 2\u03b3\u03b5) X * F + \u03b5 \u221a \u03bb U k,\u22a5 B F , where \u03b3 2 = B 2 F AA + X * 2 F\n. Further, an entry Y ij for given i, j can be computed in O(m S log(nd)) = O(\u03b5 \u22122\u03ba2 \u03c8 \u03bb (log(nd)) 2 ) time. The time taken to compute Y is\u00d5(d \u2032 \u03b5 \u22124\u03ba2 \u03c8 2 \u03bb \u03ba \u03bb log(d)).\n\nProof. Let\nX 1 = argmin X\u2208R d\u00d7d \u2032 SAX \u2212 SB 2 F + \u03bb X 2 F . We first show that X 1 \u2212 X * F \u2264 \u03b5\u03b3 A (\u03bb) ,B X * F ,(6)\nwhich follows from Lemma 23, applied to A (\u03bb) andB, after showing that, for \u03b5 p = \u03b5 2 /\u03ba 2 , X 1 satisfies\nAX 1 \u2212 B 2 F + \u03bb X 1 2 F \u2264 (1 + \u03b5 p /4)\u2206 * , where \u2206 * = AX * \u2212 B 2 F + \u03bb X * 2 F ,(7)\nwhich in turn follows from Lemma 17 of [ACW17]. That lemma considers a matrix U 1 , comprising the first n rows of the left singular matrix of\u00c2 (\u03bb) = A \u221a \u03bbI d , noting that the ridge objective can be expressed as min X \u00c2 (\u03bb) X \u2212 B 0 2 F . The matrix U 1 = U \u03a3D in our terminology, as in Lemma 18, so the observations of that lemma apply.\n\nLemma 17 of [ACW17] requires that S satisfies\nU \u22a4 1 S \u22a4 SU 1 \u2212 U \u22a4 1 U 1 \u2264 1/4,(8)\nand\nU \u22a4 1 S \u22a4 S(B \u2212 AX * ) \u2212 U \u22a4 1 (B \u2212 AX * ) F \u2264 \u03b5 p \u2206 * .(9)\nWe have A + (\u03bb) 2 = 1/(\u03bb + 1/ A + 2 ) \u2264 Z \u03bb . With the given call to LenSqSample to construct S,\nthe number of rows sampled is m S = O(\u03b5 \u22121 p Z 2 \u03bb A 2 F log(d))\n, so the expected number of times that row i of A is sampled is, up to a factor of O(log d),\n\u03b5 \u22121 p Z 2 \u03bb A 2 F A i, * 2 A 2 F = \u03b5 \u22121 p Z 2 \u03bb A i, * 2 \u2265 \u03b5 \u22121 p (U 1 ) i, * 2 = \u03b5 \u22121 p U 1 2 F (U 1 ) i, * 2 U 1 2 F\n, and so row i is sampled at least the expected number of times it would be sampled under \u03b5 \u22121 p U 1 2 F log d rounds of length-squared sampling of U 1 . As shown by Rudelson and Vershynin ([RV07], see also [KV17], Theorem 4.4), this suffices to have, with high probability, a bound on the normed expression in (8) \nof U 1 U 1 F \u03b5 \u22121 p U 1 2 F = \u221a \u03b5 p U 1 \u2264 \u221a \u03b5 p ,\nso by adjusting constant factors in sample size, (8) holds, for small enough \u03b5 p . To show that (9) holds, we use the discussion of the basic matrix multiplication algorithm discussed in [KV17], Section 2.1, which implies that\nE[ U \u22a4 1 S \u22a4 S(B \u2212 AX * ) \u2212 U \u22a4 1 (B \u2212 AX * ) 2 F ] \u2264 U 1 2 F B \u2212 AX * 2 F s\nwhere s is the number of length-squared samples of U 1 . Here s = \u03b5 \u22121 p U 1 2 F log d, so (9) follows with constant probability by Chebyshev's inequality, noting that B \u2212 AX * F \u2264 \u221a \u2206 * . Thus (8) and (9) hold, so that by Lemma 17 of [ACW17], (7) holds. We now apply Lemma 23, which with (7) and \u03b5 p = \u03b5 2 /\u03ba 2 , implies (6).\n\nNext we show that the (implicit) returned solution is close to the solution of (7), that is,\nA \u22a4 S \u22a4X \u2212 X 1 2 F \u2264 \u03b5 X 1 2 F .(10)\nThis is implied by Theorem 2 of [CYD18], since A \u22a4 S \u22a4X is the output for t = 1 of their Algorithm 1.\n\n(Or rather, it is their output for each column ofX and corresponding column of B.) To invoke their Theorem 2, we need to show that their equation (8) holds, which per their discussion following Theorem 3, holds for ridge leverage score sampling, with O(\u03b5 \u22122 sd \u03bb log sd \u03bb ) samples, which our given m R yields. When we invoke their Theorem 2, we obtain\nA \u22a4 S \u22a4X \u2212 X 1 F \u2264 \u03b5( X 1 F + 1 \u221a \u03bb U k,\u22a5 B F ).(11)\nCombining with (6) and using the triangle inequality, we have that, abbreviating \u03b3\u00c2 (\u03bb) ,B as \u03b3, up to the additive U k,\u22a5 term in (11), we have\nA \u22a4 S \u22a4X \u2212 X * F \u2264 A \u22a4 S \u22a4X \u2212 X 1 F + X 1 \u2212 X * F \u2264 \u03b5 X 1 F + X 1 \u2212 X * F + \u03b5 \u221a \u03bb U k,\u22a5 B F \u2264 \u03b5 X * F + (1 + \u03b5) X 1 \u2212 X * F + \u03b5 \u221a \u03bb U k,\u22a5 B F \u2264 \u03b5 X * F + 2\u03b5\u03b3 X * F + \u03b5 \u221a \u03bb U k,\u22a5 B F \u2264 \u03b5 (1 + 2\u03b3) X * F + \u03b5 \u221a \u03bb U k,\u22a5 B F\nfor small enough \u03b5, as claimed. The time is dominated by that for computing\u00c2 \u22121 SB, where\u00c2 = SARR \u22a4 A \u22a4 S \u22a4 , which we do via the conjugate gradient method. Via standard results (see, e.g., [Vis15], Thm 1.1), in O((T + m S ) \u03ba(\u00c2) log(1/\u03b1))d \u2032 time, where T is the time to compute the product of\u00c2 with a vector, we can obtainX with X \u2212\u00c2 \u22121 SB \u00c2 \u2264 \u03b1 \u00c2 \u22121 SB \u00c2 , where the\u00c2-norm is x \u00c2 = x \u22a4\u00c2 x. Since S and R are (at least) constant-factor subspace embeddings, the singular values of SAR are within a constant factor of those of A, and so \u03ba(\u00c2) is within a constant factor of \u03ba(AA \u22a4 + \u03bbI) = (\u03bb + \u03c3 1 (A) 2 )/(\u03bb + \u03c3 1 (A) 2 ) = \u03ba 2 \u03bb .\n\nWe have\nT = O(m R m S ) =\u00d5(\u03b5 \u22122 log m S Z 2 \u03bb A 2 F \u03b5 \u22122\u03ba2 Z 2 \u03bb A 2 F log(d)) =\u00d5(\u03b5 \u22124\u03ba2 Z 4 \u03bb A 4 F log(d))\nOur running time is\u00d5(T \u03ba \u03bb log(1/\u03b5))d \u2032 , with T as above. Translating to the notation using \u03c8 terms, the result follows.\n\n\nC Sampling from a Low-Rank Approximation\n\nWe need the following lemma, implied by the algorithm and analysis in Section 5.2 of [BWZ16]; for completeness we include a proof.\n\nLemma 25. If S \u2208 R m S \u00d7n and R are such that SA is a PCP of A \u2208 R n\u00d7d , and SAR is a PCP of SA, for error \u03b5 and rank t, and U \u2208 R m S \u00d7k has orthonormal columns such that\n(I \u2212 U U \u22a4 )SAR F \u2264 (1 + \u03b5) SAR \u2212 [SAR] t , then Y * = argmin Y Y U \u22a4 SA \u2212 A F has Y * U \u22a4 SA \u2212 A F \u2264 (1 + O(\u03b5)) A \u2212 A t F .(12)\nWe also have\nU \u22a4 SA 2 F \u2265 A t 2 F \u2212 O(\u03b5) A 2 F . Proof. Note that for matrix Y , Y (I \u2212 Y + t Y t ) = (I \u2212 Y t Y + t )Y\n, and that U U \u22a4 SA is no closer to SA than is the projection of SA to the rowspace of U \u22a4 SA, and that U U \u22a4 = (SAR) t (SAR) + t we have\nA \u2212 Y * U \u22a4 SA F = A(I \u2212 (U \u22a4 SA) + U \u22a4 SA) F \u2264 (1 + \u03b5) SA(I \u2212 (U \u22a4 SA) + U \u22a4 SA) F \u2264 (1 + \u03b5) (I \u2212 U U \u22a4 )SA F \u2264 (1 + \u03b5) 2 (I \u2212 U U \u22a4 )SAR F \u2264 (1 + \u03b5) 3 (I \u2212 (SAR) t (SAR) + t )SAR F \u2264 (1 + \u03b5) 3 (I \u2212 (SA) t (SA) + t )SAR F \u2264 (1 + \u03b5) 4 (I \u2212 (SA) t (SA) + t )SA F = (1 + \u03b5) 4 SA(I \u2212 (SA) + t (SA) t ) F \u2264 (1 + \u03b5) 4 SA(I \u2212 A + t A t ) F \u2264 (1 + \u03b5) 5 A(I \u2212 A + t A t ) F = (1 + \u03b5) 5 A \u2212 A t F = (1 + O(\u03b5)) A \u2212 A t F , as claimed.\nFor the last statement: we have SA 2 F \u2265 (1 \u2212 \u03b5) A 2 F , since SA is a PCP, and by considering the projection of A onto the rowspans of blocks of t of its rows. We have also SA \u2212\n[SA] t 2 F \u2264 (1 + \u03b5) A \u2212 [A] t\n2 , using that SA is a PCP. Using these observations, we have\n[SA] t 2 F = SA 2 F \u2212 SA \u2212 [SA] t 2 F \u2265 (1 \u2212 \u03b5) A 2 F \u2212 (1 + \u03b5) A \u2212 [A] t 2 F = [A] t 2 F \u2212 \u03b5( A 2 F + A \u2212 [A] t 2 F ) \u2265 [A] t 2 F \u2212 3\u03b5 A 2 F . Similarly, [SAR] t 2 F \u2265 [SA] t 2\nF \u2212 3\u03b5 SA 2 F , using that SAR is a PCP of SA. We then have, using these inequalities, the PCP properties, and the hypothesis for U , that\nU \u22a4 SA 2 F = U U \u22a4 SA 2 F = SA 2 F \u2212 (I \u2212 U U \u22a4 )SA 2 F \u2265 (1 \u2212 \u03b5) SAR 2 F \u2212 (1 + \u03b5) 2 SAR \u2212 [SAR] t 2 F \u2265 [SAR] t 2 F \u2212 4\u03b5 SAR 2 F \u2265 ( [SA] t 2 F \u2212 3\u03b5 SA 2 F ) \u2212 4(1 + \u03b5)\u03b5 SA 2 F \u2265 ( [A] t 2 F \u2212 3\u03b5 A 2 F ) \u2212 3\u03b5(1 + \u03b5) A 2 F \u2212 4(1 + \u03b5) 2 \u03b5 A 2 F \u2265 [A] t 2 F \u2212 13\u03b5 A 2 F ,\nfor small enough \u03b5, and the last statement of the lemma follows.\n\nBefore a proof, we give a re-statement of Theorem 5.\n\nTheorem 26 (Dynamic Data Structure for LRA). Given a matrix A \u2208 R n\u00d7d , target rank k, and estimate\u03c3 k \u2264 \u03c3 k (A), error parameter \u03b5 > 0, and estimate \u03c4 of A \u2212 A k 2 F , there exists a data structure representing a matrix Z \u2208 R n\u00d7d with rank k such that if A k 2 F \u2265 \u03b5 A 2 F , with probability at least 99/100,\nA \u2212 Z 2 F \u2264 (1 + O(\u03b5)) A \u2212 A k 2 F\n, where A k is the best rank-k approximation to A. Further, the time taken to construct the representation of Z is\nO(\u03b5 \u22126 k 3 + \u03b5 \u22124 \u03c8 \u03bb (\u03c8 \u03bb + k 2 + k\u03c8 k )), where \u03c8 \u03bb = A 2 F /(\u03c4 /k +\u03c3 2 k ) and \u03c8 k = A 2 F /\u03c3 k (A). Given j \u2208 [d], i \u2208 [n] can be generated with probability (Z) 2 ij / Z) * ,j 2 in expected time O( A 2 F /\u03c3 2 k + m 2 R \u03ba 2 ),\nwhere \u03ba is the condition number of A, and m R = O(k log k + \u03b5 \u22121 k).\n\nProof. The matrix Z is the implicit output of Algorithm 3. In that algorithm, the choice of m S = O(m S Z 2 \u03bb A 2 F ) rows constitutes an effective km S = O(\u03b5 \u22122 k log k) ridge-leverage score samples of the rows of A. We assume that the input \u03c4 is within a constant factor of A \u2212 A k 2 F , so that \u03bb = \u03c4 /k is within a constant factor of A \u2212 A k 2 F /k. Theorem 6 of [CMM17] implies that under these conditions, SA will be a rank-k Projection-Cost Preserving (PCP) sketch of A with error parameter \u03b5, a (k, \u03b5)-PCP.\n\nSimilarly to S, R 1 will be a (column) rank-k PCP of SA, here using that the PCP properties of SA imply that (S(A \u2212 A k ) 2 F = (1 \u00b1 \u03b5) A \u2212 A k 2 F , and so the appropriate \u03bb, and Z \u03bb , for SA are within constant factors of those for A. Let\u00c2 = SAR 1 . Lemma 16 and Theorem 1 of [CMM17] imply that applying their Algorithm 1 to\u00c2 yields S 2 \u2208 R m S 2 \u00d7m S so that S 2\u00c2 is a (k, \u03b5)-PCP for A, and similarly S 2\u00c2 R 2 is a (k, \u03b5)-PCP for S 2\u00c2 .\n\nWe apply Lemma 25 with\u00c2 \u22a4 , R \u22a4 2 , S \u22a4 2 , and V \u22a4 in the roles of A, S, R, and U in the lemma. We obtain that\u1ef8 = (\n\u00c2R 2 V ) +\u00c2 = argmin Y \u00c2 R 2 V Y \u2212\u00c2 F has \u00c2 R 2 V\u1ef8 \u2212\u00c2 F \u2264 (1 + O(\u03b5)) \u00c2 \u2212\u00c2 k F ,\nthat is, U as constructed in Algorithm 3 has U U \u22a4\u00c2 =\u00c2R 2 V\u1ef8 , and therefore satisfies the conditions of Lemma 25 for A, S, R 1 . This implies that Y * = A(U \u22a4 SA)\n+ = argmin Y Y U \u22a4 SA \u2212 A F has Y * U \u22a4 SA \u2212 A F \u2264 (1 + O(\u03b5)) A \u2212 A k F .(13)\nIt remains to solve the multiple-response regression problem min Y Y U \u22a4 SA \u2212 A F , which we do more quickly using the samplers R 3 and R 4 . We next show that R \u22a4 3 is a subspace \u03b5 0 -embedding of (U \u22a4 SA) \u22a4 , and supports low-error matrix product estimation, so that Thm. 36 of [CW13] can be applied. Per Lemma 6 and per Lemma 32 of [CW13], km R 3 = O(\u03b5 \u22122 0 k log k + \u03b5 \u22121 k) leverage-score samples of the columns of U \u22a4 SA suffice for these conditions to hold.\n\nTo obtain km R 3 leverage score samples, we show that 1/\u03b5 length-squared samples of the columns of SA suffice to contain one length-squared sample of U \u22a4 SA, and also that (U \u22a4 SA) + \u2264 1/\u03c3 k , using the input condition on\u03c3 k that \u03c3 k (A) \u2265\u03c3 k , so that the given value of m R 3 in the call to LenSqSample for R 3 is valid.\n\nFor the first claim, by hypothesis A k 2 F \u2265 \u03b5 A 2 F , and by adjusting constants, U as computed satisfies the conditions of Lemma 25 for some \u03b5 \u2032 = \u03b1\u03b5 for constant \u03b1 > 0, so by that lemma and by hypothesis\nU \u22a4 SA 2 F \u2265 A k \u2212 O(\u03b1\u03b5) A 2 F \u2265 \u03b5(1 \u2212 O(\u03b1)) A 2 F \u2265 \u03b5(1 \u2212 O(\u03b1))(1 \u2212 \u03b5) SA 2 F ,\nso adjusting constants, we have U \u22a4 SA 2 F \u2265 \u03b5 SA 2 F . Using that U has orthonormal columns, we have for j \u2208 [d] that U \u22a4 SA * ,j / U \u22a4 SA 2 F \u2264 SA * ,j /\u03b5 SA 2 F , so the probability of sampling j using length-squared probabilities for SA is least \u03b5 times that for U \u22a4 SA.\n\nFor the claim for the value of m R 3 used for R 3 , using the PCP properties of SA and SAR 1 , we have \u03c3 k (U \u22a4 SA) = \u03c3 k (U U \u22a4 SAR 1 ) = \u03c3 k (SAR 1 ) \u2265 (1 \u2212 \u03b5)\u03c3 k (SA) \u2265 (1 \u2212 O(\u03b5))\u03c3 k (A).\n\nso the number of length-squared samples returned by LenSqSample suffice. So using Thm. 36 of [CW13],\n\u1ef8 3 = argmin Y (Y U \u22a4 SA \u2212 A)R 3 F satisfies \u1ef8 3 U \u22a4 SA \u2212 A F \u2264 (1 + \u03b5) min Y Y U \u22a4 SA \u2212 A F \u2264 (1 + O(\u03b5)) A \u2212 A k F ,\nwhere the last inequality follows from (12). Similar conditions and results can be applied to direct leverage-score sampling of the columns of U \u22a4 SAR 3 , resulting in\u1ef8 4 = min Y (Y U \u22a4 SAR 3 \u2212 AR 3 )R 4 F , where there is m R 4 = O(\u03b5 \u22122 0 k log k + \u03b5 \u22121 k) such that these conditions hold for R 4 . This implies\u1ef8 4 is an approximate solution to min Y (Y U \u22a4 SA \u2212 A)R 3 F , and therefore AR\u1ef8 4 = AR(U \u22a4 SAR) + has AR\u1ef8 4 U \u22a4 SA \u2212 A F \u2264 (1 + O(\u03b5)) A \u2212 A k F , as claimed. We have W \u2190 (U \u22a4 SAR) + U \u22a4 =\u1ef8 4 U \u22a4 , so the claimed output condition on ARW SA holds.\n\nTurning to the time needed, Lemma 16 and Theorem 1 of [CMM17] imply that the time needed to construct S 2 and R 2 is\nO(m R 1 m S + k 2 m S ) = O(m S (m S + k 2 ))(14)\nThe time needed to construct V from S 2 SAR 1 R 2 \u2208 R m S 2 \u00d7m S 2 is\nO(m 3 S 2 ) =\u00d5(\u03b5 \u22126 k 3 )(15)\nThe time needed to construct U from V \u2208 R m R 2 \u00d7k and SAR 1 R 2 \u2208 R m S \u00d7m R 2 by multiplication and QR factorization is O(km R 2 m S + k 2 m S ) =\u00d5(m S k 2 \u03b5 \u22122 )\n\nComputation of U \u22a4 SAR 3 requires O(km S m R 3 ) time, where m R 3 = O(m R 3 \u03b5 \u22121 Z 2 k A 2 F , and m R 3 = O(log k + \u03b5 \u22121 ), that is,\u00d5 (m S k\u03b5 \u22122 Z 2 k A 2 F )\n\ntime. Leverage-score sampling of the rows of (U \u22a4 SAR 3 ) \u22a4 \u2208 R m R 3 \u00d7k takes, applying Theorem 17 and using m R 4 = O(k(log k + \u03b5 \u22121 )), time at most\nO(log(m R 3 )km R 3 + k \u03c9 log log(km R 3 ) + k 2 log m R 3 + m R 4 km 1/log(m R 3 ) R 3 )(18)=\u00d5(k\u03b5 \u22122 Z 2 k A 2 F + k \u03c9 + k 2 \u03b5 \u22121 )(19)\nComputation of (U \u22a4 SAR) + from U \u22a4 SAR requires O(k 2 m R 4 ) =\u00d5(\u03b5 \u22121 k 3 ) time. (With notation that R has m R = m R 4 columns.) Given (U \u22a4 SAR) + , computation of W = (U \u22a4 SAR) + U \u22a4 requires O(m S (m S + k 2 )) +\u00d5(m S k 2 \u03b5 \u22122 ) +\u00d5(m S k\u03b5 \u22122 Z 2 k A 2 F ) +\u00d5(m S k 2 \u03b5 \u22121 ) +\u00d5(\u03b5 \u22126 k 3 ) + tO(k\u03b5 \u22122 Z 2 k A 2 F + k \u03c9 + k 2 \u03b5 \u22121 ) =\u00d5(m S (m S + k 2 \u03b5 \u22122 + k\u03b5 \u22122 Z 2 k A 2 F ) + \u03b5 \u22126 k 3 )\nO(km R m S ) =\u00d5(m S k 2 \u03b5 \u22121 )(20)\nHere m S = O(m S Z 2 \u03bb A 2 F ) =\u00d5(\u03b5 \u22122 Z 2 \u03bb A 2 F ), so the time is\nO(\u03b5 \u22126 k 3 + \u03b5 \u22124 Z 2 \u03bb A 2 F (Z 2 \u03bb A 2 F + k 2 + kZ 2 k A 2 F ))\nQueries as in the theorem statement for given j \u2208 [d] can be answered as in [Tan19], in the time given. Briefly: given j, let v \u2190 (W SA) * ,j \u2208 R m R . Let\u00c2 denote AR. Using LenSqSample (and large m R ), generate a sampling matrix S 3 with O(Z 2 \u00c2 2 ) rows, and estimate \u00c2 v 2 \u2248 \u03b2 v \u2190 S\u00c2v 2 . Generate i \u2208 [n] via rejection sampling as follows. In a given trial, pick j * \u2208\n\n[d] with probability proportional to \u00c2 * ,j 2 v 2 j using DynSamp(A), then pick i \u2208 [m R ] with probability\u00c2 2 i,j * / \u00c2 * ,j * 2 . This implies that i \u2208 [n] has been picked with probability p i = j\u00c2 2\n\nij v 2 j / j \u00c2 * ,j 2 v 2 j . Now for a value \u03b1 > 0, accept with probability q i /\u03b1p i , where q i = (\u00c2 i, * v) 2 /\u03b2 v , otherwise reject. This requires \u03b1 \u2265 q i /p i . and takes expected trials \u03b1. So an upper bound is needed for\nq i p i = (\u00c2 i, * v) 2 \u03b2 v j \u00c2 * ,j 2 v 2 j j\u00c2 2 ij v 2 j .\nWe have (\u00c2 i, * v) 2 \u2264 m R j\u00c2 2 ij v 2 j using Cauchy-Schwarz, and \u03b2 v \u2248 \u00c2 v 2 \u2265 v 2 / \u00c2 + 2 , and also j \u00c2 * ,j 2 v 2 j \u2264 v 2 max j \u00c2 * ,j 2 \u2264 v 2 \u00c2 2 . Putting this together \u03b1 \u2265 m R \u00c2 2 \u00c2 + 2 = O(m R \u03ba(A)) will do. The work per trial is O(m R log(nd)) ,and putting that together with the time to compute \u03b2 v , the theorem follows.\n\n\nD Additional Experiments\n\n\nD.1 Low-Rank Approximation\n\nAs we stated in Section 6.1, although the accuracy of our algorithm is slightly worse, by increasing the sample size slightly, our algorithm achieves a similar accuracy as [ADBL20], but still has a faster runtime. The results are shown in Table 5. Here we set (r, c) = (500, 800) for MovieLens 100K and (r, c) = (700, 1100) for KOS data for our algorithms, but do not change (r, c) for the algorithm in [ADBL20] as in Section 6.1. \n\n\ntime. Putting together (14),(16), (17), (20), (15), (18), we have\n\nTable 2 :\n2Performance of our algorithm and ADBL on MovieLen 100K and KOS data, respectively.k = 10 k = 15 k = 20 \n\u03b5(Ours) \n0.0416 0.0557 0.0653 \n\u03b5(ADBL) \n0.0262 0.0424 0.0538 \nRuntime \n0.125s 0.131s 0.135s \n(Ours, Query) \nRuntime \n0.181s 0.183s 0.184s \n(Ours, Total) \nRuntime \n0.867s 0.913s 1.024s \n(ADBL, Query) \nRuntime \n0.968s 1.003s 1.099s \n(ADBL, Total) \nRuntime of SVD \n2.500s \n\nk = 10 k = 15 k = 20 \n\u03b5(Ours) \n0.0397 0.0478 0.0581 \n\u03b5(ADBL) \n0.0186 0.0295 0.0350 \nRuntime \n0.292s 0.296s 0.295s \n(Ours, Query) \nRuntime \n0.452s 0.455s 0.452s \n(Ours, Total) \nRuntime \n1.501s 1.643s 1.580s \n(ADBL, Query) \nRuntime \n1.814s 1.958s 1.897s \n(ADBL, Total) \nRuntime of SVD \n36.738s \n\n\n\nTable 3 :\n3Performance of our algorithm on synthetic data.(r, c) \n300, 500 500, 800 1000, 1500 \n\u03b5(Ours) \n0.1392 \n0.0953 \n0.0792 \nRuntime \n0.021s \n0.042s \n0.148s \n(Query) \nRuntime \n0.557s \n0.568s \n0.667s \n(Total) \nExact X *  \n24.074s \n\n\n\nTable 4 :\n4Performance of our algorithm on YearPrediction data and PEMS data, respectively. Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix multiplication time. In Proceedings of the 51st annual ACM SIGACT symposium on theory of computing, pages 938-942, 2019. 4 Cohen and Richard Peng. L p row sampling by Lewis weights. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 183-192. http://arxiv.org/abs/1207.6365. Final version J. ACM, Vol 63, 2017, http://doi.acm.org/10.1145/3019134. 26, 27 Jelani Nelson and Huy L Nguy\u00ean. OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings. In 2013 ieee 54th annual symposium on foundations of computer science, pages 117-126. IEEE, 2013. 4r = \n1000 \n3000 \n5000 \n\u03b5(Ours) \n0.1070 0.0633 0.0447 \nRuntime 0.031s 0.037s 0.059s \n(Query) \nRuntime 0.213s 0.229s 0.245s \n(Total) \nExact X  *  \n0.251s \n\nc = \n15000 25000 35000 \n\u03b5(Ours) \n0.1778 0.1397 0.1130 \nRuntime 0.234s 0.381s 0.532s \n(Query) \nRuntime 0.473s 0.628s 0.777s \n(Total) \nExact X *  \n0.972s \n\n\nTable 5 :\n5Performance of our algorithm and ADBL on MovieLens 100K and KOS data, respectively.k = 10 k = 15 k = 20 \n\u03b5(Ours) \n0.0323 0.0439 0.0521 \n\u03b5(ADBL) \n0.0262 0.0424 0.0538 \nRuntime \n0.341s 0.365s 0.370s \n(Ours, Query) \nRuntime \n0.412s 0.417s 0.415s \n(Ours, Total) \nRuntime \n0.863s 0.917s 1.024s \n(ADBL, Query) \nRuntime \n0.968s 1.003s 1.099s \n(ADBL, Total) \nRuntime of SVD \n2.500s \n\nk = 10 k = 15 k = 20 \n\u03b5(Ours) \n0.0291 0.0390 0.0476 \n\u03b5(ADBL) \n0.0186 0.0295 0.0350 \nRuntime \n0.826s 0.832s 0.831s \n(Ours, Query) \nRuntime \n0.979s 0.994s 0.986s \n(Ours, Total) \nRuntime \n1.501s 1.643s 1.580s \n(ADBL, Query) \nRuntime \n1.814s 1.958s 1.897s \n(ADBL, Total) \nRuntime of SVD \n36.738s \n\nThroughout, we define \u03ba(A) = A A + , that is, the ratio of largest to smallest nonzero singular values of A, so that, in particular, it will never be infinite or undefined.\nThe Bag of Words Data Set from the UCI Machine Learning Repository.\nAcknowledgements.Honghao Lin and David Woodruff would like to thank for partial support from the National Science Foundation (NSF) under Grant No. CCF-1815840.", "annotations": {"author": "[{\"end\":142,\"start\":83},{\"end\":175,\"start\":143},{\"end\":243,\"start\":176},{\"end\":303,\"start\":244},{\"end\":371,\"start\":304},{\"end\":437,\"start\":372},{\"end\":470,\"start\":438}]", "publisher": null, "author_last_name": "[{\"end\":98,\"start\":90},{\"end\":194,\"start\":186},{\"end\":255,\"start\":249},{\"end\":315,\"start\":312},{\"end\":388,\"start\":380}]", "author_first_name": "[{\"end\":89,\"start\":83},{\"end\":146,\"start\":143},{\"end\":183,\"start\":176},{\"end\":185,\"start\":184},{\"end\":248,\"start\":244},{\"end\":311,\"start\":304},{\"end\":377,\"start\":372},{\"end\":379,\"start\":378},{\"end\":441,\"start\":438}]", "author_affiliation": "[{\"end\":141,\"start\":115},{\"end\":174,\"start\":148},{\"end\":242,\"start\":216},{\"end\":302,\"start\":276},{\"end\":370,\"start\":344},{\"end\":436,\"start\":410},{\"end\":469,\"start\":443}]", "title": "[{\"end\":69,\"start\":1},{\"end\":539,\"start\":471}]", "venue": null, "abstract": "[{\"end\":1430,\"start\":552}]", "bib_ref": "[{\"end\":4402,\"start\":4394},{\"end\":4825,\"start\":4819},{\"end\":4831,\"start\":4825},{\"end\":4837,\"start\":4831},{\"end\":6161,\"start\":6155},{\"end\":6356,\"start\":6349},{\"end\":6389,\"start\":6380},{\"end\":7393,\"start\":7386},{\"end\":24072,\"start\":24069},{\"end\":46518,\"start\":46508},{\"end\":46524,\"start\":46518},{\"end\":46585,\"start\":46578},{\"end\":50979,\"start\":50972},{\"end\":51990,\"start\":51960},{\"end\":52007,\"start\":52001},{\"end\":58359,\"start\":58352}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":64481,\"start\":64414},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":65163,\"start\":64482},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":65400,\"start\":65164},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":66485,\"start\":65401},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":67167,\"start\":66486}]", "paragraph": "[{\"end\":1635,\"start\":1446},{\"end\":2826,\"start\":1637},{\"end\":3738,\"start\":2828},{\"end\":4463,\"start\":3740},{\"end\":5155,\"start\":4465},{\"end\":5304,\"start\":5157},{\"end\":5805,\"start\":5320},{\"end\":7042,\"start\":5807},{\"end\":7967,\"start\":7044},{\"end\":8717,\"start\":7969},{\"end\":8871,\"start\":8719},{\"end\":8959,\"start\":8909},{\"end\":9108,\"start\":8961},{\"end\":9333,\"start\":9144},{\"end\":9634,\"start\":9462},{\"end\":10132,\"start\":9636},{\"end\":10186,\"start\":10134},{\"end\":10424,\"start\":10286},{\"end\":10623,\"start\":10448},{\"end\":11294,\"start\":10625},{\"end\":11473,\"start\":11351},{\"end\":11573,\"start\":11527},{\"end\":12182,\"start\":11709},{\"end\":12418,\"start\":12184},{\"end\":12580,\"start\":12431},{\"end\":13429,\"start\":12582},{\"end\":13688,\"start\":13431},{\"end\":14218,\"start\":14039},{\"end\":14740,\"start\":14283},{\"end\":14801,\"start\":14742},{\"end\":16188,\"start\":14818},{\"end\":16204,\"start\":16200},{\"end\":17540,\"start\":16516},{\"end\":17590,\"start\":17572},{\"end\":17874,\"start\":17592},{\"end\":18565,\"start\":17876},{\"end\":18753,\"start\":18567},{\"end\":19051,\"start\":18813},{\"end\":19431,\"start\":19063},{\"end\":20684,\"start\":19449},{\"end\":21107,\"start\":20686},{\"end\":21504,\"start\":21109},{\"end\":22001,\"start\":21506},{\"end\":22576,\"start\":22491},{\"end\":22783,\"start\":22578},{\"end\":22951,\"start\":22785},{\"end\":23176,\"start\":22983},{\"end\":23264,\"start\":23178},{\"end\":23444,\"start\":23312},{\"end\":23971,\"start\":23493},{\"end\":24027,\"start\":23973},{\"end\":24330,\"start\":24029},{\"end\":24529,\"start\":24332},{\"end\":24609,\"start\":24531},{\"end\":24708,\"start\":24611},{\"end\":24780,\"start\":24710},{\"end\":24880,\"start\":24782},{\"end\":24969,\"start\":24882},{\"end\":25058,\"start\":24971},{\"end\":25344,\"start\":25090},{\"end\":26396,\"start\":26242},{\"end\":26572,\"start\":26398},{\"end\":27012,\"start\":26615},{\"end\":27673,\"start\":27028},{\"end\":27769,\"start\":27675},{\"end\":28111,\"start\":27985},{\"end\":29613,\"start\":29003},{\"end\":29689,\"start\":29640},{\"end\":29839,\"start\":29691},{\"end\":29858,\"start\":29841},{\"end\":30352,\"start\":29860},{\"end\":30449,\"start\":30374},{\"end\":30519,\"start\":30451},{\"end\":31209,\"start\":30521},{\"end\":31526,\"start\":31211},{\"end\":31587,\"start\":31547},{\"end\":31718,\"start\":31664},{\"end\":32068,\"start\":31720},{\"end\":32571,\"start\":32070},{\"end\":33096,\"start\":32605},{\"end\":33696,\"start\":33166},{\"end\":34100,\"start\":33716},{\"end\":34307,\"start\":34102},{\"end\":34480,\"start\":34309},{\"end\":34649,\"start\":34482},{\"end\":34776,\"start\":34687},{\"end\":35062,\"start\":34912},{\"end\":35354,\"start\":35113},{\"end\":35541,\"start\":35443},{\"end\":35622,\"start\":35548},{\"end\":36066,\"start\":35624},{\"end\":36310,\"start\":36068},{\"end\":36623,\"start\":36312},{\"end\":37235,\"start\":36749},{\"end\":37463,\"start\":37365},{\"end\":37818,\"start\":37506},{\"end\":37915,\"start\":37820},{\"end\":37993,\"start\":37962},{\"end\":38051,\"start\":38048},{\"end\":38092,\"start\":38053},{\"end\":38244,\"start\":38155},{\"end\":38408,\"start\":38373},{\"end\":38695,\"start\":38620},{\"end\":39060,\"start\":38697},{\"end\":39509,\"start\":39062},{\"end\":40016,\"start\":39663},{\"end\":40865,\"start\":40018},{\"end\":40985,\"start\":40876},{\"end\":41321,\"start\":40987},{\"end\":42090,\"start\":41323},{\"end\":42401,\"start\":42092},{\"end\":42634,\"start\":42584},{\"end\":42697,\"start\":42669},{\"end\":42862,\"start\":42764},{\"end\":43060,\"start\":42940},{\"end\":43370,\"start\":43188},{\"end\":43635,\"start\":43452},{\"end\":44066,\"start\":43773},{\"end\":44205,\"start\":44194},{\"end\":44617,\"start\":44207},{\"end\":44904,\"start\":44619},{\"end\":45028,\"start\":44906},{\"end\":45189,\"start\":45030},{\"end\":45252,\"start\":45191},{\"end\":45436,\"start\":45312},{\"end\":45599,\"start\":45491},{\"end\":45692,\"start\":45601},{\"end\":46076,\"start\":45694},{\"end\":46306,\"start\":46078},{\"end\":46586,\"start\":46372},{\"end\":46696,\"start\":46588},{\"end\":46982,\"start\":46928},{\"end\":47088,\"start\":47010},{\"end\":47369,\"start\":47315},{\"end\":47712,\"start\":47371},{\"end\":47961,\"start\":47890},{\"end\":48121,\"start\":48015},{\"end\":48263,\"start\":48198},{\"end\":48514,\"start\":48443},{\"end\":49236,\"start\":48565},{\"end\":49875,\"start\":49238},{\"end\":49961,\"start\":49877},{\"end\":50372,\"start\":49963},{\"end\":50622,\"start\":50452},{\"end\":50634,\"start\":50624},{\"end\":50845,\"start\":50739},{\"end\":51270,\"start\":50933},{\"end\":51317,\"start\":51272},{\"end\":51358,\"start\":51355},{\"end\":51515,\"start\":51419},{\"end\":51673,\"start\":51581},{\"end\":52109,\"start\":51794},{\"end\":52386,\"start\":52160},{\"end\":52790,\"start\":52464},{\"end\":52884,\"start\":52792},{\"end\":53023,\"start\":52922},{\"end\":53377,\"start\":53025},{\"end\":53574,\"start\":53431},{\"end\":54425,\"start\":53794},{\"end\":54434,\"start\":54427},{\"end\":54657,\"start\":54536},{\"end\":54832,\"start\":54702},{\"end\":55005,\"start\":54834},{\"end\":55147,\"start\":55135},{\"end\":55392,\"start\":55255},{\"end\":55996,\"start\":55818},{\"end\":56089,\"start\":56028},{\"end\":56406,\"start\":56268},{\"end\":56742,\"start\":56678},{\"end\":56796,\"start\":56744},{\"end\":57107,\"start\":56798},{\"end\":57257,\"start\":57143},{\"end\":57556,\"start\":57488},{\"end\":58072,\"start\":57558},{\"end\":58513,\"start\":58074},{\"end\":58631,\"start\":58515},{\"end\":58875,\"start\":58712},{\"end\":59418,\"start\":58954},{\"end\":59742,\"start\":59420},{\"end\":59950,\"start\":59744},{\"end\":60306,\"start\":60032},{\"end\":60498,\"start\":60308},{\"end\":60600,\"start\":60500},{\"end\":61276,\"start\":60719},{\"end\":61394,\"start\":61278},{\"end\":61514,\"start\":61445},{\"end\":61709,\"start\":61545},{\"end\":61871,\"start\":61711},{\"end\":62024,\"start\":61873},{\"end\":62553,\"start\":62162},{\"end\":62657,\"start\":62589},{\"end\":63098,\"start\":62725},{\"end\":63301,\"start\":63100},{\"end\":63531,\"start\":63303},{\"end\":63924,\"start\":63592},{\"end\":64413,\"start\":63982}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8908,\"start\":8872},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9143,\"start\":9109},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9461,\"start\":9334},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10285,\"start\":10187},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10447,\"start\":10425},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11350,\"start\":11295},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11526,\"start\":11474},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11708,\"start\":11574},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12430,\"start\":12419},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14038,\"start\":13689},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14282,\"start\":14219},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16498,\"start\":16205},{\"attributes\":{\"id\":\"formula_12\"},\"end\":17571,\"start\":17541},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18812,\"start\":18754},{\"attributes\":{\"id\":\"formula_14\"},\"end\":22490,\"start\":22002},{\"attributes\":{\"id\":\"formula_15\"},\"end\":22982,\"start\":22952},{\"attributes\":{\"id\":\"formula_16\"},\"end\":23311,\"start\":23265},{\"attributes\":{\"id\":\"formula_17\"},\"end\":25089,\"start\":25059},{\"attributes\":{\"id\":\"formula_18\"},\"end\":26241,\"start\":25345},{\"attributes\":{\"id\":\"formula_19\"},\"end\":27984,\"start\":27770},{\"attributes\":{\"id\":\"formula_20\"},\"end\":29002,\"start\":28112},{\"attributes\":{\"id\":\"formula_21\"},\"end\":30373,\"start\":30353},{\"attributes\":{\"id\":\"formula_22\"},\"end\":31663,\"start\":31588},{\"attributes\":{\"id\":\"formula_23\"},\"end\":32604,\"start\":32572},{\"attributes\":{\"id\":\"formula_24\"},\"end\":33165,\"start\":33097},{\"attributes\":{\"id\":\"formula_25\"},\"end\":34686,\"start\":34650},{\"attributes\":{\"id\":\"formula_26\"},\"end\":34911,\"start\":34777},{\"attributes\":{\"id\":\"formula_27\"},\"end\":35442,\"start\":35355},{\"attributes\":{\"id\":\"formula_28\"},\"end\":36748,\"start\":36624},{\"attributes\":{\"id\":\"formula_29\"},\"end\":37364,\"start\":37236},{\"attributes\":{\"id\":\"formula_30\"},\"end\":37505,\"start\":37464},{\"attributes\":{\"id\":\"formula_31\"},\"end\":37961,\"start\":37916},{\"attributes\":{\"id\":\"formula_32\"},\"end\":38047,\"start\":37994},{\"attributes\":{\"id\":\"formula_33\"},\"end\":38154,\"start\":38093},{\"attributes\":{\"id\":\"formula_34\"},\"end\":38372,\"start\":38245},{\"attributes\":{\"id\":\"formula_35\"},\"end\":38619,\"start\":38409},{\"attributes\":{\"id\":\"formula_36\"},\"end\":39662,\"start\":39510},{\"attributes\":{\"id\":\"formula_37\"},\"end\":42583,\"start\":42485},{\"attributes\":{\"id\":\"formula_38\"},\"end\":42668,\"start\":42635},{\"attributes\":{\"id\":\"formula_39\"},\"end\":42763,\"start\":42698},{\"attributes\":{\"id\":\"formula_40\"},\"end\":42939,\"start\":42863},{\"attributes\":{\"id\":\"formula_41\"},\"end\":43187,\"start\":43061},{\"attributes\":{\"id\":\"formula_42\"},\"end\":43451,\"start\":43371},{\"attributes\":{\"id\":\"formula_43\"},\"end\":43772,\"start\":43636},{\"attributes\":{\"id\":\"formula_44\"},\"end\":44193,\"start\":44067},{\"attributes\":{\"id\":\"formula_45\"},\"end\":45311,\"start\":45253},{\"attributes\":{\"id\":\"formula_46\"},\"end\":45490,\"start\":45437},{\"attributes\":{\"id\":\"formula_47\"},\"end\":46371,\"start\":46307},{\"attributes\":{\"id\":\"formula_48\"},\"end\":46927,\"start\":46697},{\"attributes\":{\"id\":\"formula_49\"},\"end\":47009,\"start\":46983},{\"attributes\":{\"id\":\"formula_50\"},\"end\":47314,\"start\":47089},{\"attributes\":{\"id\":\"formula_51\"},\"end\":47889,\"start\":47713},{\"attributes\":{\"id\":\"formula_52\"},\"end\":48014,\"start\":47962},{\"attributes\":{\"id\":\"formula_53\"},\"end\":48197,\"start\":48122},{\"attributes\":{\"id\":\"formula_54\"},\"end\":48442,\"start\":48264},{\"attributes\":{\"id\":\"formula_55\"},\"end\":50451,\"start\":50373},{\"attributes\":{\"id\":\"formula_56\"},\"end\":50738,\"start\":50635},{\"attributes\":{\"id\":\"formula_57\"},\"end\":50932,\"start\":50846},{\"attributes\":{\"id\":\"formula_58\"},\"end\":51354,\"start\":51318},{\"attributes\":{\"id\":\"formula_59\"},\"end\":51418,\"start\":51359},{\"attributes\":{\"id\":\"formula_60\"},\"end\":51580,\"start\":51516},{\"attributes\":{\"id\":\"formula_61\"},\"end\":51793,\"start\":51674},{\"attributes\":{\"id\":\"formula_62\"},\"end\":52159,\"start\":52110},{\"attributes\":{\"id\":\"formula_63\"},\"end\":52463,\"start\":52387},{\"attributes\":{\"id\":\"formula_64\"},\"end\":52921,\"start\":52885},{\"attributes\":{\"id\":\"formula_65\"},\"end\":53430,\"start\":53378},{\"attributes\":{\"id\":\"formula_66\"},\"end\":53793,\"start\":53575},{\"attributes\":{\"id\":\"formula_67\"},\"end\":54535,\"start\":54435},{\"attributes\":{\"id\":\"formula_68\"},\"end\":55134,\"start\":55006},{\"attributes\":{\"id\":\"formula_69\"},\"end\":55254,\"start\":55148},{\"attributes\":{\"id\":\"formula_70\"},\"end\":55817,\"start\":55393},{\"attributes\":{\"id\":\"formula_71\"},\"end\":56027,\"start\":55997},{\"attributes\":{\"id\":\"formula_72\"},\"end\":56267,\"start\":56090},{\"attributes\":{\"id\":\"formula_73\"},\"end\":56677,\"start\":56407},{\"attributes\":{\"id\":\"formula_74\"},\"end\":57142,\"start\":57108},{\"attributes\":{\"id\":\"formula_75\"},\"end\":57487,\"start\":57258},{\"attributes\":{\"id\":\"formula_76\"},\"end\":58711,\"start\":58632},{\"attributes\":{\"id\":\"formula_77\"},\"end\":58953,\"start\":58876},{\"attributes\":{\"id\":\"formula_78\"},\"end\":60031,\"start\":59951},{\"attributes\":{\"id\":\"formula_79\"},\"end\":60718,\"start\":60601},{\"attributes\":{\"id\":\"formula_80\"},\"end\":61444,\"start\":61395},{\"attributes\":{\"id\":\"formula_81\"},\"end\":61544,\"start\":61515},{\"attributes\":{\"id\":\"formula_84\"},\"end\":62118,\"start\":62025},{\"attributes\":{\"id\":\"formula_85\"},\"end\":62161,\"start\":62118},{\"attributes\":{\"id\":\"formula_86\"},\"end\":62588,\"start\":62554},{\"attributes\":{\"id\":\"formula_87\"},\"end\":62724,\"start\":62658},{\"attributes\":{\"id\":\"formula_88\"},\"end\":63591,\"start\":63532}]", "table_ref": "[{\"end\":2122,\"start\":2110},{\"end\":14800,\"start\":14793},{\"end\":15769,\"start\":15762},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":30553,\"start\":30546},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":33198,\"start\":33191},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":64228,\"start\":64221}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1444,\"start\":1432},{\"attributes\":{\"n\":\"1.1\"},\"end\":5318,\"start\":5307},{\"attributes\":{\"n\":\"1.2\"},\"end\":14816,\"start\":14804},{\"end\":16198,\"start\":16191},{\"end\":16514,\"start\":16500},{\"attributes\":{\"n\":\"2\"},\"end\":19061,\"start\":19054},{\"attributes\":{\"n\":\"3\"},\"end\":19447,\"start\":19434},{\"attributes\":{\"n\":\"4\"},\"end\":23491,\"start\":23447},{\"attributes\":{\"n\":\"5\"},\"end\":26613,\"start\":26575},{\"attributes\":{\"n\":\"6\"},\"end\":27026,\"start\":27015},{\"attributes\":{\"n\":\"6.1\"},\"end\":29638,\"start\":29616},{\"attributes\":{\"n\":\"6.2\"},\"end\":31545,\"start\":31529},{\"end\":33714,\"start\":33699},{\"end\":35111,\"start\":35065},{\"end\":35546,\"start\":35544},{\"end\":40874,\"start\":40868},{\"end\":42484,\"start\":42404},{\"end\":48563,\"start\":48517},{\"end\":54700,\"start\":54660},{\"end\":63951,\"start\":63927},{\"end\":63980,\"start\":63954},{\"end\":64492,\"start\":64483},{\"end\":65174,\"start\":65165},{\"end\":65411,\"start\":65402},{\"end\":66496,\"start\":66487}]", "table": "[{\"end\":65163,\"start\":64576},{\"end\":65400,\"start\":65223},{\"end\":66485,\"start\":66178},{\"end\":67167,\"start\":66581}]", "figure_caption": "[{\"end\":64481,\"start\":64416},{\"end\":64576,\"start\":64494},{\"end\":65223,\"start\":65176},{\"end\":66178,\"start\":65413},{\"end\":66581,\"start\":66498}]", "figure_ref": null, "bib_author_first_name": null, "bib_author_last_name": null, "bib_entry": null, "bib_title": null, "bib_author": null, "bib_venue": null}}}, "year": 2023, "month": 12, "day": 17}