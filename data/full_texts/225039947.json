{"id": 225039947, "updated": "2023-10-06 10:46:12.857", "metadata": {"title": "An Industry Evaluation of Embedding-based Entity Alignment", "authors": "[{\"first\":\"Ziheng\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Jiaoyan\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Xi\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Hualuo\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Yuejia\",\"last\":\"Xiang\",\"middle\":[]},{\"first\":\"Bo\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Yefeng\",\"last\":\"Zheng\",\"middle\":[]}]", "venue": "COLING", "journal": "Proceedings of the 28th International Conference on Computational Linguistics: Industry Track", "publication_date": {"year": 2020, "month": 10, "day": 22}, "abstract": "Embedding-based entity alignment has been widely investigated in recent years, but most proposed methods still rely on an ideal supervised learning setting with a large number of unbiased seed mappings for training and validation, which significantly limits their usage. In this study, we evaluate those state-of-the-art methods in an industrial context, where the impact of seed mappings with different sizes and different biases is explored. Besides the popular benchmarks from DBpedia and Wikidata, we contribute and evaluate a new industrial benchmark that is extracted from two heterogeneous knowledge graphs (KGs) under deployment for medical applications. The experimental results enable the analysis of the advantages and disadvantages of these alignment methods and the further discussion of suitable strategies for their industrial deployment.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2010.11522", "mag": "3117053671", "acl": "2020.coling-industry.17", "pubmed": null, "pubmedcentral": null, "dblp": "conf/coling/ZhangLCCLXZ20", "doi": "10.18653/v1/2020.coling-industry.17"}}, "content": {"source": {"pdf_hash": "54f7b8472eebf0a926be1f51b6384b4adbef4708", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.11522v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2010.11522", "status": "GREEN"}}, "grobid": {"id": "a869a1a9475ce124914d59fa186ce894ff082b98", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/54f7b8472eebf0a926be1f51b6384b4adbef4708.txt", "contents": "\nAn Industry Evaluation of Embedding-based Entity Alignment\n\n\nZiheng Zhang \nTencent Jarvis Lab\nShenzhenChina\n\nJiaoyan Chen jiaoyan.chen@cs.ox.ac.uk \nDepartment of Computer Science\nUniversity of Oxford\nUK\n\nXi Chen \nTencent Jarvis Lab\nShenzhenChina\n\nHualuo Liu \nTencent Jarvis Lab\nShenzhenChina\n\nYuejia Xiang \nTencent Jarvis Lab\nShenzhenChina\n\nBo Liu \nTencent Jarvis Lab\nShenzhenChina\n\nYefeng Zheng yefengzheng@tencent.com \nTencent Jarvis Lab\nShenzhenChina\n\nAn Industry Evaluation of Embedding-based Entity Alignment\n\nEmbedding-based entity alignment has been widely investigated in recent years, but most proposed methods still rely on an ideal supervised learning setting with a large number of unbiased seed mappings for training and validation, which significantly limits their usage. In this study, we evaluate those state-of-the-art methods in an industrial context, where the impact of seed mappings with different sizes and different biases is explored. Besides the popular benchmarks from DBpedia and Wikidata, we contribute and evaluate a new industrial benchmark that is extracted from two heterogeneous knowledge graphs (KGs) under deployment for medical applications. The experimental results enable the analysis of the advantages and disadvantages of these alignment methods and the further discussion of suitable strategies for their industrial deployment.\n\nEdit distance between entity names\n\nAverage number of attributes Seed mappings Figure 1: Distribution of mappings of two sampled medical KGs. The horizontal axis denotes the average number of attributes and the vertical axis denotes the edit distance between entity names. though a small number of seed mappings can be annotated, they are usually biased in comparison with the remaining for prediction with respect to entity name, attribute, graph structure and so on. Figure 1 shows the distribution of all the mappings of two sampled medical KGs from Tencent Technology (cf. Section 3.1 for more details), with two dimensions -the similarity between names of mapping entities and the average attribute number of mapping entities. When we directly invited experts or utilized downstream applications to annotate mappings, the annotated mappings, which could act as the seed mappings for training, usually lie in the bottom right area (seen in the red block in Figure 1) with high name similarity and large attribute number. Thus, we believe that the seed mappings should have the following characteristics to make the evaluation of these supervised methods more practical. Firstly, the seed mappings should take a small proportion of all the mappings, such as 3% that is far smaller than previous experimental settings. Secondly, the seed mappings should be biased towards the remaining mappings with respect to the entity name similarity, the average attribute number, or both. Such biases are ignored in the current evaluation.\n\nIn this work, we systematically evaluate four state-of-the-art embedding-based KG alignment methods in an industrial context. The experiment is conducted with one open benchmark from DBpedia and Wikidata, one industry benchmark from two enterprise medical KGs with heterogeneous contents, and a series of seed mappings with different sizes, name biases and attribute biases. The performance analysis considers all the testing mappings as well as different splits of them for fine-grained observations. These methods are also compared with the traditional system PARIS. To the best of our knowledge, this is the first work to evaluate and analyse the embedding-based entity alignment methods from an industry perspective. We find that these methods heavily rely on an ideal supervised learning setting and suffer from a dramatic performance drop when being tested in an industrial context. Based on these results, we can further discuss the possibility to deploy them for real-world applications as well as suitable sampling strategies. The new benchmark and seed mappings can also benefit the research community for future studies, which are publicly available at https://github.com/ZihengZZH/industry-eval-EA.\n\n\nPreliminaries and Related Work\n\n\nEmbedding-based Entity Alignment\n\nMost of the existing embedding based entity alignment methods conform to the following three-step paradigm: (i) embedding the entities into a vector space by either a translation based method such as TransE (Bordes et al., 2013) or Graph Neural Networks (GNNs) (Scarselli et al., 2008) which recursively aggregate the embeddings of the neighbouring entities and relations; (ii) mapping the entity embeddings in the space of one KG to the space of another KG by learning a transformation matrix, sharing embeddings of the aligned entities, or swapping the aligned entities in the associated triples; (iii) searching an entity's counterpart in another KG by calculating the distance in the embedding space using metrics such as the cosine similarity. It is worth noting that the role of the seed mappings mainly lies in the second step, aligning the embeddings of two KGs.\n\nSpecifically, we evaluate four methods, namely BootEA , MultiKE (Zhang et al., 2019b), RDGCN (Wu et al., 2019) and RSN4EA (Guo et al., 2018). On the one hand, they have achieved the state-of-the-art performance in the ideal supervised learning setting, according to their own evaluation and the benchmarking study (Sun et al., 2020); on the other hand, they are representative to different techniques that are widely used in the literature. The four methods are introduced as follows.\n\nBootEA is a semi-supervised approach, which adopts translation-based models for embedding and iteratively trains a classifier by bootstrapping. In each iteration, new likely mappings are labelled by the classifier and those causing no conflict are added for training in the following iteration.\n\nMultiKE utilizes multi-view learning to encode different semantics into the prediction model. Specifically, three views are developed for entity names, entity attributes, and the graph structure respectively.\n\nRDGCN applies a GCN variant, Dual-Primal GCN (Monti et al., 2018) to utilize the relation information in KG embedding. It can better utilize the graph structure than those translation-based embedding methods, especially in dealing with the triangular structures.\n\nRSN4EA firstly generates biased random walks (long paths) of both KGs as sequences and then learns the embeddings by a sequential model named Recurrent Skipping Network. The seed mappings here are used to generate cross-KG walks, thus exploring correlations between cross-KG entities.\n\n\nSeed Mappings\n\nAs far as we know, the current embedding-based entity alignment methods mostly rely on the seed mappings, whose roles are introduces in Section 2.1, for supervised or semi-supervised learning. Specially, we can consider some heuristic rules with, for example, string and attribute matching to generate the seed mappings, as done by the method IMUSE (He et al., 2019), but the impact of the seed mappings is similar and the study of such impact also benefit the distant supervision methods.\n\nIn addition, although some semi-supervised approaches such as BootEA  and SEA (Pei et al., 2019) are less dependent on the seed mappings, their performance, when trained on a small set of seed mappings, may vary from data to data and be impacted by the bias of the seed mappings.\n\nIn the own evaluation of these methods and the recent benchmark study (Sun et al., 2020), 20% and 10% of all the ground truth mappings are used for training and validation respectively, and more importantly, they are randomly selected, thus maintaining the same distribution as the testing mappings. This violates the real-world scenarios in the industry, where annotating seed mappings is costly and the annotated ones are usually biased, as discussed in Section 1. Actually, there are relatively few studies that investigate the seed mappings and those investigated only consider the proportion of the seeding mappings. In  and Wu et al. (2019), the proposed methods are evaluated with the proportion of the seed mapping for training varying from 10% to 40%. However, the minimum proportion still leads to a very large number (e.g., 1.5K) of seed mappings in aligning two big KGs.\n\n\nBenchmarks\n\nThe current benchmarks used to evaluate the embedding-based methods are typically extracted from DBpedia, Wikidata, and YAGO. They can be divided into two categories. The first includes those for cross-lingual entity alignment such as DBP15K (Sun et al., 2017) and WK3l60k (Chen et al., 2018), both of which support the alignment between DBpedia entities in English and DBpedia entities in other languages, such as Chinese or French. These benchmarks usually only support within KG alignment. The second includes those for cross-KG entity alignment such as DWY15K (Guo et al., 2018) and DWY100K , both of which are for the alignment between DBpedia and Wikidata/YAGO.\n\nAs discussed in Sun et al. (2020), entities in these aforementioned benchmarks have a significant bias in comparison with normal entities in the original KGs; for example, those DBpedia entities in WK3l60k have an average connection degrees of 22.77 while that of all DBpedia entities is 6.93. Thus, these benchmarks are not representative to DBpedia, Wikidata, and YAGO. To address this issue, Sun et al. (2020) proposed a new iterative degree-based sampling algorithm to extract new benchmarks for both cross-lingual entity alignment within DBpedia and cross-KG entity alignment between DBpedia and Wikidata/YAGO. Although the new benchmarks are more representative w.r.t. the graph structure, the entity labels defined by rdfs:label are removed, which include important name information, which makes them less representative to real-world alignment contexts. More importantly, since DBpedia, Wikidata, and YAGO are constructed from the same source Wikipedia, the entities for alignment often have similar names, attributes, or graph structures. These benchmarks are therefore not applicable in the real-world alignment which in contrast, aims at KGs from different sources to complement each other. To make an industry evaluation, we constructed a new benchmark from two industrial KGs (cf. Section 3.1).\n\nIt is worth noting that Ontology Alignment Evaluation Initiatives 1 has been organizing a KG track since 2018 (Hertling and Paulheim, 2020). The benchmarks used are those KGs extracted from several different Wikis from Fandom; 2 for example, starwars-swg is a benchmark with mappings between two KGs from Star Wars Wiki and Star Wars Galaxies Wiki. Multiple benchmarks are adopted, but their scales are limited; for example, 4 out of 5 used in 2019 have less than 2K entity mappings. As the two KGs of a benchmark are about two hubs of one concrete topic (such as the movie and the game of Star Wars), the entity name has little ambiguity and becomes a superior indicator for alignment. Thus they are not suitable industrial benchmarks for evaluating the embedding-based entity alignment methods.\n\n\nData Generation\n\n\nIndustrial Benchmark\n\nTo evaluate the embedding-based entity alignment methods in an industrial context as discussed above, we first extract a benchmark from two real-world medical KGs for alignment. One KG is built upon multiple authoritative medical resources, covering fine-grained knowledge about illness, symptoms, medicine, etc. It is deployed to support applications such as question answering and medical assistants in our company. However, some of its entities have incomplete information with many important attributes missing, which limits its usability. We extract around 10K such entities according to the feedback from downstream applications. They are then aligned with another KG to improve the information completeness. That KG is extracted from the information boxes of Baidu Baike 3 , the largest Chinese encyclopedia, via NLP techniques (such as NER and RE) as well as some handcrafted engineering work. We refer to crowdsourcing for annotating the mappings, where heuristic rules, based on labels and synonyms, and a friendly interface for supporting information check are used for assistance. Finally, we obtain 9, 162 oneto-one entity mappings, based on which one sub-KG is extracted from one original KG. Specifically, the sub-KG includes triples that are composed of entities associated with these mappings. The two sub-KGs are named as MED and BBK, and the new benchmark is named as MED-BBK-9K. More details of MED-BBK-9K and another benchmark D-W-15K, which is extracted by the iterative degree-based sampling method under the setting of V2 (Sun et al., 2020), are shown in Table 1, where # denotes the number and degree is the rate between the triple number and the entity number. Statistics of relation triples and attribute triples are separately presented in Table 1. Note that a relation is equivalent to an object property connecting two entities, while an attribute is equivalent to a data property associating an entity with a value of some data type. Two entity mapping examples of MED-BBK-9K are depicted in Figure 2, where the green ellipses indicate the aligned entities across KGs, the white ellipses and the solid arrows indicate their relation triples 4 , and the red rectangles and the dash arrows indicate the attributes which include normal values, sentence descriptions, and noisy values. Through the statistics and the examples, we can conclude that KGs in MED-BBK-9K are quite different from KGs in D-W-15K, with a higher relation degree, less attributes, higher heterogeneity, etc.\n\n\nBiased Seed Mappings\n\nBesides the industrial benchmark, we also develop a new approach to extract biased seed mappings for the industrial context. We first introduce two variables, s name and n attr , in which s name is the normalized Levenshtein Distance -an edit distance metric (Navarro, 2001) in [0, 1] for the name strings of entities of each mapping, and n attr is the average number of attributes of entities of each mapping. For Wikidata entities in D-W-15K, we use the attribute values of P373 and P1476 as the entity names, while for DBpedia entities we use the entity name in the URI. Note when one or both entities in one mapping has multiple names, we adopt the two names leading to the highest similarity i.e., the lowest s name . Meanwhile, all the names are pre-processed before calculating s name : dash, underline and backslash are replaced by the white space, punctuation marks are removed, letters are transformed into lowercase.\n\nWith s name and n attr calculated, we divide all the mappings into three different splits according to either the name similarity or the attribute number. For the name similarity, the mappings are divided into \"same\" (s name =1.0), \"close\" (s name < 1.0) and \"different\" (s name is NA, i.e., no valid entity name) for both MED-BBK-9K and D-W-15K. From the attribute number, the mappings are divided into \"large\" (n attr \u2265 k 1 ), \"medium\" (k 2 \u2264 n attr < k 1 ) and \"small\" (n attr < k 2 ), where (k 1 , k 2 ) are set to (5, 2) for MED-BBK-9K and set to (10, 4) for D-W-15K.\n\nWe further develop an iterative algorithm to extract the seed mappings with name bias and attribute bias. Its steps are shown below, with two inputs, namely the set of all the mappings M all and the size of seed mappings N seed , and one output, namely the set of biased seed mappings M seed .\n\n(1) Initialize the biased seed mapping set M seed .\n\n(2) Assign each mapping in M all a score: z = z name + z attr , where z name is set to 4, 3 and 1 if the mapping belongs to \"same\", \"close\" and \"different\" respectively, and z attr is set to 4, 3 and 1 if the mapping belongs to \"large\", \"medium\" and \"small\" respectively. Note all the mappings in M all are assigned a score of 8, 7, 6, 5, 4, or 2.\n\n(3) Move the mapping with the highest score in M all to M seed . Randomly select one if multiple mappings in M all have the highest score. (4) Check whether the size of M seed has been equal to or larger than N seed . If yes, return M seed ; otherwise, go to Step (3).\n\nWith the above procedure, we can also obtain seed mappings that are name biased alone by setting z = z name , and seed mappings that are attribute biased alone by setting z = z attr . Note the seed mappings M seed include both training mappings and validation mappings. In our experiment, the former occupies two thirds of the seed mappings while the latter occupies one third.\n\n\nEvaluation\n\n\nExperimental Setting\n\nWe first conduct the overall evaluation (cf. Section 4.2). Specifically, the methods BootEA, MultiKE, RDGCN, and RSN4EA are tested under (i) an industrial context where the seed mappings are both name biased and attribute biased, and the rate of training (resp. validation) mappings is 2% (resp. 1%), and (ii) an ideal context where the seed mappings are randomly selected without bias, and the rate of training (resp. validating) mappings is 20% (resp. 10%). We then conduct ablation studies where three impacts of seed mappings are independently analysed, including size, name bias, and attribute bias.\n\nIn both overall evaluation and ablation studies, we calculate metrics Hits@1, Hits@5, and mean reciprocal rank (MRR) with all the testing mappings. For each testing mapping, the candidate entities (i.e., all the entities in the target KG) are ranked according to their predicted scores; Hits@1 (resp. Hits@5) is the ratio of testing mappings whose ground truths are ranked in the top 1 (resp. 5) entities; MRR is the Mean Reciprocal Rank of the ground truth entity. Meanwhile, to further analyse the impact of the seed mappings on different kinds of testing mappings, we divide the testing mappings into two three-fold splits -\"same\", \"close\" and \"different\" from the name biased aspect, and \"small\", \"medium\" and \"large\" from the attribute biased aspect.\n\nWe adopt the implementation of BootEA, MultiKE, RDGCN, and RSN4EA in OpenEA, while their hyperparameters are adjusted with the validation set. Specifically, the batch size is set to 5000, the early stopping criterion is set to when Hits@1 begins to drop on the validation set (checked for every 10 epochs), the maximum epoch number is set to 2000. As MultiKE and RDGCN utilize literals, the word embeddings are produced using a fastText model pre-trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset 5 . To run them on MED-BBK-9K, the Chinese word embeddings are obtained via a medical-specific BERT model pre-trained on big medical corpora from Tencent Technology 6 .\n\nWe finally compare these embedding-based methods with a state-of-the-art conventional system named PARIS (v0.3) 7 , which is based on lexical matching and iterative calculation of relation mappings, class mappings and entity mappings with their correlations (logic consistency) considered (Suchanek et al., 2011). We adopt the default hyperparameters to PARIS. Note that PARIS requires no seed mappings for supervision. As PARIS does not rank all the candidate entities, we use Precision, Recall, and F1score as the evaluation metrics. For the embedding-based methods, Hits@1 in our one-to-one mapping evaluation is equivalent to Precision, Recall, and F1-score. Table 2 presents the results of those embedding-based methods on both D-W-15K and MED-BBK-9K under the ideal context and the industrial context. On one hand, we find that the performance of all four methods dramatically decreases when the testing context is moved from the ideal to the industrial, the latter of which is much more challenging with less and biased seed mappings. For instance, considering the average MRR of all four methods on all testing mappings, it drops from 0.661 to 0.262 on D-W-15K, and from 0.327 to 0.118 on MED-BBK-9K.\n\n\nOverall Results\n\nWe also find that the performance decreasement, when moved to the industrial context, varies from one testing mapping split to another. Considering the name-based splitting, the decreasement is the most significant on the \"different\" split, and the least significant on the \"same\" split. Take MultiKE on MED-BBK-9K as an example, its Hits@1 decreases by 11.4%, 13.9% and 43.1% on the \"same\", \"close\" and \"different\" splits respectively. As a result, the methods including MultiKE and RDGCN perform better on the \"same\" split than on the \"close\" and the \"different\" splits. It meets our expectations because the seed mappings in the industrial context, which are sampled with a bias toward those with high name similarity, are close to the \"same\" split and far away from the \"different\" split. However, such a regular is violated when we consider the attribute based seed mapping splits. As to MultiKE tested by the \"large\" testing split, its performance decreasement when moved to the industrial context is the least significant on D-W-15K, which is as expected, but is the most significant on MED-BBK-9K. Thus MultiKE performs worse on the \"large\" testing split than on the \"small\" testing split (with 28.9% lower Hits@1), although the former is more close to the seed mappings. One potential explanation is that mappings with more than 5 attributes (mappings in the \"large\" testing split) in MED-BBK-9K tend to have duplicate attributes and some attribute values are sentences that cannot be fully utilized by these methods.\n\nOn the other hand, we find that MultiKE and RDGCN are much more robust than BootEA and RSN4EA in the industrial context on both D-W-15K and MED-BBK-9K. Although MultiKE and RDGCN do not perform as well as in the ideal context, their performance is still promising. Specifically, when measured by all testing mappings, RDGCN performs better than MultiKE on D-W-15K with 27.9% higher MRR and 32.9% higher Hits@1 but performs worse than MultiKE on MED-BBK-9K with 21.3% lower MRR and 11.7% lower Hits@1. The performance of BootEA and RSN4EA is poor in the industrial context; their Hits@1, Hits@5, and MRR on all testing mappings or on different testing splits are all lower than 0.1 for both benchmarks. This means that they are very sensitive to the size or/and the bias of the seed mappings (cf. Section 4.3 for the ablation studies).\n\n\nAblation Studies\n\n\nSize Impact\n\nAccording to the results in the \"With No Bias\" setting in Table 3, we can first find that MultiKE and RDGCN are relatively robust w.r.t. a small training mapping size. Considering their Hits@1 measured on all the test mappings, it drops slightly from 0.484 to 0.394 and from 0.629 to 0.513 respectively when the training mapping size is significantly reduced from 20% to 2%. On the \"same\" testing split and the \"large\" testing split, both of which are close to the training mappings, the performance of MultiKE and RDGCN keeps relatively good when trained by 2% of the mappings. On the other two splits, which are more biased compared with training mappings, the performance of MultiKE and RDGCN, however, decreases more significantly.\n\nFurthermore, we find that BootEA and RSN4EA are very sensitive to the training mapping size. For example, the MRR of BootEA (resp. RSN4EA) measured by all the test mappings decreases from 0.864 to 0.153 to 0.051 (resp. from 0.717 to 0.132 to 0.044) when the training ratio decreases from 20% to 4% to 2%. The performance of BootEA is beyond our expectation as it is a semi-supervised algorithm designed for a limited number of training samples. Besides all the testing mappings, their performance decreasement is also quite significant on different testing splits including the \"same\" and the \"large\". \n\n\nName Bias Impact\n\nThe name bias impact from the seed mappings can be evaluated by comparing the settings of \"With Name Bias\" and \"With No Bias\" in Table 3. With 20% of the mappings for training, MultiKE and RDGCN are more negatively impacted by the name bias than BootEA and RSN4EA; for example, the MRR measured by all the test mappings drops by 52.9% and 41.5% respectively, while that of BootEA and RSN4EA drops only by 18.8% and 19.1% respectively. Specifically, considering different testing mapping splits, the negative impact on MultiKE and RDGCN mainly lies in the \"different\" split (e.g., Hits@1 of RDGCN drops from 0.305 to 0.111), while the impact on the \"same\" and the \"close\" is relatively limited and sometimes even positive. Mappings in the \"different\" testing split, which have very biased distributions as the training mappings, are sometimes known as long-tail prediction cases, and the above phenomena indicate their universality and difficulty in an industrial context. On the other hand, the negative impact of name bias on MultiKE and RDGCN is still much less than the negative impact of the small size on BootEA and RSN4EA. Thus when impacted by both small size (using 2% of the mappings for training) and name bias, BootEA and RSN4EA perform poorly. It is also worth noting that RDGCN outperforms other methods by a large margin in the \"close\" split under all the experimental settings; for example, its Hits@1 reaches 0.905 and 0.871 with 4% and 2% training mappings while that for MultiKE is only 0.209 and 0.195 respectively.\n\n\nAttribute Bias Impact\n\nThe attribute bias impact from the seed mappings can be analysed by comparing the settings of \"With Attribute Bias\" and \"With No Bias\" in Table 3. When 20% mappings are used for training, its negative impact on all four methods are similar; for example, the MRR of BootEA, MultiKE, RDGCN, and RSN4EA on all testing mappings drops by 28.1%, 28.2%, 30.3%, and 23.8% respectively. The negative impact is especially significant on the \"small\" testing split as its average attribute number is very different from that of the training mappings. In contrast, the impact on the \"large\" testing split is even positive for all four methods; for example, when trained by 4% of the mappings, Hits@1 of RSN4EA increases from 0.133 to 0.228. Especially, under the attribute bias, reducing the training mappings size has limited impact on MultiKE and RDGCN, and sometimes the impact is even positive that for example, the MRR of MultiKE and RDGCN on all testing mappings increases by 5.8% and 10.4% respectively when the training mapping ratio drops from 20% to 4%.\n\n\nComparison with Conventional System\n\nThis subsection presents the comparison between the embedding-based methods and the conventional system PARIS (Suchanek et al., 2011), using results in both Table 2 and Table 4. Note that Hits@1 in Table 2 is equivalent to Precision, Recall, and F1-Score in our evaluation with all one-to-one mappings. Although PARIS is an automatic system needing no supervision, it still significantly outperforms all four embedding based methods on both D-W-15K and MED-BBK-9K. On MED-BBK-9K whose two KGs for alignment are more heterogeneous, the outperformance of PARIS is even more significant; for example, the F1-score of PARIS is 0.493, while the best of the four embedding based methods is 0.307 (resp. 0.179) when trained in the ideal (resp. industrial) context. One important reason we believe is that these embedding based methods ignore the overall reasoning and the correlation of different mappings, while PARIS utilizes them by an iterative workflow and makes holistic decisions. Luckily, such reasoning capability and inter-mapping correlations can also be considered in the embedding-based methods, and this indicates an important direction for the future industrial application. \n\n\nConclusion and Discussion\n\nIn this study, we evaluate four state-of-the-art embedding-based entity alignment methods in an ideal context and an industrial context. To build the industrial context, a new benchmark is constructed with two real-world KGs, and the seed mappings are extracted with different sizes, different name and attribute biases. The performance of all four investigated methods dramatically drops when being evaluated in the industrial context, worse than the traditional system PARIS. Specifically, MultiKE and RDGCN are sensitive to name and attribute bias but robust to seed mapping size; BootEA and RSN4EA are extremely sensitive to seed mappings size, leading to poor performance in the industrial context. Based on these empirical findings, we recommend to specifically design strategies in crowdsourcing (with tool assistance) to ensure the annotated samples in different name and attribute distributions. In our industrial context where the seed mappings are limited, adopting MultiKE or RDGCN is demonstrated to be a better choice for cross-KG alignments. Meanwhile, as mentioned in the evaluation, an ensemble of such embedding based methods with PARIS or LogMap, which considers the correlation between mappings, is also a promising solution for better performance. Finally, we also plan to develop a robust model that can utilize a complete set of attributes, especially those with values of textual descriptions.\n\nFigure 2 :\n2Two mapping examples from MED-BBK-9K with English translations.\n\nTable 1 :\n1Statistics of MED-BBK-9K and D-W-15K.Benchmark \nKGs \n#Entities \nRelation \nAttribute \n#Relations \n#Triples \nDegree #Attributes \n#Triples \nDegree \n\nMED-BBK-9K \nMED \n9,162 \n32 \n158,357 \n34.04 \n19 \n11,467 \n1.24 \nBBK \n9,162 \n20 \n50,307 \n10.96 \n21 \n44,987 \n4.91 \n\nD-W-15K \nDBpedia \n15,000 \n167 \n73,983 \n8.55 \n175 \n66,813 \n4.40 \nWikidata \n15,000 \n121 \n83,365 \n10.31 \n457 \n175,686 \n11.59 \n\n\n\nTable 2 :\n2Overall results under the ideal context and the industrial context.Models \nName-based Splits (Hits@1) \nAttr-based Splits (Hits@1) \nAll Test Mappings \nSame \nClose \nDiff. \nSmall \nMedium \nLarge \nHits@1 Hits@5 \nMRR \n\nD-W-15K \nIdeal \n\nBootEA \n.868 \n.902 \n.753 \n.721 \n.821 \n.912 \n.818 \n.922 \n.864 \nMultiKE \n.977 \n.254 \n.216 \n.306 \n.488 \n.661 \n.484 \n.622 \n.554 \nRDGCN \n.942 \n.934 \n.305 \n.330 \n.734 \n.827 \n.629 \n.756 \n.687 \nRSN4EA \n.718 \n.718 \n.579 \n.536 \n.663 \n.753 \n.650 \n.797 \n.717 \n\nIndustrial \nBootEA \n.050 \n.051 \n.023 \n.015 \n.040 \n.053 \n.037 \n.092 \n.065 \nMultiKE \n.968 \n.211 \n.036 \n.086 \n.392 \n.605 \n.368 \n.426 \n.402 \nRDGCN \n.945 \n.872 \n.062 \n.110 \n.559 \n.759 \n.489 \n.539 \n.514 \nRSN4EA \n.055 \n.060 \n.029 \n.016 \n.046 \n.065 \n.043 \n.092 \n.068 \n\nMED-BBK-9K \nIdeal \n\nBootEA \n.334 \n.259 \n.328 \n.388 \n.201 \n.265 \n.307 \n.495 \n.399 \nMultiKE \n.342 \n.173 \n.072 \n.269 \n.149 \n.195 \n.213 \n.367 \n.289 \nRDGCN \n.550 \n.217 \n.056 \n.348 \n.270 \n.242 \n.306 \n.425 \n.365 \nRSN4EA \n.238 \n.121 \n.226 \n.277 \n.114 \n.095 \n.195 \n.311 \n.253 \n\nIndustrial \nBootEA \n.006 \n.003 \n.003 \n.006 \n.002 \n.004 \n.004 \n.011 \n.010 \nMultiKE \n.303 \n.149 \n.041 \n.218 \n.137 \n.155 \n.179 \n.322 \n.252 \nRDGCN \n.329 \n.083 \n.013 \n.201 \n.120 \n.086 \n.158 \n.239 \n.199 \nRSN4EA \n.008 \n.002 \n.007 \n.009 \n.001 \n.000 \n.005 \n.013 \n.011 \n\n\n\nTable 3 :\n3Results on D-W-15K under different settings (biases and ratios) of the training mappings.Settings Models \nName-based Splits (Hits@1) \nAttr-based Splits (Hits@1) \nAll Test Mappings \nSame \nClose \nDiff. \nSmall \nMedium \nLarge \nHits@1 Hits@5 \nMRR \n\nWith No Bias \n\n20% \n\nBootEA \n.868 \n.902 \n.753 \n.721 \n.821 \n.912 \n.818 \n.922 \n.864 \nMultiKE \n.977 \n.254 \n.216 \n.306 \n.488 \n.661 \n.484 \n.622 \n.554 \nRDGCN \n.942 \n.934 \n.305 \n.330 \n.734 \n.827 \n.629 \n.756 \n.687 \nRSN4EA \n.718 \n.718 \n.579 \n.536 \n.663 \n.753 \n.650 \n.797 \n.717 \n\n4% \n\nBootEA \n.104 \n.087 \n.092 \n.078 \n.085 \n.125 \n.096 \n.206 \n.153 \nMultiKE \n.975 \n.217 \n.088 \n.159 \n.440 \n.647 \n.413 \n.513 \n.467 \nRDGCN \n.898 \n.901 \n.123 \n.163 \n.650 \n.754 \n.521 \n.605 \n.562 \nRSN4EA \n.105 \n.079 \n.090 \n.071 \n.078 \n.133 \n.093 \n.168 \n.132 \n\n2% \n\nBootEA \n.024 \n.022 \n.030 \n.028 \n.025 \n.026 \n.026 \n.073 \n.051 \nMultiKE \n.969 \n.224 \n.048 \n.121 \n.428 \n.639 \n.394 \n.463 \n.433 \nRDGCN \n.900 \n.895 \n.107 \n.147 \n.636 \n.761 \n.513 \n.582 \n.547 \nRSN4EA \n.026 \n.015 \n.031 \n.025 \n.021 \n.034 \n.027 \n.056 \n.044 \n\nWith Name Bias \n\n20% \n\nBootEA \n.871 \n.903 \n.535 \n.433 \n.737 \n.931 \n.645 \n.766 \n.702 \nMultiKE \n.978 \n.285 \n.080 \n.085 \n.230 \n.318 \n.185 \n.335 \n.261 \nRDGCN \n.966 \n.924 \n.111 \n.102 \n.521 \n.641 \n.362 \n.441 \n.402 \nRSN4EA \n.786 \n.800 \n.391 \n.271 \n.631 \n.827 \n.514 \n.656 \n.580 \n\n4% \n\nBootEA \n.733 \n.817 \n.358 \n.260 \n.633 \n.802 \n.554 \n.642 \n.596 \nMultiKE \n.971 \n.209 \n.053 \n.106 \n.391 \n.609 \n.358 \n.427 \n.398 \nRDGCN \n.956 \n.905 \n.076 \n.128 \n.616 \n.766 \n.491 \n.544 \n.518 \nRSN4EA \n.198 \n.185 \n.087 \n.051 \n.147 \n.228 \n.138 \n.228 \n.182 \n\n2% \n\nBootEA \n.031 \n.031 \n.017 \n.013 \n.026 \n.034 \n.024 \n.069 \n.049 \nMultiKE \n.968 \n.195 \n.027 \n.093 \n.389 \n.617 \n.360 \n.404 \n.388 \nRDGCN \n.956 \n.871 \n.056 \n.118 \n.606 \n.766 \n.490 \n.541 \n.516 \nRSN4EA \n.054 \n.040 \n.027 \n.018 \n.036 \n.062 \n.038 \n.084 \n.062 \n\nWith Attribute Bias \n\n20% \n\nBootEA \n.789 \n.870 \n.397 \n.365 \n.734 \n.936 \n.565 \n.682 \n.621 \nMultiKE \n.975 \n.358 \n.078 \n.145 \n.488 \n.767 \n.334 \n.459 \n.398 \nRDGCN \n.946 \n.919 \n.109 \n.168 \n.667 \n.885 \n.437 \n.522 \n.479 \nRSN4EA \n.725 \n.816 \n.309 \n.277 \n.670 \n.834 \n.489 \n.611 \n.546 \n\n4% \n\nBootEA \n.704 \n.819 \n.337 \n.245 \n.622 \n.800 \n.538 \n.611 \n.574 \nMultiKE \n.972 \n.211 \n.057 \n.115 \n.430 \n.662 \n.383 \n.450 \n.421 \nRDGCN \n.922 \n.908 \n.091 \n.133 \n.630 \n.798 \n.501 \n.557 \n.529 \nRSN4EA \n.192 \n.213 \n.083 \n.056 \n.156 \n.228 \n.141 \n.232 \n.185 \n\n2% \n\nBootEA \n.052 \n.051 \n.023 \n.017 \n.039 \n.059 \n.037 \n.094 \n.066 \nMultiKE \n.968 \n.229 \n.041 \n.104 \n.426 \n.651 \n.384 \n.449 \n.421 \nRDGCN \n.915 \n.895 \n.078 \n.122 \n.615 \n.785 \n.497 \n.552 \n.524 \nRSN4EA \n.068 \n.073 \n.027 \n.018 \n.050 \n.083 \n.049 \n.096 \n.073 \n\n\n\nTable 4 :\n4Results of conventional system PARIS on D-W-15K and MED-BBK-9K.Benchmark \nMetric \nName-based Splits \nAttr-based Splits \nAll Test Mappings \nSame \nClose \nDiff. \nSmall \nMedium \nLarge \n\nD-W-15K \n\nPrecision \n.998 \n.998 \n.900 \n.868 \n.980 \n.999 \n.956 \nRecall \n.980 \n.975 \n.707 \n.640 \n.914 \n.987 \n.846 \nF1-score \n.989 \n.986 \n.792 \n.736 \n.946 \n.993 \n.898 \n\nMED-BBK-9K \n\nPrecision \n.910 \n.669 \n.778 \n.879 \n.748 \n.757 \n.814 \nRecall \n.505 \n.248 \n.258 \n.417 \n.293 \n.314 \n.354 \nF1-score \n.649 \n.362 \n.388 \n.565 \n.422 \n.444 \n.493 \n\n\nhttp://oaei.ontologymatching.org/ 2 http://www.fandom.com/ 3 https://baike.baidu.com/ 4 label here indicates a specific relation. Please do not be confused with rdfs:label of the W3C standard.\nThe word embeddings are publicly available at https://fasttext.cc/docs/en/english-vectors.html. 6 Other Chinese word embedding models would suffice to reproduce comparable experimental results. 7 http://webdam.inria.fr/paris/\nAcknowledgmentsJiaoyan Chen's contribution is supported by the AIDA project (Alan Turing Institute), the SIRIUS Centre for Scalable Data Access (Research Council of Norway), Samsung Research UK, Siemens AG, and the EPSRC projects AnaLOG (EP/P025943/1), OASIS (EP/S032347/1) and UK FIRES (EP/S019111/1).\nDBpedia: A nucleus for a web of open data. S\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, The Semantic Web. SpringerS\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. DB- pedia: A nucleus for a web of open data. In The Semantic Web, pages 722-735. Springer.\n\nActive learning for entity alignment. Max Berrendorf, Evgeniy Faerman, Volker Tresp, arXiv:2001.08943arXiv PreprintMax Berrendorf, Evgeniy Faerman, and Volker Tresp. 2020. Active learning for entity alignment. arXiv Preprint, January. arXiv: 2001.08943.\n\nTranslating embeddings for modeling multi-relational data. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, Oksana Yakhnenko, Advances in Neural Information Processing Systems. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Trans- lating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems, pages 2787-2795.\n\nCo-training embeddings of knowledge graphs and entity descriptions for cross-lingual entity alignment. Muhao Chen, Yingtao Tian, Kai-Wei Chang, Steven Skiena, Carlo Zaniolo, Proceedings of the 27th International Joint Conference on Artificial Intelligence. the 27th International Joint Conference on Artificial IntelligenceMuhao Chen, Yingtao Tian, Kai-Wei Chang, Steven Skiena, and Carlo Zaniolo. 2018. Co-training embeddings of knowledge graphs and entity descriptions for cross-lingual entity alignment. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, pages 3998-4004.\n\nCorrecting knowledge base assertions. Jiaoyan Chen, Xi Chen, Ian Horrocks, Erik B Myklebust, Ernesto Jimenez-Ruiz, Proceedings of The Web Conference 2020, WWW '20. The Web Conference 2020, WWW '20New York, NY, USAAssociation for Computing MachineryJiaoyan Chen, Xi Chen, Ian Horrocks, Erik B. Myklebust, and Ernesto Jimenez-Ruiz. 2020. Correcting knowledge base assertions. In Proceedings of The Web Conference 2020, WWW '20, page 1537-1547, New York, NY, USA. Association for Computing Machinery.\n\nImplicit bias in crowdsourced knowledge graphs. Gianluca Demartini, Companion Proceedings of The 2019 World Wide Web Conference. Gianluca Demartini. 2019. Implicit bias in crowdsourced knowledge graphs. In Companion Proceedings of The 2019 World Wide Web Conference, pages 624-630.\n\nLinked data quality of DBpedia, Freebase, OpenCyc, Wikidata, and YAGO. Semantic Web. Michael F\u00e4rber, Frederic Bartscherer, Carsten Menne, Achim Rettinger, 9Michael F\u00e4rber, Frederic Bartscherer, Carsten Menne, and Achim Rettinger. 2018. Linked data quality of DBpe- dia, Freebase, OpenCyc, Wikidata, and YAGO. Semantic Web, 9(1):77-129.\n\nRecurrent skipping networks for entity alignment. Lingbing Guo, Zequn Sun, Ermei Cao, Wei Hu, arXiv:1811.02318arXiv PreprintLingbing Guo, Zequn Sun, Ermei Cao, and Wei Hu. 2018. Recurrent skipping networks for entity alignment. arXiv Preprint arXiv:1811.02318.\n\nUnsupervised entity alignment using attribute triples and relation triples. Fuzhen He, Zhixu Li, Yang Qiang, An Liu, Guanfeng Liu, Pengpeng Zhao, Lei Zhao, Min Zhang, Zhigang Chen, International Conference on Database Systems for Advanced Applications. SpringerFuzhen He, Zhixu Li, Yang Qiang, An Liu, Guanfeng Liu, Pengpeng Zhao, Lei Zhao, Min Zhang, and Zhigang Chen. 2019. Unsupervised entity alignment using attribute triples and relation triples. In International Confer- ence on Database Systems for Advanced Applications, pages 367-382. Springer.\n\nThe knowledge graph track at OAEI. Sven Hertling, Heiko Paulheim, European Semantic Web Conference. SpringerSven Hertling and Heiko Paulheim. 2020. The knowledge graph track at OAEI. In European Semantic Web Conference, pages 343-359. Springer.\n\nLogMap: Logic-based and scalable ontology matching. Ernesto Jim\u00e9nez, - Ruiz, Bernardo Cuenca Grau, International Semantic Web Conference. SpringerErnesto Jim\u00e9nez-Ruiz and Bernardo Cuenca Grau. 2011. LogMap: Logic-based and scalable ontology matching. In International Semantic Web Conference, pages 273-288. Springer.\n\nA Survey on Deep Learning for Named Entity Recognition. Jing Li, Aixin Sun, Jianglei Han, Chenliang Li, abs/1812.09449CoRRJing Li, Aixin Sun, Jianglei Han, and Chenliang Li. 2018. A Survey on Deep Learning for Named Entity Recog- nition. CoRR, abs/1812.09449.\n\nFederico Monti, Oleksandr Shchur, Aleksandar Bojchevski, Or Litany, Stephan G\u00fcnnemann, Michael M Bronstein, arXiv:1806.00770Dual-primal graph convolutional networks. arXiv preprintFederico Monti, Oleksandr Shchur, Aleksandar Bojchevski, Or Litany, Stephan G\u00fcnnemann, and Michael M Bron- stein. 2018. Dual-primal graph convolutional networks. arXiv preprint arXiv:1806.00770.\n\nA guided tour to approximate string matching. Gonzalo Navarro, ACM Computing Surveys. 331Gonzalo Navarro. 2001. A guided tour to approximate string matching. ACM Computing Surveys, 33(1):31-88.\n\nOntology matching: A literature review. Lorena Otero-Cerdeira, Francisco J Rodr\u00edguez-Mart\u00ednez, Alma G\u00f3mez-Rodr\u00edguez, Expert Systems with Applications. 422Lorena Otero-Cerdeira, Francisco J. Rodr\u00edguez-Mart\u00ednez, and Alma G\u00f3mez-Rodr\u00edguez. 2015. Ontology match- ing: A literature review. Expert Systems with Applications, 42(2):949-971.\n\nSemi-supervised entity alignment via knowledge graph embedding with awareness of degree difference. Shichao Pei, Lu Yu, Robert Hoehndorf, Xiangliang Zhang, The World Wide Web Conference. Shichao Pei, Lu Yu, Robert Hoehndorf, and Xiangliang Zhang. 2019. Semi-supervised entity alignment via knowledge graph embedding with awareness of degree difference. In The World Wide Web Conference, pages 3130-3136.\n\nThe graph neural network model. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, Gabriele Monfardini, IEEE Transactions on Neural Networks. 201Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2008. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80.\n\nYAGO: A core of semantic knowledge. Fabian M Suchanek, Gjergji Kasneci, Gerhard Weikum, Proceedings of the 16th international conference on World Wide Web. the 16th international conference on World Wide WebFabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. YAGO: A core of semantic knowledge. In Proceedings of the 16th international conference on World Wide Web, pages 697-706.\n\nPARIS: Probabilistic alignment of relations, instances, and schema. Fabian M Suchanek, Serge Abiteboul, Pierre Senellart, Proceedings of the VLDB Endowment. 53Fabian M. Suchanek, Serge Abiteboul, and Pierre Senellart. 2011. PARIS: Probabilistic alignment of relations, instances, and schema. Proceedings of the VLDB Endowment, 5(3).\n\nCross-lingual entity alignment via joint attribute-preserving embedding. Zequn Sun, Wei Hu, Chengkai Li, International Semantic Web Conference. SpringerZequn Sun, Wei Hu, and Chengkai Li. 2017. Cross-lingual entity alignment via joint attribute-preserving embed- ding. In International Semantic Web Conference, pages 628-644. Springer.\n\nBootstrapping entity alignment with knowledge graph embedding. Zequn Sun, Wei Hu, Qingheng Zhang, Yuzhong Qu, Proceedings of the 27th International Joint Conference on Artificial Intelligence. the 27th International Joint Conference on Artificial IntelligenceZequn Sun, Wei Hu, Qingheng Zhang, and Yuzhong Qu. 2018. Bootstrapping entity alignment with knowledge graph embedding. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, pages 4396-4402.\n\nA Benchmarking Study of Embedding-Based Entity Alignment for Knowledge Graphs. Zequn Sun, Qingheng Zhang, Wei Hu, Chengming Wang, Muhao Chen, Farahnaz Akrami, Chengkai Li, Proc. VLDB Endow. 1312Zequn Sun, Qingheng Zhang, Wei Hu, Chengming Wang, Muhao Chen, Farahnaz Akrami, and Chengkai Li. 2020. A Benchmarking Study of Embedding-Based Entity Alignment for Knowledge Graphs. Proc. VLDB Endow., 13(12):2326-2340, July.\n\nWikidata: A Free Collaborative Knowledge Base. Denny Vrande\u010di\u0107, Markus Kr\u00f6tzsch, Communications of the ACM. 5710Denny Vrande\u010di\u0107 and Markus Kr\u00f6tzsch. 2014. Wikidata: A Free Collaborative Knowledge Base. Communica- tions of the ACM, 57(10):78-85.\n\nRelation-aware entity alignment for heterogeneous knowledge graphs. Yuting Wu, Xiao Liu, Yansong Feng, Zheng Wang, Rui Yan, Dongyan Zhao, Proceedings of the 28th International Joint Conference on Artificial Intelligence. the 28th International Joint Conference on Artificial IntelligenceYuting Wu, Xiao Liu, Yansong Feng, Zheng Wang, Rui Yan, and Dongyan Zhao. 2019. Relation-aware entity alignment for heterogeneous knowledge graphs. In Proceedings of the 28th International Joint Conference on Artificial Intelligence, pages 5278-5284.\n\nA survey on entity alignment of knowledge base. Zhuang Yan, Li Guoliang, Feng Jianhua, Journal of Computer Research and Development. 1Zhuang Yan, Li Guoliang, and Feng Jianhua. 2016. A survey on entity alignment of knowledge base. Journal of Computer Research and Development, 1:165-192.\n\nLong-tail relation extraction via knowledge graph embeddings and graph convolution networks. Ningyu Zhang, Shumin Deng, Zhanlin Sun, Guanying Wang, Xi Chen, Wei Zhang, Huajun Chen, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Ningyu Zhang, Shumin Deng, Zhanlin Sun, Guanying Wang, Xi Chen, Wei Zhang, and Huajun Chen. 2019a. Long-tail relation extraction via knowledge graph embeddings and graph convolution networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long and Short Papers), pages 3016-3025, Minneapolis, Minnesota, June. Association for Computational Linguistics.\n\nMulti-view knowledge graph embedding for entity alignment. Qingheng Zhang, Zequn Sun, Wei Hu, Muhao Chen, Lingbing Guo, Yuzhong Qu, Proceedings of the 28th International Joint Conference on Artificial Intelligence. the 28th International Joint Conference on Artificial IntelligenceQingheng Zhang, Zequn Sun, Wei Hu, Muhao Chen, Lingbing Guo, and Yuzhong Qu. 2019b. Multi-view knowl- edge graph embedding for entity alignment. In Proceedings of the 28th International Joint Conference on Artificial Intelligence, pages 5429-5435.\n", "annotations": {"author": "[{\"end\":109,\"start\":62},{\"end\":204,\"start\":110},{\"end\":247,\"start\":205},{\"end\":293,\"start\":248},{\"end\":341,\"start\":294},{\"end\":383,\"start\":342},{\"end\":455,\"start\":384}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":69},{\"end\":122,\"start\":118},{\"end\":212,\"start\":208},{\"end\":258,\"start\":255},{\"end\":306,\"start\":301},{\"end\":348,\"start\":345},{\"end\":396,\"start\":391}]", "author_first_name": "[{\"end\":68,\"start\":62},{\"end\":117,\"start\":110},{\"end\":207,\"start\":205},{\"end\":254,\"start\":248},{\"end\":300,\"start\":294},{\"end\":344,\"start\":342},{\"end\":390,\"start\":384}]", "author_affiliation": "[{\"end\":108,\"start\":76},{\"end\":203,\"start\":149},{\"end\":246,\"start\":214},{\"end\":292,\"start\":260},{\"end\":340,\"start\":308},{\"end\":382,\"start\":350},{\"end\":454,\"start\":422}]", "title": "[{\"end\":59,\"start\":1},{\"end\":514,\"start\":456}]", "venue": null, "abstract": "[{\"end\":1369,\"start\":516}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4411,\"start\":4390},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4468,\"start\":4444},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5140,\"start\":5119},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5165,\"start\":5148},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5195,\"start\":5177},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5387,\"start\":5369},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6112,\"start\":6092},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6979,\"start\":6962},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7200,\"start\":7182},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7473,\"start\":7455},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8031,\"start\":8015},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8542,\"start\":8524},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8574,\"start\":8555},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8864,\"start\":8846},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8984,\"start\":8967},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9363,\"start\":9346},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10399,\"start\":10370},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12663,\"start\":12645},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13906,\"start\":13891},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18884,\"start\":18861},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26340,\"start\":26317}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28914,\"start\":28838},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29309,\"start\":28915},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30592,\"start\":29310},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33192,\"start\":30593},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33722,\"start\":33193}]", "paragraph": "[{\"end\":2901,\"start\":1407},{\"end\":4113,\"start\":2903},{\"end\":5053,\"start\":4183},{\"end\":5539,\"start\":5055},{\"end\":5835,\"start\":5541},{\"end\":6045,\"start\":5837},{\"end\":6309,\"start\":6047},{\"end\":6595,\"start\":6311},{\"end\":7102,\"start\":6613},{\"end\":7383,\"start\":7104},{\"end\":8267,\"start\":7385},{\"end\":8949,\"start\":8282},{\"end\":10258,\"start\":8951},{\"end\":11056,\"start\":10260},{\"end\":13607,\"start\":11099},{\"end\":14559,\"start\":13632},{\"end\":15133,\"start\":14561},{\"end\":15428,\"start\":15135},{\"end\":15481,\"start\":15430},{\"end\":15830,\"start\":15483},{\"end\":16100,\"start\":15832},{\"end\":16479,\"start\":16102},{\"end\":17121,\"start\":16517},{\"end\":17878,\"start\":17123},{\"end\":18570,\"start\":17880},{\"end\":19780,\"start\":18572},{\"end\":21326,\"start\":19800},{\"end\":22162,\"start\":21328},{\"end\":22932,\"start\":22197},{\"end\":23536,\"start\":22934},{\"end\":25091,\"start\":23557},{\"end\":26167,\"start\":25117},{\"end\":27390,\"start\":26207},{\"end\":28837,\"start\":27420}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":12685,\"start\":12678},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":12874,\"start\":12867},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19242,\"start\":19235},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22262,\"start\":22255},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23693,\"start\":23686},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25262,\"start\":25255},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26371,\"start\":26364},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26383,\"start\":26376},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26412,\"start\":26405}]", "section_header": "[{\"end\":1405,\"start\":1371},{\"attributes\":{\"n\":\"2\"},\"end\":4146,\"start\":4116},{\"attributes\":{\"n\":\"2.1\"},\"end\":4181,\"start\":4149},{\"attributes\":{\"n\":\"2.2\"},\"end\":6611,\"start\":6598},{\"attributes\":{\"n\":\"2.3\"},\"end\":8280,\"start\":8270},{\"attributes\":{\"n\":\"3\"},\"end\":11074,\"start\":11059},{\"attributes\":{\"n\":\"3.1\"},\"end\":11097,\"start\":11077},{\"attributes\":{\"n\":\"3.2\"},\"end\":13630,\"start\":13610},{\"attributes\":{\"n\":\"4\"},\"end\":16492,\"start\":16482},{\"attributes\":{\"n\":\"4.1\"},\"end\":16515,\"start\":16495},{\"attributes\":{\"n\":\"4.2\"},\"end\":19798,\"start\":19783},{\"attributes\":{\"n\":\"4.3\"},\"end\":22181,\"start\":22165},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":22195,\"start\":22184},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":23555,\"start\":23539},{\"attributes\":{\"n\":\"4.3.3\"},\"end\":25115,\"start\":25094},{\"attributes\":{\"n\":\"4.4\"},\"end\":26205,\"start\":26170},{\"attributes\":{\"n\":\"5\"},\"end\":27418,\"start\":27393},{\"end\":28849,\"start\":28839},{\"end\":28925,\"start\":28916},{\"end\":29320,\"start\":29311},{\"end\":30603,\"start\":30594},{\"end\":33203,\"start\":33194}]", "table": "[{\"end\":29309,\"start\":28964},{\"end\":30592,\"start\":29389},{\"end\":33192,\"start\":30694},{\"end\":33722,\"start\":33268}]", "figure_caption": "[{\"end\":28914,\"start\":28851},{\"end\":28964,\"start\":28927},{\"end\":29389,\"start\":29322},{\"end\":30694,\"start\":30605},{\"end\":33268,\"start\":33205}]", "figure_ref": "[{\"end\":1458,\"start\":1450},{\"end\":1848,\"start\":1840},{\"end\":2340,\"start\":2332},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13130,\"start\":13122}]", "bib_author_first_name": "[{\"end\":34493,\"start\":34488},{\"end\":34509,\"start\":34500},{\"end\":34523,\"start\":34517},{\"end\":34539,\"start\":34535},{\"end\":34556,\"start\":34549},{\"end\":34574,\"start\":34567},{\"end\":34843,\"start\":34840},{\"end\":34863,\"start\":34856},{\"end\":34879,\"start\":34873},{\"end\":35123,\"start\":35116},{\"end\":35139,\"start\":35132},{\"end\":35156,\"start\":35149},{\"end\":35176,\"start\":35171},{\"end\":35191,\"start\":35185},{\"end\":35592,\"start\":35587},{\"end\":35606,\"start\":35599},{\"end\":35620,\"start\":35613},{\"end\":35634,\"start\":35628},{\"end\":35648,\"start\":35643},{\"end\":36140,\"start\":36133},{\"end\":36149,\"start\":36147},{\"end\":36159,\"start\":36156},{\"end\":36174,\"start\":36170},{\"end\":36176,\"start\":36175},{\"end\":36195,\"start\":36188},{\"end\":36650,\"start\":36642},{\"end\":36969,\"start\":36962},{\"end\":36986,\"start\":36978},{\"end\":37007,\"start\":37000},{\"end\":37020,\"start\":37015},{\"end\":37272,\"start\":37264},{\"end\":37283,\"start\":37278},{\"end\":37294,\"start\":37289},{\"end\":37303,\"start\":37300},{\"end\":37558,\"start\":37552},{\"end\":37568,\"start\":37563},{\"end\":37577,\"start\":37573},{\"end\":37587,\"start\":37585},{\"end\":37601,\"start\":37593},{\"end\":37615,\"start\":37607},{\"end\":37625,\"start\":37622},{\"end\":37635,\"start\":37632},{\"end\":37650,\"start\":37643},{\"end\":38070,\"start\":38066},{\"end\":38086,\"start\":38081},{\"end\":38336,\"start\":38329},{\"end\":38347,\"start\":38346},{\"end\":38369,\"start\":38354},{\"end\":38656,\"start\":38652},{\"end\":38666,\"start\":38661},{\"end\":38680,\"start\":38672},{\"end\":38695,\"start\":38686},{\"end\":38865,\"start\":38857},{\"end\":38882,\"start\":38873},{\"end\":38901,\"start\":38891},{\"end\":38916,\"start\":38914},{\"end\":38932,\"start\":38925},{\"end\":38951,\"start\":38944},{\"end\":38953,\"start\":38952},{\"end\":39286,\"start\":39279},{\"end\":39474,\"start\":39468},{\"end\":39500,\"start\":39491},{\"end\":39502,\"start\":39501},{\"end\":39527,\"start\":39523},{\"end\":39869,\"start\":39862},{\"end\":39877,\"start\":39875},{\"end\":39888,\"start\":39882},{\"end\":39910,\"start\":39900},{\"end\":40205,\"start\":40199},{\"end\":40222,\"start\":40217},{\"end\":40231,\"start\":40229},{\"end\":40250,\"start\":40244},{\"end\":40273,\"start\":40265},{\"end\":40550,\"start\":40544},{\"end\":40552,\"start\":40551},{\"end\":40570,\"start\":40563},{\"end\":40587,\"start\":40580},{\"end\":40975,\"start\":40969},{\"end\":40977,\"start\":40976},{\"end\":40993,\"start\":40988},{\"end\":41011,\"start\":41005},{\"end\":41313,\"start\":41308},{\"end\":41322,\"start\":41319},{\"end\":41335,\"start\":41327},{\"end\":41640,\"start\":41635},{\"end\":41649,\"start\":41646},{\"end\":41662,\"start\":41654},{\"end\":41677,\"start\":41670},{\"end\":42139,\"start\":42134},{\"end\":42153,\"start\":42145},{\"end\":42164,\"start\":42161},{\"end\":42178,\"start\":42169},{\"end\":42190,\"start\":42185},{\"end\":42205,\"start\":42197},{\"end\":42222,\"start\":42214},{\"end\":42527,\"start\":42522},{\"end\":42545,\"start\":42539},{\"end\":42795,\"start\":42789},{\"end\":42804,\"start\":42800},{\"end\":42817,\"start\":42810},{\"end\":42829,\"start\":42824},{\"end\":42839,\"start\":42836},{\"end\":42852,\"start\":42845},{\"end\":43314,\"start\":43308},{\"end\":43322,\"start\":43320},{\"end\":43337,\"start\":43333},{\"end\":43648,\"start\":43642},{\"end\":43662,\"start\":43656},{\"end\":43676,\"start\":43669},{\"end\":43690,\"start\":43682},{\"end\":43699,\"start\":43697},{\"end\":43709,\"start\":43706},{\"end\":43723,\"start\":43717},{\"end\":44598,\"start\":44590},{\"end\":44611,\"start\":44606},{\"end\":44620,\"start\":44617},{\"end\":44630,\"start\":44625},{\"end\":44645,\"start\":44637},{\"end\":44658,\"start\":44651}]", "bib_author_last_name": "[{\"end\":34498,\"start\":34494},{\"end\":34515,\"start\":34510},{\"end\":34533,\"start\":34524},{\"end\":34547,\"start\":34540},{\"end\":34565,\"start\":34557},{\"end\":34579,\"start\":34575},{\"end\":34854,\"start\":34844},{\"end\":34871,\"start\":34864},{\"end\":34885,\"start\":34880},{\"end\":35130,\"start\":35124},{\"end\":35147,\"start\":35140},{\"end\":35169,\"start\":35157},{\"end\":35183,\"start\":35177},{\"end\":35201,\"start\":35192},{\"end\":35597,\"start\":35593},{\"end\":35611,\"start\":35607},{\"end\":35626,\"start\":35621},{\"end\":35641,\"start\":35635},{\"end\":35656,\"start\":35649},{\"end\":36145,\"start\":36141},{\"end\":36154,\"start\":36150},{\"end\":36168,\"start\":36160},{\"end\":36186,\"start\":36177},{\"end\":36208,\"start\":36196},{\"end\":36660,\"start\":36651},{\"end\":36976,\"start\":36970},{\"end\":36998,\"start\":36987},{\"end\":37013,\"start\":37008},{\"end\":37030,\"start\":37021},{\"end\":37276,\"start\":37273},{\"end\":37287,\"start\":37284},{\"end\":37298,\"start\":37295},{\"end\":37306,\"start\":37304},{\"end\":37561,\"start\":37559},{\"end\":37571,\"start\":37569},{\"end\":37583,\"start\":37578},{\"end\":37591,\"start\":37588},{\"end\":37605,\"start\":37602},{\"end\":37620,\"start\":37616},{\"end\":37630,\"start\":37626},{\"end\":37641,\"start\":37636},{\"end\":37655,\"start\":37651},{\"end\":38079,\"start\":38071},{\"end\":38095,\"start\":38087},{\"end\":38344,\"start\":38337},{\"end\":38352,\"start\":38348},{\"end\":38374,\"start\":38370},{\"end\":38659,\"start\":38657},{\"end\":38670,\"start\":38667},{\"end\":38684,\"start\":38681},{\"end\":38698,\"start\":38696},{\"end\":38871,\"start\":38866},{\"end\":38889,\"start\":38883},{\"end\":38912,\"start\":38902},{\"end\":38923,\"start\":38917},{\"end\":38942,\"start\":38933},{\"end\":38963,\"start\":38954},{\"end\":39294,\"start\":39287},{\"end\":39489,\"start\":39475},{\"end\":39521,\"start\":39503},{\"end\":39543,\"start\":39528},{\"end\":39873,\"start\":39870},{\"end\":39880,\"start\":39878},{\"end\":39898,\"start\":39889},{\"end\":39916,\"start\":39911},{\"end\":40215,\"start\":40206},{\"end\":40227,\"start\":40223},{\"end\":40242,\"start\":40232},{\"end\":40263,\"start\":40251},{\"end\":40284,\"start\":40274},{\"end\":40561,\"start\":40553},{\"end\":40578,\"start\":40571},{\"end\":40594,\"start\":40588},{\"end\":40986,\"start\":40978},{\"end\":41003,\"start\":40994},{\"end\":41021,\"start\":41012},{\"end\":41317,\"start\":41314},{\"end\":41325,\"start\":41323},{\"end\":41338,\"start\":41336},{\"end\":41644,\"start\":41641},{\"end\":41652,\"start\":41650},{\"end\":41668,\"start\":41663},{\"end\":41680,\"start\":41678},{\"end\":42143,\"start\":42140},{\"end\":42159,\"start\":42154},{\"end\":42167,\"start\":42165},{\"end\":42183,\"start\":42179},{\"end\":42195,\"start\":42191},{\"end\":42212,\"start\":42206},{\"end\":42225,\"start\":42223},{\"end\":42537,\"start\":42528},{\"end\":42554,\"start\":42546},{\"end\":42798,\"start\":42796},{\"end\":42808,\"start\":42805},{\"end\":42822,\"start\":42818},{\"end\":42834,\"start\":42830},{\"end\":42843,\"start\":42840},{\"end\":42857,\"start\":42853},{\"end\":43318,\"start\":43315},{\"end\":43331,\"start\":43323},{\"end\":43345,\"start\":43338},{\"end\":43654,\"start\":43649},{\"end\":43667,\"start\":43663},{\"end\":43680,\"start\":43677},{\"end\":43695,\"start\":43691},{\"end\":43704,\"start\":43700},{\"end\":43715,\"start\":43710},{\"end\":43728,\"start\":43724},{\"end\":44604,\"start\":44599},{\"end\":44615,\"start\":44612},{\"end\":44623,\"start\":44621},{\"end\":44635,\"start\":44631},{\"end\":44649,\"start\":44646},{\"end\":44661,\"start\":44659}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7278297},\"end\":34800,\"start\":34445},{\"attributes\":{\"doi\":\"arXiv:2001.08943\",\"id\":\"b1\"},\"end\":35055,\"start\":34802},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14941970},\"end\":35482,\"start\":35057},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":49299019},\"end\":36093,\"start\":35484},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":210838962},\"end\":36592,\"start\":36095},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":153314107},\"end\":36875,\"start\":36594},{\"attributes\":{\"id\":\"b6\"},\"end\":37212,\"start\":36877},{\"attributes\":{\"doi\":\"arXiv:1811.02318\",\"id\":\"b7\"},\"end\":37474,\"start\":37214},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":129946212},\"end\":38029,\"start\":37476},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":211531458},\"end\":38275,\"start\":38031},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6601844},\"end\":38594,\"start\":38277},{\"attributes\":{\"doi\":\"abs/1812.09449\",\"id\":\"b11\"},\"end\":38855,\"start\":38596},{\"attributes\":{\"doi\":\"arXiv:1806.00770\",\"id\":\"b12\"},\"end\":39231,\"start\":38857},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":207551224},\"end\":39426,\"start\":39233},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15304024},\"end\":39760,\"start\":39428},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":86510052},\"end\":40165,\"start\":39762},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206756462},\"end\":40506,\"start\":40167},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":207163173},\"end\":40899,\"start\":40508},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1687246},\"end\":41233,\"start\":40901},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":19197215},\"end\":41570,\"start\":41235},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":51605357},\"end\":42053,\"start\":41572},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":212737039},\"end\":42473,\"start\":42055},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14494942},\"end\":42719,\"start\":42475},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":198354047},\"end\":43258,\"start\":42721},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":215966070},\"end\":43547,\"start\":43260},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":67856607},\"end\":44529,\"start\":43549},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":174802832},\"end\":45059,\"start\":44531}]", "bib_title": "[{\"end\":34486,\"start\":34445},{\"end\":35114,\"start\":35057},{\"end\":35585,\"start\":35484},{\"end\":36131,\"start\":36095},{\"end\":36640,\"start\":36594},{\"end\":37550,\"start\":37476},{\"end\":38064,\"start\":38031},{\"end\":38327,\"start\":38277},{\"end\":39277,\"start\":39233},{\"end\":39466,\"start\":39428},{\"end\":39860,\"start\":39762},{\"end\":40197,\"start\":40167},{\"end\":40542,\"start\":40508},{\"end\":40967,\"start\":40901},{\"end\":41306,\"start\":41235},{\"end\":41633,\"start\":41572},{\"end\":42132,\"start\":42055},{\"end\":42520,\"start\":42475},{\"end\":42787,\"start\":42721},{\"end\":43306,\"start\":43260},{\"end\":43640,\"start\":43549},{\"end\":44588,\"start\":44531}]", "bib_author": "[{\"end\":34500,\"start\":34488},{\"end\":34517,\"start\":34500},{\"end\":34535,\"start\":34517},{\"end\":34549,\"start\":34535},{\"end\":34567,\"start\":34549},{\"end\":34581,\"start\":34567},{\"end\":34856,\"start\":34840},{\"end\":34873,\"start\":34856},{\"end\":34887,\"start\":34873},{\"end\":35132,\"start\":35116},{\"end\":35149,\"start\":35132},{\"end\":35171,\"start\":35149},{\"end\":35185,\"start\":35171},{\"end\":35203,\"start\":35185},{\"end\":35599,\"start\":35587},{\"end\":35613,\"start\":35599},{\"end\":35628,\"start\":35613},{\"end\":35643,\"start\":35628},{\"end\":35658,\"start\":35643},{\"end\":36147,\"start\":36133},{\"end\":36156,\"start\":36147},{\"end\":36170,\"start\":36156},{\"end\":36188,\"start\":36170},{\"end\":36210,\"start\":36188},{\"end\":36662,\"start\":36642},{\"end\":36978,\"start\":36962},{\"end\":37000,\"start\":36978},{\"end\":37015,\"start\":37000},{\"end\":37032,\"start\":37015},{\"end\":37278,\"start\":37264},{\"end\":37289,\"start\":37278},{\"end\":37300,\"start\":37289},{\"end\":37308,\"start\":37300},{\"end\":37563,\"start\":37552},{\"end\":37573,\"start\":37563},{\"end\":37585,\"start\":37573},{\"end\":37593,\"start\":37585},{\"end\":37607,\"start\":37593},{\"end\":37622,\"start\":37607},{\"end\":37632,\"start\":37622},{\"end\":37643,\"start\":37632},{\"end\":37657,\"start\":37643},{\"end\":38081,\"start\":38066},{\"end\":38097,\"start\":38081},{\"end\":38346,\"start\":38329},{\"end\":38354,\"start\":38346},{\"end\":38376,\"start\":38354},{\"end\":38661,\"start\":38652},{\"end\":38672,\"start\":38661},{\"end\":38686,\"start\":38672},{\"end\":38700,\"start\":38686},{\"end\":38873,\"start\":38857},{\"end\":38891,\"start\":38873},{\"end\":38914,\"start\":38891},{\"end\":38925,\"start\":38914},{\"end\":38944,\"start\":38925},{\"end\":38965,\"start\":38944},{\"end\":39296,\"start\":39279},{\"end\":39491,\"start\":39468},{\"end\":39523,\"start\":39491},{\"end\":39545,\"start\":39523},{\"end\":39875,\"start\":39862},{\"end\":39882,\"start\":39875},{\"end\":39900,\"start\":39882},{\"end\":39918,\"start\":39900},{\"end\":40217,\"start\":40199},{\"end\":40229,\"start\":40217},{\"end\":40244,\"start\":40229},{\"end\":40265,\"start\":40244},{\"end\":40286,\"start\":40265},{\"end\":40563,\"start\":40544},{\"end\":40580,\"start\":40563},{\"end\":40596,\"start\":40580},{\"end\":40988,\"start\":40969},{\"end\":41005,\"start\":40988},{\"end\":41023,\"start\":41005},{\"end\":41319,\"start\":41308},{\"end\":41327,\"start\":41319},{\"end\":41340,\"start\":41327},{\"end\":41646,\"start\":41635},{\"end\":41654,\"start\":41646},{\"end\":41670,\"start\":41654},{\"end\":41682,\"start\":41670},{\"end\":42145,\"start\":42134},{\"end\":42161,\"start\":42145},{\"end\":42169,\"start\":42161},{\"end\":42185,\"start\":42169},{\"end\":42197,\"start\":42185},{\"end\":42214,\"start\":42197},{\"end\":42227,\"start\":42214},{\"end\":42539,\"start\":42522},{\"end\":42556,\"start\":42539},{\"end\":42800,\"start\":42789},{\"end\":42810,\"start\":42800},{\"end\":42824,\"start\":42810},{\"end\":42836,\"start\":42824},{\"end\":42845,\"start\":42836},{\"end\":42859,\"start\":42845},{\"end\":43320,\"start\":43308},{\"end\":43333,\"start\":43320},{\"end\":43347,\"start\":43333},{\"end\":43656,\"start\":43642},{\"end\":43669,\"start\":43656},{\"end\":43682,\"start\":43669},{\"end\":43697,\"start\":43682},{\"end\":43706,\"start\":43697},{\"end\":43717,\"start\":43706},{\"end\":43730,\"start\":43717},{\"end\":44606,\"start\":44590},{\"end\":44617,\"start\":44606},{\"end\":44625,\"start\":44617},{\"end\":44637,\"start\":44625},{\"end\":44651,\"start\":44637},{\"end\":44663,\"start\":44651}]", "bib_venue": "[{\"end\":34597,\"start\":34581},{\"end\":34838,\"start\":34802},{\"end\":35252,\"start\":35203},{\"end\":35739,\"start\":35658},{\"end\":36257,\"start\":36210},{\"end\":36721,\"start\":36662},{\"end\":36960,\"start\":36877},{\"end\":37262,\"start\":37214},{\"end\":37727,\"start\":37657},{\"end\":38129,\"start\":38097},{\"end\":38413,\"start\":38376},{\"end\":38650,\"start\":38596},{\"end\":39021,\"start\":38981},{\"end\":39317,\"start\":39296},{\"end\":39577,\"start\":39545},{\"end\":39947,\"start\":39918},{\"end\":40322,\"start\":40286},{\"end\":40662,\"start\":40596},{\"end\":41056,\"start\":41023},{\"end\":41377,\"start\":41340},{\"end\":41763,\"start\":41682},{\"end\":42243,\"start\":42227},{\"end\":42581,\"start\":42556},{\"end\":42940,\"start\":42859},{\"end\":43391,\"start\":43347},{\"end\":43872,\"start\":43730},{\"end\":44744,\"start\":44663},{\"end\":35807,\"start\":35741},{\"end\":36308,\"start\":36259},{\"end\":40715,\"start\":40664},{\"end\":41831,\"start\":41765},{\"end\":43008,\"start\":42942},{\"end\":44023,\"start\":43874},{\"end\":44812,\"start\":44746}]"}}}, "year": 2023, "month": 12, "day": 17}