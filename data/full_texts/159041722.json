{"id": 159041722, "updated": "2023-10-07 02:42:10.621", "metadata": {"title": "HellaSwag: Can a Machine Really Finish Your Sentence?", "authors": "[{\"first\":\"Rowan\",\"last\":\"Zellers\",\"middle\":[]},{\"first\":\"Ari\",\"last\":\"Holtzman\",\"middle\":[]},{\"first\":\"Yonatan\",\"last\":\"Bisk\",\"middle\":[]},{\"first\":\"Ali\",\"last\":\"Farhadi\",\"middle\":[]},{\"first\":\"Yejin\",\"last\":\"Choi\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as \u201cA woman sits at a piano,\u201d a machine must select the most likely followup: \u201cShe sets her fingers on the keys.\u201d With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical \u2018Goldilocks\u2019 zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1905.07830", "mag": "2950700477", "acl": "P19-1472", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/ZellersHBFC19", "doi": "10.18653/v1/p19-1472"}}, "content": {"source": {"pdf_hash": "292161a55562f69e95426ba61897e0e684557f11", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1905.07830v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/P19-1472.pdf", "status": "HYBRID"}}, "grobid": {"id": "8c9a31c0a285ed1e13c7bbe4ff3d0db213da0e04", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/292161a55562f69e95426ba61897e0e684557f11.txt", "contents": "\nHellaSwag: Can a Machine Really Finish Your Sentence?\n\n\nRowan Zellers \nSchool of Computer Science & Engineering\nAllen Institute for Artificial Intelligence\nUniversity of Washington\n\n\nAri Holtzman \nSchool of Computer Science & Engineering\nAllen Institute for Artificial Intelligence\nUniversity of Washington\n\n\nYonatan Bisk \nSchool of Computer Science & Engineering\nAllen Institute for Artificial Intelligence\nUniversity of Washington\n\n\nAli Farhadi \nSchool of Computer Science & Engineering\nAllen Institute for Artificial Intelligence\nUniversity of Washington\n\n\nYejin Choi \nSchool of Computer Science & Engineering\nAllen Institute for Artificial Intelligence\nUniversity of Washington\n\n\n\u2660 \u2665 \u2660 \nSchool of Computer Science & Engineering\nAllen Institute for Artificial Intelligence\nUniversity of Washington\n\n\nPaul G Allen \nSchool of Computer Science & Engineering\nAllen Institute for Artificial Intelligence\nUniversity of Washington\n\n\nHellaSwag: Can a Machine Really Finish Your Sentence?\n\nRecent work byZellers et al. (2018)introduced a new task of commonsense natural language inference: given an event description such as \"A woman sits at a piano,\" a machine must select the most likely followup: \"She sets her fingers on the keys.\" With the introduction of BERT (Devlin et al., 2018), near human-level performance was reached. Does this mean that machines can perform human level commonsense inference?In this paper, we show that commonsense inference still proves difficult for even stateof-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (\u010595% accuracy), state-of-the-art models struggle (\u010348%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models.Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.\n\nIntroduction\n\nImagine a woman chasing a dog around outside, trying to give it a bath. What might happen next? Humans can read a narrative like this, shown in Figure 1, and connect it to a rich model of the world: the dog is currently dry and not soapy, and it actively doesn't want to be bathed. Thus, one A woman is outside with a bucket and a dog. The dog is running around trying to avoid a bath. She\u2026 A. rinses the bucket off with soap and blow dry the dog's head. B. uses a hose to keep it from getting soapy. C. gets the dog wet, then it runs away again. D. gets into a bath tub with the dog.\n\nCome to a complete halt at a stop sign or red light. At a stop sign, come to a complete halt for about 2 seconds or until vehicles that arrived before you clear the intersection. If you're stopped at a red light, proceed when the light has turned green. \u2026\n\nA. Stop for no more than two seconds, or until the light turns yellow. A red light in front of you indicates that you should stop. B. After you come to a complete stop, turn off your turn signal.\n\nAllow vehicles to move in different directions before moving onto the sidewalk. C. Stay out of the oncoming traffic. People coming in from behind may elect to stay left or right. D. If the intersection has a white stripe in your lane, stop before this line. Wait until all traffic has cleared before crossing the intersection. Figure 1: Models like BERT struggle to finish the sentences in HellaSwag, even when they come from the same distribution as the training set. While the wrong endings are on-topic, with words that relate to the context, humans consistently judge their meanings to be either incorrect or implausible. For example, option A of the WikiHow passage suggests that a driver should stop at a red light for no more than two seconds. plausible next event is option C-that she'll get the dog wet and it will run away again. When the SWAG dataset was first announced (Zellers et al., 2018), this new task of commonsense natural language inference seemed trivial for humans (88%) and yet challenging for thenstate-of-the-art models (\u010360%), including ELMo (Peters et al., 2018). However, BERT (Devlin et al., 2018) soon reached over 86%, almost human-level performance. One news article on this development was headlined \"finally, a machine that can finish your sentence.\" 1 In this paper, we investigate the following question: How well do deep pretrained models, like BERT, perform at commonsense natural language inference (NLI)? Our surprising conclusion is that the underlying task remains unsolved. Indeed, we find that deep models such as BERT do not demonstrate robust commonsense reasonining ability by themselves. Instead, they operate more like rapid surface learners for a particular dataset. Their strong performance on SWAG is dependent on the finetuning process, wherein they largely learn to pick up on dataset-specific distributional biases. When the distribution of language shifts slightly, performance drops drastically -even if the domain remains identical.\n\nWe study this question by introducing Hella-Swag, 2 a new benchmark for commonsense NLI. We use Adversarial Filtering (AF), a datacollection paradigm in which a series of discriminators is used to select a challenging set of generated wrong answers. AF is surprisingly effective towards this goal: the resulting dataset of 70k problems is easy for humans (95.6% accuracy), yet challenging for machines (\u010350%q. This result holds even when models are given a significant number of training examples, and even when the test data comes from the exact same distribution as the training data. Machine performance slips an additional 5% when evaluated on examples that cover novel concepts from the same domain.\n\nTo make this dataset robust to deep pretrained models, we use a trifecta of state-of-theart generators (Radford et al., 2018), state-ofthe-art discriminators (BERT), and high quality source text. We expand on the SWAG's original video-captioning domain by using WikiHow articles, greatly increasing the context diversity and generation length. Our investigation reveals a Goldilocks zone -roughly three sentences of context, and two generated sentences -wherein generations are largely nonsensical, even though state-of-the-art discriminators cannot reliably tell the difference between these generations and the ground truth.\n\nMore broadly, our paper presents a case-study towards a future of verified progress in NLP, via iterative rounds of building and breaking datasets. If our ultimate goal is to provide reliable benchmarks for challenging tasks, such as commonsense NLI, these benchmarks cannot be static. Instead, they must evolve together with the evolving state-of-2 Short for Harder Endings, Longer contexts, and Lowshot Activities for Situations With Adversarial Generations. Dataset and code at https://rowanzellers.com/hellaswag. the-art. Continued evolution in turn requires principled dataset creation algorithms. Whenever a new iteration of a dataset is created, these algorithms must leverage existing modeling advancements to filter out spurious biases. Only once this cycle becomes impossible can we say that the underlying task -as opposed an individual datasetis solved.\n\n\nBackground\n\nSWAG is a dataset for commonsense NLI. For each question, a model is given a context from a video caption and four ending choices for what might happen next. Only one choice is right -the actual next caption of the video. Obtaining interesting negatives is challenging. Prior work (e.g. Gururangan et al., 2018;Poliak et al., 2018) has found that when humans write the endings to NLI questions, they introduce subtle yet strong class-conditional biases known as annotation artifacts. 3 To address this, Zellers et al. (2018) introduced Adversarial Filtering (AF). An overview is shown in Figure 2. The key idea is to produce a dataset D which is adversarial for any arbitrary split of pD train , D test q. This requires a generator of negative candidates (i.e., wrong endings that vi- olate human notions about how the world works), which we achieve by using a language model. Potential candidates of incorrect answers were massively oversampled from a language model trained on in-domain data, and then selected using an ensemble of adversaries. The selection process happens iteratively: on each iteration, the dataset is randomly partitioned into D train and D test . The ensemble is trained to classify endings as real or generated on D train , then, AF replaces easy-toclassify generations in D test . This process continues until the accuracy of these adversaries converges. Last, humans validate the data to remove adversarial endings that seem realistic.\n\nImportantly, AF creates a final dataset that is challenging to models regardless of the final dataset split. In Section 4, we will use AF as the underlying workhorse to construct an NLI dataset that is easy for humans, yet challenging for machines. This difficulty persists even when models are provided significant training data, and even when this data comes from the same distribution as the test set. This contrasts with past work on adversarial examples (e.g. Jia and Liang, 2017; Glockner et al., 2018;Belinkov and Bisk, 2018) which consider cases where an out-of-distribution test set is constructed to be adversarial.\n\n\nInvestigating SWAG\n\nIn this section, we investigate why SWAG was solved. We focus on BERT, since it is the best  While GPT converges at random, the LM used for SWAG converges at 75%. Right: AF applied to WikiHow generations from GPT, while varying the ending length from one to three sentences. They converge to random, \"40%, and \"50%, respectively. structural patterns, we consider a new scenario, Shuffled. Here the shared context is provided, but the words in each ending choice are randomly permuted. Surprisingly, this reduces BERT performance by less than 10%. Even though BERT was never exposed to randomly shuffled text during pretraining, it easily adapts to this setting, which suggests that BERT is largely performing lexical reasoning over each (context, answer) pair. Finally, when the context is removed and the words in each ending are shuffled, performance drops to 60.4%. While low, this is still higher than ELMo's performance (\u010360% from Zellers et al., 2018). As neither context nor structure is needed to discriminate between human and machine-written endings in a majority of cases, it is likely that systems primarily learn to detect distributional stylistic patterns during finetuning.\n\n\nWhere do the stylistic biases come from?\n\nSWAG was constructed via Adversarial Filtering (AF). Endings were generated via a language model, and then selected to fool a discriminator. To understand why it was solved requires understanding the interplay of AF with respect to SWAG's generators and discriminators.\n\nZellers et al. (2018) used a two-layer LSTM for generation, with shallow stylistic adversarial filters. 6 This setup was robust against ELMo models, but has the shallow LM in particular produced distributional artifacts that BERT picks up on?\n\nTo investigate this, we perform AF using BERT-Large as the discriminator 7 in two settings, comparing generations from Zellers et al. (2018) with those from a finetuned GPT (Radford et al., 2018).\n\nStrikingly, the results, Figure 5 (left), show that the generations used in SWAG are so different from the human-written endings that AF never drops the accuracy to chance; instead, it converges to roughly 75%. On the other hand, GPT's generations are good enough that BERT accuracy drops below 30% over many random subsplits of the data, revealing the importance of the generator.\n\n\nHellaSwag\n\nThe success of BERT implies that high-quality generators and discriminators are crucial to AF's success. However, it does not imply that the underlying task of commonsense NLI -as opposed to a single dataset -is solved. To evaluate this claim requires us to try making a new evolution of the SWAG dataset, one in which artifacts are removed. In this section, we do just that by introducing HellaSwag.\n\n\nActivityNet Captions\n\nWe start by including video captions from the ActivityNet Captions dataset (Krishna et al., 2017). The original SWAG dataset contains these, along with captions from LSMDC (Rohrbach et al., 2017), but for HellaSwag we solely used ActivityNet. In addition to temporal descriptions, ActivityNet also provides activity labels for each caption (e.g. jumping rope). We will use these activity labels as additional structure to test generalization ability.\n\n\nWikiHow: A New Testbed\n\nWe next consider a new and challenging testbed for commonsense reasoning: completing how-to articles from WikiHow, an online how-to manual. We scrape 80k context and follow-up paragraphs from WikiHow, covering such diverse topics as \"how to make an origami owl\" to \"how to survive a bank robbery.\" Each context has at most three sentences, as do the follow-ups.\n\nAF's effectiveness in this new setting is shown in Figure 5 (right). We consider three settings, corresponding to endings that are either one, two, or three sentences long. In all cases, BERT performance begins high (70-90%), but there are enough generations for Adversarial Filtering to lower the final accuracy considerably. While the one-sentence case converges to slightly higher than random -35% when it converges -the two and three sentence cases are higher, at 40% and 50% respectively. Given more context, it becomes easier to classify an ending as machine-or humanwritten. We compromise and use two-sentence generations. Particularly in the two-sentence case, we find ourselves in a Goldilocks zone wherein generations are challenging for deep models, yet as we shall soon see, easy for humans.\n\n\nObtaining high human agreement\n\nHow well can humans distinguish human-written endings from machine generations refined with Adversarial Filtering? In Figure 6, we compare human performance with that of BERT on a random 80%/20% split. We see a contrast between the ActivityNet and WikiHow performance. While ActivityNet starts off harder for BERT (25.5%), it also proves difficult for humans (60%). In contrast, WikiHow starts easier for BERT (41.1%) and humans find the domain almost trivial (93.5%). We hypothesis this discrepancy is due to the lengths of both datasets (Figure 7). WikiHow's 2-sentence generations average 41 tokens, versus 13 for ActivityNet. This gives WikiHow generations three times as many opportunities to make a detectable mistake.\n\nTo ensure high agreement on ActivityNet, we perform several rounds of human filtering, in- : For HellaSwag, we ensure high human agreement through several rounds of annotation. By collecting how likely each ending is we can filter false negative endings -machine generations that sound realistic -and replace them with true negatives. On both subdatasets, BERT performance increases during validation, but the gap to human performance remains wide.  Figure 7: Lengths of ActivityNet and WikiHow; the latter with two-sentence generations. WikiHow is much longer, which corresponds to being easier for humans, while taking longer for AF to converge.\n\ncreasing human performance to 94%. During human validation, crowd workers are given a context and six ending choices, of which one is the true ending, and the other five are from AF. On each iteration, we replace machine-written endings that the worker rated as realistic with new samples. In the end, we keep the 25k best ActivityNet contexts (i.e. those with highest agreement among workers 8 ) and the 45k best WikiHow contexts.  Table 1: Performance of models, evaluated with accuracy (%).We report results on the full validation and test sets (Overall), as well as results on informative subsets of the data: evaluated on in-domain, versus zero-shot situations, along with performance on the underlying data sources (ActivityNet versus WikiHow). All models substantially underperform humans: the gap is over 45% on in-domain categories, and 50% on zero-shot categories. \n\n\nZero-shot categories for evaluation\n\n\nResults\n\nWe evaluate the difficulty of HellaSwag using a variety of strong baselines, with and without massive pretraining. The models share the same format: given a context and an ending, return a logit for that ending. Accordingly, we train our models using a four-way cross-entropy loss, where the objective is to predict the correct ending. In addition to BERT-Large, our comparisons include: a. OpenAI GPT (Radford et al., 2018): A finetuned 12-layer transformer that was pre-trained on the BookCorpus (Zhu et al., 2015). b. Bert-Base: A smaller version of the BERT model whose architecture size matches GPT. c. ESIM+ELMo (Chen et al., 2017;Peters et al., 2018): This is the best-performing ELMo model for NLI, modified slightly so the final output layer is now a four-way softmax over endings. d. LSTM sentence encoder: This is a randomly initialized two-layer bi-LSTM; the second layer's hidden states are max-pooled and fed into an MLP to predict the logit. We consider three variations: GloVe embeddings, ELMo embeddings, or (frozen) BERT-Base embeddings. 9 e. FastText: (Joulin et al., 2017) An off-the-shelf library for bag-of-words text classification. 10 We compare all models to human performance by asking five independent crowd workers to solve the same four-way multiple choice problems; their predictions are combined via majority vote.\n\nOur results, shown in Table 1, hint at the difficulty of the dataset: human performance is over 95%, while overall model performance is below 50% for every model. Surprisingly, despite BERT-Large having been used as the adversarial filter, it still performs the strongest at 47.3% overall. By making the dataset adversarial for BERT, it seems to also have become adversarial for every other model. For instance, while ESIM+ELMo obtained 59% accuracy on SWAG, it obtains only 33.3% accuracy on HellaSwag.\n\nIn addition to pretraining being critical, so too is end-to-end finetuning. Freezing BERT-Base and adding an LSTM on top lowers its overall performance 4.3%. This may help explain why models such as ESIM+ELMo struggled on SWAG, as ELMo isn't updated during finetuning.\n\nWhile BERT is the best model, it still struggles on HellaSwag, and especially so on zero-shot cat-9 For ELMo and BERT-Base, the model learns scalar weights to combine each internal layer of the encoder. 10 This model is trained with binary cross entropy loss.  Figure 9: Transfer experiments from SWAG to Hella-Swag and vice versa, evaluated on the validation sets.\n\nOverall, a BERT-Large that is trained on SWAG hardly generalizes to HellaSwag: it scores 34.6%.\n\negories. Performance drops roughly 5% on the test fold, which suggests that the finetuning is not enough for BERT to learn to generalize to novel activities or how-to categories. Last, we see that WikiHow is a much harder domain that ActivityNet for machines: 45% Bert-Large performance, versus 96.5% for humans. Curiously, it is on this source dataset that we see the smallest gap between OpenAI GPT and BERT. In fact, OpenAI GPT outperforms BERT on Wiki-How, but the reverse is true for ActivityNet. One possibility is that the left-to-right structure of GPT is the right inductive bias for WikiHow -perhaps reasoning bidirectionally over long contexts is too much for a 12-layer transformer to learn.\n\n\nSWAG to HellaSwag transfer\n\nGiven the shared goals and partial domains of SWAG and HellaSwag, it is natural to ask to what extent models can transfer between the two datasets. In Figure 9 we show the results from transfer experiments: models are trained on one dataset and evaluated on the other. 11 The best models are trained on the same dataset that they are evaluated on: training on SWAG and evaluating on HellaSwag lowers performance by 12%; vice versa lowers performance by 15%. The missing domain for HellaSwag models is movie descriptions (LSMDC), still, Hella-Swag models obtain 69% accuracy. On the other hand, SWAG models do not generalize at all to their missing domain, WikiHow (28%), suggesting that learning general commonsense reasoning 11 Note that the ActivityNet splits are different for each dataset. To avoid skewing the results, we report only on the validation video captions that are not in the training sets of either dataset. The overall accuracy is then a weighted average, where ActivityNet examples are weighted proportionately more. This gives a slight advantage to training on SWAG, as it sees all the ActivityNet categories when training.\n\nCategory: Shaving (ActivityNet; In-domain) A bearded man is seen speaking to the camera and making several faces. the man a) then switches off and shows himself via the washer and dryer rolling down a towel and scrubbing the floor. (0.0%) b) then rubs and wipes down an individual's face and leads into another man playing another person's flute. (0.0%) c) is then seen eating food on a ladder while still speaking. (0.0%) d) then holds up a razor and begins shaving his face. (100.0%) Category: Sharpening knives (ActivityNet; Zero-Shot) Two men are in a room and the man with a blue shirt takes out a bench stone and with a little lubricant on the stone takes an knife and explains how to sharpen it. then he a) uses a sharpener to smooth out the stone using the knife. (100.0%) b) shows how to cut the bottom with the knife and place a tube on the inner and corner. (0.0%) c) bends down and grabs the knife and remove the appliance. was hardly necessary to solve SWAG.\n\n\nQualitative examples\n\nWe show several qualitative examples in Figure 10, along with BERT-Large's predictions. BERT does well on some ActivityNet contexts, such as in the first row, where it correctly predicts the ending for a shaving caption. Whereas shaving is in-domain, the second example about sharpening knives is zero-shot. In this context, BERT's answer suggests that one would use a knife to sharpen a stone, rather than vice versa. The last example comes from WikiHow, which appears to be incredibly challenging for BERT. BERT picks answer d, which has more words that match the context of technology (planes, traffic, laptop), but is incoherent. 12 \n\n\nStylistic\n\n\nDiscussion\n\nOur results suggest that HellaSwag is a challenging testbed for state-of-the-art NLI models, even those built on extensive pretraining. The question still remains, though, of where will the field go next?\n\n6.1 How easy might HellaSwag be for future discriminators?\n\nIn this paper, we showed the existence of a Goldilocks zone of text complexity -in which generations are nonsensical, but existing stateof-the-art NLP models cannot tell the difference. How hard will the dataset be for future, even more powerful, models? Answering this question is challenging because these models don't exist (or are unavailable) at the time of writing. However, one remedy is to perform an ablation study on the Adversarial Filtering model used, comparing weaker filters with stronger discriminators. We present our results in Figure 11, and find that while weak discriminators (like the stylistic ensemble used to make SWAG) only marginally reduce the accuracy of BERT-Large, increasing the gap between the filter and the final discriminator is not enough to solve the task. For instance, using a discriminator with 3x the parameters as the adversarial filter (BERT-Large vs. BERT-Base) results in 63% machine accuracy.  Figure 12: Estimated pretraining hours required to reach a desired accuracy on HellaSwag. We estimate perfomance with respect to a RTX 2080 Ti -a modern, fast GPU, and fit a log-linear regression line. An extrapolation suggests that to reach human-level performance on HellaSwag, without algorithmic or computational improvements, would require 10 9 GPU-hours of pretraining (over 100k GPU years).\n\n\nHow well does pretraining scale?\n\nOverall, the current paradigm of pretraining large models on lots of data has made immense progress on NLP benchmarks. Though we expect this trend to continue, it also behooves us to consider its limits. If more compute is indeed the answer for human-level commonsense inference, what would the compute requirements of this hypothetical massive model look like? We investigate this in Figure 12 by comparing the accuracies of known models on Hella-Swag with their computational needs. This estimation is a rough estimate: we convert reported TPU runtimes to our benchmark RTX 2080 Ti GPU using the Roofline model (Williams et al., 2009), which focuses primarily on the bottleneck of loading tensors into GPU memory. Extrapolating from an exponential fit suggests that reaching humanlevel performance on our dataset would require 10 9 GPU hours, or 100k years -unless algorithmic improvements are made.\n\nWhat might these algorithmic improvements look like? These could include architectural advances, better pretraining objectives, and beyond. However, these improvements share the bottleneck of the data source. To answer some Hella-Swag questions correctly without reasoning deeply -like knowing that it is a bad idea to stop at a red light for 'at most two seconds' -might require an exponential number of samples, due to prob-lems of reporting bias (Gordon and Van Durme, 2013). Alternatively, future models might answer correctly only by picking up on spurious patterns, in which case a new development of the benchmark -using these models as adversaries -would place us in the same position as we are right now.\n\nPut another way, for humans to answer Hella-Swag questions requires abstracting away from language and modeling world states instead. We postulate that this is what separates solving the task of commonsense NLI, as opposed to a particular dataset. Indeed, we find that existing deep methods often get fooled by lexical false friends. For example, in the WikiHow example from Figure 10, BERT chooses an ending that matches the technology words in the context, rather than matching the deeper topic: using technology as an excuse for not doing homework.\n\n\nTowards a future of evolving benchmarks\n\nWhat happens when HellaSwag gets solved? We believe the answer is simple: crowdsource another dataset, with the same exact format, and see where models fail. Indeed, in our work we found this to be straightforward from an algorithmic perspective: by throwing in the best known generator (GPT) and the best known discriminator (BERT-Large), we made a dataset that is adversarial -not just to BERT, but to all models we have access to.\n\nWhile this was easy algorithmically, care must be taken from a data curation standpoint. Indeed, we find success exists within a Goldilocks zone: the data source must be complex enough that stateof-the-art generators often make mistakes, while simple enough such that discriminators often fail to catch them. This ties the future of SWAGstyle benchmarks to progress on language generation: until generation is solved, commonsense NLI will remain unsolved. Even recent promising results on scaling up language models (Radford et al., 2019) find problems in terms of consistency, with the best curated examples requiring 25 random seeds.\n\n\nConclusion\n\nIn this paper, we presented HellaSwag, a new dataset for physically situated commonsense reasoning. By constructing the dataset through adversarial filtering, combined with state-of-the-art models for language generation and discrimination, we produced a dataset that is adversarial to the most robust models available -even when models are evaluated on items from the training distribution. In turn, we provided insight into the inner workings of pretrained models, and suggest a path for NLP progress going forward: towards benchmarks that adversarially co-evolve with evolving state-of-the-art models.\n\nings of the 2013 workshop on Automated knowledge\n\n\nSupplemental Material\n\n\nA Adversarial Filtering Setup\n\nIn this subsection, we provide some more details regarding the Adversarial Filtering experiments.\n\nOur version of Adversarial Filtering is mostly the same as Zellers et al. (2018). Details: a. On each iteration, we split the dataset up into 80% training and 20% testing. We don't do anything special for this split (like looking at the video/article IDs). b. For ActivityNet, we use k \" 9 assigned indices for every example. (This corresponds to the number of red columns in Figure 2). For WikiHow, we used k \" 5, since we found that there were fewer good endings produced by the generators after scaling up the sequence length. c. Similarly to Zellers et al. (2018), we train the AF models in a multi-way fashion. Since we use BERT-Large as the discriminator, this matches Devlin et al. (2018)'s model for SWAG: on each training example, the model is given exactly one positive ending and several negative endings, and the model computes probability distribution over the endings through a softmax. However, we also wanted to always report 4-way probability for simplicity. To do this, we train in a 4-way setting (the training set is constructed by subsampling 3 wrong answers from the set of k that are currently assigned to each example). The accuracy values that are reported are done so using the first 3 assigned negatives in dataset D test . d. Sometimes, BERT never converges (accuracy around 25%), so when this happens, we don't do the reassignment.\n\n\nB GPT Setup\n\nWe generate our dataset examples from OpenAI GPT. We finetune the model for two epochs on WikiHow, and 5 epochs on ActivityNet, using the default learning rate of (Radford et al., 2018). Importantly, we generate randomly according to the language model distribution, rather than performing beam search -this would bias the generations towards common words. For the WikiHow endings, we used Nucleus Sampling with p \" 0.98, which means that the probability weights for the tail (those tokens with cumulative probability mass \u0103 0.02) are zeroed out (Holtzman et al., 2019).\n\n\nC BERT setup\n\nWe extensively study BERT in this paper, and make no changes to the underlying architecture or pretraining. For all of the experiments where we provide context, we set up the input to the BERT model like this:\n\n[CLS] A woman is outside with a bucket and a dog. The dog is running around trying to avoid a bath.\n\n[SEP] She gets the dog wet, then it runs away again [SEP] In the case where only the ending is provided, we adopt the BERT-style 'single-span' setting: [CLS] She gets the dog wet, then it runs away again [SEP] D A discussion on BERT Hyperparameters and Instability\n\nIt is worth noting that many of our experiments some instability. On the SWAG experiments, we use the same hyperparameters as (Devlin et al., 2018) -these generally work very well. 13 However, we find that they become a bit unstable when crossing over to make HellaSwag. Here, we discuss some strategies and insight that we picked up on. a. We use a batch size of 64 examples rather than 16, and warm the model up for 20% of the dataset (rather than 10%). This helps the model adapt to SWAG more gradually, without diverging early on. b. For the Adversarial Filtering experiments (for both WikiHow and ActivityNet), we randomize some of the hyperaparmeters on each iteration. We sample a learning rate between 1e-5 and 4e-5, using a log-uniform distribution. These outer ranges were recommended from the original BERT paper. Additionally, with probability 0.5 we use the cased model (where the input isn't originally lowercased before tokenization), rather than the uncased model. c. During adversarial filtering, we used 3 epochs.\n\nHowever, we found that adding more epochs helped the model during fine-tuning on the final dataset HellaSwag. Our best configuration uses 10 epochs. d. While fine-tuning on HellaSwag we used a learning rate of 2e-5.\n\n\nE Human validation\n\nWe performed human validation using the same setup as (Zellers et al., 2018). Humans get six answers to choose from, of which exactly one is the true ending and the other five are from AF. We found that multiple rounds of human validation were especially helpful on ActivityNet. However, it helps to do the human validation in an intelligent way: if the first worker is confused, the answer should be replaced before it goes to the next worker. This is a hard problem, so we adopt the following approach: a. We use best practices on mechanical turk, paying workers fairly (up to 37 cents per HIT on WikiHow). We also used a qualification HIT that was autograded to help filter for workers who are good at the task. Workers who tended to prefer the generated endings over the real ones were dequalified from participating. b. For each worker, we use the summary of their performance so far to estimate Ppanswer i is right|worker rates i as bestq. We can then use this to estimate how confident we are in each answer choice: we want to be confident that workers will not prefer the wrong answers. Also, this allows us to aggregate performance across crowd workers, by multiplying the probabilities for each answer choice. c. On each round of filtering, we keep the 3 wrong endings that workers least prefer (based on the probability scores, along with the right ending. The other two endings are new ones. Particularly on ActivityNet, we found that there are some contexts where the ground truth answer isn't liked by workers. To fix this, we end up taking the best 25k examples from ActivityNet and the best 45k from WikiHow. (By best, we mean the ones with the highest probability that workers will predict the true answer, versus the three easiest-to-guess negatives, as judged by the Naive Bayes model). We make Figure 7 ('The road to HellaSwag') by doing this process (taking the best examples) for each dataset, while varying the number of annotators that are used for getting the scores for each ending. (In the case where there are 0 annotators, we get a random sample).\n\n\nF Human Evaluation\n\nWe do a human evaluation while giving workers the exact same task as is given to the models. Workers are given five endings, and must pick the best one. We obtain human evaluation numbers by combining 5 turkers together, with a majority vote.\n\nWe found that the biggest differences in difficulty in humans were due to domain (WikiHow is easier than ActivityNet). To account for this, we did the human evaluation over 200 examples from WikiHow, and 200 examples from ActivityNet, for each number of previous validators as shown in Figure 7 (0, 1, or 2). To report the accuracy of a split that's mixed between WikiHow and Activity-Net, we use the following formula:\n\nacc WikiHow\u00a8NWikiHow`a cc ActivityNet\u00a8NActivityNet N WikiHow`NActivityNet\n\nHere, acc refers to the accuracy on each dataset as judged by humans, and N is the number of examples from that dataset in the split.\n\n\nG More examples\n\nWe additionally have more validation examples, shown in Figure 2.\n\n\nH In-Domain and Zero-Shot categories\n\nSee Figure 13 for a closer look at the dataset categories.\n\nCategory: Preparing pasta (activitynet; indomain) A kitchen is shown followed by various ingredients and a woman speaking to the camera. She begins showing the ingredients and putting them into a hot boiling pot and stirring around. she a) shows off the oven and begins assembling the cookies in the oven by pushing a button on the oven.    \n\nFigure 5 :\n5Adversarial Filtering (AF) results with BERT-Large as the discriminator. Left: AF applied to ActivityNet generations produced by Zellers et al. (2018)'s language model versus OpenAI GPT.\n\nFigure 6\n6Figure 6: For HellaSwag, we ensure high human agreement through several rounds of annotation. By collecting how likely each ending is we can filter false negative endings -machine generations that sound realistic -and replace them with true negatives. On both subdatasets, BERT performance increases during validation, but the gap to human performance remains wide.\n\nFigure 8 :\n8Examples on the in-domain validation set of HellaSwag, grouped by category label. Our evaluation setup equally weights performance on categories seen during training as well as out-of-domain.\n\n( 0 .Figure 10 :\n0100%) d) stops sharpening the knife and takes out some pieces of paper to show how sharp the knife is as he cuts slivers of paper with the knife. (0.0%) Category: Youth (WikiHow; In-Domain) How to make up a good excuse for your homework not being finished Blame technology. One of the easiest and most believable excuses is simply blaming technology. You can say your computer crashed, your printer broke, your internet was down, or any number of problems. a) Your excuses will hardly seem believable.[substeps] This doesn't mean you are lying, just only that you don't have all the details of how your computer ran at the time of the accident. (0.0%) b) The simplest one to have in a classroom is to blame you entire classroom, not just lab. If you can think of yourself as the victim, why not blame it on technology. (9.4%) c) Most people, your teacher included, have experienced setbacks due to technological problems.[substeps] This is a great excuse if you had a paper you needed to type and print. (29.1%) d) It may also be more believable if you are fully aware that you may be flying at high speed on a plane and need someone to give you traffic report. Your problem might be your laptop failing to charge after a long flight. (61.5%) Example questions answered by BERT-Large. Correct model predictions are blue, incorrect predictions are red. The right answers are bolded.\n\n( 2 .\n22%) b) continues mixing up more ingredients and then puts them all together in a bowl, serving the dish ad sprinkling olive oil around it. (97.8%) c) shows raising and lowering the pot until adding more water and corn syrup. (0.0%) d) places an omelette onto the screen and puts it in the oven to bake. (0.0%) Category: Doing crunches (activitynet; indomain) We see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. the man a) demonstrates how to increase efficient exercise work by running up and down balls. (0.0%) b) moves all his arms and legs and builds up a lot of muscle. (80.9%) c) then plays the ball and we see a graphics and hedge trimming demonstration. (0.0%) d) performs sits ups while on the ball and talking. (19.1%) Category: Sharpening knives (activitynet; zeroshot) A man is seen spinning a blade with his foot on a machine and moving his hands up with down holding a knife. the camera a) pans around and shows a woman moving around in a jump rope machine. (0.0%) b) captures him from several angles while he sharpens the knife with complete concentration. (81.6%) c) pans around and points to a man standing inside the machine as the man continues to move on the machine. (18.4%) d) then pans around to a woman and her daughter who also dance at the show. (0.0%) Category: Layup drill in basketball (activitynet; zeroshot) A female basketball coach is seen instructing a group of girl basketball players who are standing in line on a basketball court. the first girl a) passes to another coach and then runs to the net and takes a layup. (0.0%) b) trying to get the ball to go far past the basket and hit it back towards the basket while her coach continues teaching her. (100.0%) c) walks across the court with the ball and keeps walking then pulling the girls to the other side of the court and the girls begin playing volleyball rhythmically rolling on the floor as the coach helps them follow how to properly do things. (0.0%) d) line up and stand behind a dummy dummy. (0.0%)\n\nFigure 13 :\n13Examples on the in-domain validation set of HellaSwag, grouped by category label. Our evaluation setup equally weights performance on categories seen during training as well as out-of-domain.\n\n\nNo context is provided and each ending is shuffled.known approach at the time of writing. 4 Core to our analysis is investigating how a model trained on Wikipedia and books can be so effectively finetuned for SWAG, a dataset from video captions.Default Ending Only Shuffled Shuffled+ \nEnding Only \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nBERT-Large Accuracy (%) \n\n86.7% \n\n74.8% \n77.0% \n\n60.4% \n\n46.7% \n41.4% \n36.2% \n31.6% \n\nSWAG \n\nHellaSwag \n\nFigure 4: BERT validation accuracy when trained and \nevaluated under several versions of SWAG, with the \nnew dataset HellaSwag as comparison. We compare: \n\nEnding Only No context is provided; just the endings. \nShuffled \nEndings that are indidivually tokenized, \nshuffled, and then detokenized. \nShuffled+ \nEnding Only \n\n3.1 How much innate knowledge does BERT \nhave about SWAG? \n\nWe investigate this question by measuring BERT's \nperformance on SWAG while varying the size of \nthe training dataset; results are shown in Fig-\nure 3. While the best known ELMo NLI model \n(ESIM+ELMo; Chen et al., 2017) requires the en-\ntire training set to reach 59%, BERT outperforms \nthis given only 64 examples. However, BERT still \nneeds upwards of 16k examples to approach hu-\nman performance, around which it plateaus. \n\n3.2 What is learned during finetuning? \n\nFigure 4 compares BERT's performance when \ntrained and evaluated on variants of SWAG. \nContext: BERT's performance only slips 11.9 \npoints (86.7%\u00d174.8%) when context is omitted \n(Ending Only), suggesting a bias exists in the \nendings themselves. 5 If a followup event seems \nunreasonable absent of context, then there must be \nsomething markedly different between the space \nof human-written and machine-generated endings. \nStructure: To distinguish word usage from \n\n\n\nTo evaluate a model's ability to generalize to new situations, we use category labels from WikiHow and ActivityNet to make 'zero-shot' evaluation sets. For each set (validation or test), we craft two subsets: one containing 5k 'in-domain' examples that come from categories as seen during training(Figure 8), and another with 5k 'zero-shot' examples from randomly chosen held-out categories. In total, there are 70k dataset examples.Overall \n\nIn-Domain \nZero-Shot \nActivityNet \nWikiHow \nModel \nVal \nTest \nVal \nTest \nVal \nTest \nVal \nTest \nVal \nTest \nSplit Size\u00d1 10K 10K \n5K \n5K \n5K \n5K \n3.2K 3.5K \n6.8K 6.5K \n\nChance \n25.0 \n\nfastText \n30.9 31.6 \n33.8 32.9 \n28.0 30.2 \n27.7 \n28.4 \n32.4 \n33.3 \nLSTM+GloVe \n31.9 31.7 \n34.3 32.9 \n29.5 30.4 \n34.3 \n33.8 \n30.7 \n30.5 \nLSTM+ELMo \n31.7 31.4 \n33.2 32.8 \n30.4 30.0 \n33.8 \n33.3 \n30.8 \n30.4 \nLSTM+BERT-Base \n35.9 36.2 \n38.7 38.2 \n33.2 34.1 \n40.5 \n40.5 \n33.7 \n33.8 \nESIM+ELMo \n33.6 33.3 \n35.7 34.2 \n31.5 32.3 \n37.7 \n36.6 \n31.6 \n31.5 \nOpenAI GPT \n41.9 41.7 \n45.3 44.0 \n38.6 39.3 \n46.4 \n43.8 \n39.8 \n40.5 \nBERT-Base \n39.5 40.5 \n42.9 42.8 \n36.1 38.3 \n48.9 \n45.7 \n34.9 \n37.7 \nBERT-Large \n46.7 47.3 \n50.2 49.7 \n43.3 45.0 \n54.7 \n51.7 \n42.9 \n45.0 \n\nHuman \n95.7 95.6 \n95.6 95.6 \n95.8 95.7 \n94.0 \n94.0 \n96.5 \n96.5 \n\n\n\n\nCategory: Youth (wikihow; indomain)[header]  How to make up a good excuse for your homework not being finished [title] Blame technology.[step] One of the easiest and most believable excuses is simply blaming technology. You can say your computer crashed, your printer broke, your internet was down, or any number of problems. a) Your excuses will hardly seem believable.[substeps]  This doesn't mean you are lying, just only that you don't have all the details of how your computer ran at the time of the accident. (0.0%) b) The simplest one to have in a classroom is to blame you entire classroom, not just lab. If you can think of yourself as the victim, why not blame it on technology. (9.4%) c) Most people, your teacher included, have experienced setbacks due to technological problems.[substeps] This is a great excuse if you had a paper you needed to type and print. (29.1%) d) It may also be more believable if you are fully aware that you may be flying at high speed on a plane and need someone to give you traffic report. Your problem might be your laptop failing to charge after a long flight. (61.5%) Category: Family Life (wikihow; zeroshot) [header] How to raise your children to be helpers [title] Call them helpers when you ask for things.[step] Instead of asking for help, ask your child to \" be a helper. \" all people, children included, are more motivated when their identity is in play. a) You can start doing this with your children as early as two years old.[substeps] You might say, \" jayden, can you be a helper and clean your bedroom before grandma comes over? \" or \" please be a helper and stay quiet while your sister naps. (0.1%) b) When you call your child helpers, describe what they do and what they need to be helped for.[substeps]  You could say, \" i need you to help dad during his lunch break at work. (99.9%) c) If you ask your child for things they have access to, it encourages them to put more effort into making things happen. [substeps] To make sure they understand exactly what's expected of them, you could try saying, \" i'm looking for helpers who can be helpers. (0.0%) d) Call them when you need them for help or for monetary help. [substeps] For example, if you need help with something you don't know how to do, let your child know you're excited to help with this. (0.0%)\n\nTable 2 :\n2Example questions answered by BERT-Large. Correct model predictions are in blue, incorrect model predictions are red. The right answers are bolded.\nThese biases simply inflate model performance, but past work has also shown that are unwanted social biases induced when humans write the endings, in terms of gender and race(Rudinger et al., 2015).\nSee the appendix for a discussion of the BERT architecture and hyperparameter settings we used in our experiments. 5 These biases are similar to those in NLI datasets, as found by Gururangan et al. (2018); Poliak et al. (2018).\nThe discriminator was an ensemble that featured a bag of words model, a shallow CNN, a multilayer perceptron operating on language model perplexities.\nOn each iteration, BERT-Large is re-initialized from its pretrained checkpoint, finetuned, and then evaluated in a four-way setting on the dummy test set of held-out data. See Supp A for a details of our BERT-Large AF setup.\nSee the appendix for details about how we estimate this.\nAmong other issues, why would someone suddenly be aware that they are 'flying at high speed on a plane...?'\nThe only exception is for the plots where we vary the number of training examples. In this case, we don't want to disadvantage the trials without much training data (since this would allow for fewer parameter updates). To remedy this, we continue training for 10 epochs and report the best validation performance over the entire training history.\nAcknowledgmentsWe thank the reviewers, as well as Jesse Thomason, for their helpful feedback. We thank the Mechanical Turk workers for their great work during dataset collection. Thanks also to Zak Stone and the Google Cloud TPU team for help with the computing infrastructure. This work was supported by the National Science Foundation through a Graduate Research Fellowship (DGE-1256082) and NSF grants (IIS-1524371, 1637479,  165205, 1703166), the DARPA CwC program through ARO (W911NF-15-1-0543), the IARPA DIVA program through D17PC00343, the Sloan Research Foundation through a Sloan Fellowship, the Allen Institute for Artificial Intelligence, the NVIDIA Artificial Intelligence Lab, and gifts by Google and Facebook. The views and conclusions contained herein are those of the authors and should not be interpreted as representing endorsements of IARPA, DOI/IBC, or the U.S. Government.\nSynthetic and natural noise both break neural machine translation. Yonatan Belinkov, Yonatan Bisk, ICLR. ICLRYonatan Belinkov and Yonatan Bisk. 2018. Synthetic and natural noise both break neural machine transla- tion. In ICLR. ICLR.\n\nEnhanced lstm for natural language inference. Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, Diana Inkpen, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational Linguistics1Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced lstm for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), vol- ume 1, pages 1657-1668.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.\n\nBreaking nli systems with sentences that require simple lexical inferences. Max Glockner, Vered Shwartz, Yoav Goldberg, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsShort Papers2Max Glockner, Vered Shwartz, and Yoav Goldberg. 2018. Breaking nli systems with sentences that re- quire simple lexical inferences. In Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics (Volume 2: Short Papers), pages 650-655.\n\nReporting bias and knowledge acquisition. Jonathan Gordon, Benjamin Van Durme, Proceed. eedJonathan Gordon and Benjamin Van Durme. 2013. Re- porting bias and knowledge acquisition. In Proceed-\n", "annotations": {"author": "[{\"end\":183,\"start\":57},{\"end\":309,\"start\":184},{\"end\":435,\"start\":310},{\"end\":560,\"start\":436},{\"end\":684,\"start\":561},{\"end\":803,\"start\":685},{\"end\":929,\"start\":804}]", "publisher": null, "author_last_name": "[{\"end\":70,\"start\":63},{\"end\":196,\"start\":188},{\"end\":322,\"start\":318},{\"end\":447,\"start\":440},{\"end\":571,\"start\":567},{\"end\":816,\"start\":811}]", "author_first_name": "[{\"end\":62,\"start\":57},{\"end\":187,\"start\":184},{\"end\":317,\"start\":310},{\"end\":439,\"start\":436},{\"end\":566,\"start\":561},{\"end\":686,\"start\":685},{\"end\":690,\"start\":687},{\"end\":808,\"start\":804},{\"end\":810,\"start\":809}]", "author_affiliation": "[{\"end\":182,\"start\":72},{\"end\":308,\"start\":198},{\"end\":434,\"start\":324},{\"end\":559,\"start\":449},{\"end\":683,\"start\":573},{\"end\":802,\"start\":692},{\"end\":928,\"start\":818}]", "title": "[{\"end\":54,\"start\":1},{\"end\":983,\"start\":930}]", "venue": null, "abstract": "[{\"end\":2410,\"start\":985}]", "bib_ref": "[{\"end\":4370,\"start\":4348},{\"end\":4556,\"start\":4535},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4593,\"start\":4572},{\"end\":4753,\"start\":4752},{\"end\":6290,\"start\":6268},{\"end\":7984,\"start\":7960},{\"end\":8004,\"start\":7984},{\"end\":8158,\"start\":8157},{\"end\":8197,\"start\":8176},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9645,\"start\":9623},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9669,\"start\":9645},{\"end\":11728,\"start\":11702},{\"end\":12648,\"start\":12626},{\"end\":16953,\"start\":16927},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17166,\"start\":17147},{\"end\":17186,\"start\":17166},{\"end\":17687,\"start\":17685},{\"end\":18856,\"start\":18854},{\"end\":20120,\"start\":20118},{\"end\":24931,\"start\":24908},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25675,\"start\":25647},{\"end\":28484,\"start\":28463},{\"end\":28971,\"start\":28950},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29099,\"start\":29079},{\"end\":29965,\"start\":29943},{\"end\":30349,\"start\":30326},{\"end\":30736,\"start\":30731},{\"end\":30836,\"start\":30831},{\"end\":30888,\"start\":30883},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31091,\"start\":31071},{\"end\":32292,\"start\":32270}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35915,\"start\":35716},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36292,\"start\":35916},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36497,\"start\":36293},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37898,\"start\":36498},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39963,\"start\":37899},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40170,\"start\":39964},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41935,\"start\":40171},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":43179,\"start\":41936},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":45502,\"start\":43180},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":45662,\"start\":45503}]", "paragraph": "[{\"end\":3010,\"start\":2426},{\"end\":3267,\"start\":3012},{\"end\":3464,\"start\":3269},{\"end\":5457,\"start\":3466},{\"end\":6163,\"start\":5459},{\"end\":6791,\"start\":6165},{\"end\":7658,\"start\":6793},{\"end\":9135,\"start\":7673},{\"end\":9762,\"start\":9137},{\"end\":10973,\"start\":9785},{\"end\":11287,\"start\":11018},{\"end\":11531,\"start\":11289},{\"end\":11729,\"start\":11533},{\"end\":12112,\"start\":11731},{\"end\":12526,\"start\":12126},{\"end\":13001,\"start\":12551},{\"end\":13389,\"start\":13028},{\"end\":14194,\"start\":13391},{\"end\":14953,\"start\":14229},{\"end\":15602,\"start\":14955},{\"end\":16479,\"start\":15604},{\"end\":17874,\"start\":16529},{\"end\":18379,\"start\":17876},{\"end\":18649,\"start\":18381},{\"end\":19016,\"start\":18651},{\"end\":19113,\"start\":19018},{\"end\":19818,\"start\":19115},{\"end\":20992,\"start\":19849},{\"end\":21965,\"start\":20994},{\"end\":22627,\"start\":21990},{\"end\":22858,\"start\":22654},{\"end\":22918,\"start\":22860},{\"end\":24258,\"start\":22920},{\"end\":25196,\"start\":24295},{\"end\":25911,\"start\":25198},{\"end\":26464,\"start\":25913},{\"end\":26941,\"start\":26508},{\"end\":27578,\"start\":26943},{\"end\":28197,\"start\":27593},{\"end\":28247,\"start\":28199},{\"end\":28402,\"start\":28305},{\"end\":29764,\"start\":28404},{\"end\":30350,\"start\":29780},{\"end\":30576,\"start\":30367},{\"end\":30677,\"start\":30578},{\"end\":30943,\"start\":30679},{\"end\":31976,\"start\":30945},{\"end\":32193,\"start\":31978},{\"end\":34292,\"start\":32216},{\"end\":34557,\"start\":34315},{\"end\":34978,\"start\":34559},{\"end\":35053,\"start\":34980},{\"end\":35188,\"start\":35055},{\"end\":35273,\"start\":35208},{\"end\":35372,\"start\":35314},{\"end\":35715,\"start\":35374}]", "formula": null, "table_ref": "[{\"end\":16044,\"start\":16037},{\"end\":17905,\"start\":17898}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2424,\"start\":2412},{\"attributes\":{\"n\":\"2\"},\"end\":7671,\"start\":7661},{\"attributes\":{\"n\":\"3\"},\"end\":9783,\"start\":9765},{\"attributes\":{\"n\":\"3.3\"},\"end\":11016,\"start\":10976},{\"attributes\":{\"n\":\"4\"},\"end\":12124,\"start\":12115},{\"attributes\":{\"n\":\"4.1\"},\"end\":12549,\"start\":12529},{\"attributes\":{\"n\":\"4.2\"},\"end\":13026,\"start\":13004},{\"attributes\":{\"n\":\"4.3\"},\"end\":14227,\"start\":14197},{\"attributes\":{\"n\":\"4.4\"},\"end\":16517,\"start\":16482},{\"attributes\":{\"n\":\"5\"},\"end\":16527,\"start\":16520},{\"attributes\":{\"n\":\"5.1\"},\"end\":19847,\"start\":19821},{\"attributes\":{\"n\":\"5.2\"},\"end\":21988,\"start\":21968},{\"end\":22639,\"start\":22630},{\"attributes\":{\"n\":\"6\"},\"end\":22652,\"start\":22642},{\"attributes\":{\"n\":\"6.2\"},\"end\":24293,\"start\":24261},{\"attributes\":{\"n\":\"6.3\"},\"end\":26506,\"start\":26467},{\"attributes\":{\"n\":\"7\"},\"end\":27591,\"start\":27581},{\"end\":28271,\"start\":28250},{\"end\":28303,\"start\":28274},{\"end\":29778,\"start\":29767},{\"end\":30365,\"start\":30353},{\"end\":32214,\"start\":32196},{\"end\":34313,\"start\":34295},{\"end\":35206,\"start\":35191},{\"end\":35312,\"start\":35276},{\"end\":35727,\"start\":35717},{\"end\":35925,\"start\":35917},{\"end\":36304,\"start\":36294},{\"end\":36515,\"start\":36499},{\"end\":37905,\"start\":37900},{\"end\":39976,\"start\":39965},{\"end\":45513,\"start\":45504}]", "table": "[{\"end\":41935,\"start\":40418},{\"end\":43179,\"start\":42371}]", "figure_caption": "[{\"end\":35915,\"start\":35729},{\"end\":36292,\"start\":35927},{\"end\":36497,\"start\":36306},{\"end\":37898,\"start\":36519},{\"end\":39963,\"start\":37907},{\"end\":40170,\"start\":39979},{\"end\":40418,\"start\":40173},{\"end\":42371,\"start\":41938},{\"end\":45502,\"start\":43182},{\"end\":45662,\"start\":45515}]", "figure_ref": "[{\"end\":2578,\"start\":2570},{\"end\":3801,\"start\":3793},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":8269,\"start\":8261},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11764,\"start\":11756},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13450,\"start\":13442},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14355,\"start\":14347},{\"end\":14778,\"start\":14768},{\"end\":15413,\"start\":15405},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17045,\"start\":17027},{\"end\":18920,\"start\":18912},{\"end\":20008,\"start\":20000},{\"end\":23475,\"start\":23466},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23870,\"start\":23861},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24689,\"start\":24680},{\"end\":26297,\"start\":26288},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28788,\"start\":28780},{\"end\":34038,\"start\":34030},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34866,\"start\":34845},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35272,\"start\":35264},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":35327,\"start\":35318}]", "bib_author_first_name": "[{\"end\":47947,\"start\":47940},{\"end\":47965,\"start\":47958},{\"end\":48158,\"start\":48154},{\"end\":48172,\"start\":48165},{\"end\":48186,\"start\":48178},{\"end\":48195,\"start\":48193},{\"end\":48204,\"start\":48201},{\"end\":48217,\"start\":48212},{\"end\":48669,\"start\":48664},{\"end\":48686,\"start\":48678},{\"end\":48700,\"start\":48694},{\"end\":48714,\"start\":48706},{\"end\":49107,\"start\":49104},{\"end\":49123,\"start\":49118},{\"end\":49137,\"start\":49133},{\"end\":49639,\"start\":49631},{\"end\":49656,\"start\":49648}]", "bib_author_last_name": "[{\"end\":47956,\"start\":47948},{\"end\":47970,\"start\":47966},{\"end\":48163,\"start\":48159},{\"end\":48176,\"start\":48173},{\"end\":48191,\"start\":48187},{\"end\":48199,\"start\":48196},{\"end\":48210,\"start\":48205},{\"end\":48224,\"start\":48218},{\"end\":48676,\"start\":48670},{\"end\":48692,\"start\":48687},{\"end\":48704,\"start\":48701},{\"end\":48724,\"start\":48715},{\"end\":49116,\"start\":49108},{\"end\":49131,\"start\":49124},{\"end\":49146,\"start\":49138},{\"end\":49646,\"start\":49640},{\"end\":49666,\"start\":49657}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":48106,\"start\":47873},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":34032948},\"end\":48662,\"start\":48108},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b2\"},\"end\":49026,\"start\":48664},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":19204066},\"end\":49587,\"start\":49028},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":16567195},\"end\":49781,\"start\":49589}]", "bib_title": "[{\"end\":48152,\"start\":48108},{\"end\":49102,\"start\":49028},{\"end\":49629,\"start\":49589}]", "bib_author": "[{\"end\":47958,\"start\":47940},{\"end\":47972,\"start\":47958},{\"end\":48165,\"start\":48154},{\"end\":48178,\"start\":48165},{\"end\":48193,\"start\":48178},{\"end\":48201,\"start\":48193},{\"end\":48212,\"start\":48201},{\"end\":48226,\"start\":48212},{\"end\":48678,\"start\":48664},{\"end\":48694,\"start\":48678},{\"end\":48706,\"start\":48694},{\"end\":48726,\"start\":48706},{\"end\":49118,\"start\":49104},{\"end\":49133,\"start\":49118},{\"end\":49148,\"start\":49133},{\"end\":49648,\"start\":49631},{\"end\":49668,\"start\":49648}]", "bib_venue": "[{\"end\":48387,\"start\":48315},{\"end\":49309,\"start\":49237},{\"end\":49680,\"start\":49677},{\"end\":47938,\"start\":47873},{\"end\":48313,\"start\":48226},{\"end\":48822,\"start\":48742},{\"end\":49235,\"start\":49148},{\"end\":49675,\"start\":49668}]"}}}, "year": 2023, "month": 12, "day": 17}