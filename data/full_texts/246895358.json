{"id": 246895358, "updated": "2022-10-19 13:23:08.376", "metadata": {"title": "Spatio-Temporal Feature Encoding for Traffic Accident Detection in VANET Environment", "authors": "[{\"first\":\"Zhili\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Xiaohua\",\"last\":\"Dong\",\"middle\":[]},{\"first\":\"Zhetao\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Keping\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Chun\",\"last\":\"Ding\",\"middle\":[]},{\"first\":\"Yimin\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "IEEE Transactions on Intelligent Transportation Systems", "journal": "IEEE Transactions on Intelligent Transportation Systems", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "In the Vehicular Ad hoc Networks (VANET) environment, recognizing traffic accident events in the driving videos captured by vehicle-mounted cameras is an essential task. Generally, traffic accidents have a short duration in driving videos, and the backgrounds of driving videos are dynamic and complex. These make traffic accident detection quite challenging. To effectively and efficiently detect accidents from the driving videos, we propose an accident detection approach based on spatio\u2013temporal feature encoding with a multilayer neural network. Specifically, the multilayer neural network is used to encode the temporal features of video for clustering the video frames. From the obtained frame clusters, we detect the border frames as the potential accident frames. Then, we capture and encode the spatial relationships of the objects detected from these potential accident frames to confirm whether these frames are accident frames. The extensive experiments demonstrate that the proposed approach achieves promising detection accuracy and efficiency for traffic accident detection, and meets the real-time detection requirement in the VANET environment.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tits/ZhouDLYDY22", "doi": "10.1109/tits.2022.3147826"}}, "content": {"source": {"pdf_hash": "2f304c1ebd71fce501e75542c5a14a2a2051a162", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2db8cd497abff1d87d98fdd1330ee33e1c8c9716", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2f304c1ebd71fce501e75542c5a14a2a2051a162.txt", "contents": "\nSpatio-Temporal Feature Encoding for Traffic Accident Detection in VANET Environment\nOCTOBER 2022\n\nMember, IEEEZhili Zhou \nXiaohua Dong \nMember, IEEEZhetao Li \nMember, IEEEKeping Yu \nChun Ding \nSenior Member, IEEEYimin Yang \nSpatio-Temporal Feature Encoding for Traffic Accident Detection in VANET Environment\n\nIEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS\n2310OCTOBER 202210.1109/TITS.2022.314782619772Index Terms-Neural networksecurity communicationtraffic accident detectiontraffic safetyVANETs\nIn the Vehicular Ad hoc Networks (VANET) environment, recognizing traffic accident events in the driving videos captured by vehicle-mounted cameras is an essential task. Generally, traffic accidents have a short duration in driving videos, and the backgrounds of driving videos are dynamic and complex. These make traffic accident detection quite challenging. To effectively and efficiently detect accidents from the driving videos, we propose an accident detection approach based on spatio-temporal feature encoding with a multilayer neural network. Specifically, the multilayer neural network is used to encode the temporal features of video for clustering the video frames. From the obtained frame clusters, we detect the border frames as the potential accident frames. Then, we capture and encode the spatial relationships of the objects detected from these potential accident frames to confirm whether these frames are accident frames. The extensive experiments demonstrate that the proposed approach achieves promising detection accuracy and efficiency for traffic accident detection, and meets the real-time detection requirement in the VANET environment.\n\nroad by short-distance communication technologies [1], [2]. By monitoring the vehicle running statuses and surrounding conditions and sharing the information among those close vehicles, VANETs have been mainly used in the field of road safety in recent years. They mainly aim to reduce the possibility of traffic accidents and the losses caused by traffic accidents. There are three kinds of applications of VANETs: collision avoidance [3], [4], traffic sign notification [5], and incident management [6].\n\nIn the VANET environment, recognizing traffic accidents/abnormalities including the vehicle and pedestrian abnormalities in the natural driving videos captured by vehiclemounted cameras is an essential task [7], [8]. Traffic accident/abnormality detection can be applied in a variety of applications such as road safety warning [9], [10], autonomous driving [11], traffic flow management [12], and pedestrian protection [13], [14]. In the past decades, many researchers have dedicated themselves to the study of accident/abnormality detection. The main challenges of accident/abnormality detection are the problems of long-tailed distribution and heterogeneous anomaly classes. To address these problems, many approaches have been proposed, which can be roughly classified into three categories: video-level, segment-level, and frame-level detection approaches.\n\nThe conventional accident/abnormality detection approaches are generally based on video-level [15], [16]. Generally, an accident/abnormality detector is directly learned from the whole traffic video using a variety of deep learning models for accident/abnormality detection. The deep learning models can be divided into two categories, i.e., classifier-based and reconstruction error-based models. Classifier-based models assume that all normal instances are from the same category, while the basic assumption of reconstruction error-based models is that the reconstruction error for abnormal samples would be higher than that for normal samples. However, it is worth noting that the accidents/abnormalities usually have a short duration and follow a long-tailed distribution in driving videos, and the backgrounds of driving videos are dynamic and complex. Thus, it is very hard for these video-level approaches to achieve desirable performance for accident/abnormality detection.\n\nTo address the problem of long-tailed distribution in traffic accident/abnormality detection, some segment-level approaches [17], [18] have been proposed based on a weakly-supervised framework with Multiple Instances Learning (MIL). They use the high-level class labels to learn the anomaly score for each video segment directly. Some other frame-level approaches have been proposed [19], [20], which use prediction models for vehicles and pedestrians' abnormality detection. These frame-level approaches usually construct the current frames by the deep learning models according to the previous frames. Then, they compute the construction error between the generated current frame and the original one to determine whether the current frame is an accident/abnormality frame. The frame-level approaches can more accurately localize the abnormalities in both the temporal and the spatial domains. However, these approaches generally need to construct each current frame and compute the corresponding construction error, which make them very time-consuming.\n\nIn summary, although these segment-level and frame-level approaches generally achieve superior performance than the video-level approaches to deal with the long-tailed distribution problem, they suffer from the issue of inefficiency in traffic accident/abnormality detection. That makes them inapplicable for traffic accident/abnormality detection in the VANET environment, in which real-time detection is usually required.\n\nIn this paper, we mainly focus on the detection of vehicle accidents/abnormalities in the VANET environment. To effectively and efficiently detect accident frames from the driving videos, we propose a novel frame-level accident detection approach based on spatio-temporal feature encoding with a multilayer neural network [21]. The framework of the proposed approach is based on the concept of coarse-tofine detection, as shown in Fig. 1. Specifically, we first encode the two temporal features: the Histogram of Optical Flow (HOF) [22] features and the temporal ordinal features of frames as a temporal coding matrix by the multilayer neural network for clustering the frames. From the obtained frame clusters, we detect the border frames as the potential accident frames. Then, we encode the Convolutional Neural Network (CNN) [23] features and spatial relationships of the objects detected from these potential accident frames to confirm whether these frames are accident frames. The main contributions of this paper are summarized as follows:\n\n1) A coarse-to-fine detection framework for traffic accident detection is proposed based on Spatial-temporal feature encoding. We first efficiently detect the potential accident frames, and then further confirm whether these frames are accident ones. That can ensure both high efficiency and accuracy in traffic accident detection. Also, the proposed approach can meet the real-time detection requirement in the VANET environment. Compared with the existing accident/abnormality detection approaches, our approach can effectively address a major challenge in the field of traffic accident detection-the long-tail distribution problem.\n\n2) A novel unsupervised clustering approach is designed to detect abnormal frames, which can significantly reduce the time cost of temporal localization in videos. Thus, it can greatly improve time efficiency.\n\n3) The superiority of the proposed approach has been demonstrated by extensive experiments. The experiment results show that the proposed approach increases the accuracy of temporal localization by 15.2% compared to the recent approaches. In our framework, we combine the temporal features with the spatial features to determine abnormal traffic video frames and identify the accident classes. Overall, the proposed approach increases the accuracy of accident detection significantly.\n\nThe remainder of this paper is organized as follows. Section II introduces the related work. Section III elaborates the proposed traffic accident detection approach. Section IV presents and discusses the experimental results. Conclusions are drawn in Section V.\n\nII. RELATED WORK Traffic accident/abnormality detection including vehicle and pedestrian abnormality detection has received considerable attention in the field of intelligent transportation. In the past two decades, a lot of traffic accident/abnormality detection approaches have been proposed. These approaches can be roughly classified into three categories: videolevel [15], [16], segment-level [17], [18], and frame-level detection approaches [19], [20]. Generally, these approaches focus on accident/abnormality detection in surveillance videos.\n\n\nA. Video-Level Detection Approaches\n\nIn the early work, some video-level approaches have been proposed [15], [16], in which each video is treated as a whole for accident/abnormality detection. Specifically, each video is fed into a well-designed normality detection model. Such a model is learned by many training videos in a variety of normal instances. The videos are determined as abnormal ones if they deviate from the normality model. In general, these approaches treat the task of accident/abnormality detection as an outlier detection problem. The one-class classification models such as Support Vector Machines (SVM) [24] and Support Vector Data Description (SVDD) [16], [25] are popular for video-level accident/abnormality detection. However, these approaches require preprocessing, i.e., feature extraction.\n\nSome end-to-end approaches have been also proposed to directly learn a reconstruction model to distinguish abnormal instances. Specifically, these approaches first train a reconstruction model to reconstruct the feature map of an input video. If the input video is an abnormal one, the reconstruction error is relatively large; otherwise, the reconstruction error is small. Following this idea, Sabokrou et al. [15] proposed an adversarially learned one-class (ALOCC) reconstruction model. This approach trains an Encoder-Decoder network to reconstruct the feature map of an input video for accident/abnormality detection. Some other approaches [24]- [26] have been proposed based on the Generative Adversarial Networks (GANs). Zheng et al. [26] proposed one-class adversarial nets (OCAN), which use the LSTM-Autoencoder to generate the feature map of an input video for accident/abnormality detection. In addition, some similar approaches have also been proposed, such as Fence Generative Adversarial Networks (FGAN) [27] and Single-Objective Generative Adversarial Active Learning (SOGAAL) [28].\n\nAlthough the video level approaches can effectively solve the uneven distribution of samples in the dataset. However, as the traffic accidents have a short duration in driving videos, the accident/abnormality detection suffers from the long-tailed distribution problem. Moreover, the backgrounds of driving videos are dynamic and complex. Therefore, it is hard for these video-level detection approaches to achieve desirable performance for accident/abnormality detection.\n\n\nB. Segment-Level Detection Approaches\n\nTo address the problem of long-tailed distribution, some segment-level accident/abnormality detection approaches have been proposed. Motivated by the technique of object detection in images such as Faster Region-CNN (Faster R-CNN) [29], the segment-level detection approaches are usually the two-stage framework. First, a set of proposals are segmented from each video, and these segment proposals are fed into a pre-trained classifier to detect the accidents/ abnormalities.\n\nSultani et al. [17] adopted the 3D Convolutional Networks (C3D) with the multiple-instance learning (MIL) technique to build the framework of segment-level accident/abnormality detection. This approach divided each video into 32 segments and treated the videos as bags and segments as instances. Then, it inputs these segments into the 3D Convolutional Networks pretrained by the MIL technique to extract the segment features, and then fed these segment features into fully connected layers of C3D to obtain the anomaly scores. Finally, the anomaly segments are determined by their anomaly scores. Instead of extracting C3D features, Zhu and Newsam [18] computed the optical flows of the segments as the segment features and adopted the attention mechanism with the MIL technique to improve the detection accuracy. Similarly, Lin et al. [30] proposed a segment-level accident/abnormality detection approach based on the dualbranch network, which consists of spatial-temporal dynamic subnetwork and the interactive dynamic subnetwork. The input of the spatial-temporal dynamic network is the original video segments, while the input of the interactive dynamic network is the feature maps that capture the interactive information between pedestrians and surrounding environments. Then, the two networks are trained simultaneously by the principle of MIL for accident/abnormality detection. In addition, Landi et al. [31] further considered the locality of the occurrence of anomalies for segment-level accident/abnormality detection.\n\nSegment-level approaches can localize and detect accident/abnormality segments in complex long videos. However, if there is more than one abnormality instance in a long and complex video, these segment-level approaches are inefficient for accident/abnormality detection in the VANET environment.\n\n\nC. Frame-Level Detection Approaches\n\nGenerally, the frame-level approaches constructed the current frames using a variety of image generation models with the input of the previous frames. Then, the construction errors are computed by comparing the constructed current frame and the original current frame for accident/abnormality detection. These approaches can be classified into two categories, i.e., the GAN network-based and Autoencoder-based approaches.\n\nIn the early work, to detect pedestrian violations of traffic rules in crowded scenes, Liu et al. [32] proposed a framelevel detection approach based on generator-discriminator structure. They used the U-Net network as the generator to predict the current frame and used a discriminator network to determine whether or not the predicted frame is an abnormal one. Subsequently, with the development of the GANs, GAN-based anomaly detection has become one of the most popular frame-level anomaly detection techniques. Schlegl et al. [19] proposed the Anomaly Detection with Generative Adversarial Networks (AnoGAN) to learn the distribution of normal frames to generate the current frame. Then, the generated current frame and the original current frame were compared to determine whether the current frame is an abnormal one. However, the training of AnoGAN is quite timeconsuming. To solve the computational inefficiency problem of AnoGAN, some other GAN-based approaches have been proposed, such as Efficient Generative Adversarial Networks-Based (EBGAN), fast Anomaly Detection with Generative Adversarial Networks (fast AnoGAN) [33] and Bi-directional Generative Adversarial Networks (BiGAN) [34]. Those GANs have improved the accuracy of anomaly detection because of the superior capability of generating realistic frames. However, in most cases, training a GAN-based anomaly detection model is very difficult, mainly due to the failure converge and mode collapse. To address these problems, some autoencoder-based approaches have been proposed. Hasan et al. [35] proposed an end-to-end framework based on a fully convolutional feed-forward autoencoder. The current frame was generated by the autoencoder with the input of the previous frames. Similarly, Medel and Savakis [36] proposed Convolutional Long Short-Term Memory Networks (ConvLSTM) for framelevel accident/abnormality detection.\n\nIn these frame-level detection approaches, since the frame construction process should be conducted many times, they suffer from the problem of high computational complexity. Therefore, it is still not feasible to directly adopt these approaches for accident/abnormality detection in VANET environment, since real-time detection is usually required.\n\nIII. THE PROPOSED APPROACH In this section, the proposed approach is elaborated. In Section III-A, the framework of the proposed approach is first introduced. In Section III-B, temporal features of frames are encoded for coarse detection. In Section III-C, spatial features of objects detected from frames are encoded for fine detection.\n\n\nA. The Framework of Proposed Approach\n\nThe framework of the proposed approach is based on coarseto-fine detection using spatial-temporal feature encoding with a multilayer neural network [21], as shown in Fig. 1. It consists of two main stages: temporal feature encoding for coarse detection and spatial feature encoding for fine detection.\n\n1) In the stage of coarse detection, for a given traffic video, we first encode the two temporal features, i.e., the HOF features [22] and the ordinal features of frames as the feature map by the multilayer neural network to cluster the frames. From the frame clusters, we detect the potential accident frames.\n\n2) In the stage of fine detection, we encode the CNN features and spatial relationships of the objects detected from the potential accident frames through the multilayer neural network. The encoded spatial features are fed into a trained SVM to confirm whether there is an accident happen in the potential accident frames. Consequently, we can localize and detect the accident frames in the given video.\n\nAccording to the literature [21], the multilayer neural network provides a representation learning platform with unsupervised/supervised and compressed/sparse learning. Thus, we employed the multilayer neural network for encoding the features of frames and objects.\n\n\nB. Temporal Feature Encoding for Coarse Detection\n\nIn the stage of coarse detection, we extract and encode temporal features from a given video as a coding matrix. Then, based on the coding matrix, we cluster the video frames. Finally, from the frame clusters, we coarsely detect the potential accident frames. Thus, there are three main steps: temporal feature encoding, frame clustering, and potential accident frame detection.\n\n1) Temporal Feature Encoding: Given a video with t frames, we first extract the motion feature vectors, i.e., the histogram of optical flow (HOF) descriptor [22], which represents the temporal trajectory information of these frames. According to [22], the HOF descriptor is extracted from each frame and its 15 consecutive frames and thus the whole video is presented as\nV = {v 1 , v 2 , . . . , v m }, where m = t \u2212 15.\nThen, the feature vector sequence and the temporal ordinal information of the video frames are encoded for the coding matrix generated by a multilayer neural network. In coding matrix generation, we label the feature vectors of video frames V = {v 1 , v 2 , . . . , v m } according to the indices of frames denoted as L = {l 1 , l 2 , . . . , l m }. Then, the feature vectors and their labels are fed into the multilayer neural network, and then the parameters of the model are used as a coding matrix for frame clustering. The following five steps are performed to generate the coding matrix A, as shown in Fig. 2.\n\nAlgorithm 1 Frame Clustering Using Self-Representation Constrained Low-Rank Representation (SRLRR) Input: Coding Matrix A = [a 1 , a 2 , . . . , a m ] \u2208 R d\u00d7m , parameters \u03b2, \u03bb; Output: The coefficient matrix Z * and noise E * ; 1: Initialize the parameters.\nY 0 1 , Y 0 2 , Y 0 3 , Y 0 4 = 0, \u03bc 0 = 10 \u22128 , \u03bc max = 10 30 , \u03c1 = 1.1, \u03b5 = 10 \u22128 , k = 0 2: Initialize the variables. Z 0 , J 0 , T 0 \u2208 R m\u00d7m and E 0 \u2208 R d\u00d7m 3: while (A \u2212 AZ k \u2212 E k \u221e > \u03b5 and Z k \u2212 J k \u221e > \u03b5 and Z k \u2212 T k \u221e > \u03b5 and 1 n Z k \u2212 1 n \u221e > \u03b5) do: 4: Update J k ; 5: Update T k ; 6: Update Z k ; 7: Update E k ; 8: Update Y k 1 , Y k 2 , Y k 3 , Y k 4\n, \u03bc k ; 9: end while 10: return the coefficient matrices Z * = Z k , E * = E k 11: Build an affinity graph using Eq. (14) 12: Use NCut algorithm to generate clusters\n\nStep 1: We use the vectors {(v k , l k )} m k=1 , v k \u2208 R D , l k \u2208 R m to represent the training data, and every hidden node is formed by several sub-nodes. The parameters of sub-nodes and the coding matrix are initialized randomly as\nA i c = g \u0175 j c \u00b7 V +b j c , \u0175 j c T = \u0175 j c \u22121 , b j c = 1 (1)\nwhere\u0175 j c \u2208 R D\u00d7d andb j c \u2208 R are weights and bias of nodes in the coding layer, respectively, d represents the number of sub-network nodes in each generally hidden node in the coding layer, and D is the dimensions of video features.\n\nStep 2: We use a sigmoid or sine function as the activation function g (\u00b7) for continuous desired output L = {l 1 , l 2 , . . . , l m }, the weights in the learning layer are computed a\u015d\nw l = u n (L)\u00b7 g \u22121 A i c T (C/I )+g \u22121 A i c g \u22121 A i c T \u22121 w l \u2208 R m\u00d7d(2)\nwhere C is a constant value, u n is a normalized function while u n (L) : R \u2192 (0 , 1], and g \u22121 (\u00b7) represents the inverse function of g (\u00b7). The formulation is given as follows:\ng \u22121 (\u00b7) = \u23a7 \u23a8 \u23a9 ar csi n (\u00b7) i f g (\u00b7) = si n (\u00b7) \u2212log 1 (\u00b7) \u2212 1 i f g (\u00b7) = 1/ 1 + e (\u00b7)(3)\nThe bias in the learning layer is computed a\u015d  Step 3: Calculate the residual error e j and error feedback data P j as\nb l = mse \u0175 j l \u00b7 g \u22121 A i c \u2212 u n (L) ,b l \u2208 R(4)e j = L \u2212 u \u22121 n g \u0175 l \u00b7 A i c +b l (5) P j = e j \u00b7 \u0175 T l C/I +\u0175 l \u0175 l T \u22121 (6) P j = u n P j + g \u22121 A i c(7)\nwhere u \u22121 n (\u00b7) represents the inverse function of u n (\u00b7).\n\nStep 4: In the output layers, the multilayer neural network aims to obtain the smallest training error and smallest output weights by minimizing:\n\u0175 i o u 1 p +C D i=1\u0175 i o g A i c ,\u0175 l ,b l \u2212 L u 2 q , i = 1, . . . , m(8)\nwhere u 1 > 0, u 2 > 0, p, q = {0, 1 2 , 1, 2, . . . \u2212, +\u221e}, and C is a constant value. In the output layer, the weight\u0175 i o is computed by\u0175\ni o = e i\u22121 , g(A i c , w r o , b r o ) g A i c , w r o , b r o 2(9)\nwhere \u00b7, \u00b7 means the Moore-Penrose function, and w r o , b r o represents the parameters between hidden nodes and output nodes.\n\nStep 5: Set j = j + 1, and then add a new general nod\u00ea w j c ,b j c in the coding layer a\u015d\nw j c = g \u22121 u n P j \u22121 \u00b7 V \u22121 ,\u0175 j c \u2208 R D\u00d7d (10) b j c = mse \u0175 j c \u00b7 V \u2212 P j \u22121 ,b j c \u2208 R(11)\nSubsequently, update the coding matrix by\nA i c = j S=1 u \u22121 S g V,\u0175 S c ,b S c(12)\nStep 6: Repeat Step 2 to Step 5 N \u2212 1 times where N is the number of general nodes in the coding layer. The\nparameters \u00e2 j c ,b j c N j =1\nare the optimal projecting parameters of the coding matrix in the coding layer. In this paper, N is equal to the number of feature vectors m. Finally, we can obtain the coding matrix of temporal features A N c by\nA N c = N w=1 u \u22121 w g V,\u0175 w c ,b w c = A *(13)\n2) Frame Clustering: Then, to facilitate the coarse detection, we try to cluster the video frames according to the obtained coding matrix.\n\nSubspace clustering has been adopted in many real-world applications such as motion segmentation [37], face recognition [38], and image retrieval [39]. Recently, graph-based clustering algorithms have been proposed to obtain low-rank coefficient matrices for clustering. The most typical algorithms are Low-Rank Representation (LRR) [40] and Sparse Subspace Clustering (SSC) [41]. LRR has been proven to be superior to SSC. Thus, many extended versions of LRR have been proposed such as self-representation constrained lowrank representation (SRLRR) [42]. In these approaches, the obtained low-rank coefficient matrices are essentially affinity matrices, in which the similarities between homogeneous data samples are enhanced while the similarities between heterogeneous data samples are decreased. Thus, the lowrank coefficient matrix will be more suitable for revealing the subspace relationship of data and clustering the data. Therefore, in the proposed approach, we adopt SRLRR for the obtained coding matrix to compute the low-rank coefficient matrix to cluster video frames.\n\nThe SRLRR algorithm aims to compute a coefficient matrix with minimal rank, denoted as Z , which satisfies A = AZ + E and Z = Z Z = Z 2 . Where A represents the input matrix and E represents the residual between coefficient matrix Z and input matrix A. The optimal solution by the SRLRR is a blockdiagonal matrix. In the proposed approach, the algorithm of frame clustering using SRLRR is summarized in Algorithm 1. Reference [42] consequently, an affinity graph G can be obtained by\nG (i, j ) = Z (i, j ) + Z ( j,i) 2 (14)\nG (i, j ) and Z (i, j ) are the (i, j )-th elements of G and Z , respectively.\n\n\n3) Potential Accident Frame Detection:\n\nIn the affinity graph, if two feature vectors are highly correlated, they tend to be in the same diagonal block. Then a spectral clustering algorithm, i.e., normalize cut (Ncut) [43], is employed to produce the final segmentation results and the indices of the potential accident frames. Specifically, the border frames in the clusters are detected by the Ncut algorithm as the potential accident frames.\n\n\nC. Spatial Feature Encoding for Fine Detection\n\nOn the fine detection stage, we encode the spatial features of the objects detected from the potential accident frames, including the CNN features of the objects generated by Faster-RCNN and the spatial relationships captured among these objects to compute the spatial coding matrix. Then, we feed the spatial coding matrix into trained SVM to confirm the potential accident frames for fine detection. Thus, there are three main steps: object detection, spatial feature encoding, and accident frame confirmation.\n\n1) Object Detection: After the coarse detection, the potential accident frames have been obtained. Since the spatial features of frames should be captured, it is necessary to detect objects from the obtained potential accident frames.\n\nIn the literature, many object detection networks have been proposed, such as YOLO [44] and Faster-RCNN [29]. In the proposed approach, we adopt Faster-RCNN to detect the objects from the potential accident frames since it has high efficiency with desirable accuracy in object detection. The Faster-RCNN consists of four parts: Convolution layers, Region Proposal Network (RPN), RoIAlign (Roi) Pooling, and Classification.\n\nFrom the last convolution layer of Faster-RCNN [29], we can get the feature vector F \u2208 R N of a single video frame, where N means the dimension of the feature vector. Then the feature vector is fed into the RPN to generate a set of object proposals. As the DoTA dataset used in the experiments has provided us with ground-truth boxes of abnormal objects in the frames, we can assign the positives and negatives according to the overlapping area between anchor and ground-truth. The loss function of training is defined as:\nL ({ p i } , {b i }) = 1 N cls i L cls p i , p * i +\u03bb 1 N reg i p * i L reg b i , b * i(15)\nwhere i is the index of anchor, p i is the predicted probability of anchor i being an object, p * i is the ground-truth label, b i is a vector of 4 parameters representing the predicted bounding box, b * i is a vector of 4 parameters representing the groundtruth box associated with a positive anchor, L cls is the log loss over two classes, and L reg is the regression loss.\n\nConsequently, we can obtain a set of objects detected from each potential accident frame. Each object can be represented by four parameters (x, y, w, h). Where, x, y are the coordinates of the center point of the object bounding box, and w and h are the weight and height of the object bounding box, respectively.\n\n2) Spatial Feature Encoding: In this step, we encode the CNN features and the spatial relationships of objects detected from each potential accident frame, where the CNN features are the convolutional features extracted from the last convolutional layer of Faster-RCNN, and the spatial relationships of objects are the spatial relative positions among these objects and the relative size relationship among these objects. We separately encode the two spatial relationships with the CNN features of objects to generate two corresponding coding matrixes, as shown in Fig. 4. More details are given as follows.\n\nThe CNN feature vectors of objects are represented by I = {I 1 , I 2 , . . . , I n }, where n is the number of objects in the frame, and the bounding boxes of each object are represented by\n{(x i , y i , w i , h i )} n i .\nThen, we label the feature vectors of the target boxes based on the order of the target boxes along the x-axis positive direction as o = {1, 2, . . . , n}. Next, the feature vectors I and their corresponding order labels o are fed into the multilayer neural network. By the same steps, i.e., Step 1 to Step 5 in Section III-B, a new coding matrix A 1 can be obtained.\n\nSimilarly, for {(x i , y i , w i , h i )} n i , we also label the feature vectors of the target boxes based on the descending order of the sizes of target boxes. Next, the feature vectors I = {I 1 , I 2 , . . . , I n } and their corresponding order labels are input into a multilayer neural network to generate another coding matrix A 2 .\n\n3) Accident Frame Confirmation: In natural driving videos, the recognition of traffic accidents is a typical problem of multi-classification. Thus, we employ a trained SVM classifier with the input of coding matrices A 1 and A 2 in the proposed approach. In this step, we concatenate A 1 and A 2 to form a single vector and then feed it into the SVM. The DoTA dataset used in the experiments has provided detailed annotation of accident events, and the objective of SVM is to find a hyperplane that can distinguish the samples of input feature vectors. The hyperplane can be defined by\ng (x) = w T x + b(16)\nThe geometric distance from the sample points to the hyperplane is computed by\nd i = y i ( |wx i + b| w )(17)\nwhere d i is the geometric distance of i-th sample to the hyperplane, and y i is the label of i-th sample. The maximum partitioned hyperplane problem of the SVM model can be solved by the following constrained optimization problem:\nmi n (w,b) 1 2 w 2(18)\nAs a result, by the SVM, we can finally obtain the detected accident frames from a given traffic video.\n\n\nIV. EXPERIMENTS AND ANALYSIS\n\n\nA. Datasets\n\nThere are some publicly available video datasets for traffic/abnormality detection. However, to the best of our knowledge, most of those traffic datasets are surveillance videos  captured by the cameras on the road. And in the VANET environment, the videos are usually the driving videos captured by the vehicle-mounted cameras for a variety of tasks such as road safety warning [9], autonomous driving [11], traffic flow management [12], and pedestrian anomaly detection [45]. Different from the surveillance videos, in which the backgrounds are usually static and simple, the backgrounds of driving videos are dynamic and complex.\n\nIn [46], a new publicly available driving dataset, named DoTA, is introduced for traffic accident detection and recognition. It consists of more than 6000 video clips with the resolution of 1280 \u00d7 720 derived from YouTube, which is captured from different countries under different weather and lighting conditions. Table I compares DoTA with other traffic video datasets. In DoTA, there are a lot of temporal, spatial, and categorical annotations of accidents for frames and objects in the videos.\n\nEach video is labeled with a set of the start and end times, which divides the video into three parts: normal frames preceding the accident, the accident frames, and post-accident frames. DoTA is the first traffic video dataset that provides detailed spatio-temporal annotations of anomalous objects. The dataset has provided each anomalous object with a unique track ID, and the label of their bounding box. Moreover, each video is assigned by one of 9 categories listed in Table II. Thus, we adopt this dataset in the experiments to evaluate the performances of different approaches.\n\n\nB. Evaluation of Video Clustering Algorithm\n\nIn this section, we evaluate the accuracy of the proposed approach for traffic accident detection. The dataset has provided the ground-truth classification labels of the individual frames in the video samples. Then, we use the pair-counting measurement to evaluate the accuracy of the clustering approaches. The employed pair-counting measurement includes two major variables ( p 1 , p 2 ). The p 1 refers to the percentage of both the ground-truth and clustered frames are assigned to same clusters. The p 2 refers to the percentage of both the ground-truth and clustered frames are assigned to different clusters. Furthermore, the average value of p 1 and p 2 indicates the clustering performance.\n\nThere are three key parameters related to the performance of the video clustering algorithm. In the step of temporal feature encoding, the parameter C in Eq. (8) has an impact on the generation of diagonal A A T and the frame clustering results. Thus, we test a set of values for parameter C, i.e., C \u2208 2 \u22124 , . . . , 2 \u22128 . Fig. 5 shows the performance of video frame clustering by using different parameters C on the DoTA dataset. From Fig. 5, it is clear that the clustering algorithm achieves the best performance when C = 8. Also, we test the impacts of parameters \u03bb and \u03b2 used in the SRLRR algorithm on the clustering performance, when \u03bb and \u03b2 vary in the range of [0.001,7] and [0.01,10], respectively, as shown in Fig. 6. From Fig. 6, \u03bb = 5 and \u03b2 = 10 provide the optimal performance in frame clustering. Thus, we set C = 5, \u03bb = 5, and \u03b2 = 10 in the following experiments.\n\nMoreover, we compare the performance of our clustering algorithm with the three other clustering algorithms. In all   of those clustering algorithms, the multilayer neural network is used to encode the temporal features, and then a clustering algorithm is used to cluster frames. The difference among them is that our clustering algorithm is based on LRRSR, while the three others use SSC, LSR, and Ordinal Subspace Clustering (OSC), respectively. Table III lists the accuracy of our clustering algorithm and the state-of-the-art on the DoTA dataset. It is clear that our clustering algorithm achieves high accuracy, and is superior to the three others for the following reasons. Since the temporal features including the trajectories and ordinal information of frames are encoded, the generated code matrix can sufficiently capture the relationships among the frames. Moreover, the powerful SRLRR algorithm can refine the code matrix to reflect the relationships among frames. As a result, the potential traffic accident frames can be easily obtained in the affinity graph G. That is beneficial for the final accident detection performance. \n\n\nC. Performance Evaluation and Comparison\n\nIn this section, we compare the accuracy of the proposed approach with those of the 11 state-of-the-art approaches, which are denoted as Convolutional AutoEncoder (ConvAE) [35], Convolutional Long Short-Trem Memory AutoEncoder (ConvLSTMAE) [36], AnoPred [33], AnoPred+Mask [46], Fully Connected (FC) [46], Traffic Accident Detection (TAD) [47], TAD+Margin Learning (ML) [46], Ensemble [46], LSTM [48], Encoder-Decoder [49], and Temporal Recurrent Networks (TRN) [50]. Where, FC is the video-level detection approach, and TAD, TAD+ML and Ensemble are the segment-level detection approaches based on localization, while ConvAE, ConvLSTMAE, AnoPred, AnoPred + Mask, LSTM, and Encoder-Decoder are the framelevel detection approaches based on construction error. Table IV shows the detection accuracy of different approaches on the DoTA dataset. In Table IV, \"Gray, Flow, RGB\" mean that the Gray images, Gradient histogram, and RGB images are used as the inputs in different approaches, while \"Masked RGB and Box\" mean the RGB images with masks and the bounding box are used as the inputs, respectively. As shown in Table IV, it is clear that the performance of approaches with optical flow input is better than that with grayscale input. Moreover, supervised approaches such as FC, LSTM, Encoder-Decoder generally achieve higher Area Under Curve (AUC) than unsupervised approaches. That is because the proposed approach combines the advantages of optical flow input and supervised learning manner, and thus it achieves the highest AUC. Table V shows the detection results of supervised approaches, i.e., FC, LSTM, Encoder-Decoder, and TRN for each class. It can be clearly observed that TRN achieves higher AUC only on the classes of LA, TC, AH * , and OC * , while the proposed approach achieves higher AUCs on more classes including ST, AH, OC, VP, ST * , LA * , TC * , VO * , and OO * . Table V also shows that detecting the classes including LA, VP, AH * , LA * , and OO * is challenging for all approaches. However, it is clear that our approach still achieves the highest accuracy, and is superior to the three others.  It is clear that the training time of the proposed approach is much less than that of the three others. The average detection time of the proposed approach is only about 0.201s, mainly because of efficient feature encoding and frame clustering algorithm in coarse detection stage. Therefore, the proposed approach can meet the real-time detection requirement in VANET environment.\n\n\nV. CONCLUSION\n\nThis paper has presented an accident detection approach to detect the accident frames from traffic videos in the VANET environment. We proposed a coarse-to-fine detection framework based on the spatial-temporal feature encoding with the multilayer neural network. The proposed approach has solved the main challenges of abnormality/accident detection, including the problems of long-tailed distribution and heterogeneous anomaly classes. The experiment results demonstrate the proposed approach not only achieves higher detection accuracy than the state-of-the-arts, but also needs less training time and detection time. Consequently, the requirement of abnormality/accident detection can be satisfied in the VANET environment.\n\nMoreover, it is notable that the existing accident/ abnormality detection approaches usually ignore the detection of pedestrian abnormality, which is very helpful for the persons with impaired vision and the elderly groups. In future, we will extend the proposed approach with the help of mobile devices for pedestrian abnormality detection. Specifically, the locations, statuses, and surrounding environments of pedestrians can be detected by the Global Positioning System (GPS), sensors, and cameras of their mobile devices, respectively, and these data is then utilized to detect pedestrian abnormalities.\n\nN\nOWADAYS, intelligent transportation systems (ITSs) have played an important role in enhancing road safety, traffic sustainability and efficiency, and vehicle management capacity. ITSs have become an indispensable part of our daily lives. As an essential paradigm in ITSs, Vehicular Ad hoc Networks (VANETs) offer connection and communication services among close vehicles and infrastructures on Manuscript received 7 August 2021; revised 6 December 2021 and 23 January 2022; accepted 26 January 2022. Date of publication 15 February 2022; date of current version 11 October 2022. This work was supported in part by the National Natural Science Foundation of China under Grant 61972205, Grant 62032020, and Grant 62122032; in part by the Hunan Science and Technology Planning Project under Grant 2019RS3019; in part by the Priority Academic Program Development of Jiangsu Higher Education Institutions (PAPD) Fund; and in part by the Collaborative Innovation Center of Atmospheric Environment and Equipment Technology (CICAEET) Fund.\n\nFig. 1 .\n1The framework of the proposed approach, which mainly includes the temporal encoding for coarse detection and the spatial encoding for fine detection.\n\nFig. 2 .\n2The illustration of temporal feature encoding for coarse detection.\n\nFig. 3 .\n3The structure of multilayer neural network[21].\n\nFig. 4 .\n4The illustration of spatial feature encoding for coarse detection.\n\nFig. 5 .\n5Evaluation of parameter C for video action clustering.\n\nFig. 6 .\n6Impacts of parameter \u03bb and \u03b2 on video clips clustering.\n\n\nThe Associate Editor for this article was B. Gupta. (Corresponding author: Zhili Zhou.) Zhili Zhou, Xiaohua Dong, and Chun Ding are with the Engineering Research Center of Digital Forecasts, Ministry of Education, School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing 210044, China (e-mail: zhou_zhili@163.com; dongxiaohua@nuist.edu.cn; dingchun@nuist.edu.cn). Zhetao Li is with the College of Information Science and Technology, Jinan University, Guangzhou 510632, China (e-mail: liztchina@ hotmail.com). Keping Yu is with the Global Information and Telecommunication Institute, Waseda University, Tokyo 169-8555, Japan (e-mail: keping.yu@aoni.waseda.jp). Yimin Yang is with the Computer Science Department, Lakehead University, Thunder Bay, ON P7B 5E1, Canada (e-mail: yyang48@lakeheadu.ca). Digital Object Identifier 10.1109/TITS.2022.3147826\n\nTABLE I COMPARISON\nIBETWEEN DIFFERENT TRAFFIC ACCIDENT DATASETS\n\nTABLE II TRAFFIC\nIIACCIDENT CATEGORIES IN THE DOTA DATASET\n\nTABLE III COMPARISON\nIIIOF CLUSTERING ALGORITHMS\n\nTABLE IV THE\nIVAUC OF DIFFERENT APPROACHES ON THE DOTA DATASET\n\nTable\nVI also lists the number of parameters of different networks including AlexNet, VGG16, ResNet50, GoogleNet\n\nTABLE V\nVDETECTION ACCURACY FOR EACH INDIVIDUAL ACCIDENT CATEGORY (AUC)TABLE VI COMPARISON OF THE NUMBER OF PARAMETERS AND COMPUTATION TIME and the training time on the DoTA dataset.\n\nIdentity-based authentication mechanism for secure information sharing in the maritime transport system. B B Gupta, A Gaurav, C.-H Hsu, B Jiao, 10.1109/TITS.2021.3125402IEEE Trans. Intell. Transp. Syst., early access. B. B. Gupta, A. Gaurav, C.-H. Hsu, and B. Jiao, \"Identity-based authentication mechanism for secure information sharing in the maritime transport system,\" IEEE Trans. Intell. Transp. Syst., early access, Nov. 15, 2021, doi: 10.1109/TITS.2021.3125402.\n\nA finegrained access control and security approach for intelligent vehicular transport in 6G communication system. Z Zhou, A Gaurav, B B Gupta, M D Lytras, I Razzak, 10.1109/TITS.2021.3106825IEEE Trans. Intell. Transp. Syst. early accessZ. Zhou, A. Gaurav, B. B. Gupta, M. D. Lytras, and I. Razzak, \"A fine- grained access control and security approach for intelligent vehicular transport in 6G communication system,\" IEEE Trans. Intell. Transp. Syst., early access, Sep. 2, 2021, doi: 10.1109/TITS.2021.3106825.\n\nCombining linked open data similarity and relatedness for cross OSN recommendation. M Boubenia, A Belkhir, F M Bouyakoub, Int. J. Semantic Web Inf. Syst. 162M. Boubenia, A. Belkhir, and F. M. Bouyakoub, \"Combining linked open data similarity and relatedness for cross OSN recommendation,\" Int. J. Semantic Web Inf. Syst., vol. 16, no. 2, pp. 59-90, Apr. 2020.\n\nMoMAC: Mobility-aware and collision-avoidance MAC for safety applications in VANETs. F Lyu, IEEE Trans. Veh. Technol. 6711F. Lyu et al., \"MoMAC: Mobility-aware and collision-avoidance MAC for safety applications in VANETs,\" IEEE Trans. Veh. Technol., vol. 67, no. 11, pp. 10590-10602, Nov. 2018.\n\nA review of applications, characteristics and challenges in vehicular ad hoc networks (VANETs). M M Hamdi, L Audah, S A Rashid, A H Mohammed, S Alani, A S Mustafa, Proc. Inter. Congr. Hum.-Comput. Intera, Optim. Rob. Applic (HORA). Inter. Congr. Hum.-Comput. Intera, Optim. Rob. Applic (HORA)M. M. Hamdi, L. Audah, S. A. Rashid, A. H. Mohammed, S. Alani, and A. S. Mustafa, \"A review of applications, characteristics and challenges in vehicular ad hoc networks (VANETs),\" in Proc. Inter. Congr. Hum.- Comput. Intera, Optim. Rob. Applic (HORA), Jun. 2020, pp. 1-7.\n\nTechniques of early incident detection and traffic monitoring centre in VANETs: A review. M M Hamdi, L Audah, S A Rashid, M A Al-Shareeda, J. Commun. 1512M. M. Hamdi, L. Audah, S. A. Rashid, and M. A. Al-shareeda, \"Techniques of early incident detection and traffic monitoring centre in VANETs: A review,\" J. Commun., vol. 15, no. 12, pp. 896-904, 2020.\n\nEfficient and secure routing protocol based on artificial intelligence algorithms with UAV-assisted for vehicular ad hoc networks in intelligent transportation systems. H Fatemidokht, M K Rafsanjani, B B Gupta, C.-H Hsu, IEEE Trans. Intell. Transp. Syst. 227H. Fatemidokht, M. K. Rafsanjani, B. B. Gupta, and C.-H. Hsu, \"Efficient and secure routing protocol based on artificial intelligence algorithms with UAV-assisted for vehicular ad hoc networks in intelli- gent transportation systems,\" IEEE Trans. Intell. Transp. Syst., vol. 22, no. 7, pp. 4757-4769, Jul. 2021.\n\nA trust infrastructure based authentication method for clustered vehicular ad hoc networks. F Mirsadeghi, M K Rafsanjani, B B Gupta, Peer-Peer Netw. Appl. 144F. Mirsadeghi, M. K. Rafsanjani, and B. B. Gupta, \"A trust infrastructure based authentication method for clustered vehicular ad hoc networks,\" Peer-Peer Netw. Appl., vol. 14, no. 4, pp. 2537-2553, 2020.\n\nA novel lane departure warning system for improving road safety. Y Chen, A Boukerche, Proc. IEEE Int. Conf. Commun. (ICC). IEEE Int. Conf. Commun. (ICC)Y. Chen and A. Boukerche, \"A novel lane departure warning system for improving road safety,\" in Proc. IEEE Int. Conf. Commun. (ICC), Jun. 2020, pp. 1-6.\n\nSecure timestamp-based mutual authentication protocol for IoT devices using RFID tags. A Tewari, B B Gupta, Int. J. Semantic Web Inf. Syst. 163A. Tewari and B. B. Gupta, \"Secure timestamp-based mutual authenti- cation protocol for IoT devices using RFID tags,\" Int. J. Semantic Web Inf. Syst., vol. 16, no. 3, pp. 20-34, Jul. 2020.\n\nExploring the limitations of behavior cloning for autonomous driving. F Codevilla, E Santana, A Lopez, A Gaidon, Proc. nullF. Codevilla, E. Santana, A. Lopez, and A. Gaidon, \"Exploring the limitations of behavior cloning for autonomous driving,\" in Proc.\n\nIeee/Cvf, Int, Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 9329-9338.\n\nAir traffic flow management slot allocation to minimize propagated delay and improve airport slot adherence. N Ivanov, F Netjasov, R Jovanovi\u0107, S Starita, A Strauss, Transp. Res. A, Policy Pract. 95N. Ivanov, F. Netjasov, R. Jovanovi\u0107, S. Starita, and A. Strauss, \"Air traffic flow management slot allocation to minimize propagated delay and improve airport slot adherence,\" Transp. Res. A, Policy Pract., vol. 95, pp. 183-197, Jan. 2017.\n\nModel of guidance for visually impaired persons in the traffic network. D Perakovi\u0107, M Peri\u0161a, V Remenar, Transp. Res. F, Traffic Psychol. Behav. 31D. Perakovi\u0107, M. Peri\u0161a, and V. Remenar, \"Model of guidance for visually impaired persons in the traffic network,\" Transp. Res. F, Traffic Psychol. Behav., vol. 31, pp. 1-11, May 2015.\n\nBeacon technology for real-time informing the traffic network users about the environment. M Peri\u0161a, I Cviti\u0107, D Perakovi\u0107, S Husnjak, 34TransportM. Peri\u0161a, I. Cviti\u0107, D. Perakovi\u0107, and S. Husnjak, \"Beacon technology for real-time informing the traffic network users about the environment,\" Transport, vol. 34, no. 3, pp. 373-382, Jun. 2019.\n\nAdversarially learned one-class classifier for novelty detection. M Sabokrou, M Khalooei, M Fathy, E Adeli, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitM. Sabokrou, M. Khalooei, M. Fathy, and E. Adeli, \"Adversarially learned one-class classifier for novelty detection,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 3379-3388.\n\nSupport vector data description. D M J Tax, R P W Duin, Mach. Learn. 541D. M. J. Tax and R. P. W. Duin, \"Support vector data description,\" Mach. Learn., vol. 54, no. 1, pp. 45-66, Jan. 2004.\n\nReal-world anomaly detection in surveillance videos. W Sultani, C Chen, M Shah, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitW. Sultani, C. Chen, and M. Shah, \"Real-world anomaly detection in surveillance videos,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 6479-6488.\n\nMotion-aware feature for improved video anomaly detection. Y Zhu, S Newsam, arXiv:1907.10211Y. Zhu and S. Newsam, \"Motion-aware feature for improved video anomaly detection,\" 2019, arXiv:1907.10211.\n\nUnsupervised anomaly detection with generative adversarial networks to guide marker discovery. T Schlegl, P Seeb\u00f6ck, S M Waldstein, U Schmidt-Erfurth, G Langs, Proc. Int. Conf. Inf. Process. Med. Imag. Int. Conf. Inf. ess. Med. ImagCham, SwitzerlandSpringerT. Schlegl, P. Seeb\u00f6ck, S. M. Waldstein, U. Schmidt-Erfurth, and G. Langs, \"Unsupervised anomaly detection with generative adversarial networks to guide marker discovery,\" in Proc. Int. Conf. Inf. Process. Med. Imag. Cham, Switzerland: Springer, Jun. 2017, pp. 146-157.\n\nEfficient GAN-based anomaly detection. H Zenati, C Foo, B Lecouat, G Manek, V R Chandrasekhar, arXiv:1802.06222H. Zenati, C. Sheng Foo, B. Lecouat, G. Manek, and V. R. Chandrasekhar, \"Efficient GAN-based anomaly detection,\" 2018, arXiv:1802.06222.\n\nMultilayer extreme learning machine with subnetwork nodes for representation learning. Y Yang, Q M J Wu, IEEE Trans. Cybern. 4611Y. Yang and Q. M. J. Wu, \"Multilayer extreme learning machine with subnetwork nodes for representation learning,\" IEEE Trans. Cybern., vol. 46, no. 11, pp. 2570-2583, Nov. 2016.\n\nAction recognition with improved trajectories. H Wang, C Schmid, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisH. Wang and C. Schmid, \"Action recognition with improved trajecto- ries,\" in Proc. IEEE Int. Conf. Comput. Vis., Dec. 2013, pp. 3551-3558.\n\nAn introduction to convolutional neural networks. K Shea, R Nash, arXiv:1511.08458K. O'Shea and R. Nash, \"An introduction to convolutional neural networks,\" 2015, arXiv:1511.08458.\n\nSupport-vector networks. C Cortes, V Vapnik, Mach. Learn. 203C. Cortes and V. Vapnik, \"Support-vector networks,\" Mach. Learn., vol. 20, no. 3, pp. 273-297, 1995.\n\nPredicting students' performance with school and family tutoring using generative adversarial network-based deep support vector machine. K T Chui, R W Liu, M Zhao, P O D Pablos, IEEE Access. 8K. T. Chui, R. W. Liu, M. Zhao, and P. O. D. Pablos, \"Predicting students' performance with school and family tutoring using generative adversarial network-based deep support vector machine,\" IEEE Access, vol. 8, pp. 86745-86752, 2020.\n\nOne-class adversarial nets for fraud detection. P Zheng, S Yuan, X Wu, J Li, A Lu, Proc. AAAI Conf. AAAI Conf33P. Zheng, S. Yuan, X. Wu, J. Li, and A. Lu, \"One-class adversarial nets for fraud detection,\" in Proc. AAAI Conf. Artif. Intell., vol. 33, Jul. 2019, pp. 1286-1293.\n\nFence GAN: Towards better anomaly detection. P C Ngo, A A Winarto, C K L Kou, S Park, F Akram, H K Lee, Proc. IEEE 31st Int. Conf. Tools Artif. Intell. (ICTAI). IEEE 31st Int. Conf. Tools Artif. Intell. (ICTAI)P. C. Ngo, A. A. Winarto, C. K. L. Kou, S. Park, F. Akram, and H. K. Lee, \"Fence GAN: Towards better anomaly detection,\" in Proc. IEEE 31st Int. Conf. Tools Artif. Intell. (ICTAI), Nov. 2019, pp. 141-148.\n\nGenerative adversarial active learning for unsupervised outlier detection. Y Liu, Z Li, C Zhou, Y Jiang, J Sun, M Wang, X He, IEEE Trans. Knowl. Data Eng. 328Y. Liu, Z. Li, C. Zhou, Y. Jiang, J. Sun, M. Wang, and X. He, \"Generative adversarial active learning for unsupervised outlier detection,\" IEEE Trans. Knowl. Data Eng., vol. 32, no. 8, pp. 1517-1528, Aug. 2020.\n\nFaster R-CNN: Towards realtime object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, Proc. null28S. Ren, K. He, R. Girshick, and J. Sun, \"Faster R-CNN: Towards real- time object detection with region proposal networks,\" in Proc. Adv. Neural Inf. Process. Syst., vol. 28, 2015, pp. 91-99.\n\nSocial MIL: Interactionaware for crowd anomaly detection. S Lin, H Yang, X Tang, T Shi, L Chen, Proc. 16th IEEE Int. Conf. Adv. Video Signal Based Surveill. (AVSS). 16th IEEE Int. Conf. Adv. Video Signal Based Surveill. (AVSS)S. Lin, H. Yang, X. Tang, T. Shi, and L. Chen, \"Social MIL: Interaction- aware for crowd anomaly detection,\" in Proc. 16th IEEE Int. Conf. Adv. Video Signal Based Surveill. (AVSS), Sep. 2019, pp. 1-8.\n\nAnomaly locality in video surveillance. F Landi, C G M Snoek, R Cucchiara, arXiv:1901.10364F. Landi, C. G. M. Snoek, and R. Cucchiara, \"Anomaly locality in video surveillance,\" 2019, arXiv:1901.10364.\n\nFuture frame prediction for anomaly detection-A new baseline. W Liu, W Luo, D Lian, S Gao, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitW. Liu, W. Luo, D. Lian, and S. Gao, \"Future frame prediction for anomaly detection-A new baseline,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 6536-6545.\n\nf-AnoGAN: Fast unsupervised anomaly detection with generative adversarial networks. T Schlegl, P Seeb\u00f6ck, S M Waldstein, G Langs, U Schmidt-Erfurth, Med. Image Anal. 54T. Schlegl, P. Seeb\u00f6ck, S. M. Waldstein, G. Langs, and U. Schmidt- Erfurth, \"f-AnoGAN: Fast unsupervised anomaly detection with gen- erative adversarial networks,\" Med. Image Anal., vol. 54, pp. 30-44, May 2019.\n\nAdversarial feature learning. J Donahue, P Kr\u00e4henb\u00fchl, T Darrell, arXiv:1605.09782J. Donahue, P. Kr\u00e4henb\u00fchl, and T. Darrell, \"Adversarial feature learn- ing,\" 2016, arXiv:1605.09782.\n\nLearning temporal regularity in video sequences. M Hasan, J Choi, J Neumann, A K Roy-Chowdhury, L S Davis, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitM. Hasan, J. Choi, J. Neumann, A. K. Roy-Chowdhury, and L. S. Davis, \"Learning temporal regularity in video sequences,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2016, pp. 733-742.\n\nAnomaly detection in video using predictive convolutional long short-term memory networks. J , Ryan Medel, A Savakis, arXiv:1612.00390J. Ryan Medel and A. Savakis, \"Anomaly detection in video using predictive convolutional long short-term memory networks,\" 2016, arXiv:1612.00390.\n\nTemporal subspace clustering for human motion segmentation. S Li, K Li, Y Fu, Proc. IEEE Int. Conf. Comput. Vis. (ICCV). IEEE Int. Conf. Comput. Vis. (ICCV)S. Li, K. Li, and Y. Fu, \"Temporal subspace clustering for human motion segmentation,\" in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Dec. 2015, pp. 4453-4461.\n\nSubspace clustering. R Vidal, IEEE Signal Process. Mag. 282R. Vidal, \"Subspace clustering,\" IEEE Signal Process. Mag., vol. 28, no. 2, pp. 52-68, Mar. 2011.\n\nA survey on: Content based image retrieval systems using clustering techniques for large data sets. M Jain, S K Singh, Int. J. Manag. Inf. Technol. 34M. Jain and S. K. Singh, \"A survey on: Content based image retrieval systems using clustering techniques for large data sets,\" Int. J. Manag. Inf. Technol., vol. 3, no. 4, pp. 23-39, Nov. 2011.\n\nRobust recovery of subspace structures by low-rank representation. G Liu, Z Lin, S Yan, J Sun, Y Yu, Y Ma, IEEE Trans. Pattern Anal. Mach. Intell. 351G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, \"Robust recovery of subspace structures by low-rank representation,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 1, pp. 171-184, Jan. 2013.\n\nSparse subspace clustering: Algorithm, theory, and applications. E Elhamifar, R Vidal, IEEE Trans. Pattern Anal. Mach. Intell. 3511E. Elhamifar and R. Vidal, \"Sparse subspace clustering: Algorithm, theory, and applications,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 11, pp. 2765-2781, Mar. 2013.\n\nRobust subspace segmentation by self-representation constrained low-rank representation. L Wei, X Wang, A Wu, R Zhou, C Zhu, Neural Process. Lett. 483L. Wei, X. Wang, A. Wu, R. Zhou, and C. Zhu, \"Robust subspace seg- mentation by self-representation constrained low-rank representation,\" Neural Process. Lett., vol. 48, no. 3, pp. 1671-1691, Dec. 2018.\n\nNormalized cuts and image segmentation. J Shi, J Malik, IEEE Trans. Pattern Anal. Mach. Intell. 228J. Shi and J. Malik, \"Normalized cuts and image segmentation,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, no. 8, pp. 888-905, Aug. 2000.\n\nYou only look once: Unified, real-time object detection. J Redmon, S Divvala, R Girshick, A Farhadi, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitJ. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \"You only look once: Unified, real-time object detection,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2016, pp. 779-788.\n\nRegionbased scalable smart system for anomaly detection in pedestrian walkways. B S Murugan, M Elhoseny, K Shankar, J Uthayakumar, Comput. Electr. Eng. 75B. S. Murugan, M. Elhoseny, K. Shankar, and J. Uthayakumar, \"Region- based scalable smart system for anomaly detection in pedestrian walk- ways,\" Comput. Electr. Eng., vol. 75, pp. 146-160, May 2019.\n\nWhen, where, and what? A new dataset for anomaly detection in driving videos. Y Yao, X Wang, M Xu, Z Pu, E Atkins, D Crandall, arXiv:2004.030442020Y. Yao, X. Wang, M. Xu, Z. Pu, E. Atkins, and D. Crandall, \"When, where, and what? A new dataset for anomaly detection in driving videos,\" 2020, arXiv:2004.03044.\n\nUnsupervised traffic accident detection in first-person videos. Y Yao, M Xu, Y Wang, D J Crandall, E M Atkins, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS). IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS)Y. Yao, M. Xu, Y. Wang, D. J. Crandall, and E. M. Atkins, \"Unsuper- vised traffic accident detection in first-person videos,\" in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), Nov. 2019, pp. 273-280.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural Comput. 98S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\" Neural Comput., vol. 9, no. 8, pp. 1735-1780, 1997.\n\nA deep neural network for unsupervised anomaly detection and diagnosis in multivariate time series data. C Zhang, Proc. AAAI. AAAI2019C. Zhang et al., \"A deep neural network for unsupervised anomaly detection and diagnosis in multivariate time series data,\" in Proc. AAAI. Artif. Intell., vol. 2019, vol. 33, no. 1, pp. 1409-1416.\n\nMulti-head CNN-RNN for multi-time series anomaly detection: An industrial case study. M Canizo, I Triguero, A Conde, E Onieva, Neurocomputing. 363M. Canizo, I. Triguero, A. Conde, and E. Onieva, \"Multi-head CNN-RNN for multi-time series anomaly detection: An industrial case study,\" Neurocomputing, vol. 363, pp. 246-260, Oct. 2019.\n", "annotations": {"author": "[{\"end\":123,\"start\":100},{\"end\":137,\"start\":124},{\"end\":160,\"start\":138},{\"end\":183,\"start\":161},{\"end\":194,\"start\":184},{\"end\":225,\"start\":195}]", "publisher": null, "author_last_name": "[{\"end\":122,\"start\":118},{\"end\":136,\"start\":132},{\"end\":159,\"start\":157},{\"end\":182,\"start\":180},{\"end\":193,\"start\":189},{\"end\":224,\"start\":220}]", "author_first_name": "[{\"end\":117,\"start\":112},{\"end\":131,\"start\":124},{\"end\":156,\"start\":150},{\"end\":179,\"start\":173},{\"end\":188,\"start\":184},{\"end\":219,\"start\":214}]", "author_affiliation": null, "title": "[{\"end\":85,\"start\":1},{\"end\":310,\"start\":226}]", "venue": "[{\"end\":367,\"start\":312}]", "abstract": "[{\"end\":1671,\"start\":509}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1726,\"start\":1723},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1731,\"start\":1728},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2112,\"start\":2109},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2117,\"start\":2114},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2148,\"start\":2145},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2177,\"start\":2174},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2390,\"start\":2387},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2395,\"start\":2392},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2511,\"start\":2508},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2517,\"start\":2513},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2542,\"start\":2538},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2572,\"start\":2568},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2604,\"start\":2600},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2610,\"start\":2606},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3141,\"start\":3137},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3147,\"start\":3143},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4154,\"start\":4150},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4160,\"start\":4156},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4413,\"start\":4409},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4419,\"start\":4415},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5834,\"start\":5830},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6044,\"start\":6040},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6341,\"start\":6337},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8528,\"start\":8524},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8534,\"start\":8530},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8554,\"start\":8550},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8560,\"start\":8556},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8603,\"start\":8599},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8609,\"start\":8605},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8812,\"start\":8808},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8818,\"start\":8814},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9334,\"start\":9330},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9382,\"start\":9378},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9388,\"start\":9384},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9940,\"start\":9936},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10174,\"start\":10170},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10180,\"start\":10176},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10270,\"start\":10266},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10547,\"start\":10543},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10621,\"start\":10617},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11373,\"start\":11369},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11634,\"start\":11630},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12268,\"start\":12264},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12456,\"start\":12452},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13033,\"start\":13029},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14008,\"start\":14004},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14441,\"start\":14437},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15041,\"start\":15037},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15105,\"start\":15101},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15473,\"start\":15469},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":15687,\"start\":15683},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16684,\"start\":16680},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16969,\"start\":16965},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17584,\"start\":17580},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18412,\"start\":18408},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18501,\"start\":18497},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22969,\"start\":22965},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":22992,\"start\":22988},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23018,\"start\":23014},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23205,\"start\":23201},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":23247,\"start\":23243},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":23422,\"start\":23418},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":24382,\"start\":24378},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":24779,\"start\":24775},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25889,\"start\":25885},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25910,\"start\":25906},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26277,\"start\":26273},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30579,\"start\":30576},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30604,\"start\":30600},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30634,\"start\":30630},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":30673,\"start\":30669},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":30838,\"start\":30834},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":34909,\"start\":34905},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34977,\"start\":34973},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":34991,\"start\":34987},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":35010,\"start\":35006},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":35037,\"start\":35033},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":35076,\"start\":35072},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":35107,\"start\":35103},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":35122,\"start\":35118},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":35133,\"start\":35129},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":35155,\"start\":35151},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":35199,\"start\":35195},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":39924,\"start\":39920}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39626,\"start\":38591},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39787,\"start\":39627},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39866,\"start\":39788},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39925,\"start\":39867},{\"attributes\":{\"id\":\"fig_4\"},\"end\":40003,\"start\":39926},{\"attributes\":{\"id\":\"fig_5\"},\"end\":40069,\"start\":40004},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40136,\"start\":40070},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":41029,\"start\":40137},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":41094,\"start\":41030},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41154,\"start\":41095},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":41204,\"start\":41155},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":41268,\"start\":41205},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":41382,\"start\":41269},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":41566,\"start\":41383}]", "paragraph": "[{\"end\":2178,\"start\":1673},{\"end\":3041,\"start\":2180},{\"end\":4024,\"start\":3043},{\"end\":5081,\"start\":4026},{\"end\":5506,\"start\":5083},{\"end\":6554,\"start\":5508},{\"end\":7190,\"start\":6556},{\"end\":7401,\"start\":7192},{\"end\":7887,\"start\":7403},{\"end\":8150,\"start\":7889},{\"end\":8702,\"start\":8152},{\"end\":9523,\"start\":8742},{\"end\":10622,\"start\":9525},{\"end\":11096,\"start\":10624},{\"end\":11613,\"start\":11138},{\"end\":13146,\"start\":11615},{\"end\":13443,\"start\":13148},{\"end\":13904,\"start\":13483},{\"end\":15800,\"start\":13906},{\"end\":16151,\"start\":15802},{\"end\":16490,\"start\":16153},{\"end\":16833,\"start\":16532},{\"end\":17145,\"start\":16835},{\"end\":17550,\"start\":17147},{\"end\":17817,\"start\":17552},{\"end\":18249,\"start\":17871},{\"end\":18621,\"start\":18251},{\"end\":19287,\"start\":18672},{\"end\":19547,\"start\":19289},{\"end\":20078,\"start\":19913},{\"end\":20315,\"start\":20080},{\"end\":20615,\"start\":20380},{\"end\":20803,\"start\":20617},{\"end\":21059,\"start\":20881},{\"end\":21272,\"start\":21154},{\"end\":21493,\"start\":21433},{\"end\":21640,\"start\":21495},{\"end\":21857,\"start\":21717},{\"end\":22054,\"start\":21927},{\"end\":22146,\"start\":22056},{\"end\":22285,\"start\":22244},{\"end\":22435,\"start\":22328},{\"end\":22679,\"start\":22467},{\"end\":22866,\"start\":22728},{\"end\":23950,\"start\":22868},{\"end\":24435,\"start\":23952},{\"end\":24554,\"start\":24476},{\"end\":25001,\"start\":24597},{\"end\":25564,\"start\":25052},{\"end\":25800,\"start\":25566},{\"end\":26224,\"start\":25802},{\"end\":26748,\"start\":26226},{\"end\":27216,\"start\":26841},{\"end\":27531,\"start\":27218},{\"end\":28140,\"start\":27533},{\"end\":28331,\"start\":28142},{\"end\":28732,\"start\":28365},{\"end\":29072,\"start\":28734},{\"end\":29659,\"start\":29074},{\"end\":29760,\"start\":29682},{\"end\":30023,\"start\":29792},{\"end\":30150,\"start\":30047},{\"end\":30829,\"start\":30197},{\"end\":31328,\"start\":30831},{\"end\":31915,\"start\":31330},{\"end\":32662,\"start\":31963},{\"end\":33544,\"start\":32664},{\"end\":34688,\"start\":33546},{\"end\":37235,\"start\":34733},{\"end\":37980,\"start\":37253},{\"end\":38590,\"start\":37982}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18671,\"start\":18622},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19912,\"start\":19548},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20379,\"start\":20316},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20880,\"start\":20804},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21153,\"start\":21060},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21323,\"start\":21273},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21432,\"start\":21323},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21716,\"start\":21641},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21926,\"start\":21858},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22243,\"start\":22147},{\"attributes\":{\"id\":\"formula_10\"},\"end\":22327,\"start\":22286},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22466,\"start\":22436},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22727,\"start\":22680},{\"attributes\":{\"id\":\"formula_13\"},\"end\":24475,\"start\":24436},{\"attributes\":{\"id\":\"formula_14\"},\"end\":26840,\"start\":26749},{\"attributes\":{\"id\":\"formula_15\"},\"end\":28364,\"start\":28332},{\"attributes\":{\"id\":\"formula_16\"},\"end\":29681,\"start\":29660},{\"attributes\":{\"id\":\"formula_17\"},\"end\":29791,\"start\":29761},{\"attributes\":{\"id\":\"formula_18\"},\"end\":30046,\"start\":30024}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31153,\"start\":31146},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31813,\"start\":31805},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34003,\"start\":33994},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35499,\"start\":35491},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35585,\"start\":35577},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35852,\"start\":35844},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":36272,\"start\":36265}]", "section_header": "[{\"end\":8740,\"start\":8705},{\"end\":11136,\"start\":11099},{\"end\":13481,\"start\":13446},{\"end\":16530,\"start\":16493},{\"end\":17869,\"start\":17820},{\"end\":24595,\"start\":24557},{\"end\":25050,\"start\":25004},{\"end\":30181,\"start\":30153},{\"end\":30195,\"start\":30184},{\"end\":31961,\"start\":31918},{\"end\":34731,\"start\":34691},{\"end\":37251,\"start\":37238},{\"end\":38593,\"start\":38592},{\"end\":39636,\"start\":39628},{\"end\":39797,\"start\":39789},{\"end\":39876,\"start\":39868},{\"end\":39935,\"start\":39927},{\"end\":40013,\"start\":40005},{\"end\":40079,\"start\":40071},{\"end\":41049,\"start\":41031},{\"end\":41112,\"start\":41096},{\"end\":41176,\"start\":41156},{\"end\":41218,\"start\":41206},{\"end\":41275,\"start\":41270},{\"end\":41391,\"start\":41384}]", "table": null, "figure_caption": "[{\"end\":39626,\"start\":38594},{\"end\":39787,\"start\":39638},{\"end\":39866,\"start\":39799},{\"end\":39925,\"start\":39878},{\"end\":40003,\"start\":39937},{\"end\":40069,\"start\":40015},{\"end\":40136,\"start\":40081},{\"end\":41029,\"start\":40139},{\"end\":41094,\"start\":41051},{\"end\":41154,\"start\":41115},{\"end\":41204,\"start\":41180},{\"end\":41268,\"start\":41221},{\"end\":41382,\"start\":41276},{\"end\":41566,\"start\":41393}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5945,\"start\":5939},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16704,\"start\":16698},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19286,\"start\":19280},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19427,\"start\":19409},{\"end\":27370,\"start\":27347},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28104,\"start\":28098},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32995,\"start\":32989},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33108,\"start\":33097},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":33405,\"start\":33386}]", "bib_author_first_name": "[{\"end\":41674,\"start\":41673},{\"end\":41676,\"start\":41675},{\"end\":41685,\"start\":41684},{\"end\":41698,\"start\":41694},{\"end\":41705,\"start\":41704},{\"end\":42154,\"start\":42153},{\"end\":42162,\"start\":42161},{\"end\":42172,\"start\":42171},{\"end\":42174,\"start\":42173},{\"end\":42183,\"start\":42182},{\"end\":42185,\"start\":42184},{\"end\":42195,\"start\":42194},{\"end\":42637,\"start\":42636},{\"end\":42649,\"start\":42648},{\"end\":42660,\"start\":42659},{\"end\":42662,\"start\":42661},{\"end\":42999,\"start\":42998},{\"end\":43307,\"start\":43306},{\"end\":43309,\"start\":43308},{\"end\":43318,\"start\":43317},{\"end\":43327,\"start\":43326},{\"end\":43329,\"start\":43328},{\"end\":43339,\"start\":43338},{\"end\":43341,\"start\":43340},{\"end\":43353,\"start\":43352},{\"end\":43362,\"start\":43361},{\"end\":43364,\"start\":43363},{\"end\":43866,\"start\":43865},{\"end\":43868,\"start\":43867},{\"end\":43877,\"start\":43876},{\"end\":43886,\"start\":43885},{\"end\":43888,\"start\":43887},{\"end\":43898,\"start\":43897},{\"end\":43900,\"start\":43899},{\"end\":44300,\"start\":44299},{\"end\":44315,\"start\":44314},{\"end\":44317,\"start\":44316},{\"end\":44331,\"start\":44330},{\"end\":44333,\"start\":44332},{\"end\":44345,\"start\":44341},{\"end\":44794,\"start\":44793},{\"end\":44808,\"start\":44807},{\"end\":44810,\"start\":44809},{\"end\":44824,\"start\":44823},{\"end\":44826,\"start\":44825},{\"end\":45130,\"start\":45129},{\"end\":45138,\"start\":45137},{\"end\":45458,\"start\":45457},{\"end\":45468,\"start\":45467},{\"end\":45470,\"start\":45469},{\"end\":45774,\"start\":45773},{\"end\":45787,\"start\":45786},{\"end\":45798,\"start\":45797},{\"end\":45807,\"start\":45806},{\"end\":46179,\"start\":46178},{\"end\":46189,\"start\":46188},{\"end\":46201,\"start\":46200},{\"end\":46214,\"start\":46213},{\"end\":46225,\"start\":46224},{\"end\":46582,\"start\":46581},{\"end\":46595,\"start\":46594},{\"end\":46605,\"start\":46604},{\"end\":46935,\"start\":46934},{\"end\":46945,\"start\":46944},{\"end\":46955,\"start\":46954},{\"end\":46968,\"start\":46967},{\"end\":47253,\"start\":47252},{\"end\":47265,\"start\":47264},{\"end\":47277,\"start\":47276},{\"end\":47286,\"start\":47285},{\"end\":47625,\"start\":47624},{\"end\":47629,\"start\":47626},{\"end\":47636,\"start\":47635},{\"end\":47640,\"start\":47637},{\"end\":47837,\"start\":47836},{\"end\":47848,\"start\":47847},{\"end\":47856,\"start\":47855},{\"end\":48191,\"start\":48190},{\"end\":48198,\"start\":48197},{\"end\":48427,\"start\":48426},{\"end\":48438,\"start\":48437},{\"end\":48449,\"start\":48448},{\"end\":48451,\"start\":48450},{\"end\":48464,\"start\":48463},{\"end\":48483,\"start\":48482},{\"end\":48899,\"start\":48898},{\"end\":48909,\"start\":48908},{\"end\":48916,\"start\":48915},{\"end\":48927,\"start\":48926},{\"end\":48936,\"start\":48935},{\"end\":48938,\"start\":48937},{\"end\":49196,\"start\":49195},{\"end\":49204,\"start\":49203},{\"end\":49208,\"start\":49205},{\"end\":49464,\"start\":49463},{\"end\":49472,\"start\":49471},{\"end\":49734,\"start\":49733},{\"end\":49742,\"start\":49741},{\"end\":49891,\"start\":49890},{\"end\":49901,\"start\":49900},{\"end\":50166,\"start\":50165},{\"end\":50168,\"start\":50167},{\"end\":50176,\"start\":50175},{\"end\":50178,\"start\":50177},{\"end\":50185,\"start\":50184},{\"end\":50193,\"start\":50192},{\"end\":50197,\"start\":50194},{\"end\":50506,\"start\":50505},{\"end\":50515,\"start\":50514},{\"end\":50523,\"start\":50522},{\"end\":50529,\"start\":50528},{\"end\":50535,\"start\":50534},{\"end\":50780,\"start\":50779},{\"end\":50782,\"start\":50781},{\"end\":50789,\"start\":50788},{\"end\":50791,\"start\":50790},{\"end\":50802,\"start\":50801},{\"end\":50806,\"start\":50803},{\"end\":50813,\"start\":50812},{\"end\":50821,\"start\":50820},{\"end\":50830,\"start\":50829},{\"end\":50832,\"start\":50831},{\"end\":51226,\"start\":51225},{\"end\":51233,\"start\":51232},{\"end\":51239,\"start\":51238},{\"end\":51247,\"start\":51246},{\"end\":51256,\"start\":51255},{\"end\":51263,\"start\":51262},{\"end\":51271,\"start\":51270},{\"end\":51600,\"start\":51599},{\"end\":51607,\"start\":51606},{\"end\":51613,\"start\":51612},{\"end\":51625,\"start\":51624},{\"end\":51894,\"start\":51893},{\"end\":51901,\"start\":51900},{\"end\":51909,\"start\":51908},{\"end\":51917,\"start\":51916},{\"end\":51924,\"start\":51923},{\"end\":52304,\"start\":52303},{\"end\":52313,\"start\":52312},{\"end\":52317,\"start\":52314},{\"end\":52326,\"start\":52325},{\"end\":52528,\"start\":52527},{\"end\":52535,\"start\":52534},{\"end\":52542,\"start\":52541},{\"end\":52550,\"start\":52549},{\"end\":52921,\"start\":52920},{\"end\":52932,\"start\":52931},{\"end\":52943,\"start\":52942},{\"end\":52945,\"start\":52944},{\"end\":52958,\"start\":52957},{\"end\":52967,\"start\":52966},{\"end\":53248,\"start\":53247},{\"end\":53259,\"start\":53258},{\"end\":53273,\"start\":53272},{\"end\":53451,\"start\":53450},{\"end\":53460,\"start\":53459},{\"end\":53468,\"start\":53467},{\"end\":53479,\"start\":53478},{\"end\":53481,\"start\":53480},{\"end\":53498,\"start\":53497},{\"end\":53500,\"start\":53499},{\"end\":53885,\"start\":53884},{\"end\":53892,\"start\":53888},{\"end\":53901,\"start\":53900},{\"end\":54136,\"start\":54135},{\"end\":54142,\"start\":54141},{\"end\":54148,\"start\":54147},{\"end\":54413,\"start\":54412},{\"end\":54650,\"start\":54649},{\"end\":54658,\"start\":54657},{\"end\":54660,\"start\":54659},{\"end\":54962,\"start\":54961},{\"end\":54969,\"start\":54968},{\"end\":54976,\"start\":54975},{\"end\":54983,\"start\":54982},{\"end\":54990,\"start\":54989},{\"end\":54996,\"start\":54995},{\"end\":55311,\"start\":55310},{\"end\":55324,\"start\":55323},{\"end\":55645,\"start\":55644},{\"end\":55652,\"start\":55651},{\"end\":55660,\"start\":55659},{\"end\":55666,\"start\":55665},{\"end\":55674,\"start\":55673},{\"end\":55950,\"start\":55949},{\"end\":55957,\"start\":55956},{\"end\":56211,\"start\":56210},{\"end\":56221,\"start\":56220},{\"end\":56232,\"start\":56231},{\"end\":56244,\"start\":56243},{\"end\":56611,\"start\":56610},{\"end\":56613,\"start\":56612},{\"end\":56624,\"start\":56623},{\"end\":56636,\"start\":56635},{\"end\":56647,\"start\":56646},{\"end\":56964,\"start\":56963},{\"end\":56971,\"start\":56970},{\"end\":56979,\"start\":56978},{\"end\":56985,\"start\":56984},{\"end\":56991,\"start\":56990},{\"end\":57001,\"start\":57000},{\"end\":57261,\"start\":57260},{\"end\":57268,\"start\":57267},{\"end\":57274,\"start\":57273},{\"end\":57282,\"start\":57281},{\"end\":57284,\"start\":57283},{\"end\":57296,\"start\":57295},{\"end\":57298,\"start\":57297},{\"end\":57643,\"start\":57642},{\"end\":57657,\"start\":57656},{\"end\":57907,\"start\":57906},{\"end\":58220,\"start\":58219},{\"end\":58230,\"start\":58229},{\"end\":58242,\"start\":58241},{\"end\":58251,\"start\":58250}]", "bib_author_last_name": "[{\"end\":41682,\"start\":41677},{\"end\":41692,\"start\":41686},{\"end\":41702,\"start\":41699},{\"end\":41710,\"start\":41706},{\"end\":42159,\"start\":42155},{\"end\":42169,\"start\":42163},{\"end\":42180,\"start\":42175},{\"end\":42192,\"start\":42186},{\"end\":42202,\"start\":42196},{\"end\":42646,\"start\":42638},{\"end\":42657,\"start\":42650},{\"end\":42672,\"start\":42663},{\"end\":43003,\"start\":43000},{\"end\":43315,\"start\":43310},{\"end\":43324,\"start\":43319},{\"end\":43336,\"start\":43330},{\"end\":43350,\"start\":43342},{\"end\":43359,\"start\":43354},{\"end\":43372,\"start\":43365},{\"end\":43874,\"start\":43869},{\"end\":43883,\"start\":43878},{\"end\":43895,\"start\":43889},{\"end\":43912,\"start\":43901},{\"end\":44312,\"start\":44301},{\"end\":44328,\"start\":44318},{\"end\":44339,\"start\":44334},{\"end\":44349,\"start\":44346},{\"end\":44805,\"start\":44795},{\"end\":44821,\"start\":44811},{\"end\":44832,\"start\":44827},{\"end\":45135,\"start\":45131},{\"end\":45148,\"start\":45139},{\"end\":45465,\"start\":45459},{\"end\":45476,\"start\":45471},{\"end\":45784,\"start\":45775},{\"end\":45795,\"start\":45788},{\"end\":45804,\"start\":45799},{\"end\":45814,\"start\":45808},{\"end\":45967,\"start\":45959},{\"end\":45972,\"start\":45969},{\"end\":46186,\"start\":46180},{\"end\":46198,\"start\":46190},{\"end\":46211,\"start\":46202},{\"end\":46222,\"start\":46215},{\"end\":46233,\"start\":46226},{\"end\":46592,\"start\":46583},{\"end\":46602,\"start\":46596},{\"end\":46613,\"start\":46606},{\"end\":46942,\"start\":46936},{\"end\":46952,\"start\":46946},{\"end\":46965,\"start\":46956},{\"end\":46976,\"start\":46969},{\"end\":47262,\"start\":47254},{\"end\":47274,\"start\":47266},{\"end\":47283,\"start\":47278},{\"end\":47292,\"start\":47287},{\"end\":47633,\"start\":47630},{\"end\":47645,\"start\":47641},{\"end\":47845,\"start\":47838},{\"end\":47853,\"start\":47849},{\"end\":47861,\"start\":47857},{\"end\":48195,\"start\":48192},{\"end\":48205,\"start\":48199},{\"end\":48435,\"start\":48428},{\"end\":48446,\"start\":48439},{\"end\":48461,\"start\":48452},{\"end\":48480,\"start\":48465},{\"end\":48489,\"start\":48484},{\"end\":48906,\"start\":48900},{\"end\":48913,\"start\":48910},{\"end\":48924,\"start\":48917},{\"end\":48933,\"start\":48928},{\"end\":48952,\"start\":48939},{\"end\":49201,\"start\":49197},{\"end\":49211,\"start\":49209},{\"end\":49469,\"start\":49465},{\"end\":49479,\"start\":49473},{\"end\":49739,\"start\":49735},{\"end\":49747,\"start\":49743},{\"end\":49898,\"start\":49892},{\"end\":49908,\"start\":49902},{\"end\":50173,\"start\":50169},{\"end\":50182,\"start\":50179},{\"end\":50190,\"start\":50186},{\"end\":50204,\"start\":50198},{\"end\":50512,\"start\":50507},{\"end\":50520,\"start\":50516},{\"end\":50526,\"start\":50524},{\"end\":50532,\"start\":50530},{\"end\":50538,\"start\":50536},{\"end\":50786,\"start\":50783},{\"end\":50799,\"start\":50792},{\"end\":50810,\"start\":50807},{\"end\":50818,\"start\":50814},{\"end\":50827,\"start\":50822},{\"end\":50836,\"start\":50833},{\"end\":51230,\"start\":51227},{\"end\":51236,\"start\":51234},{\"end\":51244,\"start\":51240},{\"end\":51253,\"start\":51248},{\"end\":51260,\"start\":51257},{\"end\":51268,\"start\":51264},{\"end\":51274,\"start\":51272},{\"end\":51604,\"start\":51601},{\"end\":51610,\"start\":51608},{\"end\":51622,\"start\":51614},{\"end\":51629,\"start\":51626},{\"end\":51898,\"start\":51895},{\"end\":51906,\"start\":51902},{\"end\":51914,\"start\":51910},{\"end\":51921,\"start\":51918},{\"end\":51929,\"start\":51925},{\"end\":52310,\"start\":52305},{\"end\":52323,\"start\":52318},{\"end\":52336,\"start\":52327},{\"end\":52532,\"start\":52529},{\"end\":52539,\"start\":52536},{\"end\":52547,\"start\":52543},{\"end\":52554,\"start\":52551},{\"end\":52929,\"start\":52922},{\"end\":52940,\"start\":52933},{\"end\":52955,\"start\":52946},{\"end\":52964,\"start\":52959},{\"end\":52983,\"start\":52968},{\"end\":53256,\"start\":53249},{\"end\":53270,\"start\":53260},{\"end\":53281,\"start\":53274},{\"end\":53457,\"start\":53452},{\"end\":53465,\"start\":53461},{\"end\":53476,\"start\":53469},{\"end\":53495,\"start\":53482},{\"end\":53506,\"start\":53501},{\"end\":53898,\"start\":53893},{\"end\":53909,\"start\":53902},{\"end\":54139,\"start\":54137},{\"end\":54145,\"start\":54143},{\"end\":54151,\"start\":54149},{\"end\":54419,\"start\":54414},{\"end\":54655,\"start\":54651},{\"end\":54666,\"start\":54661},{\"end\":54966,\"start\":54963},{\"end\":54973,\"start\":54970},{\"end\":54980,\"start\":54977},{\"end\":54987,\"start\":54984},{\"end\":54993,\"start\":54991},{\"end\":54999,\"start\":54997},{\"end\":55321,\"start\":55312},{\"end\":55330,\"start\":55325},{\"end\":55649,\"start\":55646},{\"end\":55657,\"start\":55653},{\"end\":55663,\"start\":55661},{\"end\":55671,\"start\":55667},{\"end\":55678,\"start\":55675},{\"end\":55954,\"start\":55951},{\"end\":55963,\"start\":55958},{\"end\":56218,\"start\":56212},{\"end\":56229,\"start\":56222},{\"end\":56241,\"start\":56233},{\"end\":56252,\"start\":56245},{\"end\":56621,\"start\":56614},{\"end\":56633,\"start\":56625},{\"end\":56644,\"start\":56637},{\"end\":56659,\"start\":56648},{\"end\":56968,\"start\":56965},{\"end\":56976,\"start\":56972},{\"end\":56982,\"start\":56980},{\"end\":56988,\"start\":56986},{\"end\":56998,\"start\":56992},{\"end\":57010,\"start\":57002},{\"end\":57265,\"start\":57262},{\"end\":57271,\"start\":57269},{\"end\":57279,\"start\":57275},{\"end\":57293,\"start\":57285},{\"end\":57305,\"start\":57299},{\"end\":57654,\"start\":57644},{\"end\":57669,\"start\":57658},{\"end\":57913,\"start\":57908},{\"end\":58227,\"start\":58221},{\"end\":58239,\"start\":58231},{\"end\":58248,\"start\":58243},{\"end\":58258,\"start\":58252}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1109/TITS.2021.3125402\",\"id\":\"b0\",\"matched_paper_id\":244105786},\"end\":42036,\"start\":41568},{\"attributes\":{\"doi\":\"10.1109/TITS.2021.3106825\",\"id\":\"b1\",\"matched_paper_id\":239739827},\"end\":42550,\"start\":42038},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":212895482},\"end\":42911,\"start\":42552},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":53279902},\"end\":43208,\"start\":42913},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":220939373},\"end\":43773,\"start\":43210},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":227249856},\"end\":44128,\"start\":43775},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":234197321},\"end\":44699,\"start\":44130},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":226339217},\"end\":45062,\"start\":44701},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":220889494},\"end\":45368,\"start\":45064},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":219974258},\"end\":45701,\"start\":45370},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":125953399},\"end\":45957,\"start\":45703},{\"attributes\":{\"id\":\"b11\"},\"end\":46067,\"start\":45959},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":67764251},\"end\":46507,\"start\":46069},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":111380193},\"end\":46841,\"start\":46509},{\"attributes\":{\"id\":\"b14\"},\"end\":47184,\"start\":46843},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3509717},\"end\":47589,\"start\":47186},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":114800582},\"end\":47781,\"start\":47591},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1610415},\"end\":48129,\"start\":47783},{\"attributes\":{\"doi\":\"arXiv:1907.10211\",\"id\":\"b18\"},\"end\":48329,\"start\":48131},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":17427022},\"end\":48857,\"start\":48331},{\"attributes\":{\"doi\":\"arXiv:1802.06222\",\"id\":\"b20\"},\"end\":49106,\"start\":48859},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":881524},\"end\":49414,\"start\":49108},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":753512},\"end\":49681,\"start\":49416},{\"attributes\":{\"doi\":\"arXiv:1511.08458\",\"id\":\"b23\"},\"end\":49863,\"start\":49683},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":52874011},\"end\":50026,\"start\":49865},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":218834260},\"end\":50455,\"start\":50028},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4747216},\"end\":50732,\"start\":50457},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":91184191},\"end\":51148,\"start\":50734},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":52894300},\"end\":51518,\"start\":51150},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10328909},\"end\":51833,\"start\":51520},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":208280724},\"end\":52261,\"start\":51835},{\"attributes\":{\"doi\":\"arXiv:1901.10364\",\"id\":\"b31\"},\"end\":52463,\"start\":52263},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3865699},\"end\":52834,\"start\":52465},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":73516151},\"end\":53215,\"start\":52836},{\"attributes\":{\"doi\":\"arXiv:1605.09782\",\"id\":\"b34\"},\"end\":53399,\"start\":53217},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2429016},\"end\":53791,\"start\":53401},{\"attributes\":{\"doi\":\"arXiv:1612.00390\",\"id\":\"b36\"},\"end\":54073,\"start\":53793},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":707998},\"end\":54389,\"start\":54075},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":7241355},\"end\":54547,\"start\":54391},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":637472},\"end\":54892,\"start\":54549},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":2105943},\"end\":55243,\"start\":54894},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":10102189},\"end\":55553,\"start\":55245},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":24788492},\"end\":55907,\"start\":55555},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":14848918},\"end\":56151,\"start\":55909},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":206594738},\"end\":56528,\"start\":56153},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":86633982},\"end\":56883,\"start\":56530},{\"attributes\":{\"doi\":\"arXiv:2004.03044\",\"id\":\"b46\"},\"end\":57194,\"start\":56885},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":67855477},\"end\":57616,\"start\":57196},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":1915014},\"end\":57799,\"start\":57618},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":53753975},\"end\":58131,\"start\":57801},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":199670518},\"end\":58465,\"start\":58133}]", "bib_title": "[{\"end\":41671,\"start\":41568},{\"end\":42151,\"start\":42038},{\"end\":42634,\"start\":42552},{\"end\":42996,\"start\":42913},{\"end\":43304,\"start\":43210},{\"end\":43863,\"start\":43775},{\"end\":44297,\"start\":44130},{\"end\":44791,\"start\":44701},{\"end\":45127,\"start\":45064},{\"end\":45455,\"start\":45370},{\"end\":45771,\"start\":45703},{\"end\":46176,\"start\":46069},{\"end\":46579,\"start\":46509},{\"end\":47250,\"start\":47186},{\"end\":47622,\"start\":47591},{\"end\":47834,\"start\":47783},{\"end\":48424,\"start\":48331},{\"end\":49193,\"start\":49108},{\"end\":49461,\"start\":49416},{\"end\":49888,\"start\":49865},{\"end\":50163,\"start\":50028},{\"end\":50503,\"start\":50457},{\"end\":50777,\"start\":50734},{\"end\":51223,\"start\":51150},{\"end\":51597,\"start\":51520},{\"end\":51891,\"start\":51835},{\"end\":52525,\"start\":52465},{\"end\":52918,\"start\":52836},{\"end\":53448,\"start\":53401},{\"end\":54133,\"start\":54075},{\"end\":54410,\"start\":54391},{\"end\":54647,\"start\":54549},{\"end\":54959,\"start\":54894},{\"end\":55308,\"start\":55245},{\"end\":55642,\"start\":55555},{\"end\":55947,\"start\":55909},{\"end\":56208,\"start\":56153},{\"end\":56608,\"start\":56530},{\"end\":57258,\"start\":57196},{\"end\":57640,\"start\":57618},{\"end\":57904,\"start\":57801},{\"end\":58217,\"start\":58133}]", "bib_author": "[{\"end\":41684,\"start\":41673},{\"end\":41694,\"start\":41684},{\"end\":41704,\"start\":41694},{\"end\":41712,\"start\":41704},{\"end\":42161,\"start\":42153},{\"end\":42171,\"start\":42161},{\"end\":42182,\"start\":42171},{\"end\":42194,\"start\":42182},{\"end\":42204,\"start\":42194},{\"end\":42648,\"start\":42636},{\"end\":42659,\"start\":42648},{\"end\":42674,\"start\":42659},{\"end\":43005,\"start\":42998},{\"end\":43317,\"start\":43306},{\"end\":43326,\"start\":43317},{\"end\":43338,\"start\":43326},{\"end\":43352,\"start\":43338},{\"end\":43361,\"start\":43352},{\"end\":43374,\"start\":43361},{\"end\":43876,\"start\":43865},{\"end\":43885,\"start\":43876},{\"end\":43897,\"start\":43885},{\"end\":43914,\"start\":43897},{\"end\":44314,\"start\":44299},{\"end\":44330,\"start\":44314},{\"end\":44341,\"start\":44330},{\"end\":44351,\"start\":44341},{\"end\":44807,\"start\":44793},{\"end\":44823,\"start\":44807},{\"end\":44834,\"start\":44823},{\"end\":45137,\"start\":45129},{\"end\":45150,\"start\":45137},{\"end\":45467,\"start\":45457},{\"end\":45478,\"start\":45467},{\"end\":45786,\"start\":45773},{\"end\":45797,\"start\":45786},{\"end\":45806,\"start\":45797},{\"end\":45816,\"start\":45806},{\"end\":45969,\"start\":45959},{\"end\":45974,\"start\":45969},{\"end\":46188,\"start\":46178},{\"end\":46200,\"start\":46188},{\"end\":46213,\"start\":46200},{\"end\":46224,\"start\":46213},{\"end\":46235,\"start\":46224},{\"end\":46594,\"start\":46581},{\"end\":46604,\"start\":46594},{\"end\":46615,\"start\":46604},{\"end\":46944,\"start\":46934},{\"end\":46954,\"start\":46944},{\"end\":46967,\"start\":46954},{\"end\":46978,\"start\":46967},{\"end\":47264,\"start\":47252},{\"end\":47276,\"start\":47264},{\"end\":47285,\"start\":47276},{\"end\":47294,\"start\":47285},{\"end\":47635,\"start\":47624},{\"end\":47647,\"start\":47635},{\"end\":47847,\"start\":47836},{\"end\":47855,\"start\":47847},{\"end\":47863,\"start\":47855},{\"end\":48197,\"start\":48190},{\"end\":48207,\"start\":48197},{\"end\":48437,\"start\":48426},{\"end\":48448,\"start\":48437},{\"end\":48463,\"start\":48448},{\"end\":48482,\"start\":48463},{\"end\":48491,\"start\":48482},{\"end\":48908,\"start\":48898},{\"end\":48915,\"start\":48908},{\"end\":48926,\"start\":48915},{\"end\":48935,\"start\":48926},{\"end\":48954,\"start\":48935},{\"end\":49203,\"start\":49195},{\"end\":49213,\"start\":49203},{\"end\":49471,\"start\":49463},{\"end\":49481,\"start\":49471},{\"end\":49741,\"start\":49733},{\"end\":49749,\"start\":49741},{\"end\":49900,\"start\":49890},{\"end\":49910,\"start\":49900},{\"end\":50175,\"start\":50165},{\"end\":50184,\"start\":50175},{\"end\":50192,\"start\":50184},{\"end\":50206,\"start\":50192},{\"end\":50514,\"start\":50505},{\"end\":50522,\"start\":50514},{\"end\":50528,\"start\":50522},{\"end\":50534,\"start\":50528},{\"end\":50540,\"start\":50534},{\"end\":50788,\"start\":50779},{\"end\":50801,\"start\":50788},{\"end\":50812,\"start\":50801},{\"end\":50820,\"start\":50812},{\"end\":50829,\"start\":50820},{\"end\":50838,\"start\":50829},{\"end\":51232,\"start\":51225},{\"end\":51238,\"start\":51232},{\"end\":51246,\"start\":51238},{\"end\":51255,\"start\":51246},{\"end\":51262,\"start\":51255},{\"end\":51270,\"start\":51262},{\"end\":51276,\"start\":51270},{\"end\":51606,\"start\":51599},{\"end\":51612,\"start\":51606},{\"end\":51624,\"start\":51612},{\"end\":51631,\"start\":51624},{\"end\":51900,\"start\":51893},{\"end\":51908,\"start\":51900},{\"end\":51916,\"start\":51908},{\"end\":51923,\"start\":51916},{\"end\":51931,\"start\":51923},{\"end\":52312,\"start\":52303},{\"end\":52325,\"start\":52312},{\"end\":52338,\"start\":52325},{\"end\":52534,\"start\":52527},{\"end\":52541,\"start\":52534},{\"end\":52549,\"start\":52541},{\"end\":52556,\"start\":52549},{\"end\":52931,\"start\":52920},{\"end\":52942,\"start\":52931},{\"end\":52957,\"start\":52942},{\"end\":52966,\"start\":52957},{\"end\":52985,\"start\":52966},{\"end\":53258,\"start\":53247},{\"end\":53272,\"start\":53258},{\"end\":53283,\"start\":53272},{\"end\":53459,\"start\":53450},{\"end\":53467,\"start\":53459},{\"end\":53478,\"start\":53467},{\"end\":53497,\"start\":53478},{\"end\":53508,\"start\":53497},{\"end\":53888,\"start\":53884},{\"end\":53900,\"start\":53888},{\"end\":53911,\"start\":53900},{\"end\":54141,\"start\":54135},{\"end\":54147,\"start\":54141},{\"end\":54153,\"start\":54147},{\"end\":54421,\"start\":54412},{\"end\":54657,\"start\":54649},{\"end\":54668,\"start\":54657},{\"end\":54968,\"start\":54961},{\"end\":54975,\"start\":54968},{\"end\":54982,\"start\":54975},{\"end\":54989,\"start\":54982},{\"end\":54995,\"start\":54989},{\"end\":55001,\"start\":54995},{\"end\":55323,\"start\":55310},{\"end\":55332,\"start\":55323},{\"end\":55651,\"start\":55644},{\"end\":55659,\"start\":55651},{\"end\":55665,\"start\":55659},{\"end\":55673,\"start\":55665},{\"end\":55680,\"start\":55673},{\"end\":55956,\"start\":55949},{\"end\":55965,\"start\":55956},{\"end\":56220,\"start\":56210},{\"end\":56231,\"start\":56220},{\"end\":56243,\"start\":56231},{\"end\":56254,\"start\":56243},{\"end\":56623,\"start\":56610},{\"end\":56635,\"start\":56623},{\"end\":56646,\"start\":56635},{\"end\":56661,\"start\":56646},{\"end\":56970,\"start\":56963},{\"end\":56978,\"start\":56970},{\"end\":56984,\"start\":56978},{\"end\":56990,\"start\":56984},{\"end\":57000,\"start\":56990},{\"end\":57012,\"start\":57000},{\"end\":57267,\"start\":57260},{\"end\":57273,\"start\":57267},{\"end\":57281,\"start\":57273},{\"end\":57295,\"start\":57281},{\"end\":57307,\"start\":57295},{\"end\":57656,\"start\":57642},{\"end\":57671,\"start\":57656},{\"end\":57915,\"start\":57906},{\"end\":58229,\"start\":58219},{\"end\":58241,\"start\":58229},{\"end\":58250,\"start\":58241},{\"end\":58260,\"start\":58250}]", "bib_venue": "[{\"end\":41784,\"start\":41737},{\"end\":42261,\"start\":42229},{\"end\":42704,\"start\":42674},{\"end\":43029,\"start\":43005},{\"end\":43440,\"start\":43374},{\"end\":43923,\"start\":43914},{\"end\":44383,\"start\":44351},{\"end\":44854,\"start\":44834},{\"end\":45185,\"start\":45150},{\"end\":45508,\"start\":45478},{\"end\":45820,\"start\":45816},{\"end\":45999,\"start\":45974},{\"end\":46263,\"start\":46235},{\"end\":46653,\"start\":46615},{\"end\":46932,\"start\":46843},{\"end\":47344,\"start\":47294},{\"end\":47658,\"start\":47647},{\"end\":47913,\"start\":47863},{\"end\":48188,\"start\":48131},{\"end\":48531,\"start\":48491},{\"end\":48896,\"start\":48859},{\"end\":49231,\"start\":49213},{\"end\":49514,\"start\":49481},{\"end\":49731,\"start\":49683},{\"end\":49921,\"start\":49910},{\"end\":50217,\"start\":50206},{\"end\":50555,\"start\":50540},{\"end\":50893,\"start\":50838},{\"end\":51303,\"start\":51276},{\"end\":51635,\"start\":51631},{\"end\":51998,\"start\":51931},{\"end\":52301,\"start\":52263},{\"end\":52606,\"start\":52556},{\"end\":53000,\"start\":52985},{\"end\":53245,\"start\":53217},{\"end\":53554,\"start\":53508},{\"end\":53882,\"start\":53793},{\"end\":54194,\"start\":54153},{\"end\":54445,\"start\":54421},{\"end\":54695,\"start\":54668},{\"end\":55039,\"start\":55001},{\"end\":55370,\"start\":55332},{\"end\":55700,\"start\":55680},{\"end\":56003,\"start\":55965},{\"end\":56300,\"start\":56254},{\"end\":56680,\"start\":56661},{\"end\":56961,\"start\":56885},{\"end\":57360,\"start\":57307},{\"end\":57684,\"start\":57671},{\"end\":57925,\"start\":57915},{\"end\":58274,\"start\":58260},{\"end\":43502,\"start\":43442},{\"end\":45216,\"start\":45187},{\"end\":45826,\"start\":45822},{\"end\":47390,\"start\":47346},{\"end\":47959,\"start\":47915},{\"end\":48580,\"start\":48533},{\"end\":49543,\"start\":49516},{\"end\":50566,\"start\":50557},{\"end\":50944,\"start\":50895},{\"end\":51641,\"start\":51637},{\"end\":52061,\"start\":52000},{\"end\":52652,\"start\":52608},{\"end\":53596,\"start\":53556},{\"end\":54231,\"start\":54196},{\"end\":56342,\"start\":56302},{\"end\":57409,\"start\":57362},{\"end\":57931,\"start\":57927}]"}}}, "year": 2023, "month": 12, "day": 17}