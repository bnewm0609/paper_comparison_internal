{"id": 231856277, "updated": "2023-04-04 22:51:11.594", "metadata": {"title": "Multimodal deep learning models for early detection of Alzheimer\u2019s disease stage", "authors": "[{\"first\":\"Janani\",\"last\":\"Venugopalan\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Tong\",\"middle\":[]},{\"first\":\"Hamid\",\"last\":\"Hassanzadeh\",\"middle\":[\"Reza\"]},{\"first\":\"May\",\"last\":\"Wang\",\"middle\":[\"D.\"]}]", "venue": "Scientific Reports", "journal": "Scientific Reports", "publication_date": {"year": 2021, "month": 2, "day": 5}, "abstract": "Most current Alzheimer\u2019s disease (AD) and mild cognitive disorders (MCI) studies use single data modality to make predictions such as AD stages. The fusion of multiple data modalities can provide a holistic view of AD staging analysis. Thus, we use deep learning (DL) to integrally analyze imaging (magnetic resonance imaging (MRI)), genetic (single nucleotide polymorphisms (SNPs)), and clinical test data to classify patients into AD, MCI, and controls (CN). We use stacked denoising auto-encoders to extract features from clinical and genetic data, and use 3D-convolutional neural networks (CNNs) for imaging data. We also develop a novel data interpretation method to identify top-performing features learned by the deep-models with clustering and perturbation analysis. Using Alzheimer\u2019s disease neuroimaging initiative (ADNI) dataset, we demonstrate that deep models outperform shallow models, including support vector machines, decision trees, random forests, and k-nearest neighbors. In addition, we demonstrate that integrating multi-modality data outperforms single modality models in terms of accuracy, precision, recall, and meanF1 scores. Our models have identified hippocampus, amygdala brain areas, and the Rey Auditory Verbal Learning Test (RAVLT) as top distinguished features, which are consistent with the known AD literature.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": "33547343", "pubmedcentral": "7864942", "dblp": null, "doi": "10.1038/s41598-020-74399-w"}}, "content": {"source": {"pdf_hash": "665a2d3289252d8f86c2ff8f98410dce2d787bed", "pdf_src": "PubMedCentral", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.nature.com/articles/s41598-020-74399-w.pdf", "status": "GOLD"}}, "grobid": {"id": "d9c4c49622362db7727ae709f42d76e6553706e3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/665a2d3289252d8f86c2ff8f98410dce2d787bed.txt", "contents": "\nMultimodal deep learning models for early detection of Alzheimer's disease stage\n0123456789\n\nJanani Venugopalan \nLi Tong \nHamid Reza Hassanzadeh \nSchool of Computational Science and Engineering\nGeorgia Institute of Technology\nAtlantaGAUSA\n\nMay D Wang \nSchool of Electrical and Computer Engineering\nGeorgia Institute of Technology\nAtlantaGAUSA\n\nInstitute of People and Technology\nWinship Cancer Institute\nParker H. Petit Institute for Bioengineering and Biosciences\nGeorgia Institute of Technology and Emory University\nAtlantaGAUSA\n\n\nDepartment of Biomedical Engineering\nGeorgia Institute of Technology and Emory University\nAtlantaGAUSA\n\nMultimodal deep learning models for early detection of Alzheimer's disease stage\n012345678910.1038/s41598-020-74399-w\nMost current Alzheimer's disease (AD) and mild cognitive disorders (MCI) studies use single data modality to make predictions such as AD stages. The fusion of multiple data modalities can provide a holistic view of AD staging analysis. Thus, we use deep learning (DL) to integrally analyze imaging (magnetic resonance imaging (MRI)), genetic (single nucleotide polymorphisms (SNPs)), and clinical test data to classify patients into AD, MCI, and controls (CN). We use stacked denoising auto-encoders to extract features from clinical and genetic data, and use 3D-convolutional neural networks (CNNs) for imaging data. We also develop a novel data interpretation method to identify top-performing features learned by the deep-models with clustering and perturbation analysis. Using Alzheimer's disease neuroimaging initiative (ADNI) dataset, we demonstrate that deep models outperform shallow models, including support vector machines, decision trees, random forests, and k-nearest neighbors. In addition, we demonstrate that integrating multi-modality data outperforms single modality models in terms of accuracy, precision, recall, and meanF1 scores. Our models have identified hippocampus, amygdala brain areas, and the Rey Auditory Verbal Learning Test (RAVLT) as top distinguished features, which are consistent with the known AD literature.Deep-learning (DL) has shown tremendous potential for clinical decision support for a variety of diseases, including diabetic retinopathy 1,2 , cancers 3,4, and Alzheimer's disease (for imaging analysis) 5-7 . The major strength of DL over other shallow learning models is their ability to learn the most predictive features directly from the raw data given a dataset of labeled examples. DL has shown improvement over shallow learning for single data modality such as images 8,9 , electronic health records (EHRs) 10, and SNPs 11 . DL techniques also facilitate the training and prediction in the presence of partial data 12 . In this study, we develop a novel DL architecture for clinical decision support that predicts the Alzheimer's disease (AD) stage using multi-modality data (images, clinical data, and genetic information).AD is the most common neurodegenerative disorder and the 6th leading cause of death in the United States 13,14 . The world-wide disease burden of AD is projected to reach $2 trillion by 2030 15 , which necessitates early detection. Despite extensive research and advances in clinical practice, less than 50% of the AD patients are diagnosed accurately for their pathology and disease progression based on their clinical symptoms 13 . The most conclusive evidences for AD are the presence of amyloid plaques and neurofibrillary tangles in histopathology. However, the early onset of AD is not correlated with the presence of plaque but with synaptic and neuronal loss 16 .Research on data and data mining strategies from AD initiative 17-19 are ongoing to improve our understanding of the underlying disease processes. AD biomarkers including clinical symptoms 20 (such as dementia, memory loss), neurological tests and scores such as MMSE scores are augmented with imaging, genetic, and protein biomarkers[21][22][23][24][25][26]. Most of these studies identify biomarkers using a single-modality data, which restricts a holistic assessment of AD disease progression. There have been AD multi-modal analyses that combine various imaging modalities 27-32 such as structural MRI (T1 weighted, T2 weighted), fMRI, positron emission tomography (PET) 33,34 , and imaging genetics 35 . In addition, genetics have been used with clinical data to augment data labels and phenotypes. Besides shallow learners, DL models such as auto-encoders 8 and deep-belief networks 36 (Supplementary Table A1) have been used for PET and MRI image data fusion with improved prediction. Precision: 0.79 \u00b1 0.08 Recall: 0.79 \u00b1 0.08 F1 Scores: 0.79 \u00b1 0.07 Accuracy: 0.77 Precision: 0.76 Recall: 0.77 F1 Scores: 0.77 SNP + Imaging (shallow models) Prediction (CN, MCI/AD) Random Forest Trees = 20 Mean GLCM 3 right superior temporal Sum GLCM 5 left amygdala Median GLCM 2 right hippocampus Gene10 location 108777098 Entropy intensity left hippocampus Accuracy: 0.75 \u00b1 0.11 Precision: 0.72 \u00b1 0.16 Recall: 0.65 \u00b1 0.09 F1 Scores: 0.65 \u00b1 0.12 Accuracy: 0.63 Precision: 0.62 Recall: 0.57 F1 Scores: 0.56\n\nScientific Reports\n\n| (2021) 11:3254 | https://doi.org/10.1038/s41598-020-74399-w www.nature.com/scientificreports/ In this study, we further the multi-modal AD data fusion to advance AD stage prediction by using DL to combine imaging, EHR, and genomic SNP data for the classification of patients into control (CN), MCI, and AD groups. We use stacked de-noising auto-encoders for EHR and SNP data respectively, and novel 3D convolutional neural networks (CNNs) to train MRI imaging data. After the networks are separately trained for each data modality, we combine them using different classification layers, including decision trees, random forests, support vectors machines (SVM), and k-nearest neighbors (kNN). We demonstrate the performance of our integration models using the ADNI 37 dataset that contains SNP (808 patients), MRI imaging (503 patients), and clinical and neurological test data (2004 patients).\n\nDespite superior performance in clinical decision support using multiple data types, a major drawback for widespread adoption of DL models for clinical decision making is the lack of well-defined methods for interpreting the deep models. We address this challenge by developing novel perturbations and a clustering-based approach for finding the top features contributing to the decision.\n\nIn this article, we report the major contributions for the AD stage prediction as follows:\n\n\u2022 Novel DL architectures outperform shallow learning models;\n\n\u2022 Multi-modality data analysis with DL outperforms single-modality DL models; and \u2022 Novel interpretable DL methods are capable of extracting top performing features.\n\n\nData description\n\nThis article uses Alzheimer's Disease Neuroimaging Initiative* (ADNI) database (adni.loni.usc.edu) 37 data for the analysis. ADNI aims to test whether serial MRI, PET, biological markers, and clinical and neuropsychological assessments can be combined to measure the progression of MCI and early AD. ADNI data repository contains imaging, clinical, and genetic data for over 2220 patients spanning over four studies (ADNI1, ADNI2, ADNI GO, and ADNI3). Our study focuses on ADNI1, 2 and GO because ADNI 3 is an ongoing study expected to end in 2022. The data is currently being released in phases with limited availability for unprocessed imaging data and no genetic data yet. The imaging data (ADNI1, 2 and GO) consists of MRI and PET images, of which we use crosssectional MRI data corresponding to the baseline screenings from ADNI1 (503 patients). The data publisher has standardized the images to eliminate the non-linearities caused by the scanners from different vendors. In this study, we used the cross-sectional MRI data, consisting of 9108 voxels per patient distributed over 18 \n\n\nStudy design for novel DL and multi-modality data analysis\n\nAs mentioned above, we use data from imaging (503 MRI images), SNP (808 patients) and the EHR (2004 patients) to predict AD stages. For each single data modality, we first demonstrate the superiority of deep models over shallow models such as kNN, one-vs-one coding SVM, random forests, and decision trees. The SNP and EHR features for shallow models and DL are the same. For imaging, when using DL, we apply multi-slice 3D voxels directly, while for shallow learners, we extract expert crafted features derived from the 3D voxels. Regarding AD staging, only EHR has three-stage classes CN, MCI, and AD. SNP expression does not vary between MCI and AD 38 , and only has CN vs AD/MCI prediction. On images, patients with early MCI were structurally similar to CN, and those from patients with late MCI were structurally similar to AD. As such, for imaging, only CN and AD (as seen in Ref. 39 ) are used for staging assessment. Thus, combining all three modalities can help us significantly improve AD staging prediction accuracy. As shown in Figs. 2 and 3. we have developed three data fusion strategies: (i) Feature-level combinations using shallow models, (ii) Intermediate-feature-level combinations using deep models, and (iii) Decision-level combinations using shallow models.\n\nFeature-level combinations are performed through direct concatenation of the data modalities using shallow learners (Fig. 2). The intermediate-feature-level combinations are performed by extracting intermediate features using DL, followed by concatenating and passing through a classification layer (more details are provided in methods and supplement). Decision-level combinations are performed by voting on the single-modalities. We test shallow models such as kNN, one-vs-one coding SVM, random forests, and decision trees for decision-level combinations and present the best performing model. For the intermediate-feature-level models (Fig. 3), we evaluate four combinations, (i) EHR + imaging + SNP, (ii) EHR + imaging, (iii) EHR + SNP, and (iv) imaging + SNP. For all combinations except imaging + SNP, we perform three-stage classification (CN, AD, and MCI). For imaging + SNP we perform classification into AD vs CN.\n\nAll above-mentioned cases are evaluated using an internal cross-validation and an external test set. We first remove 10% of the data as an external test set. On the remaining 90%, we perform tenfold cross-validation, with 81% of the total data being used for training and 9% for internal cross-validation. The internal cross-validation data set is used to optimize the model. \n\n\nResults for novel DL and multi-modality data analysis\n\nWe report the ADNI results for both the internal cross-validation partition and the external test dataset. For each of the DL models, or the baseline shallow models, we use mean values of accuracy, precision, recall, and meanF1 scores as metrics to show the superiority of deep models for single-modalities and the improvements gained from data integration.\n\n3D convolutional neural network (DL) is superior to shallow models on imaging MRI data. One patient's imaging data consists of 9108 3D voxels of dimension 22 \u00d7 23 \u00d7 18, corresponding to each of the five selected brain areas.  The number of nodes in DL models for the first-level fully connected layers = 5 \u00d7 20 = 100, and the number of nodes for the second level fully connected layer is 20. The results (Fig. 4a) indicate that the CNN based imaging models outperform shallow models and give the best precision and meanF1 scores.\n\nDeep autoencoder model is comparable to shallow models on EHR data. EHR    www.nature.com/scientificreports/ adequate training) of 25. After hyperparameter optimization, the regularization coefficients for initial training is fixed at 0.03 and those for fine tuning at 0.03. The dropout probability is set to 0.6 for all the layers. The results ( Fig. 4b) indicate that the autoencoders outperform shallow models such as kNN and SVM, and they are comparable to decision trees and random forests.\n\nDeep autoencoder model is superior to shallow models for SNP data. Processed SNP data consists of 808 patients with 500 features (each with levels 1, 2, 3), which we use to classify the patients into AD/ MCI vs CN (two class). The auto-encoder network consists of three hidden layers with 200, 100 and 50 nodes each. Using Adam optimization and a max epoch count of 30, the best performing models have regularization coefficients for initial training as 0.03 and those for fine tuning at 0.06. The corruption (dropouts) is 0.6 for each layer. The results (Fig. 4c) indicate that the auto-encoder models outperform all the baselines models.\n\nResults for multi-modality classification. The intermediate features generated from the single-modality deep-models are concatenated and passed to an additional classification layer for integration.   . 5a) using deep models followed by random forests as the classification layer are the best. Deep models for the combination of the three modalities outperform single-modalities DL. In addition, during combination deep model outperforms shallow models such as feature-level and decision-level for both CV and external test sets ( Table 1).\n\nCombination of SNP and EHR modalities: deep model outperforms shallow models. Internal CV accuracy of 0.78 \u00b1 0 using deep models followed by random forests as the classification layer (Fig. 5b.) are the best. The deep models for EHR + SNP combinations outperform single-modalities DL. During combination, deep model outperforms shallow models such as feature-level combination models for both CV and external test sets ( Table 1).\n\nCombination of imaging and EHR modalities: deep model outperforms shallow models. Internal CV accuracy of 0.79 \u00b1 0 using deep models followed by random forests and SVM as the classification layers (Fig. 5c) are the best. The deep models for EHR+ imaging combinations outperform single-modalities DL. In addition, during combination, DL model outperforms shallow models such as feature decision-level combination models for both CV and external test sets (Table 1). Random forests as the classification layer give the best performance on the external set.\n\nCombination of imaging and SNP modalities: shallow model outperforms deep models. We perform two-class classification using a combination of SNP and imaging intermediate features (CN vs. AD/MCI). Internal CV accuracy of 0.75 \u00b1 0.11, using feature-level combination models (Fig. 5d) is the best. However, the results on the external data are poor. The poor external validation can be attributed to having only 220 patients with both modalities of data.\n\n\nDiscussion for novel DL and multi-modality data analysis\n\nOur results suggest that the deep models outperform traditional shallow models for single-modalities. The shallow models typically require handcrafted features by experts. On the contrary, deep models can find the optimal set of features during training. In addition, deep models such as auto-encoders and CNNs can be used to perform unsupervised feature generation, and then to combine with a more sophisticated decision layer. This architecture enables the modeling of complex decision boundaries for multiclass classification problems 40 . Due to this property, deep models are particularly effective for the identification of MCI, which has been a clinical challenge in Alzheimer's research due to small differences between the three groups. Because shallow models (except random forests) do not tolerate noisy and missing data or missing modalities well, for noisy data, DL gives the best performance for single-modalities. The integration of multiple modalities improves the prediction accuracy (three of four scenarios). The deep models for integration also show improved performance over traditional feature-level and decision-level integrations. The DL's superior performance is due to its ability to extract relationships amongst features from different modalities. When the dataset is very small (e.g., the combination of imaging and SNP), deep models do not perform well. The degraded performance could be caused by the lack of training data for networks. Overall, our investigations show that:\n\n\u2022 For single-modality data (clinical, and imaging), the performances of DL models are always better than those of shallow models; and \u2022 When using DL models, predictions by multi-modality data is better than those by single-modality data. The three best fusion set ups are: EHR + SNP, EHR + Imaging + SNP, and EHR + Imaging.\n\nOne bottleneck for our proposed DL-based data integration model is the small sample size of the ADNI dataset. To mitigate the small sample size challenge, we can utilize strategies such as transfer learning and domain adaptation 41 . For each data modality, we can adopt neural networks pre-trained on other similar datasets (e.g., CNN-based MRI/CT brain imaging classification model trained for other conditions). By composing our model with these pre-trained networks and their parameters, we can perform domain adaptation or fine-tune the network parameters using our labeled ADNI data. On the other hand, we can also perform an unsupervised feature representation learning for each data modality using publicly available data (e.g., The Cancer Genome Atlas (TCGA) dataset for SNPs).Our feature extraction step is performed independently for each modality in the current DL model, which is not trained end-to-end with the integration and classification step. One future direction is to enable end-to-end training and combine auto-encoders with other integration strategies besides feature concatenation 42,43 .\n\n\nStudy design of novel feature extraction to assist in DL model interpretation\n\nModel interpretation is a major challenge for DL and is often considered as a barrier for real-world biomedical applications. Research has shown that the weights of deep models affect the results through several layers of combinations, hence do not yield clinically meaningful interpretation 44 . In this study, we develop a novel interpretation method where we mask one feature at a time and measure the drop-in accuracy (Fig. 6). The features that give the maximum drop in accuracy are ranked higher for feature extraction. \n\n\nResults and discussion of novel feature extraction to assist in DL model interpretation\n\nThe top EHR features (Table 1) include memory tests, imaging summary scores, and brain volumes. Changes to memory and brain volumes have been reported as AD biomarkers. Imaging markers such as involvement of limbic and cortical regions 45 , and changes in hippocampus volume and structure 46,47 are known biomarkers in PET and MRI studies. SNP features picked chromosome 10, 4, 19, 1, and 5. SNP + Imaging + EHR and SNP + EHR pick more EHR features (memory tests, metabolic markers and brain volume) which are known AD related features. EHR + Imaging pick EHR features including brain volumes, clinical dementia ratings, and metabolite markers. Imaging + SNP pick brain areas such as the hippocampus, and amygdala higher than SNP features.\n\nIn addition, we also cluster the intermediate features from EHR and SNP data using kmeans ( Supplementary  Information) Figs. A5, A6). Table 1. Features extraction from deep models and comparison of internal validation results with external test result. Autoencoder models are preferred for EHR and SNP data and CNN for imaging data. For multimodality models, the three modality models and two modality models (EHR + SNP, EHR + imaging gave the best prediction performance). For the multi-modality models, 3 or 4 combinations deep models outperformed shallow models. \n\n\nConclusions\n\nDiagnosing patients with AD is challenging, and the prediction accuracy remains low for staging assessment. In this study, we report the potential of DL for multi-modal data fusion, including:\n\n\u2022 Deep-models outperform shallow models for single-modality Alzheimer's stage prediction.\n\n\u2022 Novel DL framework for multi-modality data fusion outperforms single-modality DL.\n\n\u2022 Novel perturbation and clustering-based feature extraction assisting DL model interpretations are capable of AD stage prediction. \u2022 Application of 3D convolutional neural network architecture for MRI image data benefits the AD analysis.\n\nDespite the improved performance, our study suffers from short-comings such as limited dataset sizes. In the future, we will test our models on a larger and richer dataset.\n\n\nMethods\n\nIn this study, we use DL models to perform multimodal data fusion (Fig. 3) (i.e. imaging, EHR and genomic SNP data) for classifying patients into CN, MCI, and AD groups. We use stacked de-noising auto-encoders for EHR and SNP, and 3D convolutional neural networks (CNNs) for MRI imaging data. After the networks are separately trained for each data modality, we apply decision trees, random forests, support vectors machines, and k-nearest neighbors to conduct integrated classification on AD staging. Data pre-processing. As mentioned above, ADNI dataset consists of clinical data, SNP data, and imaging data.\n\nMRI imaging data. We first preprocess the 3D images to filter noise, perform skull stripping, segment different types of brain tissue, normalize and co-register the images to MNI space (Fig. 7a) 48 . Following that, we extract 3D areas of 21 brain regions (associated with Alzheimer's disease) including the right amygdala, left and right angular, left and right cerebellum, left and right Hippocampus, left and right occipital regions, and left and right superior temporal regions (Supplementary Information).\n\nClinical features. We extract 1680 common clinical features (quantitative real numbers, binary and categorical) from ADNI1, ADNI2, and ADNI GO. We normalize the quantitative data to the range 1-2, convert the categorical data into binary using one hot encoding., and finally, convert the binary data into values 1 or 2 (Fig. 7b).\n\nGenetic data. Each subject has about ~ 3 million SNPs in the raw VCF file. We apply multiple filtering and feature selection steps (Fig. 7c) to eliminate SNPs with (i) low genotype quality, (ii) low minor allele frequency, (iii) high per-site missing rate and (iv) significant Hardy-Weinberg equilibrium p-value. After filtering, we apply a two-stage feature selection: (i) we retain SNPs that located on known AD-associated genes, (ii) we select 500 SNP features using minimum redundancy maximum relevance (mRMR) 49 We chose mRMR as a feature selec- www.nature.com/scientificreports/ tion method because it works well with categorical data (such the SNP data) and has been previously reported with genetic data 50 . mRMR was chosen over other wrapper-based techniques such as sequential feature selection due to computational costs. In the future we will investigate other filter-based feature selection methods such as correlation techniques, ANOVA, and relieFF in the future (Supplementary Information).\n\nIntermediate feature generation using single-modalities. We first perform feature extraction for each modality separately (Fig. 7), then we use DL for the generation of intermediate features. The intermediate features from EHR and SNP data are generated using auto-encoders and those of images are generated using 3D-convolutional neural networks. The intermediate features generated from each single-modality are subsequently used for multi-modal analysis. As a data-driven approach, DL's performance heavily relies on a large amount of well-annotated training data. However, the ADNI dataset contains only a few thousand samples in total and even fewer samples with all three modalities. Thus, we use DL only for feature representation learning instead of end-to-end training.\n\nIntermediate features for imaging data. First, we select the regions of interest and put them into a separate 3-dimensional convolutional neural network ( Supplementary Fig. A2 in the supplementary material) with their weights shared across the CNN modules. CNN modules can extract higher level features from the abstraction of images to form concepts, that often correlate better with the targets. Each 3D CNN in the architecture above comprises ten 3D-convolutional kernels of size 5 \u00d7 5 \u00d7 5 followed by pooling layers with pooling kernels of size 3 \u00d7 3 \u00d7 3 . After the pooling layer, we feed the pooled 3D images into Rectified Linear Unit (ReLU) nonlinearities to learn complex features from the input modalities. We use volumetric batch normalization 51 that is an effective regularizer for convolutional neural networks. Next, the feature maps generated by each 3D CNN are flattened and fed into separate fully connected layers with ReLU activation functions, followed by drop-out regularizers. We integrate the features generated from each modality and feed them into the second level fully connected layer and the corresponding drop-out layer. Finally, we use a softmax layer with a negative-log-likelihood loss function to train the imaging network. We use the combined features generated from the first level fully connected layers as the intermediate features that are fed into our multi-modality DL models.\n\nIntermediate features for EHR and SNP data using auto-encoders. We represent each patient data (EHR and SNP inputs to the feature learning algorithm) as a vector of length m(where m is the number of features. Then, we pass this data through a two-layer stacked denoising auto-encoder network 52 (Supplementary Fig. A3 in supplementary material) to obtain a high level representation of the patient data. Each auto-encoder layer takes an input x of dimension n \u00d7 d , where n is the number of training samples and d is input dimensionality ( d = m for first layer). The input for each layer is first passed through an encoder to convert the input into a higher order representation of the data (1).\n\nwhere f is an activation function such as sigmoidal or tanh, [W, b] are parameters to be trained. We then pass the mapped values y through a decoder to obtain a representation of the input (x ) (2).\n\n(1) Figure 7. Data pre-processing pipeline for three data modalities: (a) Imaging data is first skull stripped, segmented into white matter, grey matter, and cerebrospinal fluid. Then the images are registered to a standard space, prior to extracting 21 brain regions using anatomical automatic labeling atlases. (b) Clinical data is normalized between 1-2 or encoded as 1-2. Then we discard features with values missing values > 70% to obtain 1680 features for 204 patients. (c) SNP data is first filtered, error corrected, feature selection using known genes and then followed by maximum relevance (maxrel) based methods, to obtain 500 SNPS for 808 patients.\ny = f (Wx + b),\n\nScientific Reports\n\n| (2021) 11:3254 | https://doi.org/10.1038/s41598-020-74399-w www.nature.com/scientificreports/ where b \u2032 needed to be trained, and the weights W T are tied with the encoder weights. We construct the network by stacking the trained encoder layers and implement denoising using dropouts, where a portion of the input values are masked (set to zero) to allow better generalization of the models in the presence of small and noisy training data. We perform training through back propagation by minimizing the average cross-entropy between the input and the reconstructed input data (3).\n\nwhere a is number of dimensions. Optimization is carried out using Adam optimization 53 with a batch size of 3.\n\nAfter the training of auto-encoder layers, we perform the network fine-tuning for each by adding a softmax layer that predicts the final class. The intermediate features are the output of the fine-tuned network after removing the softmax layer. The hyper-parameters in the model, such as the layer sizes, dropout parameters, and regularization coefficients (to prevent overfitting), are optimized using tenfold cross-validation.\n\nMultimodal data integration. We propose data integration across modalities as a method for bridging the gaps in our understanding of disease processes and improve clinical outcome predictions and model performance. The data integration from different modalities can be performed at multiple levels (raw feature-level, intermediate feature-level, and decision-level) 54 (Fig. 1). In this study, we integrate the intermediate features generated in the previous step using a concatenation layer followed by a classification layer to predict the AD stage (Fig. 3). We try k-nearest neighbors (kNN), decision trees, random forests, and support vectors machines (SVM) as alternatives for the classification layer. In the event any modality is missing for a specific patient, we mask the modality with zeros. This procedure minimizes the effect of missing values from propagating down the layers and hence allows prediction with some missing data. We evaluate our models using feature-level combinations and decision-level combinations as the baseline models. (2) x = f W T y + b \u2032 , \n(3) W, b, b \u2032 = arg min W, b, b \u2032 \u2212 a k=1 x k logx k + (1 \u2212 x k ) log 1 \u2212x k ,\nFigure 1 .\n1(a) Description of ADNI data. Clinical data consists of demographics, neurological exams and assessments, medications, imaging volumes, and biomarkers. (b) Number of patients by modality and disease stage. (CN controls, MCI mild cognitive disorder, and AD Alzheimer's disease\n\n\ndata consists of 2004 patients with 1680 normalized features per patient, which we use to classify the patients into AD, MCI, and CN (three class). We use a three-layer auto-encoder with 200, 100 and 50 nodes each. The deep networks are trained using Adam with a max epoch count (repetition of DL network training on the entire dataset to allow\n\nFigure 2 .\n2Deep model for data integration compared with shallow models of data integration. (a) Feature level integration on shallow models, where the features are concatenated before passing into shallow models. (b) Deep intermediate feature level integration where the original features are transformed separatelyusing deep models prior to integration and prediction. (c) Decision level integration where voting is performed using decisions of individual classifiers. In this study, we comparee the performance of deep intermediate level integration against shallow feature and decision levels integrations for the prediction of Alzheimer's stages.\n\nFigure 3 .\n3Intermediate-feature-level combination deep models for multimodality data integration for clinical decision support. Data from diverse sources, imaging, EHR and SNP are combined using novel deep architectures. 3D convolutional neural network architectures used on 3D MR image regions to obtain intermediate imaging features. Deep stacked denoising autoencoders are used to obtain intermediate EHR features. Deep stacked denoising autoencoders are used obtain intermediate SNP features. The 3 types of intermediate features are passed into a classification layer for classification into Alzheimer's stages (CN, MCI and AD). Scientific Reports | (2021) 11:3254 | https://doi.org/10.1038/s41598-020-74399-w\n\nFigure 4 .\n4Internal cross validation results for individual data modality to predict Alzheimer's stage (a) Imaging results: deep learning prediction performs better than shallow learning predictions (b) EHR results: deep learning outperforms shallow models kNN and SVM and is comparable to decision trees and random forests (c) SNP results: deep learning outperforms shallow models. The kNN, SVM, RF and decision trees are shallow models. (kNN k-Nearest Neighbors, SVM support vector machines, and RF random forests).\n\nFigure 6 .\n6Feature extraction for deep model interpretation. Novel feature interpretation methodology where features are masked one at a time and the effect on the classification is observed. The feature which gives the highest drop in accuracy is ranked the highest. Once we ranked the features, we checked if the intermediate features picked associations different from raw data using cluster analysis. Deep models show associations which are different from shallow models, which accounts for superior performance. Scientific Reports | (2021) 11:3254 | https://doi.org/10.1038/s41598-020-74399-w\n\n\nslices, with each slice having 22 \u00d7 23 voxels. For clinical or EHR data, we use 2004 patients (ADNI1, ADNI2, and ADNI GO) data from the clinical tests (e.g., memory tests, balance tests, and cognitive tests), medication data (e.g., usage of levodopa), imaging score summaries (e.g., levels of fluorodeoxyglucose (FDG) from PET, brain volumes from MRI), patient demographics (e.g., age and gender), and biochemical tests. The genetic data consists of the whole genome sequencing (WGS) data from 808 ADNI participants (at the time of sequencing, 128 with AD, the diagnosis from patient's last visit. As shown inFig. 1c, 220 patients have all three data modalities, 588 patients have SNP and EHR, 283 patients have imaging and EHR, the remaining patients have only EHR data.415 with \nMCI, and 267 controls) by Illumina's non-Clinical Laboratory Improvement Amendments (non-CLIA) labora-\ntory at roughly 30-40 \u00d7 coverage in 2012 and 2013. The resulting variant call files (VCFs) have been generated \nby ADNI using Broad best practices (Burrows-Wheeler Aligner (BWA) and Genome Analysis Toolkit (GATK)-\nhaplotype caller) in 2014. We use a total of 2004 patients in this study, with all 2004 patients have clinical data, \n503 patients have imaging data, and 808 patients have genetic data (Fig. 1). For participants with multiple visits, \nwe use \n\n\nCombination of all 3 modalities: (imaging + EHR + SNP): deep model outperforms shallow models. When a particular modality is not available, we mask it as zeros when using DL. The intermediate features from the three modalities are passed to the classification layer. We test kNN, decision trees, random forests, and support vectors machines as alternatives for the classification layer. Internal cross-validation (CV) accuracy(FigScientific Reports \n| \n(2021) 11:3254 | \nhttps://doi.org/10.1038/s41598-020-74399-w \n\nwww.nature.com/scientificreports/ \n\n\n\n\nRF random forests, SM shallow models, and DL deep learning).Scientific Reports \n| \n(2021) 11:3254 | \nhttps://doi.org/10.1038/s41598-020-74399-w \n\nwww.nature.com/scientificreports/ \n\nFigure 5. Internal cross validation results for integration of data modalities to predict Alzheimer's stage \n(a) Imaging + EHR + SNP. Deep learning prediction performs better than shallow learning predictions (b) \nEHR + SNP Deep learning prediction performs better than shallow learning predictions (c) Imaging + EHR \ndeep learning prediction performs better than shallow learning predictions (d) Imaging + SNP results. Shallow \nlearning gave a better prediction than deep learning due to small sample sizes. (kNN k-Nearest Neighbors, SVM \nsupport vector machines, Scientific Reports \n| \n(2021) 11:3254 | \nhttps://doi.org/10.1038/s41598-020-74399-w \n\nwww.nature.com/scientificreports/ \n\n\n\n\nto show associations in intermediate features. On plotting the clusters for intermediate and raw features, we find that the intermediate features generate better separation as compared to the original features. This indicates subtle relationships in intermediate features, which are picked by deep models (Supplementary\n\n\nhttps://doi.org/10.1038/s41598-020-74399-w www.nature.com/scientificreports/Scientific Reports \n| \n(2021) 11:3254 | \n\n\u00a9 The Author(s) 2021\nAccuracy: 0.79 \u00b1 0 Acknowledgements Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: https ://adni.loni.usc.edu/wp-conte nt/uploa ds/how_to_apply /ADNI_Ackno wledg ement _List.pdf.Author contributionsJ.V., contributed to the study design, the pre-processing, data analysis for the EHR data, the combination of the three data modalities, and the writing of the manuscript, including Figs. 1, 2, 3 and the Tables. L.T., contributed to the pre-processing and analysis of the SNP data, the writing of the manuscipt (including background and discussions, sections related to SNP results and pre-processing includingFundingThe work was supported in part by Petit Institute Faculty Fellow Fund, Carol Ann and David D. Flanagan Faculty Fellow Research Fund, Amazon Faculty Research Fellowship. This work was also supported in part by the scholarship from China Scholarship Council (CSC) under the Grant CSC NO. 201406010343. The content of this article is solely the responsibility of the authors.Competing interestsThe authors declare no competing interests.Additional informationSupplementary information is available for this paper at https ://doi.org/10.1038/s4159 8-020-74399 -w.Correspondence and requests for materials should be addressed to M.D.W.Reprints and permissions information is available at www.nature.com/reprints.Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Open Access This article is licensed under a Creative Commons Attribution 4.0 InternationalLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco mmons .org/licen ses/by/4.0/.\nDevelopment and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. V Gulshan, JAMA. 316Gulshan, V. et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. JAMA 316, 2402-2410 (2016).\n\nDevelopment and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes. D S W Ting, JAMA. 318Ting, D. S. W. et al. Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes. JAMA 318, 2211-2223 (2017).\n\nDermatologist-level classification of skin cancer with deep neural networks. A Esteva, Nature. 542115Esteva, A. et al. Dermatologist-level classification of skin cancer with deep neural networks. Nature 542, 115 (2017).\n\nCombining deep learning and coherent anti-Stokes Raman scattering imaging for automated differential diagnosis of lung cancer. S Weng, X Xu, J Li, S T Wong, J. Biomed. Opt. 22106017Weng, S., Xu, X., Li, J. & Wong, S. T. Combining deep learning and coherent anti-Stokes Raman scattering imaging for automated differential diagnosis of lung cancer. J. Biomed. Opt. 22, 106017 (2017).\n\n. H.-I Suk, D Shen, Medical Image Computing and Computer-Assisted Intervention-MICCAI. SpringerSuk, H.-I. & Shen, D. Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013 583-590 (Springer, New York, 2013).\n\nMultimodal neuroimaging feature learning for multiclass diagnosis of Alzheimer's disease. S Liu, Biomed. Eng. IEEE Trans. 62Liu, S. et al. Multimodal neuroimaging feature learning for multiclass diagnosis of Alzheimer's disease. Biomed. Eng. IEEE Trans. 62, 1132-1140 (2015).\n\nAlzheimer's Disease Neuroimaging Initiative. Deep sparse multi-task learning for feature selection in Alzheimer's disease diagnosis. H I Suk, S W Lee, D Shen, Brain Struct. Funct. 2215Suk, H. I., Lee, S. W., Shen, D. & Alzheimer's Disease Neuroimaging Initiative. Deep sparse multi-task learning for feature selection in Alzheimer's disease diagnosis. Brain Struct. Funct. 221(5), 2569-2587 (2016).\n\n. P Schulam, F Wigley, S Saria, Aaai In, Schulam, P., Wigley, F. & Saria, S. In AAAI, 2956-2964 (2015).\n\nH.-I Suk, D Shen, International Conference on Medical Image Computing and Computer-Assisted Intervention. New YorkSpringerSuk, H.-I. & Shen, D. In International Conference on Medical Image Computing and Computer-Assisted Intervention, 583-590 (Springer, New York, 2013).\n\nDoctor ai: Predicting clinical events via recurrent neural networks. E Choi, M T Bahadori, J Sun, arXiv:1511.05942arXiv preprintChoi, E., Bahadori, M.T. & Sun, J. Doctor ai: Predicting clinical events via recurrent neural networks. arXiv preprint arXiv:1511.05942 (2015).\n\nPredicting effects of noncoding variants with deep learning-based sequence model. J Zhou, O G Troyanskaya, Nat. Methods. 12Zhou, J. & Troyanskaya, O. G. Predicting effects of noncoding variants with deep learning-based sequence model. Nat. Methods 12, 931-934 (2015).\n\nMultimodal deep learning. J Ngiam, A Khosla, M Kim, J Nam, H Lee, A Y Ng, Proceedings of the 28th International Conference on Machine Learning. the 28th International Conference on Machine LearningNgiam, J., Khosla, A., Kim, M., Nam, J., Lee, H. & Ng, A. Y. Multimodal deep learning. In Proceedings of the 28th International Conference on Machine Learning (ICML-11) 689-696 (2011).\n\nAlzheimer's disease facts and figures. Alzheimer's Dement. 124Alzheimer's AssociationAlzheimer's Association. 2016 Alzheimer's disease facts and figures. Alzheimer's Dement. 12(4), 459-509 (2016).\n\nAlzheimer's disease facts and figures. 9Alzheimer's DementAlzheimer's Association. 2013 Alzheimer's disease facts and figures. Alzheimer's Dement. 9(2), 208-245 (2013).\n\nC World Patterson, Alzheimer, The State of the Art of Dementia Research: New Frontiers. (Alzheimer's Disease International (ADI). LondonPatterson, C. World Alzheimer Report 2018-The State of the Art of Dementia Research: New Frontiers. (Alzheimer's Disease International (ADI), London, 2018).\n\nMultimodal techniques for diagnosis and prognosis of Alzheimer's disease. R J Perrin, A M Fagan, D M Holtzman, Nature. 461Perrin, R. J., Fagan, A. M. & Holtzman, D. M. Multimodal techniques for diagnosis and prognosis of Alzheimer's disease. Nature 461, 916-922 (2009).\n\nClinical utility of cerebrospinal fluid biomarkers in the diagnosis of early Alzheimer's disease. Alzheimer's Dement. K Blennow, 11Blennow, K. et al. Clinical utility of cerebrospinal fluid biomarkers in the diagnosis of early Alzheimer's disease. Alzheimer's Dement. 11, 58-69 (2015).\n\nStructural imaging biomarkers of Alzheimer's disease: Predicting disease progression. S F Eskildsen, Neurobiol. Aging. 36Eskildsen, S. F. et al. Structural imaging biomarkers of Alzheimer's disease: Predicting disease progression. Neurobiol. Aging 36, S23-S31 (2015).\n\nVisual versus fully automated analyses of 18F-FDG and amyloid PET for prediction of dementia due to Alzheimer disease in mild cognitive impairment. T Grimmer, J. Nucl. Med. 57Grimmer, T. et al. Visual versus fully automated analyses of 18F-FDG and amyloid PET for prediction of dementia due to Alzheimer disease in mild cognitive impairment. J. Nucl. Med. 57, 204-207 (2016).\n\nRNN-based longitudinal analysis for diagnosis of Alzheimer's disease. R Cui, M Liu, A S D N Initiative, Comput. Med. Imaging Graph. 73Cui, R., Liu, M. & Initiative, A. S. D. N. RNN-based longitudinal analysis for diagnosis of Alzheimer's disease. Comput. Med. Imaging Graph. 73, 1-10 (2019).\n\nVascular and Alzheimer's disease markers independently predict brain atrophy rate in Alzheimer's Disease Neuroimaging Initiative controls. J Barnes, Neurobiol. Aging. 34Barnes, J. et al. Vascular and Alzheimer's disease markers independently predict brain atrophy rate in Alzheimer's Disease Neu- roimaging Initiative controls. Neurobiol. Aging 34, 1996-2002 (2013).\n\nBLood-based protein biomarkers for diagnosis of alzheimer disease. J D Doecke, Arch. Neurol. 69Doecke, J. D. et al. BLood-based protein biomarkers for diagnosis of alzheimer disease. Arch. Neurol. 69, 1318-1325 (2012).\n\nPredicting Alzheimer's disease progression using multi-modal deep learning approach. G Lee, K Nho, B Kang, K.-A Sohn, D Kim, Sci. Rep. 91952Lee, G., Nho, K., Kang, B., Sohn, K.-A. & Kim, D. Predicting Alzheimer's disease progression using multi-modal deep learning approach. Sci. Rep. 9, 1952 (2019).\n\nLearning from longitudinal data in electronic health record and genetic data to improve cardiovascular event prediction. J Zhao, Sci. Rep. 9717Zhao, J. et al. Learning from longitudinal data in electronic health record and genetic data to improve cardiovascular event predic- tion. Sci. Rep. 9, 717 (2019).\n\n11C-PIB PET image analysis for Alzheimer's diagnosis using weighted voting ensembles. W Wu, J Venugopalan, M D Wang, 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). IEEEWu, W., Venugopalan, J. & Wang, M. D. 11C-PIB PET image analysis for Alzheimer's diagnosis using weighted voting ensembles. In 2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) 3914-3917 (IEEE, 2017).\n\nFunctional genomics and proteomics in the clinical neurosciences: data mining and bioinformatics. Prog. J H Phan, C F Quo, M D Wang, Brain Res. 158Phan, J. H., Quo, C. F. & Wang, M. D. Functional genomics and proteomics in the clinical neurosciences: data mining and bioin- formatics. Prog. Brain Res. 158, 83-108 (2006).\n\nMultimodal analysis of functional and structural disconnection in Alzheimer's disease using multiple kernel SVM. Hum. M Dyrba, M Grothe, T Kirste, S J Teipel, Brain Mapp. 36Dyrba, M., Grothe, M., Kirste, T. & Teipel, S. J. Multimodal analysis of functional and structural disconnection in Alzheimer's disease using multiple kernel SVM. Hum. Brain Mapp. 36, 2118-2131 (2015).\n\nPredicting cognitive decline in subjects at risk for Alzheimer disease by using combined cerebrospinal fluid, MR imaging, and PET biomarkers. J L Shaffer, Radiology. 266Shaffer, J. L. et al. Predicting cognitive decline in subjects at risk for Alzheimer disease by using combined cerebrospinal fluid, MR imaging, and PET biomarkers. Radiology 266, 583-591 (2013).\n\nDiscriminative analysis of early Alzheimer's disease using multi-modal imaging and multi-level characterization with multi-classifier (M3). Z Dai, NeuroImage. 59Dai, Z. et al. Discriminative analysis of early Alzheimer's disease using multi-modal imaging and multi-level characterization with multi-classifier (M3). NeuroImage 59, 2187-2195 (2012).\n\nPredicting prodromal Alzheimer's disease in subjects with mild cognitive impairment using machine learning classification of multimodal multicenter diffusion-tensor and magnetic resonance imaging data. M Dyrba, J. Neuroimaging. 25Dyrba, M. et al. Predicting prodromal Alzheimer's disease in subjects with mild cognitive impairment using machine learning classification of multimodal multicenter diffusion-tensor and magnetic resonance imaging data. J. Neuroimaging 25, 738-747 (2015).\n\nMultimodal image analysis in Alzheimer's disease via statistical modelling of non-local intensity correlations. M Lorenzi, Sci. Rep. 622161Lorenzi, M. et al. Multimodal image analysis in Alzheimer's disease via statistical modelling of non-local intensity correlations. Sci. Rep. 6, 22161 (2016).\n\nBrain properties predict proximity to symptom onset in sporadic Alzheimer's disease. J W Vogel, Brain. 141Vogel, J. W. et al. Brain properties predict proximity to symptom onset in sporadic Alzheimer's disease. Brain 141, 1871-1883 (2018).\n\nRandom forest-based similarity measures for multi-modal classification of Alzheimer's disease. K R Gray, P Aljabar, R A Heckemann, A Hammers, D Rueckert, NeuroImage. 65Gray, K. R., Aljabar, P., Heckemann, R. A., Hammers, A. & Rueckert, D. Random forest-based similarity measures for multi-modal classification of Alzheimer's disease. NeuroImage 65, 167-175 (2013).\n\nMultimodal classification of Alzheimer's disease and mild cognitive impairment. D Zhang, Y Wang, L Zhou, H Yuan, D Shen, NeuroImage. 55Zhang, D., Wang, Y., Zhou, L., Yuan, H. & Shen, D. Multimodal classification of Alzheimer's disease and mild cognitive impair- ment. NeuroImage 55, 856-867 (2011).\n\nIdentifying disease sensitive and quantitative trait-relevant biomarkers from multidimensional heterogeneous imaging genetics data via sparse multimodal multitask learning. H Wang, Bioinformatics. 28Wang, H. et al. Identifying disease sensitive and quantitative trait-relevant biomarkers from multidimensional heterogeneous imaging genetics data via sparse multimodal multitask learning. Bioinformatics 28, i127-i136 (2012).\n\nHierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis. H.-I Suk, S.-W Lee, D Shen, NeuroImage. 101Suk, H.-I., Lee, S.-W. & Shen, D. Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis. NeuroImage 101, 569-582 (2014).\n\nWays toward an early diagnosis in Alzheimer's disease: the Alzheimer's Disease Neuroimaging Initiative (ADNI). S G Mueller, 1Alzheimer's DementMueller, S. G. et al. Ways toward an early diagnosis in Alzheimer's disease: the Alzheimer's Disease Neuroimaging Initiative (ADNI). Alzheimer's Dement. 1, 55-66 (2005).\n\nGenetic analysis of quantitative phenotypes in AD and MCI: Imaging, cognition and biomarkers. L Shen, Brain Imaging Behav. 8Shen, L. et al. Genetic analysis of quantitative phenotypes in AD and MCI: Imaging, cognition and biomarkers. Brain Imaging Behav. 8, 183-207 (2014).\n\nQuantitative MRI brain studies in mild cognitive impairment and Alzheimer's disease: A methodological review. S Leandrou, S Petroudi, C C Reyes-Aldasoro, P A Kyriacou, C S Pattichis, IEEE Rev. Biomed. Eng. 11Leandrou, S., Petroudi, S., Reyes-Aldasoro, C. C., Kyriacou, P. A. & Pattichis, C. S. Quantitative MRI brain studies in mild cognitive impairment and Alzheimer's disease: A methodological review. IEEE Rev. Biomed. Eng. 11, 97-111 (2018).\n\nH Mhaskar, Q Liao, T Poggio, arXiv:1603.00988Learning functions: when is deep better than shallow. arXiv preprintMhaskar, H., Liao, Q. & Poggio, T. Learning functions: when is deep better than shallow. arXiv preprint arXiv:1603.00988 (2016).\n\nDeep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning. H.-C Shin, IEEE Trans. Med. Imaging. 355Shin, H.-C. et al. Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning. IEEE Trans. Med. Imaging 35(5), 1285-1298 (2016).\n\nDeep learning based feature-level integration of multi-omics data for breast cancer patients survival analysis. L Tong, J Mitchel, K Chatlin, M D Wang, BMC Med. Inform. Decis. Mak. 20Tong, L., Mitchel, J., Chatlin, K. & Wang, M. D. Deep learning based feature-level integration of multi-omics data for breast cancer patients survival analysis. BMC Med. Inform. Decis. Mak. 20, 1-12 (2020).\n\nIntegrating multi-omics data by learning modality invariant representations for improved prediction of overall survival of cancer. L Tong, H Wu, M D Wang, 10.1016/j.ymeth.2020.07.008Tong, L., Wu, H. & Wang, M. D. Integrating multi-omics data by learning modality invariant representations for improved predic- tion of overall survival of cancer. Methods. https ://doi.org/10.1016/j.ymeth .2020.07.008 (2020).\n\nDistilling knowledge from deep networks with applications to healthcare domain. Z Che, S Purushotham, R Khemani, Y Liu, arXiv:1512.03542arXiv preprintChe, Z., Purushotham, S., Khemani, R. & Liu, Y. Distilling knowledge from deep networks with applications to healthcare domain. arXiv preprint arXiv:1512.03542 (2015).\n\nAlzheimer's disease markers, hypertension, and gray matter damage in normal elderly. L Glodzik, Neurobiol. Aging. 33Glodzik, L. et al. Alzheimer's disease markers, hypertension, and gray matter damage in normal elderly. Neurobiol. Aging 33, 1215-1227 (2012).\n\nAdvancing research diagnostic criteria for Alzheimer's disease: the IWG-2 criteria. B Dubois, Lancet Neurol. 13Dubois, B. et al. Advancing research diagnostic criteria for Alzheimer's disease: the IWG-2 criteria. Lancet Neurol. 13, 614-629 (2014).\n\nFacilitation of memory encoding in primate hippocampus by a neuroprosthesis that promotes task-specific neural firing. R E Hampson, J. Neural Eng. 1066013Hampson, R. E. et al. Facilitation of memory encoding in primate hippocampus by a neuroprosthesis that promotes task-specific neural firing. J. Neural Eng. 10, 066013 (2013).\n\nA new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data. S B Eickhoff, Neuro-Image. 25Eickhoff, S. B. et al. A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data. Neuro- Image 25, 1325-1335 (2005).\n\nFeature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. H Peng, F Long, C Ding, IEEE Trans. Pattern Anal. Mach. Intell. 27Peng, H., Long, F. & Ding, C. Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. IEEE Trans. Pattern Anal. Mach. Intell. 27, 1226-1238 (2005).\n\nMinimum redundancy feature selection from microarray gene expression data. C Ding, H Peng, J. Bioinform. Comput. Biol. 3Ding, C. & Peng, H. Minimum redundancy feature selection from microarray gene expression data. J. Bioinform. Comput. Biol. 3, 185-205 (2005).\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, arXiv:1502.03167arXiv preprintIoffe, S. & Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015).\n\nDeep patient: An unsupervised representation to predict the future of patients from the electronic health records. R Miotto, L Li, B A Kidd, J T Dudley, Sci. Rep. 626094Miotto, R., Li, L., Kidd, B. A. & Dudley, J. T. Deep patient: An unsupervised representation to predict the future of patients from the electronic health records. Sci. Rep. 6, 26094 (2016).\n\nD Kingma, J Ba, Adam, arXiv:1412.6980A method for stochastic optimization. arXiv preprintKingma, D. & Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).\n\nMethods of integrating data to uncover genotype-phenotype interactions. M D Ritchie, E R Holzinger, R Li, S A Pendergrass, D Kim, Nat. Rev. Genet. 16Ritchie, M. D., Holzinger, E. R., Li, R., Pendergrass, S. A. & Kim, D. Methods of integrating data to uncover genotype-phenotype interactions. Nat. Rev. Genet. 16, 85-97 (2015).\n\nscientificreports/ writing of the results pertaining to image processing, including the relevant figures in supplementary section. 10.1038/s41598-020-74399-wwww.nature.com/Scientific Reports |. 113254Scientific Reports | (2021) 11:3254 | https://doi.org/10.1038/s41598-020-74399-w www.nature.com/scientificreports/ writing of the results pertaining to image processing, including the relevant figures in supplementary section.\n\ncontributed to the study design, result evaluation, and extensive refining and the revision of the manuscript. M D W Prof, Prof. M.D.W., contributed to the study design, result evaluation, and extensive refining and the revision of the manuscript.\n", "annotations": {"author": "[{\"end\":113,\"start\":94},{\"end\":122,\"start\":114},{\"end\":240,\"start\":123},{\"end\":532,\"start\":241},{\"end\":637,\"start\":533}]", "publisher": null, "author_last_name": "[{\"end\":112,\"start\":101},{\"end\":121,\"start\":117},{\"end\":145,\"start\":134},{\"end\":251,\"start\":247}]", "author_first_name": "[{\"end\":100,\"start\":94},{\"end\":116,\"start\":114},{\"end\":128,\"start\":123},{\"end\":133,\"start\":129},{\"end\":244,\"start\":241},{\"end\":246,\"start\":245}]", "author_affiliation": "[{\"end\":239,\"start\":147},{\"end\":343,\"start\":253},{\"end\":531,\"start\":345},{\"end\":636,\"start\":534}]", "title": "[{\"end\":81,\"start\":1},{\"end\":718,\"start\":638}]", "venue": null, "abstract": "[{\"end\":5103,\"start\":756}]", "bib_ref": "[{\"end\":6019,\"start\":6004},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6853,\"start\":6851},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7840,\"start\":7838},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8794,\"start\":8792},{\"end\":11507,\"start\":11504},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15156,\"start\":15154},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16681,\"start\":16679},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":17559,\"start\":17556},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":17561,\"start\":17559},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":17939,\"start\":17937},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":18501,\"start\":18499},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":18555,\"start\":18552},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":18557,\"start\":18555},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":21190,\"start\":21188},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":22352,\"start\":22350},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22550,\"start\":22548},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":25338,\"start\":25336},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27222,\"start\":27219},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":28136,\"start\":28134}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29212,\"start\":28924},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29559,\"start\":29213},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30213,\"start\":29560},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30930,\"start\":30214},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31450,\"start\":30931},{\"attributes\":{\"id\":\"fig_5\"},\"end\":32050,\"start\":31451},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33393,\"start\":32051},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33948,\"start\":33394},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34820,\"start\":33949},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35142,\"start\":34821},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":35262,\"start\":35143}]", "paragraph": "[{\"end\":6020,\"start\":5125},{\"end\":6410,\"start\":6022},{\"end\":6502,\"start\":6412},{\"end\":6564,\"start\":6504},{\"end\":6731,\"start\":6566},{\"end\":7841,\"start\":6752},{\"end\":9184,\"start\":7904},{\"end\":10110,\"start\":9186},{\"end\":10488,\"start\":10112},{\"end\":10903,\"start\":10546},{\"end\":11434,\"start\":10905},{\"end\":11931,\"start\":11436},{\"end\":12572,\"start\":11933},{\"end\":13114,\"start\":12574},{\"end\":13546,\"start\":13116},{\"end\":14102,\"start\":13548},{\"end\":14555,\"start\":14104},{\"end\":16122,\"start\":14616},{\"end\":16448,\"start\":16124},{\"end\":17563,\"start\":16450},{\"end\":18171,\"start\":17645},{\"end\":19002,\"start\":18263},{\"end\":19571,\"start\":19004},{\"end\":19779,\"start\":19587},{\"end\":19870,\"start\":19781},{\"end\":19955,\"start\":19872},{\"end\":20195,\"start\":19957},{\"end\":20369,\"start\":20197},{\"end\":20991,\"start\":20381},{\"end\":21503,\"start\":20993},{\"end\":21834,\"start\":21505},{\"end\":22842,\"start\":21836},{\"end\":23622,\"start\":22844},{\"end\":25042,\"start\":23624},{\"end\":25740,\"start\":25044},{\"end\":25940,\"start\":25742},{\"end\":26602,\"start\":25942},{\"end\":27223,\"start\":26640},{\"end\":27336,\"start\":27225},{\"end\":27766,\"start\":27338},{\"end\":28845,\"start\":27768}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":26618,\"start\":26603},{\"attributes\":{\"id\":\"formula_1\"},\"end\":28924,\"start\":28846}]", "table_ref": "[{\"end\":13113,\"start\":13105},{\"end\":13545,\"start\":13537},{\"end\":14011,\"start\":14002},{\"end\":18293,\"start\":18284},{\"end\":19146,\"start\":19139}]", "section_header": "[{\"end\":5123,\"start\":5105},{\"end\":6750,\"start\":6734},{\"end\":7902,\"start\":7844},{\"end\":10544,\"start\":10491},{\"end\":14614,\"start\":14558},{\"end\":17643,\"start\":17566},{\"end\":18261,\"start\":18174},{\"end\":19585,\"start\":19574},{\"end\":20379,\"start\":20372},{\"end\":26638,\"start\":26620},{\"end\":28935,\"start\":28925},{\"end\":29571,\"start\":29561},{\"end\":30225,\"start\":30215},{\"end\":30942,\"start\":30932},{\"end\":31462,\"start\":31452}]", "table": "[{\"end\":33393,\"start\":32824},{\"end\":33948,\"start\":33826},{\"end\":34820,\"start\":34011},{\"end\":35262,\"start\":35221}]", "figure_caption": "[{\"end\":29212,\"start\":28937},{\"end\":29559,\"start\":29215},{\"end\":30213,\"start\":29573},{\"end\":30930,\"start\":30227},{\"end\":31450,\"start\":30944},{\"end\":32050,\"start\":31464},{\"end\":32824,\"start\":32053},{\"end\":33826,\"start\":33396},{\"end\":34011,\"start\":33951},{\"end\":35142,\"start\":34823},{\"end\":35221,\"start\":35145}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9310,\"start\":9302},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":9833,\"start\":9825},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":11318,\"start\":11309},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":11791,\"start\":11783},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":12497,\"start\":12488},{\"end\":12779,\"start\":12775},{\"end\":13310,\"start\":13300},{\"end\":13753,\"start\":13745},{\"end\":14385,\"start\":14376},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":18075,\"start\":18067},{\"end\":19123,\"start\":19096},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19137,\"start\":19124},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20455,\"start\":20447},{\"end\":21187,\"start\":21178},{\"end\":21833,\"start\":21824},{\"end\":21975,\"start\":21967},{\"end\":22974,\"start\":22966},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23800,\"start\":23779},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25361,\"start\":25339},{\"end\":25954,\"start\":25946},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28145,\"start\":28137},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28327,\"start\":28319}]", "bib_author_first_name": "[{\"end\":38057,\"start\":38056},{\"end\":38418,\"start\":38417},{\"end\":38422,\"start\":38419},{\"end\":38735,\"start\":38734},{\"end\":39006,\"start\":39005},{\"end\":39014,\"start\":39013},{\"end\":39020,\"start\":39019},{\"end\":39026,\"start\":39025},{\"end\":39028,\"start\":39027},{\"end\":39267,\"start\":39263},{\"end\":39274,\"start\":39273},{\"end\":39577,\"start\":39576},{\"end\":39897,\"start\":39896},{\"end\":39899,\"start\":39898},{\"end\":39906,\"start\":39905},{\"end\":39908,\"start\":39907},{\"end\":39915,\"start\":39914},{\"end\":40166,\"start\":40165},{\"end\":40177,\"start\":40176},{\"end\":40187,\"start\":40186},{\"end\":40199,\"start\":40195},{\"end\":40272,\"start\":40268},{\"end\":40279,\"start\":40278},{\"end\":40610,\"start\":40609},{\"end\":40618,\"start\":40617},{\"end\":40620,\"start\":40619},{\"end\":40632,\"start\":40631},{\"end\":40896,\"start\":40895},{\"end\":40904,\"start\":40903},{\"end\":40906,\"start\":40905},{\"end\":41109,\"start\":41108},{\"end\":41118,\"start\":41117},{\"end\":41128,\"start\":41127},{\"end\":41135,\"start\":41134},{\"end\":41142,\"start\":41141},{\"end\":41149,\"start\":41148},{\"end\":41151,\"start\":41150},{\"end\":41834,\"start\":41833},{\"end\":41840,\"start\":41835},{\"end\":42202,\"start\":42201},{\"end\":42204,\"start\":42203},{\"end\":42214,\"start\":42213},{\"end\":42216,\"start\":42215},{\"end\":42225,\"start\":42224},{\"end\":42227,\"start\":42226},{\"end\":42517,\"start\":42516},{\"end\":42772,\"start\":42771},{\"end\":42774,\"start\":42773},{\"end\":43103,\"start\":43102},{\"end\":43402,\"start\":43401},{\"end\":43409,\"start\":43408},{\"end\":43416,\"start\":43415},{\"end\":43422,\"start\":43417},{\"end\":43764,\"start\":43763},{\"end\":44060,\"start\":44059},{\"end\":44062,\"start\":44061},{\"end\":44298,\"start\":44297},{\"end\":44305,\"start\":44304},{\"end\":44312,\"start\":44311},{\"end\":44323,\"start\":44319},{\"end\":44331,\"start\":44330},{\"end\":44636,\"start\":44635},{\"end\":44909,\"start\":44908},{\"end\":44915,\"start\":44914},{\"end\":44930,\"start\":44929},{\"end\":44932,\"start\":44931},{\"end\":45406,\"start\":45405},{\"end\":45408,\"start\":45407},{\"end\":45416,\"start\":45415},{\"end\":45418,\"start\":45417},{\"end\":45425,\"start\":45424},{\"end\":45427,\"start\":45426},{\"end\":45743,\"start\":45742},{\"end\":45752,\"start\":45751},{\"end\":45762,\"start\":45761},{\"end\":45772,\"start\":45771},{\"end\":45774,\"start\":45773},{\"end\":46143,\"start\":46142},{\"end\":46145,\"start\":46144},{\"end\":46506,\"start\":46505},{\"end\":46918,\"start\":46917},{\"end\":47314,\"start\":47313},{\"end\":47585,\"start\":47584},{\"end\":47587,\"start\":47586},{\"end\":47836,\"start\":47835},{\"end\":47838,\"start\":47837},{\"end\":47846,\"start\":47845},{\"end\":47857,\"start\":47856},{\"end\":47859,\"start\":47858},{\"end\":47872,\"start\":47871},{\"end\":47883,\"start\":47882},{\"end\":48187,\"start\":48186},{\"end\":48196,\"start\":48195},{\"end\":48204,\"start\":48203},{\"end\":48212,\"start\":48211},{\"end\":48220,\"start\":48219},{\"end\":48580,\"start\":48579},{\"end\":48935,\"start\":48931},{\"end\":48945,\"start\":48941},{\"end\":48952,\"start\":48951},{\"end\":49252,\"start\":49251},{\"end\":49254,\"start\":49253},{\"end\":49549,\"start\":49548},{\"end\":49840,\"start\":49839},{\"end\":49852,\"start\":49851},{\"end\":49864,\"start\":49863},{\"end\":49866,\"start\":49865},{\"end\":49884,\"start\":49883},{\"end\":49886,\"start\":49885},{\"end\":49898,\"start\":49897},{\"end\":49900,\"start\":49899},{\"end\":50177,\"start\":50176},{\"end\":50188,\"start\":50187},{\"end\":50196,\"start\":50195},{\"end\":50554,\"start\":50550},{\"end\":50904,\"start\":50903},{\"end\":50912,\"start\":50911},{\"end\":50923,\"start\":50922},{\"end\":50934,\"start\":50933},{\"end\":50936,\"start\":50935},{\"end\":51314,\"start\":51313},{\"end\":51322,\"start\":51321},{\"end\":51328,\"start\":51327},{\"end\":51330,\"start\":51329},{\"end\":51673,\"start\":51672},{\"end\":51680,\"start\":51679},{\"end\":51695,\"start\":51694},{\"end\":51706,\"start\":51705},{\"end\":51997,\"start\":51996},{\"end\":52256,\"start\":52255},{\"end\":52540,\"start\":52539},{\"end\":52542,\"start\":52541},{\"end\":52849,\"start\":52848},{\"end\":52851,\"start\":52850},{\"end\":53144,\"start\":53143},{\"end\":53152,\"start\":53151},{\"end\":53160,\"start\":53159},{\"end\":53487,\"start\":53486},{\"end\":53495,\"start\":53494},{\"end\":53769,\"start\":53768},{\"end\":53778,\"start\":53777},{\"end\":54093,\"start\":54092},{\"end\":54103,\"start\":54102},{\"end\":54109,\"start\":54108},{\"end\":54111,\"start\":54110},{\"end\":54119,\"start\":54118},{\"end\":54121,\"start\":54120},{\"end\":54338,\"start\":54337},{\"end\":54348,\"start\":54347},{\"end\":54603,\"start\":54602},{\"end\":54605,\"start\":54604},{\"end\":54616,\"start\":54615},{\"end\":54618,\"start\":54617},{\"end\":54631,\"start\":54630},{\"end\":54637,\"start\":54636},{\"end\":54639,\"start\":54638},{\"end\":54654,\"start\":54653},{\"end\":55402,\"start\":55397}]", "bib_author_last_name": "[{\"end\":38065,\"start\":38058},{\"end\":38427,\"start\":38423},{\"end\":38742,\"start\":38736},{\"end\":39011,\"start\":39007},{\"end\":39017,\"start\":39015},{\"end\":39023,\"start\":39021},{\"end\":39033,\"start\":39029},{\"end\":39271,\"start\":39268},{\"end\":39279,\"start\":39275},{\"end\":39581,\"start\":39578},{\"end\":39903,\"start\":39900},{\"end\":39912,\"start\":39909},{\"end\":39920,\"start\":39916},{\"end\":40174,\"start\":40167},{\"end\":40184,\"start\":40178},{\"end\":40193,\"start\":40188},{\"end\":40202,\"start\":40200},{\"end\":40276,\"start\":40273},{\"end\":40284,\"start\":40280},{\"end\":40615,\"start\":40611},{\"end\":40629,\"start\":40621},{\"end\":40636,\"start\":40633},{\"end\":40901,\"start\":40897},{\"end\":40918,\"start\":40907},{\"end\":41115,\"start\":41110},{\"end\":41125,\"start\":41119},{\"end\":41132,\"start\":41129},{\"end\":41139,\"start\":41136},{\"end\":41146,\"start\":41143},{\"end\":41154,\"start\":41152},{\"end\":41850,\"start\":41841},{\"end\":41861,\"start\":41852},{\"end\":42211,\"start\":42205},{\"end\":42222,\"start\":42217},{\"end\":42236,\"start\":42228},{\"end\":42525,\"start\":42518},{\"end\":42784,\"start\":42775},{\"end\":43111,\"start\":43104},{\"end\":43406,\"start\":43403},{\"end\":43413,\"start\":43410},{\"end\":43433,\"start\":43423},{\"end\":43771,\"start\":43765},{\"end\":44069,\"start\":44063},{\"end\":44302,\"start\":44299},{\"end\":44309,\"start\":44306},{\"end\":44317,\"start\":44313},{\"end\":44328,\"start\":44324},{\"end\":44335,\"start\":44332},{\"end\":44641,\"start\":44637},{\"end\":44912,\"start\":44910},{\"end\":44927,\"start\":44916},{\"end\":44937,\"start\":44933},{\"end\":45413,\"start\":45409},{\"end\":45422,\"start\":45419},{\"end\":45432,\"start\":45428},{\"end\":45749,\"start\":45744},{\"end\":45759,\"start\":45753},{\"end\":45769,\"start\":45763},{\"end\":45781,\"start\":45775},{\"end\":46153,\"start\":46146},{\"end\":46510,\"start\":46507},{\"end\":46924,\"start\":46919},{\"end\":47322,\"start\":47315},{\"end\":47593,\"start\":47588},{\"end\":47843,\"start\":47839},{\"end\":47854,\"start\":47847},{\"end\":47869,\"start\":47860},{\"end\":47880,\"start\":47873},{\"end\":47892,\"start\":47884},{\"end\":48193,\"start\":48188},{\"end\":48201,\"start\":48197},{\"end\":48209,\"start\":48205},{\"end\":48217,\"start\":48213},{\"end\":48225,\"start\":48221},{\"end\":48585,\"start\":48581},{\"end\":48939,\"start\":48936},{\"end\":48949,\"start\":48946},{\"end\":48957,\"start\":48953},{\"end\":49262,\"start\":49255},{\"end\":49554,\"start\":49550},{\"end\":49849,\"start\":49841},{\"end\":49861,\"start\":49853},{\"end\":49881,\"start\":49867},{\"end\":49895,\"start\":49887},{\"end\":49910,\"start\":49901},{\"end\":50185,\"start\":50178},{\"end\":50193,\"start\":50189},{\"end\":50203,\"start\":50197},{\"end\":50559,\"start\":50555},{\"end\":50909,\"start\":50905},{\"end\":50920,\"start\":50913},{\"end\":50931,\"start\":50924},{\"end\":50941,\"start\":50937},{\"end\":51319,\"start\":51315},{\"end\":51325,\"start\":51323},{\"end\":51335,\"start\":51331},{\"end\":51677,\"start\":51674},{\"end\":51692,\"start\":51681},{\"end\":51703,\"start\":51696},{\"end\":51710,\"start\":51707},{\"end\":52005,\"start\":51998},{\"end\":52263,\"start\":52257},{\"end\":52550,\"start\":52543},{\"end\":52860,\"start\":52852},{\"end\":53149,\"start\":53145},{\"end\":53157,\"start\":53153},{\"end\":53165,\"start\":53161},{\"end\":53492,\"start\":53488},{\"end\":53500,\"start\":53496},{\"end\":53775,\"start\":53770},{\"end\":53786,\"start\":53779},{\"end\":54100,\"start\":54094},{\"end\":54106,\"start\":54104},{\"end\":54116,\"start\":54112},{\"end\":54128,\"start\":54122},{\"end\":54345,\"start\":54339},{\"end\":54351,\"start\":54349},{\"end\":54357,\"start\":54353},{\"end\":54613,\"start\":54606},{\"end\":54628,\"start\":54619},{\"end\":54634,\"start\":54632},{\"end\":54651,\"start\":54640},{\"end\":54658,\"start\":54655},{\"end\":55407,\"start\":55403}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":26657811},\"end\":38247,\"start\":37931},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3606254},\"end\":38655,\"start\":38249},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3767412},\"end\":38876,\"start\":38657},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":13897488},\"end\":39259,\"start\":38878},{\"attributes\":{\"id\":\"b4\"},\"end\":39484,\"start\":39261},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7183410},\"end\":39761,\"start\":39486},{\"attributes\":{\"id\":\"b6\"},\"end\":40161,\"start\":39763},{\"attributes\":{\"id\":\"b7\"},\"end\":40266,\"start\":40163},{\"attributes\":{\"id\":\"b8\"},\"end\":40538,\"start\":40268},{\"attributes\":{\"doi\":\"arXiv:1511.05942\",\"id\":\"b9\"},\"end\":40811,\"start\":40540},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":205424148},\"end\":41080,\"start\":40813},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":352650},\"end\":41463,\"start\":41082},{\"attributes\":{\"id\":\"b12\"},\"end\":41661,\"start\":41465},{\"attributes\":{\"id\":\"b13\"},\"end\":41831,\"start\":41663},{\"attributes\":{\"id\":\"b14\"},\"end\":42125,\"start\":41833},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4431382},\"end\":42396,\"start\":42127},{\"attributes\":{\"id\":\"b16\"},\"end\":42683,\"start\":42398},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":23761368},\"end\":42952,\"start\":42685},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":207408065},\"end\":43329,\"start\":42954},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":73437016},\"end\":43622,\"start\":43331},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1641792},\"end\":43990,\"start\":43624},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":22934295},\"end\":44210,\"start\":43992},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":61156321},\"end\":44512,\"start\":44212},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":59223548},\"end\":44820,\"start\":44514},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2357824},\"end\":45299,\"start\":44822},{\"attributes\":{\"id\":\"b25\"},\"end\":45622,\"start\":45301},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":32035658},\"end\":45998,\"start\":45624},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9608298},\"end\":46363,\"start\":46000},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":7328589},\"end\":46713,\"start\":46365},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":36506951},\"end\":47199,\"start\":46715},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":7537775},\"end\":47497,\"start\":47201},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":21127672},\"end\":47738,\"start\":47499},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":4513154},\"end\":48104,\"start\":47740},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":207178993},\"end\":48404,\"start\":48106},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1793044},\"end\":48830,\"start\":48406},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":207190844},\"end\":49138,\"start\":48832},{\"attributes\":{\"id\":\"b36\"},\"end\":49452,\"start\":49140},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":11428327},\"end\":49727,\"start\":49454},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":50786800},\"end\":50174,\"start\":49729},{\"attributes\":{\"doi\":\"arXiv:1603.00988\",\"id\":\"b39\"},\"end\":50417,\"start\":50176},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3333267},\"end\":50789,\"start\":50419},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":221673275},\"end\":51180,\"start\":50791},{\"attributes\":{\"doi\":\"10.1016/j.ymeth.2020.07.008\",\"id\":\"b42\"},\"end\":51590,\"start\":51182},{\"attributes\":{\"doi\":\"arXiv:1512.03542\",\"id\":\"b43\"},\"end\":51909,\"start\":51592},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":8689864},\"end\":52169,\"start\":51911},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":1939828},\"end\":52418,\"start\":52171},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":18367918},\"end\":52748,\"start\":52420},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":29213996},\"end\":53032,\"start\":52750},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":206764015},\"end\":53409,\"start\":53034},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":1831347},\"end\":53672,\"start\":53411},{\"attributes\":{\"doi\":\"arXiv:1502.03167\",\"id\":\"b50\"},\"end\":53975,\"start\":53674},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":4404566},\"end\":54335,\"start\":53977},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b52\"},\"end\":54528,\"start\":54337},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":18003714},\"end\":54856,\"start\":54530},{\"attributes\":{\"doi\":\"10.1038/s41598-020-74399-wwww.nature.com/\",\"id\":\"b54\"},\"end\":55284,\"start\":54858},{\"attributes\":{\"id\":\"b55\"},\"end\":55533,\"start\":55286}]", "bib_title": "[{\"end\":38054,\"start\":37931},{\"end\":38415,\"start\":38249},{\"end\":38732,\"start\":38657},{\"end\":39003,\"start\":38878},{\"end\":39574,\"start\":39486},{\"end\":39894,\"start\":39763},{\"end\":40893,\"start\":40813},{\"end\":41106,\"start\":41082},{\"end\":41502,\"start\":41465},{\"end\":42199,\"start\":42127},{\"end\":42769,\"start\":42685},{\"end\":43100,\"start\":42954},{\"end\":43399,\"start\":43331},{\"end\":43761,\"start\":43624},{\"end\":44057,\"start\":43992},{\"end\":44295,\"start\":44212},{\"end\":44633,\"start\":44514},{\"end\":44906,\"start\":44822},{\"end\":45403,\"start\":45301},{\"end\":45740,\"start\":45624},{\"end\":46140,\"start\":46000},{\"end\":46503,\"start\":46365},{\"end\":46915,\"start\":46715},{\"end\":47311,\"start\":47201},{\"end\":47582,\"start\":47499},{\"end\":47833,\"start\":47740},{\"end\":48184,\"start\":48106},{\"end\":48577,\"start\":48406},{\"end\":48929,\"start\":48832},{\"end\":49546,\"start\":49454},{\"end\":49837,\"start\":49729},{\"end\":50548,\"start\":50419},{\"end\":50901,\"start\":50791},{\"end\":51994,\"start\":51911},{\"end\":52253,\"start\":52171},{\"end\":52537,\"start\":52420},{\"end\":52846,\"start\":52750},{\"end\":53141,\"start\":53034},{\"end\":53484,\"start\":53411},{\"end\":54090,\"start\":53977},{\"end\":54600,\"start\":54530},{\"end\":54987,\"start\":54858}]", "bib_author": "[{\"end\":38067,\"start\":38056},{\"end\":38429,\"start\":38417},{\"end\":38744,\"start\":38734},{\"end\":39013,\"start\":39005},{\"end\":39019,\"start\":39013},{\"end\":39025,\"start\":39019},{\"end\":39035,\"start\":39025},{\"end\":39273,\"start\":39263},{\"end\":39281,\"start\":39273},{\"end\":39583,\"start\":39576},{\"end\":39905,\"start\":39896},{\"end\":39914,\"start\":39905},{\"end\":39922,\"start\":39914},{\"end\":40176,\"start\":40165},{\"end\":40186,\"start\":40176},{\"end\":40195,\"start\":40186},{\"end\":40204,\"start\":40195},{\"end\":40278,\"start\":40268},{\"end\":40286,\"start\":40278},{\"end\":40617,\"start\":40609},{\"end\":40631,\"start\":40617},{\"end\":40638,\"start\":40631},{\"end\":40903,\"start\":40895},{\"end\":40920,\"start\":40903},{\"end\":41117,\"start\":41108},{\"end\":41127,\"start\":41117},{\"end\":41134,\"start\":41127},{\"end\":41141,\"start\":41134},{\"end\":41148,\"start\":41141},{\"end\":41156,\"start\":41148},{\"end\":41852,\"start\":41833},{\"end\":41863,\"start\":41852},{\"end\":42213,\"start\":42201},{\"end\":42224,\"start\":42213},{\"end\":42238,\"start\":42224},{\"end\":42527,\"start\":42516},{\"end\":42786,\"start\":42771},{\"end\":43113,\"start\":43102},{\"end\":43408,\"start\":43401},{\"end\":43415,\"start\":43408},{\"end\":43435,\"start\":43415},{\"end\":43773,\"start\":43763},{\"end\":44071,\"start\":44059},{\"end\":44304,\"start\":44297},{\"end\":44311,\"start\":44304},{\"end\":44319,\"start\":44311},{\"end\":44330,\"start\":44319},{\"end\":44337,\"start\":44330},{\"end\":44643,\"start\":44635},{\"end\":44914,\"start\":44908},{\"end\":44929,\"start\":44914},{\"end\":44939,\"start\":44929},{\"end\":45415,\"start\":45405},{\"end\":45424,\"start\":45415},{\"end\":45434,\"start\":45424},{\"end\":45751,\"start\":45742},{\"end\":45761,\"start\":45751},{\"end\":45771,\"start\":45761},{\"end\":45783,\"start\":45771},{\"end\":46155,\"start\":46142},{\"end\":46512,\"start\":46505},{\"end\":46926,\"start\":46917},{\"end\":47324,\"start\":47313},{\"end\":47595,\"start\":47584},{\"end\":47845,\"start\":47835},{\"end\":47856,\"start\":47845},{\"end\":47871,\"start\":47856},{\"end\":47882,\"start\":47871},{\"end\":47894,\"start\":47882},{\"end\":48195,\"start\":48186},{\"end\":48203,\"start\":48195},{\"end\":48211,\"start\":48203},{\"end\":48219,\"start\":48211},{\"end\":48227,\"start\":48219},{\"end\":48587,\"start\":48579},{\"end\":48941,\"start\":48931},{\"end\":48951,\"start\":48941},{\"end\":48959,\"start\":48951},{\"end\":49264,\"start\":49251},{\"end\":49556,\"start\":49548},{\"end\":49851,\"start\":49839},{\"end\":49863,\"start\":49851},{\"end\":49883,\"start\":49863},{\"end\":49897,\"start\":49883},{\"end\":49912,\"start\":49897},{\"end\":50187,\"start\":50176},{\"end\":50195,\"start\":50187},{\"end\":50205,\"start\":50195},{\"end\":50561,\"start\":50550},{\"end\":50911,\"start\":50903},{\"end\":50922,\"start\":50911},{\"end\":50933,\"start\":50922},{\"end\":50943,\"start\":50933},{\"end\":51321,\"start\":51313},{\"end\":51327,\"start\":51321},{\"end\":51337,\"start\":51327},{\"end\":51679,\"start\":51672},{\"end\":51694,\"start\":51679},{\"end\":51705,\"start\":51694},{\"end\":51712,\"start\":51705},{\"end\":52007,\"start\":51996},{\"end\":52265,\"start\":52255},{\"end\":52552,\"start\":52539},{\"end\":52862,\"start\":52848},{\"end\":53151,\"start\":53143},{\"end\":53159,\"start\":53151},{\"end\":53167,\"start\":53159},{\"end\":53494,\"start\":53486},{\"end\":53502,\"start\":53494},{\"end\":53777,\"start\":53768},{\"end\":53788,\"start\":53777},{\"end\":54102,\"start\":54092},{\"end\":54108,\"start\":54102},{\"end\":54118,\"start\":54108},{\"end\":54130,\"start\":54118},{\"end\":54347,\"start\":54337},{\"end\":54353,\"start\":54347},{\"end\":54359,\"start\":54353},{\"end\":54615,\"start\":54602},{\"end\":54630,\"start\":54615},{\"end\":54636,\"start\":54630},{\"end\":54653,\"start\":54636},{\"end\":54660,\"start\":54653},{\"end\":55409,\"start\":55397}]", "bib_venue": "[{\"end\":40382,\"start\":40374},{\"end\":41279,\"start\":41226},{\"end\":41969,\"start\":41963},{\"end\":38071,\"start\":38067},{\"end\":38433,\"start\":38429},{\"end\":38750,\"start\":38744},{\"end\":39049,\"start\":39035},{\"end\":39346,\"start\":39281},{\"end\":39606,\"start\":39583},{\"end\":39941,\"start\":39922},{\"end\":40372,\"start\":40286},{\"end\":40607,\"start\":40540},{\"end\":40932,\"start\":40920},{\"end\":41224,\"start\":41156},{\"end\":41522,\"start\":41504},{\"end\":41700,\"start\":41663},{\"end\":41961,\"start\":41863},{\"end\":42244,\"start\":42238},{\"end\":42514,\"start\":42398},{\"end\":42802,\"start\":42786},{\"end\":43125,\"start\":43113},{\"end\":43461,\"start\":43435},{\"end\":43789,\"start\":43773},{\"end\":44083,\"start\":44071},{\"end\":44345,\"start\":44337},{\"end\":44651,\"start\":44643},{\"end\":45038,\"start\":44939},{\"end\":45443,\"start\":45434},{\"end\":45793,\"start\":45783},{\"end\":46164,\"start\":46155},{\"end\":46522,\"start\":46512},{\"end\":46941,\"start\":46926},{\"end\":47332,\"start\":47324},{\"end\":47600,\"start\":47595},{\"end\":47904,\"start\":47894},{\"end\":48237,\"start\":48227},{\"end\":48601,\"start\":48587},{\"end\":48969,\"start\":48959},{\"end\":49249,\"start\":49140},{\"end\":49575,\"start\":49556},{\"end\":49933,\"start\":49912},{\"end\":50273,\"start\":50221},{\"end\":50585,\"start\":50561},{\"end\":50970,\"start\":50943},{\"end\":51311,\"start\":51182},{\"end\":51670,\"start\":51592},{\"end\":52023,\"start\":52007},{\"end\":52278,\"start\":52265},{\"end\":52565,\"start\":52552},{\"end\":52873,\"start\":52862},{\"end\":53205,\"start\":53167},{\"end\":53528,\"start\":53502},{\"end\":53766,\"start\":53674},{\"end\":54138,\"start\":54130},{\"end\":54410,\"start\":54374},{\"end\":54675,\"start\":54660},{\"end\":55050,\"start\":55030},{\"end\":55395,\"start\":55286}]"}}}, "year": 2023, "month": 12, "day": 17}