{"id": 70115, "updated": "2022-03-28 00:36:43.47", "metadata": {"title": "Bayesian measures of model complexity and fit", "authors": "[{\"first\":\"David J.\",\"last\":\"Spiegelhalter\",\"middle\":[]},{\"first\":\"Nicola G.\",\"last\":\"Best\",\"middle\":[]},{\"first\":\"Bradley P.\",\"last\":\"Carlin\",\"middle\":[]},{\"first\":\"Angelika\",\"last\":\"Van Der Linde\",\"middle\":[]}]", "venue": null, "journal": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)", "publication_date": {"year": 2002, "month": null, "day": null}, "abstract": "Summary. We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure pD for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general pD approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the \u2018hat\u2019 matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding pD to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.", "fields_of_study": "[\"Mathematics\"]", "external_ids": {"arxiv": null, "mag": "2057765075", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.1111/1467-9868.00353"}}, "content": {"source": {"pdf_hash": "1b0be2aff765f161faa4e4fe78d002579bf053a4", "pdf_src": "Wiley", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://rss.onlinelibrary.wiley.com/doi/pdfdirect/10.1111/1467-9868.00353", "status": "BRONZE"}}, "grobid": {"id": "6f44e949d83f37d6e44def80b3240f28defc79de", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1b0be2aff765f161faa4e4fe78d002579bf053a4.txt", "contents": "\nBayesian measures of model complexity and fit\n2002\n\nDavid J Spiegelhalter \nNicola G Best \nBradley P Carlin \nAngelika Van Der Linde \n\nImperial College School of Medicine\nMedical Research Council Biostatistics Unit\nUniversity of Minnesota\nCambridge, London, MinneapolisUK, UK, USA\n\n\nUniversity of Bremen\nGermany\n\nBayesian measures of model complexity and fit\n\nJ. R. Statist. Soc. B\n642002[Read before The Royal Statistical Society at a meeting organized by the Research Section on Wednesday, March 13th, 2002, Professor D. Firth in the Chair ]Bayesian model comparisonDecision theoryDeviance information criterionEffective number of parametersHierarchical modelsInformation theoryLeverageMarkov chain Monte Carlo methodsModel dimension\nWe consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure p D for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general p D approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the 'hat' matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding p D to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.\n\nIntroduction\n\nThe development of Markov chain Monte Carlo (MCMC) methods has made it possible to fit increasingly large classes of models with the aim of exploring real world complexities of data . This ability naturally leads us to wish to compare alternative model formulations with the aim of identifying a class of succinct models which appear to describe the information in the data adequately: for example, we might ask whether we need to incorporate a random effect to allow for overdispersion, what distributional forms to assume for responses and random effects, and so on.\n\nWithin the classical modelling framework, model comparison generally takes place by defining a measure of fit, typically a deviance statistic, and complexity, the number of free parameters in the model. Since increasing complexity is accompanied by a better fit, models are compared by trading off these two quantities and, following early work of , proposals are often formally based on minimizing a measure of expected loss on a future replicate data set: see, for example, , Ripley (1996) and . A model comparison using the Bayesian information criterion also requires the specification of the number of parameters in each model , but in complex hierarchical models parameters may outnumber observations and these methods clearly cannot be directly applied (Gelfand and Dey, 1994). The most ambitious attempts to tackle this problem appear in the smoothing and neural network literature (Wahba, 1990;Moody, 1992;MacKay, 1995;Ripley, 1996). This paper suggests Bayesian measures of complexity and fit that can be combined to compare models of arbitrary structure.\n\nIn the next section we use an information theoretic argument to motivate a complexity measure p D for the effective number of parameters in a model, as the difference between the posterior mean of the deviance and the deviance at the posterior estimates of the parameters of interest. This quantity can be trivially obtained from an MCMC analysis and algebraic forms and approximations are unnecessary for its use. We nevertheless investigate some of its formal properties in the following three sections: Section 3 shows that p D is approximately the trace of the product of Fisher's information and the posterior covariance matrix, whereas in Section 4 we show that for normal models p D corresponds to the trace of the 'hat' matrix projecting observations onto fitted values and we illustrate its form for various hierarchical models. Its properties in exponential families are explored in Section 5.\n\nThe posterior mean devianceD can be taken as a Bayesian measure of fit or 'adequacy', and Section 6 shows how in exponential family models an observation's contributions toD and p D can be used as residual and leverage diagnostics respectively. In Section 7 we tentatively suggest that the adequacyD and complexity p D may be added to form a deviance information criterion DIC which may be used for comparing models. We describe how this parallels the development of non-Bayesian information criteria and provide a somewhat heuristic decision theoretic justification. In Section 8 we illustrate the use of this technique on some reasonably complex examples. Finally, Section 9 draws some conclusions concerning these proposed techniques.\n\n\nThe complexity of a Bayesian model\n\n\n'Focused' full probability models\n\nParametric statistical modelling of data y involves the specification of a probability model p.y|\u03b8/, \u03b8 \u2208 \u0398. For a Bayesian 'full' probability model, we also specify a prior distribution p.\u03b8/ which may give rise to a marginal distribution p.y/ = \u0398 p.y|\u03b8/ p.\u03b8/ d\u03b8:\n\n(1) Particular choices of p.y|\u03b8/ and p.\u03b8/ will be termed a model 'focused' on \u0398. Note that we might further parameterize our prior with unknown 'hyperparameters' \u03c8 to create a hierarchical model, so that the full probability model factorizes as p.y; \u03b8; \u03c8/ = p.y; \u03b8/ p.\u03b8|\u03c8/ p.\u03c8/:\n\nThen, depending on the parameters in focus, the model may compose the likelihood p.y|\u03b8/ and prior p.\u03b8/ = \u03a8 p.\u03b8|\u03c8/ p.\u03c8/ d\u03c8; or the likelihood p.y|\u03c8/ = \u0398 p.y|\u03b8/ p.\u03b8|\u03c8/ d\u03b8 and prior p.\u03c8/. Both these models lead to the same marginal distribution (1) but can be considered as having different numbers of parameters. A consequence is that in hierarchical modelling we cannot uniquely define a 'likelihood' or 'model complexity' without specifying the level of the hierarchy that is the focus of the modelling exercise (Gelfand and Trevisani, 2002). In fact, by focusing our models on a particular set of parameters \u0398, we essentially reduce all models to non-hierarchical structures. For example, consider an unbalanced random-effects one-way analysis of variance (ANOVA) focused on the group means: y i |\u03b8 i \u223c N.\u03b8 i ; \u03c4 \u22121 i /; \u03b8 i \u223c N.\u03c8; \u03bb \u22121 /; i = 1; : : : ; p:\n\nThis model could also be focused on the overall mean \u03c8 to give y i |\u03c8 \u223c N.\u03c8; \u03c4 \u22121 i + \u03bb \u22121 /;\n\nin which case it could reasonably be considered as having a different complexity. It is natural to wish to measure the complexity of a focused model, both in its own right, say to assess the degrees of freedom of estimators, and as a contribution to model choice: for example, criteria such as BIC (Schwarz, 1978), AIC , TIC (Takeuchi, 1976) and NIC (Murata et al., 1994) all trade off model fit against a measure of the effective number of parameters in the model. However, the foregoing discussion suggests that such measures of complexity may not be unique and will depend on the number of parameters in focus. Furthermore, the inclusion of a prior distribution induces a dependence between parameters that is likely to reduce the effective dimensionality, although the degree of reduction may depend on the data that are available. Heuristically, complexity reflects the 'difficulty in estimation' and hence it seems reasonable that a measure of complexity may depend on both the prior information concerning the parameters in focus and the specific data that are observed.\n\n\nIs there a true model?\n\nWe follow Box (1976) in believing that 'all models are wrong, but some are useful'. However, it can be useful to posit a 'true' distribution p t .Y/ of unobserved future data Y since, for any focused model, this defines a 'pseudotrue' parameter value \u03b8 t (Sawa, 1978) which specifies a likelihood p.Y |\u03b8 t / that minimizes the Kullback-Leibler distance E t [log{p t .Y/}=p.Y |\u03b8 t /] from p t .Y/. Having observed data y, under reasonably broad conditions (Berk, 1966;Bunke and Milhaud, 1998) p.\u03b8|y/ converges to \u03b8 t as information on the components of \u03b8 increases. Thus Bayesian analysis implicitly relies on p.Y |\u03b8 t / being a reasonable approximation to p t .Y/, and we shall indicate where we make use of this 'good model' assumption.\n\n\nTrue and estimated residual information\n\nThe residual information in data y conditional on \u03b8 may be defined (up to a multiplicative constant) as \u22122 log{p.y|\u03b8/} (Kullback and Leibler, 1951;) and can be interpreted as a measure of 'surprise' (Good, 1956), logarithmic penalty (Bernardo, 1979) or uncertainty. Suppose that we have an estimator\u03b8.y/ of the pseudotrue parameter \u03b8 t . Then the excess of the true over the estimated residual information will be denoted d \u0398 {y; \u03b8 t ;\u03b8.y/} = \u22122 log{p.y|\u03b8 t /} + 2 log[p{y|\u03b8.y/}]:\n\n(\n\nThis can be thought of as the reduction in surprise or uncertainty due to estimation, or alternatively the degree of 'overfitting' due to\u03b8.y/ adapting to the data y. We now argue that d \u0398 may form the basis for both classical and Bayesian measures of model dimensionality, with each approach differing in how it deals with the unknown true parameters in d \u0398 .\n\n\nClassical measures of model dimensionality\n\nIn a non-Bayesian likelihood-based context, we may take\u03b8.y/ to be the maximum likelihood estimator\u03b8.y/, expand 2 log{p.y|\u03b8 t /} around 2 log[p{y|\u03b8.y/}], take expectations with respect to the unknown true sampling distribution p t .Y/ and hence show (Ripley, 1996) \n(page 34) that E t [d \u0398 {Y; \u03b8 t ;\u03b8.Y/}] \u2248 p \u00c5 = tr.KJ \u22121 /;(4)\nwhere J = \u2212E t @ 2 log{p.Y |\u03b8 t /} @\u03b8 2 ; K = var t @ log{p.Y |\u03b8 t /} @\u03b8 :\n\nThis is the measure of complexity that is used in TIC (Takeuchi, 1976).  (page 244) pointed out that p \u00c5 = tr.J\u03a3/;\n\nwhere \u03a3 = J \u22121 KJ \u22121 is the familiar 'sandwich' approximation to the variance-covariance matrix of the\u03b8.y/ (Huber, 1967). If p t .y/ = p.y|\u03b8 t /, i.e. one of the models is true, then K = J and p \u00c5 = p, the number of independent parameters in \u0398. For example, in a fixed effect ANOVA model y i |\u03b8 i \u223c N.\u03b8 i ; \u03c4 \u22121 i /; i = 1; : : : ; p;\n\nwith \u03c4 \u22121 i s known,\nd \u0398 {y; \u03b8 t ;\u03b8.y/} = i \u03c4 i .y i \u2212 \u03b8 t i / 2 ; whose expectation under p t .Y/ is p \u00c5 = \u03a3 i \u03c4 i E t .Y i \u2212 \u03b8 t / 2 . If the model is true, E t .Y i \u2212 \u03b8 t / 2 = \u03c4 \u22121 i\nand so p \u00c5 = p. Ripley (1996) (page 140) showed how this procedure may be extended to 'regularized' models in which a specified prior term p.\u03b8/ is introduced to form a penalized log-likelihood. Replacing log.p/ by log{p.y|\u03b8/} + log{p.\u03b8/} in equations (5) yields a more general definition of p \u00c5 that was derived by Moody (1992) and termed the 'effective number of parameters'. This is the measure of dimensionality that is used in NIC (Murata et al., 1994): the estimation of p \u00c5 is generally not straightforward (Ripley, 1996).\n\nIn the random-effects ANOVA example with \u03b8 i \u223c N.\u03c8; \u03bb \u22121 /; \u03c8 and \u03bb known, let \u03c1 i = \u03c4 i =.\u03c4 i + \u03bb/ be the intraclass correlation coefficient in the ith group. We then obtain\np \u00c5 = i \u03c1 i \u03c4 i E t .Y i \u2212 \u03b8 t / 2 ;(7)\nwhich becomes\np \u00c5 = i \u03c1 i(8)\nif the likelihood is true.\n\n\nA Bayesian measure of model complexity\n\nFrom a Bayesian perspective, the unknown \u03b8 t may be replaced by a random variable \u03b8. \n\np D {y; \u0398;\u03b8.y/} is our proposal as the effective number of parameters with respect to a model with focus \u0398: we shall usually drop the arguments {y; \u0398;\u03b8.y/} from the notation. In our examples we shall generally take\u03b8.y/ = E.\u03b8|y/ =\u03b8, the posterior mean of the parameters. However, we note that it is not strictly necessary to use the posterior mean as an estimator of either d \u0398 or \u03b8, and the mode or median could be justified (Section 2.6). Taking f.y/ to be some fully specified standardizing term that is a function of the data alone, p D may be written as\np D = D.\u03b8/ \u2212 D.\u03b8/(10)\nwhere D.\u03b8/ = \u22122 log{p.y|\u03b8/} + 2 log{f.y/}:\n\nWe shall term D.\u03b8/ the 'Bayesian deviance' in general and, more specifically, for members of the exponential family with E.Y/ = \u00b5.\u03b8/ we shall use the saturated deviance D.\u03b8/ obtained by setting f.y/ = p{y|\u00b5.\u03b8/ = y}: see Section 8.1. Equation (10) shows that p D can be considered as a 'mean deviance minus the deviance of the means'. A referee has pointed out the related argument used by , who showed that such a difference, between the average of log-likelihood ratios and the likelihood ratio evaluated at the average (over multiple imputations) of the parameters, is the key quantity in estimating the degrees of freedom of a test.\n\nFor example, in the random-effects ANOVA (2) with \u03c8 and \u03bb known,\nD.\u03b8/ = i \u03c4 i .y i \u2212 \u03b8 i / 2 ;\nwhich is \u22122 log(likelihood) standardized by the term \u22122 log{f.y/} = \u03a3 i log.2\u03c0=\u03c4 i / obtained from setting \u03b8 i = y i . Now \u03b8 i |y \u223c N{\u03c1 i y i + .1 \u2212 \u03c1 i /\u03c8; \u03c1 i \u03c4 \u22121 i } and hence it can be shown that the posterior distribution of D.\u03b8/ has the form D.\u03b8/ \u223c \u03c1 i \u03c7 2 {1; .y i \u2212 \u03c8/ 2 .1 \u2212 \u03c1 i /\u03bb};\n\nwhere \u03c7 2 .a; b/ is a non-central \u03c7 2 -distribution with mean a + b. Thus, since \u03c1 i \u03bb = .1 \u2212 \u03c1 i /\u03c4 i , we have D.\u03b8/ = \u03c1 i + \u03c4 i .1 \u2212 \u03c1 i / 2 .y i \u2212 \u03c8/ 2 ; D.\u03b8/ = \u03c4 i .1 \u2212 \u03c1 i / 2 .y i \u2212 \u03c8/ 2 ; and so\np D = i \u03c1 i = i \u03c4 i \u03c4 i + \u03bb :(11)\nThe effective number of parameters is therefore the sum of the intraclass correlation coefficients, which essentially measures the sum of the ratios of the precision in the likelihood to the precision in the posterior. This exactly matches Moody's approach (8) when the model is true. If \u03c8 is unknown and given a uniform hyperprior we obtain a posterior distribution \u03c8 \u223c N{\u0233; .\u03bb \u03a3 \u03c1 i / \u22121 }, where\u0233 = \u03a3 \u03c1 i y i =\u03a3 \u03c1 i . It is straightforward to show that D.\u03b8/ = \u03c1 i + \u03bb \u03c1 i .1 \u2212 \u03c1 i /.y i \u2212\u0233/ 2 + \u03c1 i .1 \u2212 \u03c1 i /= \u03c1 i ; D.\u03b8/ = \u03bb \u03c1 i .1 \u2212 \u03c1 i /.y i \u2212\u0233/ 2 ; and so p D = \u03a3 \u03c1 i + \u03a3 \u03c1 i .1 \u2212 \u03c1 i /=\u03a3 \u03c1 i . If the groups are independent, \u03bb = 0; \u03c1 i = 1 and p D = p. If the groups all have the same mean, \u03bb \u2192 \u221e; \u03c1 i \u2192 0 and p D \u2192 1. If all group precisions are equal, p D = 1 + .p \u2212 1/\u03c1; as obtained by .\n\n\nSome observations on p D\n\n(a) Equation (10) may be rewritten as D.\u03b8/ = D.\u03b8/ + p D ;\n\nwhich can be interpreted as a classical 'plug-in' measure of fit plus a measure of complexity. Thus our Bayesian measure of fit, D.\u03b8/, could perhaps be better considered as a measure of 'adequacy', and we shall use these terms interchangeably. However, in Section 7.3 we shall suggest that an additional penalty for complexity may be reasonable when making model comparisons. (b) Simple use of the Bayes theorem reveals the expression p D = E \u03b8|y \u22122 log p.\u03b8|y/ p.\u03b8/ + 2 log p.\u03b8|y/ p.\u03b8/ ;\n\nwhich can be interpreted as (minus twice) the posterior estimate of the gain in information provided by the data about \u03b8, minus the plug-in estimate of the gain in information.\n\n(c) It is reasonable that the effective number of parameters in a model might depend on the data, the choice of focus \u0398 and the prior information (Section 2.1). Less attractive, perhaps, is that p D may also depend on the choice of estimator\u03b8.y/, since this can produce a lack of invariance of p D to apparently innocuous transformations, such as making inferences on logits instead of probabilities in Bernoulli trials. Our usual choice of the posterior mean is largely based on the subsequent ability to investigate approximate forms for p D (Section 3), and the positivity properties described below. A choice of, say, posterior medians would produce a measure of model complexity that was invariant to univariate 1-1 transformations, and we explore this possibility in Section 5. (d) It follows from equation (10) and Jensen's inequality that, when using the posterior mean as an estimator\u03b8.y/; p D 0 for any likelihood that is log-concave in \u03b8, with 0 being approached for a degenerate prior on \u03b8. Non-log-concave likelihoods can, however, give rise to a negative p D in certain circumstances. For example, consider a single observation from a Cauchy distribution with deviance D.\u03b8/ = 2 log{1 + .y \u2212 \u03b8/ 2 }, with a discrete prior assigning probability 1/11 to \u03b8 = 0 and 10/11 to \u03b8 = 3. If we observe y = 0, then the posterior probabilities are changed to 0.5 and 0.5, and so\u03b8 = 1:5. Thus p D = D.\u03b8/ \u2212 D.\u03b8/ = log.10/ \u2212 2 log.13=4/ = log.160=169/ < 0. Our experience has been that negative p D s indicate substantial conflict between the prior and data, or where the posterior mean is a poor estimator (such as a symmetric bimodal distribution). (e) The posterior distribution that is used in obtaining p D conditions on the truth of the model, and hence p D may only be considered an appropriate measure of the complexity of a model that reasonably describes the data. This is reflected in the finding that p D in the simple ANOVA example (11) will not necessarily be approximately equivalent to the classical p \u00c5 (7) if the assumptions of the model are substantially inaccurate. This good model assumption (Section 2.2) is further considered when we come to comparisons of models (Section 7.3). (f) Provided that D.\u03b8/ is available in closed form, p D may be easily calculated after an MCMC run by taking the sample mean of the simulated values of D.\u03b8/, minus the plug-in estimate of the deviance using the sample means of the simulated values of \u03b8. No 'small sample' adjustment is necessary. This ease of computation should be contrasted with the frequent difficulty within the classical framework with deriving the functional form of the measure of dimensionality and its subsequent estimation. (g) Since the complexity depends on the focus, a decision must be made whether nuisance parameters, e.g. variances, are to be included in \u0398 or integrated out before specifying the model p.y|\u03b8/. However, such a removal of nuisance parameters may create computational difficulties.\n\np D has been defined and is trivially computable by using MCMC methods, and so strictly speaking there is no need to explore exact forms or approximations. However, to provide insight into the behaviour of p D , the following three sections consider the form of p D in different situations and draw parallels with alternative suggestions: note that we are primarily concerned with the 'preasymptotic' situation in which prior opinion is still influential and the likelihood has not overwhelmed the prior.\n\n\nForms for p D based on normal approximations\n\nIn Section 2.1 we argued that focused models are essentially non-hierarchical with a likelihood p.y|\u03b8/ and prior p.\u03b8/. Before considering particular assumptions for these we examine the form of p D under two general conditions: approximately normal likelihoods and negligible prior information.\n\n3.1. p D assuming a normal approximation to the likelihood We may expand D.\u03b8/ around E \u03b8|y .\u03b8/ =\u03b8 to give, to second order,\nD.\u03b8/ \u2248 D.\u03b8/ + .\u03b8 \u2212\u03b8/ T @D @\u03b8 \u03b8 + 1 2 .\u03b8 \u2212\u03b8/ T @ 2 D @\u03b8 2 \u03b8 .\u03b8 \u2212\u03b8/; (13) = D.\u03b8/ \u2212 2.\u03b8 \u2212\u03b8/ T L \u03b8 \u2212 .\u03b8 \u2212\u03b8/ T L \u03b8 .\u03b8 \u2212\u03b8/(14)\nwhere L = log{p.y|\u03b8/} and L and L represent first and second derivatives with respect to \u03b8. This corresponds to a normal approximation to the likelihood. Taking expectations of equation (14) with respect to the posterior distribution of \u03b8 gives\nE \u03b8|y {D.\u03b8/} \u2248 D.\u03b8/ \u2212 E[tr{.\u03b8 \u2212\u03b8/ T L \u03b8 .\u03b8 \u2212\u03b8/}] = D.\u03b8/ \u2212 E[tr{L \u03b8 .\u03b8 \u2212\u03b8/.\u03b8 \u2212\u03b8/ T }] = D.\u03b8/ \u2212 tr[L \u03b8 E{.\u03b8 \u2212\u03b8/.\u03b8 \u2212\u03b8/ T }] = D.\u03b8/ + tr.\u2212L \u03b8 V/\nwhere V = E{.\u03b8 \u2212\u03b8/.\u03b8 \u2212\u03b8/ T } is the posterior covariance matrix of \u03b8, and \u2212L \u03b8 is the observed Fisher information evaluated at the posterior mean of \u03b8. Thus\np D \u2248 tr.\u2212L \u03b8 V/;(15)\nwhich can be thought of as a measure of the ratio of the information in the likelihood about the parameters as a fraction of the total information in the likelihood and the prior. We note the parallel with the classical p \u00c5 in equation (6). We also note that\nL \u03b8 = Q \u03b8 \u2212 P \u03b8\nwhere Q = @ 2 log{p.\u03b8|y/}=@\u03b8 2 and P = @ 2 log{p.\u03b8/}=@\u03b8 2 , and hence approximation (15) can be written\np D \u2248 tr.\u2212Q \u03b8 V/ \u2212 tr.\u2212P \u03b8 V/:\nUnder approximate posterior normality V \u22121 \u2248 \u2212Q \u03b8 and hence\np D \u2248 p \u2212 tr.\u2212P \u03b8 V/(16)\nwhere p is the cardinality of \u0398.\n\n\np D for approximately normal likelihoods and negligible prior information\n\nConsider a focused model in which p.\u03b8/ is assumed to be dominated by the likelihood, either because of assuming a 'flat' prior or by increasing the sample size. Assume that the approximation\n\u03b8|y \u223c N.\u03b8; \u2212L \u03b8 /(17)\nholds, where\u03b8 =\u03b8 are the maximum likelihood estimates such that L \u03b8 = 0 , section 5.3). From equation (14) D\n.\u03b8/ \u2248 D.\u03b8/ \u2212 .\u03b8 \u2212\u03b8/ T L \u03b8 .\u03b8 \u2212\u03b8/ \u2248 D.\u03b8/ + \u03c7 2 p ;(18)\nsince, by approximation (17), \u2212.\u03b8 \u2212\u03b8/ T L \u03b8 .\u03b8 \u2212\u03b8/ has an approximate \u03c7 2 -distribution with p degrees of freedom. Rearranging approximation (18) and taking expectations with respect to the posterior distribution of \u03b8 reveals that\np D = E \u03b8|y {D.\u03b8/} \u2212 D.\u03b8/ \u2248 p;\ni.e. p D will be approximately the true number of parameters: this approximation could also be derived by letting P \u03b8 \u2192 0 in approximation (16). This approximate identity is illustrated in Section 8.1.\n\nWe note in passing that we might use MCMC output to estimate the classical deviance D.\u03b8/ of any likelihood-based model byD\n.\u03b8/ = E \u03b8|y {D.\u03b8/} \u2212 p:(19)\nAlthough the maximum likelihood deviance is theoretically the minimum of D over all feasible values of \u03b8; D.\u03b8/ will generally be very badly estimated by the sample minimum over an MCMC run, and so the estimator given by equation (19) may be preferable.\n\n\np D for normal likelihoods\n\nIn this section we illustrate the formal behaviour of p D for normal likelihoods by using exact and approximate identities. However, it is important to keep in mind that in practice such forms are unnecessary for computation and that p D should automatically allow for fixed effects, random effects and unknown precisions.\n\n\nThe normal linear model\n\nWe consider the general hierarchical normal model described by Lindley and Smith (1972). Suppose that\ny \u223c N.A 1 \u03b8; C 1 /; \u03b8 \u223c N.A 2 \u03c8; C 2 /(20)\nwhere all matrices and vectors are of appropriate dimension, and C 1 and C 2 are assumed known and \u03b8 is the focus: unknown precisions are considered in Section 4.5. Then the standardized deviance is D.\u03b8/ = .y \u2212 A 1 \u03b8/ T C \u22121 1 .y \u2212 A 1 \u03b8/; and the posterior distribution for \u03b8 is normal with mean\u03b8 = Vb and covariance V : V and b will be left unspecified for the moment. Expressing\ny \u2212 A 1 \u03b8 as y \u2212 A 1\u03b8 + A 1\u03b8 \u2212 A 1 \u03b8 reveals that D.\u03b8/ = D.\u03b8/ \u2212 2.y \u2212 A 1\u03b8 / T C \u22121 1 A 1 .\u03b8 \u2212\u03b8/ + .\u03b8 \u2212\u03b8/ T A T 1 C \u22121 1 A 1 .\u03b8 \u2212\u03b8/:\nTaking expectations with respect to the posterior distribution of \u03b8 eliminates the middle term and givesD\n= D.\u03b8/ + tr.A T 1 C \u22121 1 A 1 V/;\nand thus p D = tr.A T 1 C \u22121 1 A 1 V/: We note that A T 1 C \u22121 1 A 1 is the Fisher information \u2212L ; V is the posterior covariance matrix and hence\np D = tr.\u2212L V/:(21)\nan exact version of approximation (15). It is also clear that in this context p D is invariant to affine transformations of \u03b8.\n\nIf \u03c8 is assumed known, then Lindley and Smith (1972) \nshowed that V \u22121 = A T 1 C \u22121 1 A 1 + C \u22121 2\nand hence from equation (21) p\nD = p \u2212 tr.C \u22121 2 V/(22)\nas an exact version of approximation (16); then 0 p D p, and p \u2212 p D is the measure of the 'shrinkage' of the posterior estimates towards the prior means. If .C \u22121 2 V/ \u22121 = A T 1 C \u22121 1 A 1 C 2 + I p has eigenvalues \u03bb i + 1; i = 1; : : : ; p, then\np D = p i=1 \u03bb i \u03bb i + 1 ;(23)\nand hence the upper bound for p D is approached as the eigenvalues of C 2 become large, i.e. the prior becomes flat. It can further be shown, in the case A 1 = I n , that p D is the sum of the squared canonical correlations between data Y and the 'signal' \u03b8.\n\n\nThe 'hat' matrix and leverages\n\nA revealing identity is found by noting that b = A T 1 C \u22121 1 y and the fitted values for the data are given by\u0177 = A 1\u03b8 = A 1 Vb = A 1 VA T 1 C \u22121 1 y. Thus the hat matrix that projects the data onto the fitted values is H = A 1 VA T 1 C \u22121 1 , and\np D = tr.A T 1 C \u22121 1 A 1 V/ = tr.A 1 VA T 1 C \u22121 1 / = tr.H/:(24)\nThis identity also holds assuming that \u03c8 is unknown with a uniform prior, in which case Lindley and Smith (1972) \nshowed that V \u22121 = A T 1 C \u22121 1 A 1 + C \u22121 2 \u2212 C \u22121 2 A 2 .A T 2 C \u22121 2 A 2 / \u22121 A T 2 C \u22121 2 .\nThe identification of the effective number of parameters with the trace of the hat matrix is a standard result in linear modelling and has been applied to smoothing (Wahba, 1990) (page 63) and generalized additive models (Hastie and Tibshirani (1990), section 3.5), and is also the conclusion of  in the context of general linear models. The advantage of using the deviance formulation for specifying p D is that all matrix manipulation and asymptotic approximation is avoided: see Section 4.4 for further discussion. Note that tr.H/ is the sum of terms which in regression diagnostics are identified as the individual leverages, the influence of each observation on its fitted value: we shall return to this identity in Section 6.3.  considered the independent normal model y i \u223c N.\u03b8 i ; \u03c4 \u22121 / and suggested that the effective number of parameters should be \u03a3 i h i , where\nh i .\u03b8/ = @E y|\u03b8 .\u03b8 i / @\u03b8 i :(25)\nthe average sensitivity of an unspecified estimate\u03b8 i to a small change in y i . This is a generalization of the trace of the hat matrix discussed above. In the context of the normal linear models, it is straightforward to show that E Y |\u03b8 .\u03b8/ = H\u03b8, and hence p D = tr.H/ matches Ye's suggestion for model complexity. Further connections with  are described in Section 7.2. Laird and Ware (1982) specified the mixed normal model as\n\n\nExample: Laird-Ware mixed models\ny \u223c N.X\u03b1 + Z\u03b2; C 1 /; \u03b2 \u223c N.0; D/;\nwhere the covariance matrices C 1 and D are currently assumed known. The random effects are \u03b2, and the fixed effects are \u03b1, and placing a uniform prior on \u03b1 we can write this model within the general Lindley-Smith formulation (20) by setting \u03b8 = .\u03b1; \u03b2/; A 1 = .X; Z/; \u03c8 = 0 and C 2 as a block diagonal matrix with \u221e in the top left-hand block, D in the bottom right and 0 elsewhere. We have already shown that in these circumstances p D = tr{A T 1 C \u22121 1 A 1 .A T 1 C \u22121 1 A 1 + C \u22121 2 / \u22121 }, and substituting in the appropriate entries for the Laird-Ware model gives p D = tr.\nV \u00c5 V \u22121 /, where V \u00c5 = X T C \u22121 1 X X T C \u22121 1 Z Z T C \u22121 1 X Z T C \u22121 1 Z ; V = X T C \u22121 1 X X T C \u22121 1 Z Z T C \u22121 1 X Z T C \u22121 1 Z + D \u22121\nwhich is the precision of the parameter estimates assuming that D \u22121 = 0, relative to the precision assuming informative D.\n\n\nFrequentist approaches to model complexity: smoothing and normal non-linear models\n\nA common model in semiparametric regression is\ny \u223c N.X\u03b1 + \u03b2; \u03c4 \u22121 C 1 /; \u03b2 \u223c N.0; \u03bb \u22121 D/;\nwhere \u03b2 is a vector of length n of function values of the nonparametric part of an interpolation spline (Wahba, 1990;van der Linde, 1995) and C 1 and D are assumed known. Motivated by the need to estimate the unknown scale factors \u03c4 \u22121 and \u03bb \u22121 , for many years the effective number of parameters has been taken to be the trace of the hat matrix (Wahba (1990), page 63) and so, for example,\u03c4 \u22121 is the residual sum of squares divided by the 'effective degrees of freedom' n \u2212 tr.H/. In this class of models this measure of complexity coincides with p D . Interest in regression diagnostics (Eubank, 1985;Eubank and Gunst, 1986) and cross-validation to determine the smoothing parameter \u03c4 =\u03bb (Wahba (1990), section 4.2) also drew attention to the diagonal entries of the hat matrix as leverage values. Links to partially Bayesian interpolation models have been provided by Kimeldorf and Wahba (1970) and Wahba (1978Wahba ( , 1983 and further work built on these ideas. For example, another large class of models can be formulated by using the following extension to the Lindley-Smith model:\ny \u223c N{g.\u03b8/; \u03c4 \u22121 C 1 }; \u03b8 \u223c N.A 2 \u03c8; \u03bb \u22121 D/\nwhere g is a non-linear expression as found, for example, in pharmacokinetics or neural networks: in many situations A 2 \u03c8 will be 0 and C 1 and D will be identity matrices. Define\nq.\u03b8/ = .y \u2212 g.\u03b8// T C \u22121 1 .y \u2212 g.\u03b8//; r.\u03b8/ = .\u03b8 \u2212 A 2 \u03c8/ T D \u22121 .\u03b8 \u2212 A 2 \u03c8/\nas the likelihood and prior residual variation. MacKay (1992) suggested estimating \u03c4 and \u03bb by maximizing the 'type II' likelihood p.y|\u03bb; \u03c4 / derived from integrating out the unknown \u03b8 from the likelihood. Setting derivatives equal to 0 eventually reveals that\n\u03c4 \u22121 = q.\u03b8/ n \u2212 p D ; \u03bb \u22121 = r.\u03b8/ p D ;\nwhich are the fitted likelihood and prior residual variation, divided by the appropriate effective degrees of freedom: p D = tr.H/ is the key quantity. These results were derived by MacKay (1992) in the context of 'regularization' in complex interpolation models such as neural networks, in which the parameters \u03b8 are standardized and assumed to have independent normal priors with mean 0 and precision \u03bb. Then expression (16) may be written\np D \u2248 p \u2212 \u03bb tr.V/:(26)\nHowever, MacKay's use of approximation (26) requires the evaluation of tr.V/, whereas our p D arises without any additional computation. We would also recommend including \u03bb and \u03c4 in the general MCMC estimation procedure, rather than relying on type II maximum likelihood estimates (Ripley (1996), page 167). In this and the smoothing context a fully Bayesian analysis requires prior distributions for \u03c4 \u22121 and \u03bb \u22121 to be specified (van der Linde, 2000), and this will both change the complexity of the model and require a choice of estimator of the precisions. We shall now illustrate the form of p D in the restricted situation of unknown \u03c4 \u22121 .\n\n\nNormal models with unknown sampling precision\n\nIntroducing unknown variances as part of the focus confronts us with the need to choose a form for the plug-in posterior estimates. We may illustrate this issue by extending the general hierarchical normal model (20) to the conjugate normal-gamma model with an unknown scale parameter \u03c4 in both the likelihood and the prior , section 5.2.1). Suppose that\ny \u223c N.A 1 \u03b8; \u03c4 \u22121 C 1 /; \u03b8 \u223c N.A 2 \u03c8; \u03c4 \u22121 C 2 /;(27)\nand we focus on .\u03b8; \u03c4 /. The standardized deviance is D.\u03b8; \u03c4 / = \u03c4 q.\u03b8/ \u2212 n log.\u03c4 /, where\nq.\u03b8/ = .y \u2212 A 1 \u03b8/ T C \u22121 1 .y \u2212 A 1 \u03b8/\nis the residual variation. Then, for a currently unspecified estimator\u03c4 ,\np D = E \u03b8;\u03c4 |y .D|\u03b8; \u03c4 / \u2212 D.\u03b8;\u03c4 / = E \u03c4 |y [E \u03b8|\u03c4 ;y {\u03c4 q.\u03b8/} \u2212 n log.\u03c4 /] \u2212 {\u03c4 q.\u03b8/ \u2212 n log.\u03c4 /} = tr.H/ + q.\u03b8/.\u03c4 \u2212\u03c4 / \u2212 n{log.\u03c4 / \u2212 log.\u03c4 /} (28) where H = A T 1 C \u22121 1 A 1 .A T 1 C \u22121 1 A 1 + C \u22121 2 / \u22121 is the hat matrix which does not depend on \u03c4 .\nThus the additional uncertain scale parameter adds the second two terms to the complexity of the model.\n\nA conjugate prior \u03c4 \u223c gamma.a; b/ leads to a posterior distribution \u03c4 |y \u223c gamma.a + n=2; b + S=2/, where\nS = .y \u2212 A 1 A 2 \u03c8/ T .C 1 + A T 1 C 2 A 1 / \u22121 .y \u2212 A 1 A 2 \u03c8/:\nIt remains to choose the estimator\u03c4 to place in equation (28), and we shall consider two options. Suppose that we parameterize in terms of \u03c4 and us\u00ea \u03c4 =\u03c4 = a + n=2 b + S=2 ;\n\nmaking the second term in equation (28) 0. Now if X \u223c gamma.a; b/, then E{log.X/} = \u03c8.a/ \u2212 log.b/ where \u03c8 is the digamma function, and so log.\u03c4 / = \u03c8.a + n=2/ \u2212 log.b + S=2/.\n\nHence the term contributing to p D due to the unknown precision is\np D \u2212 tr.H/ = \u2212n \u03c8 a + n 2 \u2212 log a + n 2 \u2248 1 \u2212 2a \u2212 1 3 2a + n\nusing the approximation \u03c8.x/ \u2248 log.x/ \u2212 1=2x \u2212 1=12x 2 . This term will tend to 1 + 1=3n as prior information becomes negligible and hence will be close to the 'correct' value of 1 for moderate sample sizes.\n\nIf we were to parameterize in terms of log.\u03c4 / and to use\u03c4 = exp{log.\u03c4 /}, the third term in equation (28) is 0 and the second term can be shown to be 1 \u2212 O.n \u22121 /. Thus for reasonable sample sizes the choice of parameterization of the unknown precision will make little difference to the measure of complexity. However, in Section 7 we shall argue that the log-scale may be more appropriate owing to the better approximation to likelihood normality.\n\n\nExponential family likelihoods\n\nWe assume that we have p groups of observations, where each of the n i observations in group i has the same distribution. Following McCullagh and Nelder (1989), we define a one-parameter exponential family for the jth observation in the ith group as\nlog{p.y ij |\u03b8 i ; \u03c6/} = w i {y ij \u03b8 i \u2212 b.\u03b8 i /}=\u03c6 + c.y ij ; \u03c6/;(29)\nwhere\n\u00b5 i = E.Y ij |\u03b8 i ; \u03c6/ = b .\u03b8 i /; V.Y ij |\u03b8 i ; \u03c6/ = b .\u03b8 i /\u03c6=w i ;\nand w i is a constant. If the canonical parameterization \u0398 is the focus of the model, then writin\u1e21 b i = E \u03b8 i |y {b.\u03b8 i /} we easily obtain that the contribution of the ith group to the effective number of parameters is\np \u0398 Di = 2n i w i {b i \u2212 b.\u03b8 i /}=\u03c6:(30)\nThese likelihoods highlight the issue of the lack of invariance of p D to reparameterization, since the mean parameterization \u00b5 will give a different complexity p \u00b5 Di . This is first explored within simple binomial and Poisson models with conjugate priors, and then exact and approximate forms of p D are examined for generalized linear and generalized linear mixed models.\n\n\nBinomial likelihood with conjugate prior\n\nIn the notation of equation (29), \u03c6 = 1; w i = 1 and \u03b8 = logit.\u00b5/ = log{\u00b5=.1 \u2212 \u00b5/}, and the (unstandardized) deviance is D.\u00b5 i / = \u22122y i log.\u00b5 i / \u2212 2.n i \u2212 y i / log.1 \u2212 \u00b5 i / where y i = \u03a3 j y ij . A conjugate prior \u00b5 i = {1 + exp.\u2212\u03b8 i /} \u22121 \u223c beta.a; b/ provides a posterior \u00b5 i \u223c beta.a + y i ; b + n i \u2212 y i / with mean .a + y i /=.a + b + n i /. Now, if X \u223c beta.a; b/, then E{log.X/} = \u03c8.a/ \u2212 \u03c8.a + b/ and E{log.1 \u2212 X/} = \u03c8.b/ \u2212 \u03c8.a + b/ where \u03c8 is the digamma function, and hence it can be shown that\nD.\u00b5 i / = D.\u03b8 i / = \u22122y i \u03c8.a + y i / \u2212 2.n i \u2212 y i / \u03c8.b + n i \u2212 y i / + 2n i \u03c8.a + b + n i / D.\u03bc i / = \u22122y i log.a + y i / \u2212 2.n i \u2212 y i / log.b + n i \u2212 y i / + 2n i log.a + b + n i / D.\u03b8 i / = \u22122y i \u03c8.a + y i / + 2y i \u03c8.b + n i \u2212 y i / + 2n i log[1 + exp{\u03c8.a + y i / \u2212 \u03c8.b + n i \u2212 y i /}]; D.\u00b5 med i / = D.\u03b8 med i / = \u22122y i log.\u00b5 med i / \u2212 2.n i \u2212 y i / log.1 \u2212 \u00b5 med i / where \u00b5 med i\ndenotes the posterior median of \u00b5 i . Exact p D i s are obtainable by subtraction, and Fig. 1 shows how the value of p D i depends on the parameterization, the data and the prior. We may also gain further insight into the behaviour of p D i by considering approximate formulae for the mean and canonical parameterizations by\nusing \u03c8.x/ \u2248 log.x/ \u2212 1=2x \u2248 log.x \u2212 1 2 /. This leads to p \u00b5 D i \u2248 y i a + y i + n i \u2212 y i b + n i \u2212 y i \u2212 n i a + b + n i ; 7 p \u0398 D i \u2248 n i a + b + n i \u2212 1 2 :(31)\nWe make the following observations. we are seeking agreement between alternative parameterizations with little dependence on data 5.1.1. Behaviour of p D For all three parameterizations, as the sample size in each group increases relative to the effective prior sample size, its contribution to p D i tends towards 1.\n\n\nAgreement between parameterizations\n\nThe agreement between parameterizations is generally reasonable except in the situations in which the prior sample size is 10 times that of the data. While the canonical parameterization has p D i \u2248 1=11, the mean and median give increased p D i for extreme prior means.\n\n\nDependence on data\n\nWith the exception of the sparse data and weak prior scenario for which the approximate formulae do not hold, the canonical p \u0398 D i does not depend on the data observed and is approximately the ratio of the sample size to the effective posterior sample size. When the mean and median forms depend on data (say when n i = 1 and a + b = 10), p D i is higher in situations of prior-data conflict.\n\n\nPoisson likelihood with conjugate prior\n\nIn the notation of equation (29), \u03c6 = 1; w i = 1 and \u03b8 = log.\u00b5/, and the (unstandardized) deviance is D.\u00b5 i / = \u22122y i log.\u00b5 i / + 2n i \u00b5 i . A conjugate prior \u00b5 i = exp.\u03b8 i / \u223c gamma.a; b/ gives a posterior \u00b5 i \u223c gamma.a + y i ; b + n i / with mean .a + y i /=.b + n i /. If X \u223c gamma.a; b/, then E{log.X/} = \u03c8.a/ \u2212 log.b/ and hence we can show that Exact p D i s are obtainable by subtraction. Fig. 2 shows how the value of p D i relates to the parameterization, the data and the prior. Using the same approximation as previously, approximate p D i s for the mean and canonical parameterizations are p \u00b5 D i \u2248 y i =.a + y i /; p \u0398 D i \u2248 n i =.b + n i /:\n\n\nBehaviour of p D i\n\nFor all three parameterizations, as the sample size in each group increases relative to the effective prior sample size, its contribution to p D i tends towards 1.\n\n\nAgreement between parameterizations\n\nThe agreement between parameterizations is best when there is no conflict between the prior expectation and the data, but it can be substantial when such conflict is extreme. The median estimator leads to a p D i that is intermediate between those derived from the canonical and mean parameterizations.\n\n\nDependence on data\n\nExcept in the situation of a single y i = 0 with weak prior information, the approximation for the canonical p \u0398 D i is very accurate and so p \u0398 D i does not depend on the data observed. There can be a substantial dependence for the mean parameterization, with p \u00b5 D i being higher when the prior mean underestimates the data.\n\n\nConclusion\n\nIn conclusion, for both binomial and Poisson data there is reasonable agreement between the different p D i s provided that the model provides a reasonable fit to the data, i.e. there is not strong conflict between the prior and data. The canonical parameterization appears preferable, both for its lack of dependence on the data and for its generally close approximation to the invariant p D i based on a median estimator. Thus we would not normally expect the choice of parameterization to have a strong effect, although in Section 8.3 we present an example of a Bernoulli model where this choice does prove to be important.\n\n\nGeneralized linear models with canonical link functions\n\nHere we shall focus on the canonical parameterization in terms of \u03b8 i , both for the reasons outlined above and because its likelihood should better fulfil a normal approximation (Slate, 1994): related identities are available for the mean parameterization in terms of \u00b5 i = \u00b5.\u03b8 i /. We emphasize again that the approximate identities that are derived in this and the following section are only for understanding the behaviour of p D in idealized circumstances (i.e. known precision parameters) and are not required for computation in practical situations.\n\nFollowing McCullagh and Nelder (1989) we assume that the mean \u00b5 i of y ij is related to a set of covariates x i through a link function g.\u00b5 i / = x T i \u03b1, and that g is the canonical link \u03b8.\u00b5/. The second-order Taylor series expansion of D.\u03b8 i / around D.\u03b8 i / yields an approximate normal distribution for working observations and hence derivations of Section 3 apply. We eventually obtain\np D \u2248 tr{X T WX V.\u03b1|y/} where W is diagonal with entries W i = w i \u03c6 n i b .\u03b8 i /;\nthe generalized linear model iterated weights (McCullagh and Nelder (1989), page 40): \u03c6 is assumed known. Under an N.\u03b1 0 ; C 2 / prior on \u03b1, the prior contribution to the negative Hessian matrix at the mode is just C \u22121 2 , so under the canonical link the approximate normal posterior has variance V.\u03b1|y/ = .C \u22121 2 + X T WX/ \u22121 ; again producing p D as a measure of the ratio of the 'working' likelihood to posterior information.\n\n\nGeneralized linear mixed models\n\nWe now consider the class of generalized linear mixed models with canonical link, in which g.\u00b5 i / = x T i \u03b1 + z T i \u03b2, where \u03b2 \u223c N.0; D/ (Breslow and Clayton, 1993) and D is assumed known.\n\nUsing the same argument as for generalized linear models (Section 5.3), we find that\np D \u2248 tr[.X; Z/ T W.X; Z/V {.\u03b1; \u03b2/|y}] \u2248 tr.V \u00c5 V \u22121 /; where V \u00c5 = X T W \u22121 X X T W \u22121 Z Z T W \u22121 X Z T W \u22121 Z ; V = X T W \u22121 X X T W \u22121 Z Z T W \u22121 X Z T W \u22121 Z + D \u22121 :\nThis matches the proposal of  except their D \u22121 is a diagonal matrix of the second derivatives of the prior likelihood for each random effect.\n\n\nDiagnostics for fit and influence\n\n\nPosterior expected deviance as a Bayesian measure of fit or 'adequacy'\n\nThe posterior mean of the deviance E \u03b8|y {D.\u03b8/} = D.\u03b8/ has often been used to compare models informally: see, for example, Dempster (1974) (reprinted as Dempster (1997a)), Raghunathan (1988), Zeger and Karim (1991), Gilks et al. (1993) and Richardson and Green (1997). These researchers have, however, not been explicit about whether, or how much, such a measure might be traded off against increasing complexity of a model: Dempster (1997b) suggested plotting log-likelihoods from MCMC runs but hesitated to dictate a model choice procedure. We shall discuss this further in Section 7.3. In Section 2.6 we argued that D.\u03b8/ already incorporates some penalty for complexity and hence we use the term 'adequacy' and 'Bayesian fit' interchangeably.\n\n\nSampling theory diagnostics for lack of Bayesian fit\n\nSuppose that all aspects of the model were assumed true. Then before observing data Y our expectation of the posterior expected deviance is\nE Y .D/ = E Y [E \u03b8|y {D.\u03b8/}] (32) = E \u03b8 .E Y |\u03b8 [\u22122 log{p.Y |\u03b8/} + 2 log{f.Y/}]/\nby reversing the conditioning between Y and \u03b8. If f.Y/ = p{Y |\u03b8.Y/} where\u03b8.Y/ is the standard maximum likelihood estimate, then\nE Y |\u03b8 \u2212 2 log p.Y |\u03b8/ p{Y |\u03b8.Y/}\nis simply the expected likelihood ratio statistic for the fitted values\u03b8.Y/ with respect to the true null model \u03b8 and hence under standard conditions is approximately E.\u03c7 2 p / = p, the dimensionality of \u03b8. From equation (32) we therefore expect, if the model is true, the posterior expected deviance (standardized by the maximized log-likelihood) to be E Y .D/ \u2248 E \u03b8 .p/ = p, the number of free parameters in \u03b8. This might be appropriate for checking the overall goodness of fit of the model.\n\nIn particular, consider the one-parameter exponential family where p = n, the total sample size. The likelihood is maximized by substituting y i for the mean of y i , and the posterior mean of the standardized deviance has approximate sampling expectation n if the model is true. This will be exact for normal models with known variance, but in general it will only be reliable if each observation provides considerable information about its mean (McCullagh and Nelder (1989), page 36). Note that comparingD with n is precisely the same as comparing the 'classical' fit D.\u03b8/ with n \u2212 p D , the effective degrees of freedom.\n\nIt is then natural to consider the contribution D i of each observation i to the overall mean deviance, so thatD\n= iD i = i dr 2 i\nwhere dr i = \u00b1 \u221aD i (with the sign given by the sign of y i \u2212 E.y i |\u03b8/) termed the Bayesian deviance residual, defined analogously to McCullagh and Nelder (1989), page 39. See Section 8.1 for an application of this procedure.\n\n\nLeverage diagnostics\n\nIn Section 4.1 we noted that in normal linear models the contribution p Di of each observation i to p D turned out to be its leverage, defined as the relative influence that each observation has on its own fitted value. For y i conditionally independent given \u03b8, it can be shown that\np Di = \u22122 E \u03b8|y log p.\u03b8|y i / p.\u03b8/ \u2212 log p.\u03b8|y i / p.\u03b8/\nwhich reflects its interpretation as the difficulty in estimating \u03b8 with y i . It may be possible to exploit this interpretation in general model fitting, and as a by-product of MCMC estimation to obtain estimates of leverage for each observation. Such diagnostics are illustrated in Section 8.1.\n\n\nA model comparison criterion\n\n\nModel 'selection'\n\nThere has been a long and continuing debate about whether the issue of selecting a model as a basis for inferences is amenable to a strict mathematical analysis using, for example, a decision theoretic paradigm: see, for example, . Our approach here can be considered to be semiformal. Although we believe that it is useful to have measures of fit and complexity, and to combine them into overall criteria that have some theoretical justification, we also feel that an overformal approach to model 'selection' is inappropriate since so many other features of a model should be taken into account before using it as a basis for reporting inferences, e.g. the robustness of its conclusions and its inherent plausibility. In addition, in many contexts it may not be appropriate to 'choose' a single model. Our development closely follows that of Section 2.\n\nA characteristic that is common to both Bayesian and classical approaches is the concept of an independent replicate data set Y rep , derived from the same data-generating mechanism as gave rise to the observed data. Suppose that the loss in assigning to a set of data Y a probability p.Y |\u03b8/ is L.Y ;\u03b8/: We assume that we shall favour models p.Y |\u03b8/ for which L.Y ;\u03b8/ is expected to be small, and thus a criterion can be based on an estimate of E Y rep |\u03b8 t {L.Y rep ;\u03b8/}.\n\nA natural, but optimistic, estimate of this quantity is the 'apparent' loss L{y;\u03b8.y/} that is suffered on repredicting the observed y that gave rise to\u03b8.y/. We follow  in defining the 'optimism' that is associated with this estimator as c \u0398 , where\nE Y rep |\u03b8 t [L{Y rep ;\u03b8.y/}] = L{y;\u03b8.y/} + c \u0398 {y; \u03b8 t ;\u03b8.y/}:(33)\nBoth classical and Bayesian approaches to estimating the optimism c \u0398 will now be examined when assuming a logarithmic loss function L.Y ;\u03b8/ = \u22122 log{p.Y |\u03b8/}: as in Section 2, the classical approach attempts to estimate the sampling expectation of c \u0398 , whereas the Bayesian approach is based on a direct calculation of the posterior expectation of c \u0398 .\n\n\nClassical criteria for model comparison\n\nFrom the previous discussion, approximate forms for the expected optimism\n\u03c0.\u03b8 t / = E Y |\u03b8 t [c \u0398 {Y; \u03b8 t ;\u03b8.Y/}]\nwill, from equation (33), yield criteria for a comparison of models that are based on minimizin\u011d\nE Y rep |\u03b8 t [L{Y rep ;\u03b8.y/}] = L{y;\u03b8.y/} +\u03c0.\u03b8 t /:(34)\nEfron (1986) derived the expression for \u03c0.\u03b8 t / for exponential families and for general loss functions. In particular, for the logarithmic loss function, Efron showed that\n\u03c0 E .\u03b8 t / = 2 i cov t .\u0176 i ; Y i /;(35)\nwhere\u0176 i is the fitted value arising from the estimator\u03b8: if\u03b8 corresponds to maximum likelihood estimation based on a linear predictor with p parameters, then \u03c0 E .\u03b8 t / \u2248 2p. Hence Efron's result can be thought of as generalizing , who sought to minimize the expected Kullback-Leibler distance between the true and estimated predictive distribution and showed under broad conditions that \u03c0.\u03b8 t / \u2248 2p. This in turn suggests that \u03c0 E =2, derived from equation (35), may be adopted as a measure of complexity in more complex modelling situations. Ye and Wong (1998) extended the work mentioned in Section 4.2 to show that \u03c0 E =2 for exponential families can be expressed as a sum of the average sensitivity of the fitted values\u0177 i to a small change in y i : this quantity is termed by Ye and Wong the 'generalized degrees of freedom' when using a general estimation procedure. In normal models with linear estimators\u0177 i =\u03b8 i .y/ = \u03a3 j h ij y j , and so \u03c0.\u03b8 t / = 2 tr.H/. Finally, Ripley (1996) extended the analysis described in Section 2.4 to show that if the model assumed is not true then \u03c0.\u03b8 t / \u2248 2p \u00c5 , where p \u00c5 is defined in equation (4). See  for a full and detailed review of all aspects of estimation of \u03c0.\u03b8 t /.\n\nThese classical criteria for general model comparison are thus all based on equation (34) and can all be considered as corresponding to a plug-in estimate of fit, plus twice the effective number of parameters in the model. We shall now adapt this structure to a Bayesian context. Gelfand and Ghosh (1998) and Laud and Ibrahim (1995) both attempted strict decision theoretic approaches to model choice based on expected losses on replicate data sets. Our approach is more informal, in aiming to identify models that best explain the observed data, but with the expectation that they are likely to minimize uncertainty about observations generated in the same way. Thus, by analogy with the classical results described above, we propose a deviance information criterion DIC, defined as a classical estimate of fit, plus twice the effective number of parameters, to give\n\n\nBayesian criteria for model comparison\nDIC = D.\u03b8/ + 2p D (36) =D + p D(37)\nby definition of p D (10): equation (37) shows that DIC can also be considered as a Bayesian measure of fit or adequacy, penalized by an additional complexity term p D . From the results in Section 3.2, we immediately see that in models with negligible prior information DIC will be approximately equivalent to Akaike's criterion. An approximate decision theoretic justification for DIC can be obtained by mimicking the development of Ripley (1996) (page 33) and  (chapter 6). Using the logarithmic loss function in equation (33), we obtain\nc \u0398 {y; \u03b8 t ;\u03b8.y/} = E Y rep |\u03b8 t {D rep .\u03b8/} \u2212 D.\u03b8/\nwhere \u22122 log[p{Y rep |\u03b8.y/}] is denoted D rep .\u03b8/ and so on: note in this section that D is an unstandardized deviance .f.\u00b7/ = 1/. It is convenient to expand c \u0398 into the three terms\nc \u0398 = E Y rep |\u03b8 t {D rep .\u03b8/ \u2212 D rep .\u03b8 t /} + E Y rep |\u03b8 t {D rep .\u03b8 t / \u2212 D.\u03b8 t /} + {D.\u03b8 t / \u2212 D.\u03b8/};(38)\nwe shall denote the first two terms by L 1 and L 2 respectively and, since we are taking a Bayesian perspective, replace the true \u03b8 t by a random quantity \u03b8.\n\nExpanding the first term to second order gives\nL 1 .\u03b8;\u03b8/ \u2248 E Y rep |\u03b8 {\u22122.\u03b8 \u2212 \u03b8/ T L rep;\u03b8 \u2212 .\u03b8 \u2212 \u03b8/ T L rep;\u03b8 .\u03b8 \u2212 \u03b8/} where L rep;\u03b8 = log{p.Y rep |\u03b8/}. Since E Y rep |\u03b8 .L rep;\u03b8 / = 0 from\nstandard results for score statistics, we obtain after some rearrangement\nL 1 .\u03b8;\u03b8/ \u2248 tr{I \u03b8 .\u03b8 \u2212 \u03b8/.\u03b8 \u2212 \u03b8/ T } where I \u03b8 = E Y rep |\u03b8 .\u2212L rep;\u03b8\n/ is the assumed Fisher information in Y rep , and hence also in y. Making the good model assumption (Section 2.2), this might reasonably be approximated by the observed information at the estimated parameters, so\nL 1 .\u03b8;\u03b8/ \u2248 tr{\u2212L \u03b8 .\u03b8 \u2212 \u03b8/.\u03b8 \u2212 \u03b8/ T }:(39)\nSuppose that under a particular model assumption we obtain a posterior distribution p.\u03b8|y/.\n\nThen from approximations (38) and (39) our posterior expected optimism when adopting this model and the estimator\u03b8 is\nE \u03b8|y .c \u0398 / \u2248 tr[\u2212L \u03b8 E \u03b8|y {.\u03b8 \u2212\u03b8/.\u03b8 \u2212\u03b8/ T }] + E \u03b8|y {L 2 .y; \u03b8/} + E \u03b8|y {D.\u03b8/ \u2212 D.\u03b8/}:\nUsing the posterior mean\u03b8 as our estimator makes the expected optimism\nE \u03b8|y .c \u0398 / \u2248 tr.\u2212L \u03b8 V/ + E \u03b8|y {L 2 .y; \u03b8/} + p D ;(40)\nwhere V again is defined as the posterior covariance of \u03b8, and p D =D \u2212 D.\u03b8/. Now\nL 2 .y; \u03b8/ = E Y rep |\u03b8 [\u22122 log{p.Y rep |\u03b8/}] + 2 log{p.y|\u03b8/}; and so E Y [E \u03b8|Y {L 2 .Y ; \u03b8/}] = E \u03b8 [E Y |\u03b8 {L 2 .Y ; \u03b8/}] = 0.\nWe have already shown in approximation (15) that p D \u2248 tr.\u2212L \u03b8 V/, and hence from expressions (33) and (40) the expected posterior loss when adopting a particular model is\nD.\u03b8/ + E \u03b8|y .c \u0398 / \u2248 D.\u03b8/ + 2p D = DIC;\nneglecting a term E \u03b8|y {L 2 .y; \u03b8/} which is expected to be 0. This derivation has assumed that D is an unstandardized deviance: common standardization across models will leave unchanged the property that differences in DIC are estimates of differences in expected loss in prediction. We make the following observations concerning this admittedly heuristic justification of DIC. First, for the general normal linear model (20), it is straightforward to show that L 2 .y; \u03b8/ = p\u2212.y\u2212A 1 \u03b8/ T C \u22121 1 .y\u2212A 1 \u03b8/ where p is the dimensionality of \u03b8, and hence for true \u03b8 has sampling distribution p \u2212 \u03c7 2 p with mean 0 and variance 2p. This parallels the classical development in which Ripley (1996) (page 34) pointed out that the equivalent term is O.\n\n\u221a n/: we would hope that this factor will tend to cancel when assessing differences in DIC, but this requires further investigation. Second, this development draws heavily on the approximations in Section 3 and hence encourages parameterizations in which likelihood normality is more plausible.\n\nThird, we are attempting to evaluate the consequences of assuming a particular model, using an analysis that is based on that very assumption. This use of the good model assumption (Section 2.2) argues for the use of DIC in comparing models that have already been shown to be adequate candidates for explaining the observations.\n\n\nExamples\n\np D and DIC have already been applied by other researchers in a variety of contexts, such as alternative models for diagnostic probabilities in screening studies (Erkanli et al., 1999), longitudinal binary data using Markov regression models (Erkanli et al., 2001), spline models with Bernoulli responses (Biller and Fahrmeir, 2001), multistage models for treatment usage which combine to form a total DIC (Gelfand et al., 2000), complex spatial models for Poisson counts (Green and Richardson, 2000), pharmacokinetic modelling (Rahman et al., 1999) and structures of Bayesian neural networks (Vehtari and Lampinen, 1999). The following examples illustrate the use of p D and DIC to compare alternative prior and likelihood structures.\n\n\nThe spatial distribution of lip cancer in Scotland\n\nWe consider data on the rates of lip cancer in 56 districts in Scotland (Clayton and Kaldor, 1987;Breslow and Clayton, 1993). The data include observed (y i ) and expected (E i ) numbers of cases for each county i (where the expected counts are based on the age-and sex-standardized national rate applied to the population at risk in each county) plus the 'location' of each county expressed as a list (A i ) of its n i adjacent counties. We assume that the cancer counts within each county y i follow a Poisson distribution with mean exp.\u03b8 i /E i where exp.\u03b8 i / denotes the underlying true area-specific relative risk of lip cancer. We then consider the following set of candidate models for \u03b8 i , reflecting different assumptions about the between-county variation in (log-) relative risk of lip cancer: model 1,\n\u03b8 i = \u03b1 0 ; model 2, \u03b8 i = \u03b1 0 + \u03b3 i ; model 3, \u03b8 i = \u03b1 0 + \u03b4 i ; model 4, \u03b8 i = \u03b1 0 + \u03b3 i + \u03b4 i ; model 5, \u03b8 i = \u03b1 i :\nAn improper uniform prior is placed on \u03b1 0 , independent (proper) normal priors with large variance are specified for each \u03b1 i .i = 1; : : : ; 56/, \u03b3 i are exchangeable random effects with a normal prior distribution having zero mean and precision \u03bb \u03b3 , and \u03b4 i are spatial random effects with a conditional autoregressive prior (Besag, 1974) given by\n\u03b4 i |\u03b4 \\i \u223c normal 1 n i j\u2208A i \u03b4 j ; 1 n i \u03bb \u03b4 :\nA sum-to-zero constraint is imposed on the {\u03b4 i } for identifiability, and weakly informative gamma(0.5,0.0005) priors are assumed for the random effects precision parameters \u03bb \u03b3 and \u03bb \u03b4 . These five models cover the spectrum between the pooled model 1 that makes no allowance for variation between the true risk ratios in each county and the saturated model 5 that assumes independence between the county-specific risk ratios (essentially yielding the maximum likelihood estimates\u03b8 i = log.y i =E i /). The random-effects models 2-4 allow the county-specific relative risks to be similar but not identical, with the autoregressive term allowing for the possibility of spatially correlated variation. We use the saturated deviance (McCullagh and Nelder (1989), page 34)\nD.\u03b8/ = 2 i [y i log{y i = exp.\u03b8 i /E i } \u2212 {y i \u2212 exp.\u03b8 i /E i }]\nobtained by taking \u22122 log{f.y/} = \u22122\u03a3 i log{p.y i |\u03b8 i /} = 208:0 as the standardizing factor (see Section 2.5). This allows calculation of absolute measures of fit (see Section 6.2). For model comparisons, however, it is sufficient to take the standardizing factor as f.y/ = 1. For each model we ran two independent chains of an MCMC sampler in WinBUGS (Spiegelhalter et al., 2000) for 15000 iterations each, following a burn-in period of 5000 iterations. As suggested by Dempster (1997b)  The deviance summaries proposed in this paper are shown for the lip cancer data in Table 1: D is simply the mean of the posterior samples of the saturated deviance; D.\u03bc/ is calculated by plugging the posterior mean of \u00b5 i = exp.\u03b8 i /E i into the saturated deviance; D.\u03b8/ is calculated by plugging the posterior means of the relevant parameters (\u03b1 0 , \u03b1 i , \u03b3 i and/or \u03b4 i ) into the linear predictor \u03b8 i and then evaluating the saturated deviance; D.med/ is calculated by plugging the posterior median of \u03b8 i (or, equivalently, of \u00b5 i ) into the saturated deviance. The results are remarkably similar for the three alternative parameterizations of the plug-in deviance. For fixed effects models we would expect from Section 3.2 that p D should be approximately the true number of independent parameters. For the pooled model 1, p D = 1:0 as expected, whereas, for the saturated model 5, p D ranges from 52.8 to 55.9 depending on the parameterization that is used, which is close to the true value of 56 parameters. The models containing spatial random effects (either with or without additional exchangeable effects) both have around 31 effective parameters, whereas the model with only exchangeable random effects has about 12 additional effective parameters. On the basis of the results of Section 5.2 comparing p D for Poisson likelihoods with different priors, this suggests that the spatial model provides stronger prior information than does the exchangeable model for these data.\n\nTurning to the comparison of DIC for each model, we first note that DIC is subject to Monte Carlo sampling error, since it is a function of stochastic quantities generated under an MCMC sampling scheme. Whereas computing the precise standard errors for our DIC values is a subject of on-going research, the standard errors for theD-values are readily obtained and provide a good indication of the accuracy of DIC and p D . In any case, in several runs using different initial values and random-number seeds for this example, the DIC and p D -estimates obtained never varied by more than 0.5. As such, we are confident that, even allowing for Monte Carlo error, either of models 3 or 4 is superior (in terms of DIC performance) to models 2 or 5, which are in turn superior to model 1. A comparison of DIC for models 3 and 4 suggests that the two spatial models are virtually indistinguishable in terms of the overall fit: pragmatically, we might prefer reporting model 3 since its DIC is only marginally greater than the more complex model 4.\n\nConsidering now the absolute measure of fit suggested in Section 6.2, we compare the values ofD in Table 1 with the sample size n = 56. This suggests that all models except the pooled model 1 provide an adequate overall fit to the data, and that the comparison is essentially based on their complexity alone.\n\nFollowing the discussion in Section 6, Fig. 4 shows a plot of deviance residuals dr i against leverages p Di for each of the five models considered. The broken curves marked on each plot are of the form x 2 + y = c and points lying along such a parabola will each contribute an amount DIC i = c to the overall DIC for that model. For models 2-5, parabolas are marked at values of c = 1, 2, 5, and any data point whose contribution DIC i is greater than 2 is labelled by its  observation number. For model 1, parabolas are marked at c = 1, 10, 50, since the size of the deviance residuals and individual contributions to DIC are much larger and, for clarity, only points for which DIC i is greater than 10 are marked by their observation number. Observations 55 and 56, the only districts with y i = 0, are clearly identified as potential outliers under each of the random-effects models 2-4, as is observation 1 (the district with the highest observed risk ratio y i =E i ). A few other observations (2, 3, 4, 53 and 54) have contributions DIC i that are just larger than 2 under model 2: with the exception of the three districts already discussed, these five districts have the most extreme observed risk ratios and so their estimates tend to be shrunk furthest under the exchangeable model. Observations 14, 15, 45 and 50 appear to be outliers in models 3 and 4 which have a spatial effect, but not in the remaining models. A further investigation reveals that the observed risk ratios in these districts are extreme compared with those in each of their neighbouring districts. For example district 50 has only six cases compared with 19.6 expected, whereas each of its three neighbouring districts have high observed counts (17, 16 and 16) relative to those expected (7.8, 10.5 and 14.4). The spatial prior in models 3 and 4 causes the estimated rate in district 50 to be smoothed towards the mean of its neighbours' rates, thus leading to the discrepancy between observed and fitted values, and since the observation still exercises considerable weight on its fitted value the leverage is high as well. However, overall we might not consider that there is sufficient evidence to cast doubt on any particular observations. Spiegelhalter et al. (1996) (pages 27-29) considered a variety of error structures for the oftanalysed stack loss data of Brownlee (1965). Here the response variable y, the amount of stack loss (escaping ammonia in an industrial application), is regresssed on three predictor variables: air flow x 1 , temperature x 2 and acid concentration x 3 . Assuming the usual linear regression structure\n\n\nRobust regression using the stack loss data\n\u00b5 i = \u03b2 0 + \u03b2 1 z i1 + \u03b2 2 z i2 + \u03b2 3 z i3\nwhere z ij = .x ij \u2212x :j /=sd.x :j /, the standardized covariates, the presence of a few prominent outliers among the n = 21 cases motivates a comparison of the following four error distributions: model 1,\ny i \u223c normal.\u00b5 i ; \u03c4 \u22121 /; model 2, y i \u223c DE.\u00b5 i ; \u03c4 \u22121 /; model 3, y i \u223c logistic.\u00b5 i ; \u03c4 \u22121 /; model 4, y i \u223c t d .\u00b5 i ; \u03c4 \u22121 / (where DE denotes the double-exponential (Laplace) distribution and t d denotes Student's t- distribution with d degrees of freedom).\nA well-known alternative to the direct fitting of many symmetric but non-normal error distributions is through scale mixtures of normals (Andrews and Mallows, 1974). From page 210 of Carlin and Louis (2000), we have the alternate t d -formulation model 5,\ny i \u223c normal \u00b5 i ; 1 w i \u03c4 ; w i \u223c 1 d \u03c7 2 d = gamma d 2 ; d 2 :\nUnlike our other examples the form of the likelihood changes with each model, so we must use the full normalizing constants when computing \u22122 log{p.y|\u00b5; \u03c4 /}. Following Spiegelhalter et al. (1996) we set d = 4, and for each model we placed essentially flat priors on the \u03b2 j (actually normal with mean 0 and precision 0.00001) and log.\u03c4 / (actually gamma(0.001,0.001) on \u03c4 ) and ran the Gibbs sampler in BUGS for 5000 iterations following a burn-in period of 1000 iterations.\n\nReplacing \u03c4 and w i by their posterior means where necessary for the D.\u03b8/-calculation, the resulting deviance summaries are shown in Table 2 (note that the mean parameterization and the canonical parameterization are equivalent here, since the mean \u00b5 i is a linear function of the canonical \u03b2-parameters). Beginning with a comparison of the first four models, the estimates of p D are all just over 5, the correct number of parameters for this example. The DIC-values imply that model 2 (double exponential) is best, followed by the t 4 -, the logistic and finally the normal models. Clearly this order is consistent with the models' respective abilities to accommodate outliers.\n\nTurning to the normal scale mixture representation for the t 4 -likelihood (model 5), the p D -value is 7.6, suggesting that the w i random effects contribute only an extra 2-2.5 parameters. However, the model's smaller DIC-value implies that the extra mixing parameters are worthwhile in an overall quality-of-fit sense. We emphasize that the results from models 4 and 5 need not be equal since, although they lead to the same marginal likelihood for the y i , they correspond to different prediction problems. Finally, plots of deviance residuals versus leverages (which are not shown) clearly identify the observations determined to be 'outlying' by several previous researchers who analysed this data set.\n\n\nLongitudinal binary observations: the six-cities study\n\nTo illustrate how the mean and canonical parameterizations (introduced in Section 5 and further discussed in Section 9) can sometimes lead to different conclusions, our next example considers a subset of data from the six-cities study, a longitudinal study of the health effects of air pollution: see Fitzmaurice and Laird (1993) for the data and a likelihood-based analysis. The data consist of repeated binary measurements y ij of the wheezing status (1, yes; 0, no) of child i at time j, i = 1; : : : ; I; j = 1; : : : ; J, for each of I = 537 children living in Stuebenville, Ohio, at J = 4 time points. We are given two predictor variables: a ij , the age of child i in years at measurement point j (7, 8, 9 or 10 years), and s i , the smoking status of child i's mother (1, yes; 0, no). Following the Bayesian analysis of Chib and Greenberg (1998), we adopt the conditional response model\nY ij \u223c Bernoulli.p ij /; p ij \u2261 Pr.Y ij = 1/ = g \u22121 .\u00b5 ij /; \u00b5 ij = \u03b2 0 + \u03b2 1 z ij1 + \u03b2 2 z ij2 + \u03b2 3 z ij3 + b i ;\nwhere z ijk = x ijk \u2212x ::k ; k = 1; 2; 3, and x ij1 = a ij , x ij2 = s i and x ij3 = a ij s i , a smoking-age interaction term. The b i are individual-specific random effects, initially given an exchangeable N.0; \u03bb \u22121 / specification, which allow for dependence between the longitudinal responses for child i. The model choice issue here is to determine the most appropriate link function g.\u00b7/ among three candidates, namely the logit, the probit and the complementary log-log-links. More formally, our three models are model 1,\ng.p ij / = logit.p ij / = log{p ij =.1 \u2212 p ij /}; model 2, g.p ij / = probit.p ij / = \u03a6 \u22121 .p ij /;\nand model 3, g.p ij / = cloglog.p ij / = log{\u2212 log.1 \u2212 p ij /}: Since the Bernoulli likelihood is unaffected by this choice, in all cases the deviance takes the simple form\nD = \u22122 i;j {y ij log.p ij / + .1 \u2212 y ij / log.1 \u2212 p ij /}:\nPlacing flat priors on the \u03b2 k and a gamma(0.001,0.001) prior on \u03bb, and running the Gibbs sampler for 5000 iterations following a burn-in period of 1000 iterations produces the deviance summaries in Table 3 for the canonical and mean parameterizations: the canonical parameterization constructs\u03b8 as the mean of the linear predictors \u03b2 and b i , and then uses the appropriate linking transformation (logit, probit or complementary log-log) to obtain the imputed means for the p ij . The mean parameterization simply uses the means of the p ij themselves when computing D.\u03b8/. Natarajan and Kass (2000) have pointed out potential problems with the gamma(0.001,0.001) prior on \u03bb, but in this context the 537 random effects ensure that these findings are robust to the choice of prior for \u03bb.\n\nThe posterior standard deviation \u221a \u03bb \u22121 of the random effects is estimated to be 2.2 (standard deviation 0.2), which indicates extremely high unexplained overdispersion and hence considerable prior-data conflict: this should warn us of a potential lack of robustness in our procedure.\n\nWe have a sample size of n i = 4 for each of I = 537 individuals, and an average p D i for the canonical parameterization of around 0.4-0.5. From approximation (31), this indicates a prior sample size a + b of around 4-6. Referring to the evidence in Fig. 1 concerning low prior and observation sample sizes (n i = 1; a + b = 1), we might expect the mean parameterization to display decreased complexity compared with the canonical, and this is borne out in the results. DIC prefers the complementary log-log-link under the canonical parameterization, but the probit link under the mean parameterization. We repeat that we prefer the canonical results because of the improved normality of the likelihoods and their lack of dependence on observed data: however, none of the models explain the data very well, and the lack of consensus suggests caution in using any of the models.\n\n\nDiscussion\n\nHere we briefly discuss relationships to other suggestions and give some guidance on the practical use of the techniques described in this paper.\n\n9.1. Relationship of p D and DIC to other suggestions 9.1.1. Cross-validation Stone (1977) showed the asymptotic equivalence of model comparison based on cross-validation and AIC, whereas Wahba (1990) (page 52) showed how a generalized cross-validation criterion leads to the use of n \u2212 tr.H/ as a denominator in the estimation of residual mean-squared error. We would expect our measure of model complexity p D to be strongly related to cross-validatory assessment, but this requires further investigation.  criticized  for using a plug-in predictive distribution as we have done in Section 7.3, rather than the full predictive distribution obtained by integrating out the unknown parameters. A criterion based on this predictive distribution is also invariant to reparameterizations. Laud and Ibrahim (1995) and Gelfand and Ghosh (1998) suggested minimizing a predictive 'discrepancy measure' E{d.Y new ; y/|y}; where Y new is a draw from the posterior predictive distribution p.Y new |y/, and we might for instance take d.\n\n\nOther predictive loss functions\nY new ; y/ = .Y new \u2212 y/ T .Y new \u2212 y/.\nThey showed that their measures also have attractive interpretations as weighted sums of 'goodness of fit' and 'predictive variability penalty' terms. However, a proper choice of the criterion requires fairly involved analytic work, as well as several subjective choices about the utility function that is appropriate for the problem at hand. Furthermore, the oneway ANOVA model in Section 2.5 gives rise to a fit term equivalent to D.\u03b8/, and a predictive variability term equal to p D + p. Thus their suggestion is equivalent in this context to the comparison by our Bayesian measure of fitD which, although invariant to parameterization, does not seem to penalize complexity sufficiently.\n\nIn general the use of a plug-in estimate appears to 'cost' an extra penalty of p D .\n\n\nBayes factors\n\nBayes factors are criteria based on a comparison of the marginal likelihoods (1) , and a common approximation is the Bayesian (or Schwarz) information criterion (Schwarz, 1978), which for a model with p parameters and n observations is given by BIC = \u22122 log{p.y|\u03b8/} + p log.n/:\n\nBernardo and Smith (1994) (chapter 6) argued that this formulation may only be appropriate in circumstances where it was really believed that one and only one of the competing models was in fact true, and the crucial issue was to choose this correct model, and that in other circumstances criteria based on short-term prediction, such as cross-validation, may be more appropriate. We support this view and refer to Han and Carlin (2001)  In Section 5 we explored the use of the posterior median as an estimator leading to an invariant p D . This has two possible disadvantages: we do not have a proof that p D will be positive and some additional computational difficulty in that the full sample needs to be retained. In addition the approximate properties based on Taylor series expansions in Section 3 may not hold, although this may be only of theoretical interest. Currently we recommend calculation of DIC on the basis of several different estimators, with a preference for posterior means based on parameterizations obeying approximate likelihood normality.\n\n\nFocus of analysis\n\nAs we saw in the stack loss example of Section 8.2, there may be sensitivity to apparently innocuous restructuring of the model: this is to be expected since by making such changes we are altering the definition of a replicate data set, and hence one would expect DIC to change. For example, consider a model comprising a mixture of normal distributions. If this assumption was solely to obtain a flexible functional form, then the appropriate likelihood would comprise the mixture. If, however, we were interested in the membership of individual observations, then the likelihoods would be normal and the membership variables would contribute to the complexity of the model. Thus the parameters in the focus of a model should ideally depend on the purpose of the investigation, although in practice it is likely that the focus may be chosen on computational grounds as providing likelihoods that are available in closed form.\n\n\nNuisance parameters\n\nStrictly speaking, nuisance parameters should first be integrated out to leave a likelihood depending solely on parameters in focus. In practice, however, parameters such as variances are likely to be included in the focus and add to the estimated complexity: we would recommend posterior means of log-variances as estimators.  suggested models receiving AIC within 1-2 of the 'best' deserve consideration, and 3-7 have considerably less support: these rules of thumb appear to work reasonably well for DIC. Certainly we would like to ensure that differences are not due to Monte Carlo error: although this is straightforward forD,  have explored the difficulty of assessing the Monte Carlo error on DIC.\n\n\nWhat is an important difference in DIC?\n\n\nAsymptotic consistency\n\nAs with AIC, DIC will not consistently select the true model from a fixed set with increasing sample sizes. We are not greatly concerned about this: we neither believe in a true model nor would expect the list of models being considered to remain static as the sample size increased.\n\n\nConclusion\n\nIn conclusion, our suggestions have a similar 'information theoretic' background to frequentist measures of model complexity and criteria for model comparison but are based on expectations with respect to parameters in place of sampling expectations. DIC can thus be viewed as a Bayesian analogue of AIC, with a similar justification but wider applicability. It is also applicable to any class of model, involves negligible additional analytic work or Monte Carlo sampling and appears to perform reasonably across a range of examples. We feel that p D and DIC deserve further investigation as tools for model assessment and comparison. gramme on neural networks and machine learning that was held at the Isaac Newton Institute for Mathematical Sciences in 1997, and to Andrew Thomas for so quickly implementing our changing ideas into WinBUGS. NGB received partial support from Medical Research Council grant G9803841, and BPC received partial support from National Institute of Allergy and Infectious Diseases grant 1-R01-AI41966.\n\n\nDiscussion on the paper by Spiegelhalter, Best, Carlin and van der Linde\n\nS. P. Brooks .University of Cambridge/ This is a wonderful paper containing a wide array of interesting ideas. It seems to me very much like a first step (and in the right direction) and I am sure that it will be seen as both a focus and a source of inspiration for future developments in this area.\n\nAs the authors point out, their p D and the deviance information criterion (DIC) statistics have already been widely used within the Bayesian literature. Given this history and in the previous absence of a published source for these ideas, it is easy to misunderstand what p D actually does. Certainly, before reading this paper, but having read several others which use the DIC, I thought that the p D -statistic was a clever way of avoiding the problem that Bayesians have when it comes to calculating the number of parameters in any hierarchical model. Essentially the problem is one of deciding which variables in the posterior are model parameters and which are hyperparameters arising from the prior. However, p D does not help us here and that is why we have Section 2.1 explaining that this choice is up to the reader. The authors refer to this as choosing the 'focus' for the analysis. Sadly, in many cases the calculation of p D will be impossible for the focus of primary interest since the deviance will not be available in closed from (this includes random effects and state space models, for example), so this remains an open problem.\n\nWhat p D does do is to tell you, once you have chosen your focus, how many parameters you lose (or even gain?) by being Bayesian. The number of degrees of freedom (or parameters) in a model is clear from the (focused) likelihood. However, by combining the likelihood with the prior we almost always impose additional restrictions on the parameter space, effectively reducing the degrees of freedom of our model. Take the authors' saturated model of Section 8.1, in which parameters \u03b1 1 ; : : : ; \u03b1 56 are given a prior with some unknown mean \u00b5 and fixed variance \u03c3 2 . Clearly, in the limit as \u03c3 2 goes to 0, we essentially remove the 56 individual parameters \u03b1 i and effectively replace them with a single parameter \u00b5. I guess that this is fairly obvious with hindsight as is the case with many great ideas. None-the-less it is a credit to the authors firstly for seeing it and, more importantly, for actually deriving a procedure for dealing with it.\n\nThis prior-induced parameter reduction can be clearly observed in Fig. 5 in which we plot the value of p \u03b8 D against log(\u03c3 2 ) both for a hyperprior \u00b5 \u223c N.0; 1000/ and for \u00b5 = 0 (the authors are unclear about which, if either, they actually use in Section 8.1). We can see that, as \u03c3 2 decreases, the effective number of parameters decreases to either 1 or 0 depending on whether or not \u00b5 itself is a parameter, i.e. which prior is chosen. It is interesting to note the rapid decline in p D for variances between 1 and 0.01, but what is particularly interesting about this plot is that, as \u03c3 2 increases, p D converges to a fixed maximum well below 56, the number of parameters in the likelihood. As an experiment, if we take \u03c3 2 = 10 30 or even the Jeffreys prior for the \u00b5 i , a value for p D exceeding 53.1 is never obtained (modulo Monte Carlo error). This suggests that we automatically lose three parameters just by being Bayesian, even if we are as vague as we could possibly be with our prior. Quoting , page 298, 'every prior specification has some informative posterior or predictive implications : : : . There is no \"objective\" prior that represents ignorance.' Of course, the authors' Table 1 suggests that if we took the median as the basis for the calculation of p D then we might obtain different results; indeed we seem to regain several parameters this way! Unfortunately, analytic investigation of the p D -statistic is essentially limited to the case where we take\u03b8.y/ to be the posterior mean, so we have little idea of the extent and nature of the variability across parameterizations. This choice is likely to have a significant effect on any inference based on the corresponding p D -statistic and further (no doubt simulation-based) investigation along these lines would certainly be very helpful.\n\nAs well as the construction of the p D -statistic, the paper also derives a new criterion for model comparison labelled the DIC. The authors provide a heuristic justification for the DIC, but there are clearly several alternatives. One obvious extension of the usual Akaike information criterion (AIC) statistic to p D\n\n\nFig. 5. Plot of p \u03b8\n\nD for the saturated model of Section 8.1 demonstrating its dependence on the prior variance for the random effects:\n\n, p D -statistic with an N.0, 1000/ hyperprior for \u00b5: -----, corresponding value when we fix \u00b5 D 0I . . . . . . . , number of parameters in the likelihood the Bayesian context is to calculate its posterior expectation, EAIC = D.\u03b8/ + 2p (rather than evaluating it at the posterior mode under a flat prior), or to take the deviance calculated at the posterior mean, i.e. taking D.\u03b8/ + 2p. Of course, as with the DIC, posterior medians, modes etc. could also be taken and similar extensions could be applied to the corrected AIC statistic and the Bayesian information criterion for example. Further, the number of parameters in each of these expressions might be replaced by p D to gain even more potential criteria. Table 4 gives the posterior model probabilities and posterior-averaged information criteria (based on p, rather than p D ), including DIC, for autoregressive models of various orders fitted to the well-known lynx data (Priestley (1981), section 5.5). We note the broad agreement between the DIC, EAIC and EAIC c (as is common in my own experience and, I think, expected by the authors), but that EBIC locates an entirely different model. We note also that the posterior model probabilities correctly identify the fact that two models appear to describe the data well and it is the only criterion to identify correctly the existence of two distinct modes in the posterior.\n\nGiven the number of approximations and assumptions that are required to obtain the DIC it can only really be used as a broad brush technique for discriminating between obviously disparate models, in much the same way as any of the alternative information criteria suggested above might be used. However, in many realistic applications there may be two or more models with sufficiently similar DIC that it is impossible to choose between the two. The only sensible choice in this circumstance is to model-average (see Section 9.1.3). , section 4.2, suggested the use of AIC weights and these are also given in Table 4 together with the corresponding weights for the other criteria. Essentially, these are obtained by subtracting from each AIC the value associated with the 'best' model and then setting\nw k \u221d exp{\u2212\u2206AIC.k/=2}\nwhere \u2206AIC.k/ denotes the transformed AIC-value for model k. These weights are then normalized to sum to 1 over the models under consideration.\n\nNote the distinct differences between the weights and the posterior model probabilities given in Table 4, suggesting that only one or the other can really make any sense. We note here that similar comparisons have been made in the context of other examples. In the context of a log-linear contingency in bold indicate the model minimizing the relevant criterion, whereas those in italics denote alternative plausible models under the rules of thumb discussed in Section 9.2.4. Probabilities \u03c0 or weights w in bold denote the top two models in each case. Here, EAIC c denotes the posterior mean of the corrected EAIC , \u03c0.K = k/ the corresponding posterior model probability under a flat prior across models and the w X k the corresponding Akaike weights (or equivalent). The posterior model probabilities were kindly provided by Ricardo Ehlers. table analysis, King (2001), Table 2.5, found that two models have posterior probability 0.557 and 0.057 but corresponding DIC weights of 0.062 and 0.682 respectively. Similar examples in which the DIC and posterior model probabilities give wildly different results are provided by King and Brooks (2001). Do the authors have any feel for why these two approaches might give such different results? Which would they recommend be used and do they have any suggestions for alternative DIC-based weights for model averaging which might lead to more sensible results? Surely, the only sensible approach is to calculate posterior model probabilities via transdimensional Markov chain Monte Carlo methods. When, then, do the authors suggest that the DIC might be used? What, in practical terms is the question that the DIC is answering as opposed to the posterior model probabilities?\n\nThe incorporation of the DIC-statistic into WinBUGS 1.4 ensures its ultimate success, but I have grave misgivings concerning the blind application of a 'default' DIC-statistic for model determination problems particularly given its heuristic derivation and the series of essentially arbitrary assumptions and approximations on which it is based. The authors 'recommend calculation of DIC on the basis of several different estimators'. The option to choose different parameterizations is not available in the beta version of WinBUGS 1.4; will it be added to later versions? What about options for the all-important choice of focus? What do the authors suggest we do when the same parameterization is not calculable for all models being compared? Could not the choice of parameterization for each model adversely influence the results, particularly for models with large numbers of parameters (where a small percentage change in p D might mean a large absolute change in the corresponding DIC)?\n\nThe paper, like any good discussion paper, leaves various other open questions. For example: why take E \u03b8|y [d \u0398 ] in equation (9) and not the mode or median; how should we decide when to take\u03b8 to be the mean, median, mode etc. as this will surely lead to different comparative results for the DIC; when is p D negative and why; in an entirely practical sense, how does model comparison with the DIC compare with that via posterior model probabilities and why do they differ-can both be 'correct' in any meaningful way? On page 613, the authors write 'p D and DIC deserve further investigation as tools for model assessment and comparison' and I would certainly agree that they do. I have very much enjoyed thinking about some of these ideas over the past few weeks and I am very grateful to the authors for the opportunity and motivation to do so. It therefore gives me great pleasure to propose the vote of thanks. Jim Smith .University of Warwick, Coventry/ I shall not address technical inaccuracies but just present four foundational problems that I have with the model selection in this paper.\n\n(a) Bayesian models are designed to make plausible predictive statements about future observables.\n\nThe predictive implications of all the prior settings on variances in the worked examples in Section 8 are unbelievable. They do not represent carefully elicited expert judgments but the views of a vacuous software user. Early in Section 1 the authors state that they want to identify succinct models 'which appear to describe the information [about wrong \"true\" parameter values (see Section 2.2)?] in the data accurately'. But in a Bayesian analysis a separation between information in the data and in the prior is artificial and inappropriate. For example where do I input extraneous data used as the basis of my prior? When do I stop calling this data (and so include it in D.\u00b7// and instead call it prior information? This forces the authors to use default priors. A Bayesian analysis on behalf of a remote auditing expert (Smith, 1996) might require the selection of a prior that is robust within a class of belief of different experts (e.g. Pericchi and Walley (1991)). Default priors can sometimes be justified for simple models. Even then, models within a selection class need to have compatible parameterizations: see Moreno et al. (1998). However, in examples where 'the number of parameters outnumbers observations'-they claim their approach addresses-default priors are unlikely to exhibit any robustness. In particular, outside the domain of vague location estimation or separating variance estimation (discussed in Section 4), apparently default priors can have strong influence on model implications and hence selection. (b) Suppose that we need to select models whose predictive implications we do not believe. Surely we should try to ensure that prior information in each model corresponds to predictive statements that are comparable. Such issues, not addressed here, are considered by Madigan and Raftery (1991) for simple discrete Bayesian models. But outside linear models with known variances this is a difficult problem. Furthermore it is well known that calibration is a fast function (Cooke, 1991).\n\nIn particular apparently inconsequential deviations from the features of a model 'not in focus' tend to dominate D.\u03b8/ and D.\u03b8/. A trivial example of this occurs when we plan to forecast X 2 having observed an independent identically distributed X 1 = 0:01 which under models M1 and M2 have respective Gaussian distributions N.100; 10000/ and N.0; 0:001/. Then, for most priors, model M1 is strongly preferred although its predictions about X 2 are less 'useful' (Section 2.2). The authors' premise that all the models they entertain are 'wrong' allows these calibration issues to bite theoretically even in the limit, unlike their asymptotically consistent rivals. The authors, however, do no more than to acknowledge the existence of this core difficulty after the example in Section 8.3. (c) Suppose that problems (a) and (b) do not bite. Then the 'vector of parameters of focus' (POF)\n\nwill have a critical influence on any ensuing inference. How in practice do we specify this? The authors state without elaboration that this 'should depend on the purpose of the investigation' (Section 9.2.2). But it appears that in practice the POF is calculated on 'computational grounds', their software capability driving their inference. The high influence of the choice of the POF is illustrated in the example in Section 8.2. Here models 4 and 5 are predictively identical but model 5 has a significantly smaller deviance information criterion DIC than model 4. The authors conclude that 'the extra mixing parameters are worthwhile': why? In what practical sense is this helpful? This example illustrates that the unguided choice of the POF will often be inferentially critical. Incidentally in this example the order of DIC is not (as stated) consistent with the thickness of tails of the sample distribution, the thickest-tailed distribution being model 4. (d) But ignoring all these difficulties there still remains the acknowledged choice of (re)parameterization governing the choice of\u03b8 which initially we shall assume to be the mean. Consider the case when the POF \u03b8 is one dimensional with strictly increasing posterior distribution function F.\u03b8|y/, and G \u00b5 is a distribution function of a random variable with mean \u00b5. Then the reparameterization of \u03b8 to \u03c6 \u00b5 = G \u22121 \u00b5 {F.\u03b8|y/} has E.\u03c6 \u00b5 / = \u00b5. Thus D.\u03b8/ (or D.\u03c6// is arbitrary within the range of D.\u00b7/. Thus, contrary to Section (5.1.4), the choice of parameterization of \u03b8 with non-degenerate posterior will always be critical. But no general selection guidance is given here. In observation (c) of Section 2.6 the authors suggest the use of the posterior median instead of the mean if this can be calculated easily from their output: not a solution when the POF is more than one dimensional. Even familiar transforms of marginal medians to contrasts and means or means and variances to means and coefficients of variation will not exhibit the required sorts of invariance.\n\nThere may be theoretical reasons to use DIC but I do not believe that this paper gives them. So my suggestion to a practitioner would be: if you must use a formal selection criterion do not use DIC. I second the vote of thanks.\n\nThe vote of thanks was passed by acclamation.\n\n\nAki Vehtari .Helsinki University of Technology/\n\nThe authors mention that the deviance information criterion DIC estimates the expected loss, with deviance as the loss function. This connection should be emphasized more. It should be remembered that the estimation of the expected deviance was Akaike's motivation for deriving the very first information criterion AIC . In prediction and decision problems, it is natural to assess the predictive ability of the model by estimating the expected utilities, as the principle of rational decisions is based on maximizing the expected utility (Good, 1952) and the maximization of expected likelihood maximizes the information gained (Bernardo, 1979). It is often useful to use other than likelihood-based utilities. For example, in classification problems it is much more meaningful for the application expert to know the expected classification accuracy than just the expected deviance value (Vehtari, 2001). Given an arbitrary utility function u, it is possible to use Monte Carlo samples to estimate E \u03b8 [\u016b.\u03b8/] and\u016b.E \u03b8 [\u03b8]/, and then to compute an expected utility estimate as\nu DIC =\u016b.E \u03b8 [\u03b8]/ + 2{E \u03b8 [\u016b.\u03b8/] \u2212\u016b.E \u03b8 [\u03b8]/};\nwhich is a generalization of DIC (Vehtari, 2001).\n\nThe authors also mention the known asymptotic relationship of AIC to cross-validation (CV). Equally important is to note that the same asymptotic relationship holds also for NIC (Stone (1977), equation (4.5)). The asymptotic relationship is not surprising, as it is known that CV can also be used to estimate expected utilities with Bayesian justification , chapter 6, Vehtari (2001) and Vehtari and Lampinen (2002a)). Below some main differences between CV and DIC are listed. See Vehtari (2001) and Vehtari and Lampinen (2002b) for full discussion and empirical comparisons. CV can use full predictive distributions. In the CV approach, there are no parameterization problems, as it deals directly with predictive distributions. CV estimates the expected utility directly, but it can also be used to estimate the effective number of parameters if desired. In the CV approach, it is easy to estimate the distributions of the expected utility estimates, which can for example be used to determine automatically whether the difference between two models is 'important'. Importance sampling leave-one-out CV (Gelfand et al., 1992;Gelfand, 1996) is computationally as light as DIC, but it seems to be numerically more unstable. k-fold CV is very stable and reliable, but it requires k times more computation time to use. k-fold CV can also handle finite range dependences in the data. For example, in the six-cities study, the wheezing statuses of a single child at different ages are not independent. DIC, which assumes independence, underestimates the expected deviance. In k-fold CV it is possible to group the dependent data and to handle independent groups and thus to obtain better estimates (Vehtari, 2001;Vehtari and Lampinen, 2002b).\n\n\nMartyn Plummer .International Agency for Research on Cancer, Lyon/\n\nI congratulate the authors on their thought-provoking paper. I would like to offer one constructive suggestion and one criticism.\n\nFirstly, I have a proposal for a modified definition of the effective number of parameters p D . Starting from the Kullback-Leibler information divergence between the predictive distributions at two different values of \u03b8\nI.\u03b8 0 ; \u03b8 1 / = E Yrep|\u03b8 0 log p.Y rep |\u03b8 0 / p.Y rep |\u03b8 1 / ;\nI suggest that p D be defined as the expected value of I.\u03b8 0 ; \u03b8 1 / when \u03b8 0 and \u03b8 1 are independent samples from the posterior distribution of \u03b8. This modified definition yields exactly the same expression for p D in the normal linear model with known variance. In general, it should give a similar estimate of p D when \u03b8 has an asymptotic normal distribution. This version of p D can also be decomposed into influence diagnostics when the likelihood factorizes as in Section 6.3. It has the theoretical advantages of being non-negative and co-ordinate free. A practical advantage is that p D can be estimated via Markov chain Monte Carlo sampling using two parallel chains by taking the sample average of\nlog p.Y 0 rep |\u03b8 0 / p.Y 0 rep |\u03b8 1 /\nwhere the superscript denotes the chain to which each quantity belongs. The Monte Carlo error of this estimate is easily calculated and the difficulties discussed by  can thus be avoided. For exponential family models, I.\u03b8 0 ; \u03b8 1 / can be expressed in closed form and there is no need to simulate replicate observations Y rep . When the scale parameter \u03c6 is known, the expression for p D i simplifies to\np D i = n i w i cov{\u03b8 i ; \u00b5.\u03b8 i /|Y } =\u03c6:\nThis gives a surprising resolution to the problem of whether to use the canonical or mean parameterization to estimate p D .\n\nOn a more negative note, I am not convinced by the heuristic derivation of the deviance information criterion DIC in Section 7.3. I followed this derivation for the linear model of Section 4.1, for which it is not necessary to make any approximations. The term with expectation 0, neglected in the final expression, is p \u2212 p D \u2212 D.\u03b8/. Adding this to DIC gives an expected loss of p + p D which is not useful as a model choice criterion. I am not suggesting that the use of DIC is wrong, but a formal derivation is lacking.\n\n\nMervyn Stone .University College London/\n\nThe paper is rather economical with the 'truth'. The truth of p t .Y/ corresponds fixedly to the conditions of the experimental or observational set-up that ensures independent future replication Y rep or internal independence of y = y = .y 1 ; : : : ; y n / (not excluding an implicit concomitant x). For p t .Y/ \u2248 p.Y |\u03b8 t /; \u03b8 must parameterize a scientifically plausible family of alternative distributions of Y under those conditions and is therefore a necessary 'focus' if the 'good [true] model' idea is to be invoked: think of tossing a bent coin. Changing focus is not an option.\n\nAny connection of p D with cross-validatory assessment would need truth as p t .y/ = p t .y 1 /: : : p t .y n /. If l = log.p/ is an acceptable measure of predictive success, A = \u03a3 i l.y i |\u03b8 \u2212i / is a one-out estimate of\nE p t .Y/ [\u03a3 i l{Y i |\u03b8.y/}].\nMultiplied by \u22122, this connects with equation (33) only when the \u03b8-model is true with Y 1 ; : : : ; Y n independent.\n\nExtending Stone (1977) to the posterior mode for prior p.\u03b8/, with n large, A \u2248 L\u02dc\u03b8.y/ \u2212 \u03a0.y/ where \u03a0.y/ = \u2212tr{L \u03b8 + l .\u03b8/} \u22121 i l \u03b8 .y i /l \u03b8 .y i / T and l.\u03b8/ = log {p.\u03b8/}. If l .\u03b8/ is negative definite, the typically non-negative penalty \u03a0.y/ is smaller for the posterior mode than for the maximum likelihood estimate. For the maximum likelihood estimate, l .\u03b8/ = O gives \u03a0.y/ estimating p \u00c5 , but the general form probably gives Ripley's p \u00c5 . If Section 7.3 could be rigorously developed (the use of E Y does look suspicious!), another connection (via equation (33)) might be that DIC \u2248 \u22122A. But, since Section 7.3 invokes the 'good model' assumption and small |\u03b8 \u2212 \u03b8| for the Taylor series expansion (i.e. large n), such a connection would be as contrived as that of A with the Akaike information criterion: why not stick with the pristine (nowadays calculable) form of A-which does not need large n or truth, and which accommodates estimation of \u03b8 at the independence level of a hierarchical Bayesian model? If sensitivity of the logarithm to negligible probabilities is objectionable, Bayesians should be happy to substitute a subjectively preferable measure of predictive success.\n\n\nChristian P. Robert .Universit\u00e9 Paris Dauphine/ and D. M. Titterington .University of Glasgow/\n\nA question that arises regarding this thought challenging paper was actually raised in the discussion of Aitkin (1991), namely that the data seem to be used twice in the construction of p D . Indeed, y is used the first time to produce the posterior distribution \u03c0.\u03b8|y/ and the associated estimate\u03b8.y/. The (Bayesian) deviance criterion then computes the posterior expectation of the observed likelihood p.y|\u03b8/, log {p.y|\u03b8/} \u03c0.d\u03b8|y/ \u221d log {p.y|\u03b8/} p.y|\u03b8/ \u03c0.d\u03b8/;\n\nand thus uses y again, similarly to Aitkin's posterior Bayes factor p.y|\u03b8/ \u03c0.d\u03b8|y/:\n\nThis repeated use of y would appear to be a potential factor for overfitting.\n\nIt thus seems more pertinent (within the Bayesian paradigm) to follow an integrated approach along the lines of the posterior expected deviance of Section 6.2, E Y |\u03b8 [\u22122 log{p.Y |\u03b8/} + 2 log{f.Y/}]\u03c0.d\u03b8|y/ because this quantity would be strongly related to the posterior expected loss defined by the logarithmic deviance,\nd.\u03b8;\u03b8/ = E Y |\u03b8 [log{p.Y |\u03b8/} \u2212 log{p.Y |\u03b8/}];\nadvocated in Robert (1996) and Dupuis and Robert (2002) as an intrinsic loss adequate for model fitting.\n\nIn fact, the connection between p D , the deviance information criterion and the logarithmic deviance would suggest the use of this loss d.\u03b8;\u03b8/ to compute the estimate plugged in p D as the intrinsic Bayes estimator\n\u03b8 \u03c0 .y/ = arg mi\u00f1 \u03b8 {E \u03b8|y .E Y |\u03b8 [log{p.Y |\u03b8/} \u2212 log{p.Y |\u03b8/}]/} = arg max[E Y |y {p.Y |\u03b8/}]\nwhere the last expectation is computed under the predictive distribution. Not only does this make sense because of the aforementioned connection, but it also provides an estimator that is completely invariant to reparameterization and thus avoids the possibly difficult choice of the parameterization of the problem.\n\n(See Celeux et al. (2000) for an illustration in the set-up of mixtures.)\n\n\nJ. A. Nelder .Imperial College of Science, Technology and Medicine, London/\n\nMy colleague Professor Lee has made some general points connecting the subject of this paper to our work on likelihood-based hierarchical generalized linear models. I want to make one specific point and two general ones.\n\n(a) Professor Dodge has shown that, of the 21 observations in the stack loss data set, only five have not been declared to be outliers by someone! Yet there is a simple model in which no observation appears as an outlier. It is a generalized linear model with gamma distribution, log-link and linear predictor x 2 + log.x 1 /\u00c5 log.x 3 /: This gives the following entries for Table 2 in the paper 98:3 92:1 6:2 104:5 (I am indebted to Dr Best for calculating these). It is clearly better than the existing models used in Table 2. (b) This example illustrates my first general point. I believe that the time has passed when it was enough to assume an identity link for models while allowing the distribution only to change. We should take as our base-line set of models at least the generalized linear model class defined by distribution, link and linear predictor, with choice of scales for the covariates in the last named. (c) My second general point is that there is, for me, not nearly enough model checking in the paper (I am assuming that the use of such techniques is not against the Bayesian rules). For example, if a set of random effects is sufficiently large in number and the model postulates that they are normally distributed, their estimates should be graphed to see whether they look like a sample from such a distribution. If they look, for example, strongly bimodal, then the model must be revised.\n\n\nAnthony Atkinson .London School of Economics and Political Science/\n\nThis is an interesting paper which tackles important problems. In my comments I concentrate on regression models: the points extend to the more complicated models at the centre of the authors' presentation. It is stressed in Section 7.1 that information criteria assume a replication of the observations; in regression this would be with the same X-matrix. But, the simulations of Atkinson (1980) showed that, to predict over a different region, higher values of the penalty coefficient than two in equation (36) are needed. Do the authors know of any analytical results in this area? Information criteria for model selection are based on aggregate statistics. Fig. 4 shows an alternative and more informative breakdown of one criterion into the contributions of individual observations than that given by Weisberg (1981). However, it does not show the effect of the deletion of observations on model choice. Atkinson and Riani (2000) used the forward search to analyse the stack loss data, for which symmetrical error distributions were considered in Section 8.2. Their Fig. 4.28 shows that the square-root transformation is the only one supported by all the data. The forward plot of residuals, Fig. 3.27, is stable, with observations 4 and 21 outlying. This diagnostic technique complements the choice of a model using information criteria calculated over a set of models that is too narrow. An example of model choice potentially confounded by the presence of several outliers is provided by 108 observations on the survival of patients following liver surgery from Neter et al. (1996), pages 334 and 438. There are four explanatory variables. Fig. 6 shows the evolution of the added variable t-tests for the variables during the forward search with log(survival time) as the response: the evidence for the importance of all variables except x 4 increases steadily during the search. Atkinson and Riani (2002) modify the data to produce two different effects. The forward plots of the t-tests in Fig. 7(a) show that now x 1 is non-significant at the end of the search. The plot identifies the group of modified observations which have this effect on the t-test for x 1 . Fig. 7(b) shows the effect of a different contamination, which makes x 4 significant at the end of the search.\n\nThe use of information criteria in the selection of models is a first step, which needs to be complemented by diagnostic tests and plots. These examples show that the forward search is an extremely powerful tool for this purpose. It also requires many fits of the model to subsets of the data. Can it be combined with the appreciable computations of the authors' Markov chain Monte Carlo methods?\n\nA. P. Dawid .University College London/ This paper should have been titled 'Measures of Bayesian model complexity and fit', for it is the models, not the measures, that are Bayesian. Once the ingredients of a problem have been specified, any relevant question has a unique Bayesian answer. Bayesian methodology should focus on specification issues or on ways of calculating or approximating the answer. Nothing else is required.\n\nClassical criteria overfit complex models, necessitating some form of penalization, and this paper lies firmly in that tradition. But with Bayesian techniques  overfitting is not a problem: the marginal likelihood automatically penalizes model complexity without any need for further adjustment. In particular, Bayesian model choice is consistent in the 'good model' case (Dawid, 1992a). In Section 9.2.5 the authors brush aside the failure of their deviance information criterion procedure to share this consistency property; but should we not seek reassurance that a procedure performs well in those simple cases for which its performance can be readily assessed, before trusting it on more complex problems?\n\nI contest the view (Section 9.1.3) that likelihood is relevant only under the good model assumption: from a decision theoretic perspective, we can always regard the 'log-loss' scoring rule S.p; y/ := \u2212 log{p.y/} as a measure of the inadequacy of an assessed density p.\u00b7/ in the light of empirical data y (Dawid, 1986). Moreover, when y is a sequence y n = .y 1 ; : : : ; y n / of not necessarily independent or identically distributed variables, we have\n\u2212 log{p.y n /} = n i=1 \u2212 log{p.y i |y i\u22121 /};\n. 41/ the ith term measuring the performance of the Bayesian probability forecast for y i on the basis of analysis of earlier data only (Cowell et al. (1999), chapters 10 and 11). This representation clearly demonstrates why unadjusted marginal likelihood offers a valid measure of model fit: each 'test' observation y i is always entirely disjoint from the associated 'training' data y i\u22121 . If desired, we can generalize this prequential formulation of marginal likelihood by inserting other loss functions (Dawid, 1992b) or using other model fitting methods (Skouras and Dawid, 1999). Such procedures exhibit a natural consistency property even under model misspecification (Dawid, 1991;Skouras and Dawid, 2000). One place where a Bayesian might want a measure of model complexity is as a substitute for p in the Bayes information criterion approximation to marginal likelihood, e.g. for hierarchical models. But in such cases the definition of the sample size n can be just as problematic as that of the model dimension p. What we need is a better substitute for the whole term p log.n/.\n\n\nAndrew Lawson and Allan Clark .University of Aberdeen/\n\nWe would like to make several comments on this excellent paper.\n\nOur prime concern here is the fact that the deviance information criterion DIC is not designed to provide a sensible measure of model complexity when the parameters in the model take the form of locations in some R-dimensional space. In the spatial context, this could mean the locations of cluster centres or, more generally, the components of a mixture. Clearly the averaging of parameters in these contexts is nonsensical but is a fundamental ingredient of DIC's penalty term D.\u03b8/. Even if an alternative measure of central tendency is used it remains inappropriate to average over configurations where locations in the chosen space are parameters (e.g. cluster detection modelling in spatial epidemiology (McKeague and Loiseaux, 2002;Gangnon and Clayton, 2002). In the case of the Bayes information criterion, however, it might be possible to replace the penalty p ln.n/ by an average number of parameters (in a reversible jump context) such asp ln.n/, where p is the number of parameters and n the sample size. This would at least approximately accommodate the varying dimension but would not require the averaging of parameters (as compared with DIC). This was suggested in Lawson (2000).\n\nThe second point of concern is the relationship of the goodness of fit to convergence of the Markov chain Monte Carlo samplers for which DIC is designed. If posterior marginal distributions are multimodal then the conventional convergence diagnostic will fail (as they will usually find too much variability in individual chains), and also DIC will average over the modes.\n\nWe are also somewhat concerned and puzzled by the results for the Scottish lip cancer data set. In Table 1, excepting the saturated model, the largest penalty terms are for the exchangeable model and not those with either spatial or spatial and exchangeable components. We also note that it is not strictly appropriate to fit a spatial-only model without the exchangeable component.\n\nFinally we note that alternative approaches have recently been proposed (Plummer, 2002).\n\nJos\u00e9 M. Bernardo .Universitat de Val\u00e8ncia/ This interesting paper discusses rather polemic issues and offers some reasonable suggestions. I shall limit my comments to some points which could benefit from further analysis.\n\n(a) The authors point out that their proposal is not invariant under reparameterization and show that differences may be large. The use of the median would make the result invariant in one dimension, but it is not trivial to extend this to many dimensions. An attractive, general invariant estimator is the intrinsic estimator obtained by minimizing the reference posterior expectation of the intrinsic loss \u03b4.\u03b8; \u03b8/ (Bernardo and Suarez, 2002) defined as the minimum logarithmic divergence between p.x|\u03b8/ and p.x|\u03b8/. Under regularity conditions and moderate or large samples, this is well approximated by .E[\u03b8|x] + M[\u03b8|x]/=2, the average between the reference posterior mean and mode. Other invariant estimators may be obtained by minimizing the posterior expectation of \u03b4.\u03b8; \u03b8/ obtained from either a proper subjective prior or an improper prior which, as the reference prior, is obtained from an algorithm which is invariant under reparameterization. (b) The authors use 'essentially flat' or 'weakly informative' priors, i.e. conjugate-like priors with very small parameter values. This is dangerous and is not recommended. There is no reason to believe that those priors are weakly informative on the parameters of interest. Indeed, these limiting proper priors can have hidden undesirable features such as strong biases (cf. the Stein paradox). Moreover, they may approximate a prior function which would result in an improper posterior and using a 'vague' proper prior in that case does not solve the problem; the answer will then typically be extremely sensitive to the hyperparameters chosen for the vague proper prior and, since the Markov chain Monte Carlo algorithm will converge because the posteriors are guaranteed to be proper, one might not notice anything wrong. If full, credible, subjective elicitation is not possible then one should use formal methods to derive an appropriate reference prior. (c) The authors' brief comment (in Section 9.2.4) on the calibration of the deviance information criterion DIC is too short to offer guidance. With Bayes factors, we have a direct interpretation of the numbers obtained. The Bayesian reference criterion (Bernardo, 1999) is defined in terms of natural information units (and may also be described in terms of log-odds). Is there a natural interpretation for DIC? (d) The important particular case of nested models is not discussed in the paper. Would the authors comment on the behaviour on DIC in that case (and hence on their implication on precise hypothesis testing)? For instance, what is DIC's recommendation for the simple canonical problem of testing a value for a normal mean? It seems to me that, like Akaike's information criterion or the Bayesian reference criterion (but not the Bayes information criterion or Bayes factors), DIC would avoid Lindley's paradox. Is this so?\n\nSujit K. Sahu .University of Southampton/ This impressive paper shows how the very complicated business of model complexity can be assessed easily by using Markov chain Monte Carlo methods. My comments mostly concern the foundational aspects of the methods proposed and the interrelationship of the deviance information criterion DIC and other Bayesian model selection criteria. The paper provides a long list of models and the associated p D , the effective number of parameters. In each of these cases p D is interpreted nicely in terms of model quantities. However, there is an unappealing feature of p D that I would like to point out in the discussion below.\n\nConsider the set-up leading to equation (23). Assume further that A 1 = 1; C 1 = 1 and C 2 = \u03c4 2 . Thus the likelihood is N.\u03b8; 1/ and the prior is N.0; \u03c4 2 /. Then equation (23) yields that\np D = 1 1 + 1=n\u03c4 2 :\nAssuming \u03c4 2 to be finite it is seen that p D increases to 1 as n \u2192 \u221e. The unappealing point is that the effective number of parameters is larger for larger sample sizes; conventional intuition suggests otherwise. The number of unknowns (i.e. the effective number of parameters) should decrease as more data are obtained under this very simple static model. In spite of the authors' views on asymptotics or consistency, this point deserves further explanation as it is valid even when small sample sizes are considered.\n\nIn Section 9.1 the relationship between DIC and other well-known Bayesian model selection criteria including the Bayes factor is discussed. Although DIC is not to be viewed as a formal model choice criterion (according to the authors), it is often (and it will be) used to perform model selection; see for example the references cited by the authors. In this regard a more precise statement about the relationship between the Bayes factor and DIC can be made. I illustrate this with the above simple example taken from the paper.\n\nAssume that the observation model is N.\u03b8; 1/ and the prior for \u03b8 is N.0; \u03c4 2 /. Suppose that model 0 specifies that H 0 : \u03b8 = 0 and model 1 says that H 1 : \u03b8 = 0: I assume that both n and \u03c4 2 are finite and thus avoid the problems with interpretation of the Bayes factor and Lindley's paradox. Using the Bayes factor, model 0 will be selected if\nn\u0233 2 < .1 + n\u03c4 2 / log.1 + n\u03c4 2 / n\u03c4 2 :\nIn contrast, DIC selects model 0 if\nn\u0233 2 < .1 + n\u03c4 2 / 2 2 + n\u03c4 2 :\nClearly, if DIC selects model 0 then the Bayes factor will also select model 0. It is also observed that the Bayes factor allows for higher |\u0233|-values without rejecting the simpler model. In effect DIC is seen to have the much discussed poor behaviour of a conventional significance test which criticizes the simpler null hypothesis too much and often rejects it when it should not. Sylvia Richardson .Imperial College School of Medicine, London/ I restrict my comments on this far-reaching paper to the use of the deviance information criterion DIC for choosing within a family of models and the behaviour of p D as a penalization.\n\nMy first remark concerns the spatial example of Section 8. The DIC-values for the 'spatial' and the 'spatial plus exchangeable' models are nearly identical. Thus, the authors resort to external pragmatic considerations for preferring the simpler model, while the more complex one is not penalized. Turning to mixture models and the comparison between models with different numbers of components, I discuss two situations. The first concerns simple Gaussian mixtures with an unknown number of components; y i \u223c \u03a3 k j=1 w j f.\u00b7|\u03b8 j /; i = 1; : : : ; n, where f.\u00b7|\u03b8 j / is Gaussian. To calculate DIC in this setting, let us focus on mixtures as flexible distributions and use the conditional density for a new observation y \u00c5 : g.y \u00c5 / = p.y \u00c5 |y; w; \u03b8; k/ to calculate the deviance D.g/ = \u22122 \u03a3 n i=1 log{g.y i /} and take its expectation over the Markov chain Monte Carlo run, conditional on k. We have p D .k/ = E{D.g/} \u2212 D.\u011d k /, where\u011d k = p.y \u00c5 |y; k/.\n\nTwo cases of Gaussian mixtures were simulated (one replication): a well-separated bimodal mixture (bimod), 0.5 N.\u22121:5; 0:5/ + 0:5 N.1:5; 0:5/, and an overlapping skewed bimodal mixture (skew): 0.75 N(0, 1) + 0.25 N(1.5, 0.33), each with 200 data points.\n\nIn the clear-cut bimod case, DIC(k) is lower for k = 2, with a small incremental increase in both E.D|y; k/ and p D as extra components are being fitted (Table 5). In the more challenging skew case, the pattern of DIC-values shows that this data set requires more than two components to be adequately fitted, but the values of DIC and p D stay surprisingly flat between three and six components. Note that the predictive density plots conditional on k = 3; 4; 5 are completely superimposed (Fig. 8), indicating that more than three components can be considered as overfitting the data, in the sense that they give alternative explanations that are no better but involve increasing numbers of parameters.\n\nThe second situation is that of spatial mixture models proposed in  in the context of disease mapping. DIC was calculated by focusing on area-specific risk. Referring, for example, to the simple north-south (two-component) contrast defined in that paper, we find that DIC stays stable as k increases, decreasing E.D|y; k/ values being compensated by increasing p D . On the basis of a mean-square error criterion between the estimated and the underlying risk surface, a deterioration of the fit would be seen with values of 0.14, 0.15 and 0.16 for k = 2; 3; 4 respectively.\n\nThus p D acts as a sufficient penalization only in the simplest case. In other cases, DIC does not distinguish between alternative fits with increasing number of parameters.\n\nPeter Green .University of Bristol/ I have two rather simple comments on this interesting, important and long-awaited paper.\n\nThe first concerns using basic distribution theory to give a surprising new perspective on p D in the normal case, perhaps identifying a missed opportunity in exposition.\n\nConsider first a decomposition of data as focus plus noise:\nY = X + Z\nwhere X and Z are independent n-vectors, normally distributed with fixed means and variances, and var(Z)\n\nis non-singular. The deviance is \nD.X/ = .Y \u2212 X/ T var.Z/ \u2212p D = tr{.A 1 C 2 A T 1 + C 1 / \u22121 A 1 C 2 A T 1 } = tr{A T 1 C \u22121 1 A 1 .A T 1 C \u22121 1 A 1 + C \u22121 2 / \u22121 };\nas in equation (21) of the paper. Turning now to hierarchical models, consider a decomposition into k independent terms Y = Z 1 + Z 2 + : : : + Z k ;\n\nwhere all Z i are normal, and var.Z k / is non-singular. These represent all the various terms of the model: fixed effects with priors, random effects with different structures, errors at various levels; again all means and variances are fixed. Then for any level l = 1; 2; : : : ; k \u2212 1 we may take the sum of the first l terms as the focus and the rest as noise. Version (42) of p D above is then not very promising:\np D .l/ = tr var k i=l+1 Z i \u22121 var k i=l+1 Z i Y ;\nbut expression (43) gives the more compelling\np D .l/ = tr var.Y/ \u22121 var l i=1 Z i :\n. 44/ Thus p D has generated a decomposition of the overall degrees of freedom n = \u03a3 l tr{var.Y/ \u22121 var.Z l /} into non-negative terms attributable to the levels l = 1; 2; : : : ; k, just as in frequentist nested model analysis of variance. (We must take care with improper priors in using expression (44), and terms should be treated as limits as precisions go to 0.) Of course, expressions (43) and (44) fail to hold with unknown variances or with non-normal models, but the observations above do provide further motivation for accepting p D as a measure of complexity, and suggest exploring more thoroughly its role in hierarchical models. My second point notes that the paper has no examples with discrete 'parameters'. Conditional distributions in hierarchical models with purely categorical variables can be computed by using probability propagation methods (Lauritzen and Spiegelhalter, 1988), avoiding Markov chain Monte Carlo methods, so that p D is again a cheap local computation. Presumably marginal posterior modes would be used for \u03b8. Certainly this is a context where p D can be negative. Can connections be drawn with existing model criticism criteria in probabilistic expert systems?\n\nThe following contributions were received in writing after the meeting. Table 6. Comparison of the three different criteria DIC 1 , DIC 2 and DIC 3 for a simulated sample of 100 observations from 0:5 N (5, 1.5) + 0:5 N (7.5, 8) with a conjugate prior \u03b8 1 N (4, 5) and \u03b8 2 N (8, 5), and of DIC based on the true complete sample (x, z) and DIC for the single-component normal model (with an N (6, 5) prior and a variance set of 6.07)  . 9. Histogram of the simulated data set and true density A third possibility is the full DIC, DIC 3 , based on the completed likelihood (45) when it incorporate\u015b z as an additional parameter, in which case the saturated deviance could be the normal standardized deviance, although we still use f.x/ = 1 for comparison.\n\n\nResults\n\nThe three possibilities above lead to rather different figures, as shown by Table 6 for the simulated data set in Fig. 9; Table 6 exhibits in addition a lack of clear domination of the mixture (k = 2) versus the normal distribution (k = 1) (second column), except when z is set to its true value (third column) or estimated (last column). Note that, for the full DIC, p D is far from 102; this may be because, for some combinations of z, the likelihood is the same. (This also relates to the fact that z is not a parameter in the classical sense.)\n\n\nDavid Draper .University of California, Santa Cruz/\n\nThe authors of this interesting paper talk about Bayesian model assessment, comparison and fit, but-if their work is to be put seriously to practical use-the real point of the paper is Bayesian model choice: we are encouraged to pick the model with the smallest deviance information criterion DIC among the class of 'good' models (those which are 'adequate candidates for explaining the observations'). (It is implicit that somehow this class has been previously specified by means that are not addressed here-would the authors comment on how this set of models is to be identified in general?) However, in the case of model selection it would seem self-evident that to choose a model you have to say to what purpose the model will be put, for how else will you know whether your model is sufficiently good? We can, perhaps, use DIC to say that model 2 is better than model 1, and we can, perhaps, compareD with 'the number of free parameters in \u03b8' to 'check the overall goodness of fit' of model 2, but we cannot use the authors' methods to say whether model 2 is sufficiently good, because the real world definition of this concept has not been incorporated into their methods. It seems hard to escape the fact that specifying the purpose to which a model will be put demands a decision theoretic basis for model choice; thus (Draper, 1999) I am firmly in the camp of .\n\nSee Draper and Fouskakis (2000) and Fouskakis and Draper (2002) for an example from health policy that puts this approach into practice, as follows. Most attempts at variable selection in generalized linear models conduct what might be termed a benefit-only analysis, in which a subset of the available predictors is chosen solely on the basis of predictive accuracy. However, if the purpose of the modelling is to create a scale that will be used-in an environment of constrained costs, which is frequently the case-to make predictions of outcome values for future observations, then the model selection process must seek a subset of predictors which trades off predictive accuracy against data collection cost. We use stochastic optimization methods to maximize the expected utility in a decision theoretic framework in the space of all 2 p possible subsets (for p of the order of 100), and because our predictors vary widely in how much they cost to collect (which will also often be true in practice) we obtain subsets which are sharply different from (and much better than) those identified by benefit-only methods for performing 'optimal' variable selection in regression, including DIC.\n\nAlan E. Gelfand .Duke University, Durham/ and Matilde Trevisani .University of Trieste/ The authors' generally informal approach motivates several remarks which we can only briefly develop here. First, in Section 2.1, we think that better terminology would be 'focused on p.y|\u03b8/' with 'interest in the models for \u03b8', as in, for example, the example in Section 8.1 where there is no \u03b8 in the likelihood for any of the given models. Even the example in Section 8.2, where \u03b8 does not change across models, emphasizes the focus on p.y|\u03b8/ since f.y/ depends on the choice of p. So, here, a relative comparison of the models depends on the choices made for the f s. Without a clear prescription for f (once we leave the exponential family), the opportunity exists to fiddle the support for a model.\n\nThough the functional form of the Bayesian deviance does not depend on p.\u03b8/, DIC and p D will. With the authors' hierarchical specification, p.y; \u03b8; \u03c8/ = p.y|\u03b8/ p.\u03b8|\u03c8/ p.\u03c8/;\n\nthe effective degrees of freedom will depend on p.\u03c8/. But, also, under this specification, rather than p.y|\u03b8/, we can put a different distribution, p.y|\u03c8/, in focus. Again, it seems preferable not to speak in terms of 'parameters in focus'.\n\nMoreover, since p.y|\u03b8/ and p.y|\u03c8/ have the same marginal distribution p.y/, a coherent model choice criterion must provide the same value under either focus. Otherwise, a particular hierarchical specification could be given more or less support according to which distribution we focus on. But let DIC 1 ; p D 1 and f 1 .y/ be associated with p.y|\u03b8/ and DIC 2 ; p D 2 and f 2 .y/ with p.y|\u03c8/. To have DIC 1 = DIC 2 requires, after some algebra, that\nln{f 2 .y/} \u2212 ln{f 1 .y/} = p D 1 \u2212 p D 2 + E[ln{p.y|\u03c8/|y}] \u2212 E[ln{p.y|\u03b8/|y}]:\nJust as the functional form of f 1 .y/ depends only on the form of p.y|\u03b8/, the form for f 2 .y/ should depend only on p.y|\u03c8/. Evidently this is not so. For instance, under the authors' example in expression (2), f 1 .y/ = 0. The above expression yields the non-intuitive choice\nln{f 2 .y/} = w i + 1 2 ln.1 \u2212 w i / \u2212 \u03bb var.\u03c8|y/ w 2 i \u2212 \u03bb 2 w 2 i {y i \u2212 E.\u03c8|y/} 2 where w i = \u03c4 i =.\u03c4 i + \u03bb/.\nThis issue is discussed further in Gelfand and Trevisani (2002).\n\n\nJim Hodges .University of Minnesota, Minneapolis/\n\nThis is a most interesting paper, presenting a method of tremendous generality and, as a bonus, a fine survey of related methods. I can think of a dozen models for which I would like to see p D , but I shall ask for just one: a balanced one-way random-effects model with unknown between-group precision, in which each group has its own unknown error precision, these latter precisions being modelled as draws from, say, a common gamma distribution with unknown parameters. Thus the precisions will be shrunk as well as the means, and presumably the two kinds of shrinkage will affect each other. The focus could be either the means or the precisions, or preferably both at once. One thing is troubling: the possibility of a negative measure of complexity (Section 2.6, comment (d)).  is linked (shackled?) to linear model theory, in which complexity is defined as the dimension of the subspace of n in which the fitted values lie. In our generalization, the fitted values may be restricted to 'using' only part of a basis vector's dimension, because they are stochastically constrained by higher levels of the model's hierarchy. (Basing complexity on fitted values may remove the need to specify a focus, although, if true, this is not obvious.) In this context, zero complexity makes sense: the fitted values lie in a space of dimension 0 specified entirely by a degenerate prior. Negative complexity, however, is uninterpretable in these terms. The authors attribute negative complexity to a poor model fit, which suggests that p D describes something more than the fitted values' complexity per se. Perhaps the authors could comment further on this.\n\nYoungjo Lee .Seoul National University/ It is very interesting to see the Bayesian view of Section 4.2 of , which used extended or h-likelihood and in which we introduced various test statistics. For a lack of fit of the model we proposed using the scaled deviance\nD r = \u22122.log{p.y|\u03b8 t /} \u2212 log[p{y|\u00b5.\u03b8/ = y}]/\nwith degrees of freedom E.D r /, estimated by n \u2212 tr.\u2212L \u03b8 V/ where \u2212L \u03b8 = V \u00c5 as in Sections 4.3 and 5.4 of this paper. We considered a wider class of models, which we called hierarchical generalized linear models (HGLMs) (see also Lee and Nelder (2001a, b)), but some of our proofs hold more widely than this, so that, for example, Section 3.1 of this paper is summarized in our Appendix D, etc. For model complexity the authors define in equation (9)  D r and D m are the scaled deviances for the residual and model respectively, whose degrees of freedom add up to the sample size n. We are very glad that the authors have pointed out the importance of the parameterization of \u03b8 in forming deviances. We extended the canonical parameters of Section 5 to arbitrary links by defining the h-likelihood on a particular scale of the random parameters, namely one in which they occur linearly in the linear predictor. In HGLMs the degrees of freedom for fixed effects are integers whereas those for random effects are fractions. Thus, a GLM has integer degrees of freedom p m = rank.X/ because C \u22121 2 \u03b4 is 0 in Section 5, whereas the estimated degrees of freedom of D m in HGLMs are fractions.  introduced the adjusted profile h-likelihood eliminating \u03b8, and this can be used to test various structures of the dispersion parameters \u03bb discussed in the examples of Section 8: see the model checking plots for the lip cancer data in Lee and Nelder (2001b). Lee and Nelder (2001a) justified the simultaneous elimination of fixed and random nuisance parameters. It will be interesting to have the Bayesian view of the adjusted profile h-likelihood.\n\n\nXavier de Luna .Ume\u00e5 University/\n\nThis interesting paper presents Bayesian measures of model complexity and fit which are useful at different stages of a data analysis. My comments will focus on their use for model selection. In this respect, one of the noticeable contributions of the paper is to propose a Bayesian analogue, the deviance information criterion DIC, to the Akaike information criterion AIC and TIC. Both DIC and TIC are generalizations of AIC. The former may be useful in a Bayesian data analysis, whereas the frequentist criterion TIC has the advantage of not requiring the 'good model' assumption discussed by the authors.\n\nSuch 'information-based' criteria use measures of model complexity (denoted p \u00c5 or p D in the paper). It should, however, be emphasized that models can be compared without having to define and compute their complexity. Instead, out-of-sample validation methods, such as cross-validation (Stone, 1974) or prequential tests (Dawid, 1984) can be used in wide generality. Moreover, to use an estimate of p \u00c5 in a model selection criterion, some characteristics of the data-generating mechanism (DGM)-'true model' in the paper-must be known. For instance, depending on the DGM either AIC-type or Bayes information type criteria are asymptotically optimal (see Shao (1997) for a formal treatment of linear models). Thus, when little is known about the DGM, out-of-sample validation provides a formal and general framework to perform model selection as was presented in de Luna and Skouras (2003), in which accumulated prediction errors (defined with a loss function chosen in accordance with the purpose of the data analysis) were advocated to compare and choose between different model selection strategies. When many models are under scrutiny, out-of-sample validation may be computationally prohibitive and generally yields high variability in the selection of a model. In such cases, different model selection strategies based on p \u00c5 (making-implicitly or explicitly-diverse DGM assumptions) can be applied to reduce the dimension of the selection problem. Accumulated prediction errors can then be used to identify the best strategy while making very few assumptions on the DGM.\n\nXiao-Li Meng .Harvard University, Cambridge, and University of Chicago/ The summary made me smile, for the 'mean of the deviance \u2212 deviance of the mean' theme once injected a small dose of excitement into my student life. I was rather intrigued by the 'cuteness' of expressions (3.4) and (3.8) of , and seeing a Bayesian analogue of our likelihood ratio version certainly brought back fond memories. My excitement back then was short lived as I quickly realized that all I was deriving was just a masked version of a well-known variance formula. Let D.x; \u00b5/ = .x \u2212 \u00b5/ 2 be the deviance, a case of realized discrepancy of Gelman et al. (1996); then\n1 n n i=1 .x i \u2212x/ 2 = D.x i ; \u00b5/ \u2212 D.x; \u00b5/:\n.46/ Although equation (46) is typically mentioned (with \u00b5 set to 0) for computational convenience, it is the back-bone of the theme under quadratic or normal approximations, or more generally with log-concave likelihoods, beyond which assumptions become much harder to justify or derive. (Obviously, equation (46) is applicable for posterior or likelihood averaging by switching x and \u00b5.) Section 1 contained a small puzzle. I wondered why  was omitted from the list of 'the most ambitious attempts', because Ye's 'data derivative' perspective goes far beyond the independent normal model cited in Section 4.2 (for example, it addresses data mining). It also provides a more original and insightful justification than normal approximations, especially considering that Markov chain Monte Carlo sampling is most needed in cases where such approximations are deemed unacceptable.\n\nSection 2.1 presented a bigger puzzle. The authors undoubtedly would agree that a statement like 'In hierarchical modelling we cannot uniquely define a \"posterior\" or \"model complexity\" without specifying the level of the hierarchy that is the focus of the modelling exercise' is tautological. Surely the 'posterior' and thus the corresponding 'model complexity' depend on the level or parameter(s) of interest. So why does the statement become a meaningful motivation when the word posterior is replaced by 'likelihood'? There is even some irony here, because hierarchical models are models where there are unambiguous and uncontroversial marginal likelihoods-both L.\u03b8|y/ = p.y|\u03b8/ and L.\u03c6|y/ = p.y|\u03c6/ in Section 2.1 are likelihoods in the original sense.\n\nAlthough limitations on space prevent me from describing my reactions when reading the rest, I do wish that DIC would stick out in the dazzling AIC-TIC alphabet contest, so we would all be less compelled to look for UIC (unified or useful information criterion?) : : : .\n\nSmith and others ask how the model focus should be chosen in practice. We argue that the focus is operationalized by the prediction problem of interest. For example, if the random effects \u03b8 in a hierarchical model relate to observation units such as schools or hospitals or geographical areas, where we might reasonably want to make future predictions for those same units, then taking p.y|\u03b8/ as the focus is sensible. The prediction problem is then to predict a new Y i;rep conditional on the posterior estimate of \u03b8 i for that unit. However, if the random effects relate to individual people, say, then we are often interested in population-average inference rather than subject-specific inference, so we may want to predict responses for a new or 'typical' individual rather than an individual who is already in the data set. In this case, it is appropriate to integrate over the \u03b8s and to predict Y rep for a new individual conditional on \u03c8, leading to a model focused on p.y|\u03c8/. A crucial insight is that a predictive probability statement such as p.Y rep |y/ is not uniquely defined without specifying the level of the hierarchy that is kept fixed in the prediction-this defines the focus of the model. In summary, we feel that the issue of focus with respect to predictive model assessment and selection is an issue in hierarchical modelling and not specifically Bayesian.\n\nWhen the forms of the likelihoods differ between models being compared, it is clearly vital to be careful that any standardizing terms that are used in the deviance are common. As observed by Smith, a comparison of models with focus at different levels of the hierarchy may not be meaningful as they correspond to different prediction problems.\n\n\nFeatures of p D\n\nSeveral discussants questioned the definition or performance of p D . As to the definition we maintain our claim (in spite of Dawid's comment) that it is in our models that there is a genuine Bayesian interest in quantifying the interaction between Y and \u0398 in probabilistic terms. One can indeed often think of p D in terms of dimensionality as Hodges suggests, but in general we prefer to think of it as a feature of the joint distribution of Y and \u0398. This frees it from the shackles imposed by normal linear model theory. Such a measure of interaction or model complexity may, for example, be used to reparameterize hyperparameters \u03c8 to facilitate an intuitively interpretable specification of model priors on \u03c8 (Holmes and Denison, 1999). Still, as suggested by Brooks, p D may turn out to be only a step towards a (better) definition of model complexity such as that suggested by Plummer: we feel that the quantity that he proposes is intuitively intriguing and that it may be particularly appropriate in exponential families, but we wonder about its general validation and justification.\n\nOur uncertainty about whether to recommend p D as a definition or as an estimate of a quantity still to be defined makes it difficult to judge proposals for an 'improvement'. For example, using an invariant estimator such as that proposed by Robert and Titterington or Bernardo instead of\u03b8 is tempting as part of a definition, but it takes into account only one feature of p D while destroying others such as the trace approximation. Similarly the occurrence of a negative value of p D , typically observed if the model fits poorly, might resemble a negative estimate for a positive parameter. We take a pragmatic point of view and look forward to theoretical progress that provides insight into why p D generally appears to work well. Green provides a valuable insight into the interpretation of p D in the normal case, using an attractive decomposition of the total predictive variance of the observables.\n\nReplying to those discussants who were concerned about observing p D < n under 'flat' priors, we reemphasize that p D = n was obtained theoretically only in the normal case or under normal approximations. There is no proof that p D = n for general distributions. In the case of Brooks's illustration using the Scottish lip cancer data, in which he shows that p D appears to 'lose' two or three (modulo Monte Carlo error) parameters under such priors, we point out that two of the 56 observations in this data set are 0 with small expected values and so contribute negligibly to the Poisson deviance. We have replicated his analysis replacing these two observations by non-zero counts, and we found that p D increases by about 2 to around 55.5.\n\nWe certainly do not recommend the unthinking use of default priors, a concern of Smith and Bernardo: on the contrary, one of our main aims is to demonstrate how an informative prior reduces model complexity. Typically a large number of parameters p relative to a small sample size n is compensated by using an informative prior, and the deviance information criterion DIC and p D adjust accordingly without any need for additional adjustment for small sample size (see Burnham, and Lawson and Clark's comment on the example in Section 8.1).\n\nThere is evidence (Daniels andKass, 1999, 2001) that, in the absence of missing data, the use of default priors for variance components typically has little effect on the posteriors for the main effects in a model. Still, Smith and Bernardo observe that the flat priors that may maximize p D are not necessarily weakly informative, and we agree. Reference priors that are least informative in an information theoretical sense can be easily studied in some of our examples. For example, Fig. 1 displays the performance of the beta. 1 2 ; 1 2 / reference prior (corresponding to a prior sample size of n i = a + b = 1) for the binomial likelihood, and the approximation (31) indicates that p \u0398 D i based on the reference prior is greater than p \u0398 D i based on the uniform beta(1, 1) prior (which has prior sample size n i = 2). Similarly for a Poisson likelihood the reference prior \u03c0.\u00b5 i / \u221d \u221a \u00b5 i yields a \u0393.y i + 1 2 ; n i / posterior distribution corresponding to a = 1 2 ; b \u2192 0. Hence p \u00b5 D i \u2248 y i =.y i + 1 2 / and p \u0398 D i \u2248 n i =n i = 1 might be compared with the values shown in Fig. 2.\n\n\nProperties of DIC\n\nAnother main part of the discussion focused on the properties and performance of DIC. Plummer doubted the usefulness of the expected loss that DIC approximates, but he has included a standardizing constant in the loss function which should not be present (we have made this clearer in the paper). The expected loss in the (independent) normal linear case is then p + p D + n log.2\u03c0\u03c3 2 /: this says that when comparing 'good' models with the same \u03c3 2 s the expected loss is minimized with a degenerate prior in which no parameters are estimated. This seems entirely reasonable, as all the models have equivalent fit, and so distinction is based on complexity alone. Of course in practice either \u03c3 2 will be estimated or \u03c3 2 will vary between models, and hence the appropriate trade-off between fit and complexity will naturally arise. A practical aspect, related to the need for 'good' models in the derivation of DIC, is that the term L 2 ignored by DIC will tend to be negative with poorly fitting models and hence to inflate DIC: the approximation of DIC to expected loss will thus tend automatically to penalize models that are not 'good'. Though we agree with Brooks that owing to its heuristic derivation DIC may be considered as a 'broad brush technique', we do not regard it to be as arbitrary as the alternatives that he suggests. In particular we do not feel that terms of 'fit' and 'complexity' can be arbitrarily combined, but we re-emphasize that a measure of model complexity results from correcting overfit due to an approximation of the expected loss that 'uses the observations twice'. Similarly we would like to see a justification of Vehtari's estimates of expected utilities as valid approximations generalizing DIC.\n\nBernardo asks for the application of DIC to nested models and hypothesis testing, in particular the occurrence of Lindley's paradox. This is an interesting question partially answered by the example discussed in Section 8.1 where some of the competing models are nested. The key point is that DIC is designed to take into account priors that are concentrated on parameters which are specified in a model, thus effectively assigning prior probability 0 to hypothetically omitted parameters (if there are remaining parameters). Let us consider Lindley's paradox in the following version: when comparing using the Bayes factorX \u223c N.\u00b5 0 ; \u03c3 2 =n/ withX \u223c N.\u00b5; \u03c3 2 =n/ where \u00b5 \u223c N.\u00b5 1 ; \u03c4 2 /, evidence in favour of H 0 : \u00b5 = \u00b5 0 becomes overwhelming as \u03c4 2 \u2192 \u221e even ifx would cause the rejection of H 0 at any arbitrary significance level. If \u03c3 2 is known \u00b5 is the only parameter in the model. To apply DIC we compare the model X \u223c N.\u00b5; \u03c3 2 =n/ with prior \u00b5 \u223c N.\u00b5 0 ; \u03c4 2 /; \u03c4 2 \u2192 0, corresponding to H 0 with the model with the same likelihood but prior \u00b5 \u223c N.\u00b5 1 ; \u03c4 2 /; \u03c4 2 \u2192 \u221e. Then D.\u00b5/ = n.x \u2212 \u00b5/ 2 =\u03c3 2 , D.\u00b5/ = .n=\u03c3 2 /{D.\u03bc/ + var.\u00b5|x/} and p D = n=\u03c3 2 var.\u00b5|x/. For \u03c4 2 \u2192 0, p D \u2192 0;\u03bc \u2192 \u00b5 0 and DIC \u2192 D.\u00b5 0 /. Similarly, for \u03c4 2 \u2192 \u221e, p D \u2192 1;\u03bc \u2192x and DIC \u2192 D.x/ + 2 = 2. Hence the model with the flat prior-the 'alternative hypothesis'-is favoured if D.\u00b5 0 / > 2 or | \u221a n.x \u2212 \u00b5 0 /=\u03c3| > 1:414 which corresponds to a rejection of H 0 at a significance level \u03b1 \u2248 0:16-exactly the behaviour of the Akaike information criterion. Thus Lindley's paradox is not observed. Similarly Sahu contrasts the prior concentrated on \u00b5 0 = 0 with an informative prior N.0; \u03c4 2 / which is centered at \u00b5 0 , also. Thus it is reasonable to reject H 0 using DIC if the data are suitably compatible with the 'alternative' prior. However, we do not accept an assessment of DIC that uses Bayes factors as a 'gold standard', since they are dealing with different prediction problems (see below). Several discussants (Brooks, Bernardo, Burnham and Smith) were concerned with the lack of calibration of DIC. However, unlike the Bayesian reference criterion (Bernardo, 1999), which is based on a Kullback-Leibler distance and therefore a relative measure, DIC is an approximation to an absolute expected loss, and we cannot calibrate it (externally). Correspondingly, 'coherence' of model choice cannot be required in terms of equal DIC-values as Gelfand and Trevisani or Smith claim but can only be discussed in terms of model ranking by DIC. Note, by the way, that Plummer's alternative measure of model complexity, as well as our p D , are defined relatively, indicating that these measures might be calibrated.\n\nFinally, we certainly do not claim that applying DIC is an exhaustive tool for model assessment. Although we feel that our Fig. 4 is a step in the right direction, additional techniques such as those discussed by Nelder and Atkinson are certainly needed for refined analyses.\n\n\nApplications\n\nThere were various comments on the interpretation of p D in the Scottish lip cancer analysis (Lawson and Clark, and Richardson) and in mixture models (Richardson, and DeIorio and Robert). Here we tend to think of p D as the estimable dimension of the parameter space or, alternatively, as the size of the parameter space that is identifiable by the data. We repeat that the spatial model 3 in the lip cancer example (Section 8.1) provides stronger prior information than the exchangeable model 2 leading to a smaller p D . Only the sum of the spatial and exchangeable random effects is uniquely identifiable in model 4 and so p D remains virtually unchanged compared with the spatial-only model 3, thus justifying the lack of an additional 'penalty' for the apparently more complex model. The same is true for mixture models, where increasing the number of components does not necessarily increase the identifiable parameter space. We do appreciate the discussion of DIC in mixture models introduced by DeIorio and Robert, and by Richardson (though Richardson does not appear to have calculated DIC as we have defined it, but a different criterion based on predictive deviances). DeIorio and Robert's example nicely illustrates a range of possibilities for defining DIC in this case, although we re-emphasize that a comparison of models with different focus (e.g. their DIC 2 versus DIC 3 ) may not be meaningful, and we further note that their integrated DIC (DIC 1 ) does not correspond to our definition of DIC.\n\nIn response to Lawson and Clark's query about averaging 'location' parameters, we point to Green's comment concerning the calculation of p D and DIC for models with discrete parameters, and his suggestion that marginal posterior modes could be used for\u03b8 in this case.\n\nWe thank Nelder and Atkinson for their refinements to the analysis of the stack loss data (Section 8.2). We disagree with Smith that our models 4 and 5 for these data are predictively identical since, as already discussed, the prediction problem addressed by model 4 integrates over the random effects and corresponds to predicting stack loss for a new chimney, whereas model 5 conditions on the random effects and corresponds to predicting future stack loss for the 21 chimneys in the data set.\n\nAlternatives to DIC Several discussants (Brooks, Dawid and Sahu) feel that DIC suffers in comparison with more traditional Bayesian model selection criteria based on posterior model probabilities and Bayes factors. Here we can only repeat that our deliberate intention was to offer an alternative to Bayes factors, which are most suitable when the entire collection of candidate models can be specified ahead of time (the 'M closed' case of ). In our practical experience, the model-building, criticism and rebuilding process is typically an iterative 'M open' one in which the ultimate model collection is rarely known ahead of time, and here DIC may emerge as more appropriate. Moreover, Bayes factors address how well the prior has predicted the observed data; this prior predictive emphasis ultimately leads to the Lindley paradox. DIC instead addresses how well the posterior might predict future data generated by the same mechanism that gave rise to the observed data; this posterior predictive outlook might be considered intuitively more appealing in many practical contexts. We emphasize that these techniques are intended to answer different questions and cannot be expected to give the same conclusions: in any case, posterior model probabilities may be highly dependent on within-and betweenmodel priors, so their comparison with DIC is not straightforward. On a related point, several discussants (Brooks, Burnham and Draper) mention the possible alternative of model averaging. We do not, however, see any justification for transforming DIC-values to relative probabilities, and in any case the prior on the model space may be difficult to develop, and might even reasonably be related to model complexity! Dawid wishes for a better definition of p log.n/ (instead of just p) for use in the Bayesian information criterion (BIC) but previous work has shown that many such definitions are justifiable asymptotically (e.g. Volinsky and Raftery (2000)), so this line of research does not appear promising. Regarding the suggestion by Lawson and Clark of usingp log.n/ as a penalty for the BIC, this of course assumes that the number of parameters p is a suitable measure of model complexity. But most spatial models of the type that they refer to will involve random effects, where such use of the raw parameter count p would be inappropriate; indeed, this is precisely the situation that p D was designed to address.\n\nVehtari and de Luna argue persuasively on behalf of cross-validation as an alternative to our posterior predictive approach that avoids a definition of complexity. Whereas no knowledge of the datagenerating mechanism is required for cross-validation, the data-generating mechanism is necessary in a fully Bayesian analysis. Still, cross-validation as an alternative estimation method was also used to estimate model complexity by . We certainly acknowledge the potential of this approach, particularly in comparisons of different model selection strategies. We agree with Stone concerning further investigation of model assessment procedures in which the model is not assumed to be correct, and we refer to Konishi and Kitagawa (1996) (whose GIC adds yet further to the alphabet).\n\nIn conclusion, it is clear that several of the discussants feel that our pragmatic aims are muddying otherwise pure Bayesian waters. We feel, however, that the huge increase in the use of Bayesian methods in complex practical problems means that full elicitation of informative priors and utilities is simply not feasible in most situations, and that reasonably simple and robust methods for prior specification, model criticism and model comparison are necessary. We hope that we have made a positive contribution to the final concern.\n\nFig. 1 .\n1Binomial likelihood-contribution of the i th group to the effective number of parameters under various parameterizations (canonical p \u0398 D i , mean p \u00b5 D i and median p med D i ) as a function of the data (sample size n i and observed proportion y i =n i ) and prior (effective prior sample size a + b and prior mean a=(a + b)):\n\n\nD.\u00b5 i / = D.\u03b8 i / = \u22122y i {\u03c8.a + y i / \u2212 log.b + n i /} + 2n i a + y i b + n i ; D.\u03bc i / = \u22122y i {log.a + y i / \u2212 log.b + n i /} + 2n i a + y i b + n i ; D.\u03b8 i / = \u22122y i {\u03c8.a + y i / \u2212 log.b + n i /} + 2n i exp{\u03c8.a + y i\n\nFig. 2 .\n2Poisson likelihood-contribution of the i th group to the effective number of parameters under various parameterizations (canonical p \u0398 D i , mean p \u00b5 D i and median p med D i ) as a function of the data (sample size n i and observed total y i ) and prior (mean n i a=b and 'sample size' b)\n\nFig. 3 .\n3, Fig. 3 shows a kernel density smoothed plot of the resulting posterior distributions of the deviance under each competing model. Apart from revealing the obvious unacceptability of model 1, this clearly illustrates the difficulty of formally comparing posterior deviances on the basis of such plots alone. Posterior distributions of the deviance for each model considered in the lip cancer example: , model 1; . . . . . . . , model 2; -------, model 3; ---, model 4; --, model 5\n\nFig. 4 .\n4Diagnostics for the lip cancer example-residuals versus leverages (the parabolas indicate contributions of 1, 2 or 5 to the total DIC (apart from model 1): (a) model 1; (b) model 2; (c) model 3; (d) model 4; (e) model 5\n\nFig. 6 .\n6Transformed surgical unit data: forward plot of the four added variable t -statistics: three variables are needed in the model-x 4 is not significant\n\nFig. 7 .\n7Modified transformed surgical unit data: (a) outliers render x 1 non-significant; (b) now the outliers make x 4 significant (both (a) and (b) show forward plots of added variable t -statistics)\n\n\n= E[D.X/|Y ] \u2212 D.E[X|Y ]/ = tr{var.Z/ \u22121 var.Z|Y/}; . 42/ using the standard expression for the expectation of a quadratic form. Several results in the paper have this form, possibly in disguise. However, var.Z|Y/ = var.Z/ \u2212 cov.Z; Y/ var.Y/ \u22121 cov.Y ; Z/ = var.Z/ \u2212 var.Z/ var.Y/ \u22121 var.Z/ = var.Z/ var.Y/ \u22121 {var.Y/ \u2212 var.Z/}; yielding the much more easily interpretable p D = tr{var.Y/ \u22121 var.X/}: . 43/ This allows a very clean derivation of examples in Sections 2.5 and 4.1-4.3. For example, in the Lindley and Smith model we have var.Z/ = C 1 and var.X/ = A 1 C 2 A T 1 , and so\n\n\nthe scaled deviance D m = \u22122[log{p.y|\u03b8/} \u2212 log{p.y|\u03b8 t /}]:\n\n\nThen d \u0398 {y; \u03b8;\u03b8.y/} can be estimated by its posterior expectation with respect to p.\u03b8|y/, denoted p D {y; \u0398;\u03b8.y/} = E \u03b8|y [d \u0398 {y; \u03b8;\u03b8.y/}] = E \u03b8|y [\u22122 log{p.y|\u03b8/}] + 2 log[p{y|\u03b8.y/}]:\n\nTable 1 .\n1Deviance summaries for the lip cancer data using three alternative parameterizations (mean, canonical and median) for the plug-in deviance \u2020 \u2020Exchangeable means an exchangeable random effect; spatial is a spatially correlated random effect.ModelD \nD(\u03bc) p \n\n\u00b5 \nD \n\nDIC \u00b5 \nD(\u03b8) p \u03b8 \n\nD \n\nDIC \u03b8 D(med) p med \n\nD \n\nDIC med \n\n1, pooled \n381.7 380.7 \n1.0 382.7 380.7 \n1.0 382.7 \n380.7 \n1.0 \n382.7 \n2, exchangeable \n61.1 \n18.2 42.9 104.0 \n17.7 43.4 104.5 \n17.6 \n43.5 \n104.6 \n3, spatial \n58.3 \n26.6 31.7 \n89.9 \n27.1 31.2 \n89.5 \n27.2 \n31.1 \n89.3 \n4, exchangeable + spatial \n57.9 \n26.1 31.8 \n89.7 \n26.5 31.4 \n89.3 \n26.6 \n31.3 \n89.2 \n5, saturated \n55.9 \n0.0 55.9 111.7 \n3.1 52.8 108.6 \n1.4 \n54.5 \n110.4 \n\n \n\nTable 2 .\n2Deviance results for the stack loss dataModelD \nD(\u03b8) p D \nDIC \n\n1, normal \n110.1 105.0 5.1 115.2 \n2, double exponential 107.9 102.3 5.6 113.5 \n3, logistic \n109.5 104.2 5.3 114.8 \n4, t 4 \n108.7 103.2 5.5 114.2 \n5, t 4 as scale mixture \n102.1 \n94.5 7.6 109.7 \n\n\n\nTable 3 .\n3Results for both parameterizations of the Bernoulli panel dataModelD \nResults for the canonical \nResults for the mean \nparameterization \nparameterization \n\nD(\u03b8) \np D \nDIC \nD(\u03b8) \np D \nDIC \n\n1, logit \n1166.4 917.7 248.7 1415.1 \n997.5 168.9 1335.3 \n2, probit \n1148.6 885.9 262.7 1411.3 \n989.9 158.7 1307.3 \n3, complementary log-log 1180.9 956.5 224.4 1405.3 1013.7 167.2 1348.1 \n\n\n\nTable 4 .\n4Effective number of parameters, values of DIC and the posterior expectation of various information criteria for fitting an autoregressive model of order k (with k C 1 parameters including the error variance) to the lynx data \u2020k \np D \nDIC \nEAIC \nEBIC \nEAIC c \n\u03c0(K = k) \nw DIC \n\nk \n\nw EAIC \n\nk \n\nw EBIC \n\nk \n\nw EAIC c \n\n1 \n1.88 \n206.66 \n206.78 \n209.51 \n206.81 \n0.000 \n0.000 \n0.000 \n0.000 \n0.000 \n2 \n2.85 \n126.58 \n127.72 \n133.19 \n127.83 \n0.243 \n0.000 \n0.003 \n0.858 \n0.011 \n3 \n3.78 \n127.06 \n129.27 \n137.48 \n129.50 \n0.016 \n0.000 \n0.001 \n0.101 \n0.005 \n4 \n4.76 \n125.52 \n128.75 \n139.70 \n129.12 \n0.007 \n0.000 \n0.002 \n0.033 \n0.006 \n5 \n5.70 \n125.23 \n129.52 \n143.20 \n130.08 \n0.002 \n0.000 \n0.001 \n0.006 \n0.004 \n6 \n6.62 \n126.30 \n131.68 \n148.09 \n132.46 \n0.001 \n0.000 \n0.004 \n0.000 \n0.001 \n7 \n7.60 \n122.34 \n128.72 \n147.88 \n129.78 \n0.002 \n0.000 \n0.002 \n0.001 \n0.004 \n8 \n8.61 \n121.81 \n129.19 \n151.08 \n130.56 \n0.002 \n0.000 \n0.001 \n0.000 \n0.003 \n9 \n9.58 \n122.75 \n131.16 \n155.79 \n132.89 \n0.001 \n0.000 \n0.001 \n0.000 \n0.001 \n10 \n10.54 \n118.94 \n128.40 \n155.76 \n130.53 \n0.002 \n0.001 \n0.002 \n0.000 \n0.003 \n11 \n11.33 \n106.51 \n117.16 \n147.26 \n119.75 \n0.154 \n0.431 \n0.566 \n0.001 \n0.624 \n12 \n12.61 \n106.89 \n118.27 \n151.10 \n121.36 \n0.268 \n0.356 \n0.325 \n0.000 \n0.280 \n13 \n13.56 \n108.74 \n121.17 \n156.74 \n124.81 \n0.135 \n0.142 \n0.076 \n0.000 \n0.050 \n14 \n14.46 \n110.77 \n124.30 \n162.61 \n128.54 \n0.067 \n0.051 \n0.016 \n0.000 \n0.008 \n15 \n15.37 \n112.896 \n127.42 \n168.47 \n132.32 \n0.000 \n0.019 \n0.003 \n0.000 \n0.001 \n\n \u2020Criterion entries \n\nTable 5 .\n5Performance of DIC for mixture models with different numbers of componentsResults for the following values of k: \n\nk = 2 \nk = 3 k = 4 \nk = 5 k = 6 \n\nBimod (n = 200) \nDIC(k) \n566.7 567.7 568.5 569.2 570.0 \nE.D|y; k/ \n563.4 563.7 564.1 564.5 565.0 \np D \n3.3 \n4 \n4.4 \n4.7 \n5 \n\nSkew (n = 200) \nDIC(k) \n545.5 535.9 535.5 535.7 535.8 \nE.D|y; k/ \n540.3 530.1 530.0 530.2 530.4 \np D \n5.2 \n5.8 \n5.5 \n5.5 \n5.4 \n\nNorth-south (n = 94) \nDIC(k) \n110.5 110.9 110.9 110.5 110.8 \nE.D|y; k/ \n94.2 \n91.9 \n89.6 \n87.7 \n86.2 \np D \n16.3 \n19.0 \n21.3 \n22.8 \n24.6 \n\n\n\nfor the following models:Normal \nComplete, \nIntegrated, \nObserved, \nFull, \n(k = 1) \n[DIC | z] \nDIC 1 \nDIC 2 \nDIC 3 \n\nDIC \n465.1 \n413.5 \n462.6 \n457.6 \n447.4 \n\u2206DIC \n-\n\u221251.6 \n\u22122.5 \n\u22127.5 \n\u221217.6 \np D \n0.99 \n1.96 \n2.27 \n1.98 \n28.06 \n\nFig\nAcknowledgementsWe are very grateful for the generous discussion and criticism of the participants in the pro-The authors replied later, in writing, as follows.We thank all the contributors for their wide-ranging and provocative discussion. Our reply is organized according to a number of recurring themes, but constraints on space mean that it is impossible to address all the points raised. Echoing Brooks's opening remarks, our hope is that discussants and readers will be sufficiently inspired to pursue the ideas proposed in this paper and to address some of the unresolved issues highlighted in the discussion.Model focus and definition of devianceOur notion of the 'focus' of a model and its relationship to the prediction problem of interest provoked some controversy. The crucial role of the model focus is to define the (parameterization of the) likelihood, and we appreciate Gelfand and Trevisani's suggestion of the term 'focus on p.y|\u03b8/', with interest in the structure of \u03b8, rather than models 'focused on \u03b8'. In all our examples the likelihood has been taken to be p.y|\u03b8/ (using the notation of Section 2.1) leading to models with a closed form likelihood but an unknown number of effective parameters that we propose to estimate by p D . However, as Brooks points out, if the focus is on p.y|\u03c8/ (i.e. integrating over the random effects \u03b8), then in general the likelihood will no longer be available in closed form, and other methods must be sought to evaluate p.y|\u03c8/: in this circumstance the number of parameters will be the dimension of \u03c8 or less, depending on the strength of the prior information on \u03c8.Kenneth P. Burnham .US Geological Survey and Colorado State University, Fort Collins/This paper is an impressive contribution to the literature and I congratulate the authors on their achievements therein. My comments focus on the model selection aspect of the deviance information criterion DIC. My perspectives on model selection are given inBurnham and Anderson (2002), which has a focus on the Akaike information criterion AIC as derived from Kullback-Leibler information theory. A lesson that we learned was that, if the sample size n is small or the number of estimated parameters p is large relative to n, a modified AIC should be used, such as AIC c = AIC + 2p.p + 1/=.n \u2212 p \u2212 1/. I wonder whether DIC needs such a modification or if it really automatically adjusts for a small sample size or large p, relative to n. This would be a useful issue for the authors to explore in detail.At a deeper level I maintain that model selection should be multimodel inference rather than just inference based on a single best model. Thus, model selection to me has become the computation of a set of model weights (probabilities in a Bayesian approach), based on the data and the set of models, that sum to 1. Given these weights and the fitted models (or posterior distributions), model selection uncertainty can be assessed and model-averaged inferences made. The authors clearly have this issue in mind as demonstrated by the last sentence of Section 9.1.3. I urge them to pursue this much more general implementation of model selection and to seek a theoretical or empirical basis for it with DIC.There is a matter that I am confused about. The authors say ': : : we essentially reduce all models to non-hierarchical structures' (third page), and 'Strictly speaking, nuisance parameters should first be integrated out : : : ' (Section 9.2.3). Does this mean that we cannot make full inferences about models with random effects? Can DIC be applied to random-effects models? It seems so on the basis of their lip cancer example (Section 8.1). Can I have a model with fixed effects \u03c4 , random effects \u03c6 1 ; : : : ; \u03c6 k , with postulated distribution g.\u03c6|\u03b8/; \u03b8 as fixed effects (plus priors on all fixed effects) and have my focus be all of \u03c4 ; \u03c6 and \u03b8? Thus, I obtain shrinkage-type inferences about the \u03c6 i ; I do not integrate out the \u03c6 (AIC has been adapted to this usage).The authors make a point (page 612) that I wish to make more strongly. It will usually not be appropriate to 'choose' a single model. Unfortunately, standard statistical model selection has been to select a single model and to ignore any selection uncertainty in the subsequent inferences. which is quite interesting, as it illustrates the versatility of the deviance information criterion DIC under different representations of the same model.In this set-up, if the p j s are known, the associated completed likelihood isL{\u03b8|.x 1 ; z 1 /; : : : ; .x n ; z n /} \u221d n i=1f.x i |\u03b8 z i / = k j=1 i:z i =j f.x i |\u03b8 j /:.45/ Therefore, conditional on the latent variables z = .z 1 ; : : : ; z n /, and setting the saturated deviance f. where Pr.z|x/ can be approximated(Casella et al., 1999). A second possibility is the observed DIC, DIC 2 , based on the observed likelihood, which does not use the latent variables z. (We note the strong dependence of DIC on the choice of the saturated function f and the corresponding lack of clear guidance outside exponential families. For instance, if f.x i / goes from the marginal density to the extreme alternative where both \u03b8 1 and \u03b8 2 are set equal to x i , DIC 2 goes from \u221231.71 to 166.6 in the following example.)\nInformation theory and an extension of the maximum likelihood principle. H Akaike, Proc. 2nd Int. Symp. Information Theory. B. N. Petrov and F. Cs\u00e1ki2nd Int. Symp. Information TheoryBudapestAkad\u00e9miai Kiad\u00f3Akaike, H. (1973) Information theory and an extension of the maximum likelihood principle. In Proc. 2nd Int. Symp. Information Theory (eds B. N. Petrov and F. Cs\u00e1ki), pp. 267-281. Budapest: Akad\u00e9miai Kiad\u00f3.\n\nScale mixtures of normal distributions. D F Andrews, C L Mallows, J. R. Statist. Soc. B. 36Andrews, D. F. and Mallows, C. L. (1974) Scale mixtures of normal distributions. J. R. Statist. Soc. B, 36, 99-102.\n\nLimiting behaviour of posterior distributions when the model is incorrect. R H Berk, Ann. Math. Statist. 37Berk, R. H. (1966) Limiting behaviour of posterior distributions when the model is incorrect. Ann. Math. Statist., 37, 51-58.\n\nExpected information as expected utility. J M Bernardo, Ann. Statist. 7Bernardo, J. M. (1979) Expected information as expected utility. Ann. Statist., 7, 686-690.\n\n. J M Bernardo, A F M Smith, Chichester: WileyBernardo, J. M. and Smith, A. F. M. (1994) Bayesian Theory. Chichester: Wiley.\n\nSpatial interaction and the statistical analysis of lattice systems (with discussion). J Besag, J. R. Statist. Soc. B. 36Besag, J. (1974) Spatial interaction and the statistical analysis of lattice systems (with discussion). J. R. Statist. Soc. B, 36, 192-236.\n\nBayesian varying-coefficient models using adaptive regression splines. C Biller, L Fahrmeir, Statist. Modlng. 1Biller, C. and Fahrmeir, L. (2001) Bayesian varying-coefficient models using adaptive regression splines. Statist. Modlng, 1, 195-211.\n\n. G E P Box, Science and statistics. J. Am. Statist. Ass. 71Box, G. E. P. (1976) Science and statistics. J. Am. Statist. Ass., 71, 791-799.\n\nApproximate inference in generalized linear mixed models. N E Breslow, D G Clayton, J. Am. Statist. Ass. 88Breslow, N. E. and Clayton, D. G. (1993) Approximate inference in generalized linear mixed models. J. Am. Statist. Ass., 88, 9-25.\n\nStatistical Theory and Methodology in Science and Engineering. K A Brownlee, WileyNew YorkBrownlee, K. A. (1965) Statistical Theory and Methodology in Science and Engineering. New York: Wiley.\n\nAsymptotic behaviour of Bayes estimates under possibly incorrect models. O Bunke, X Milhaud, Ann. Statist. 26Bunke, O. and Milhaud, X. (1998) Asymptotic behaviour of Bayes estimates under possibly incorrect models. Ann. Statist., 26, 617-644.\n\nModel Selection and Inference. K P Burnham, D R Anderson, SpringerNew YorkBurnham, K. P. and Anderson, D. R. (1998) Model Selection and Inference. New York: Springer.\n\nBayes and Empirical Bayes Methods for Data Analysis. B P Carlin, T A Louis, Chapman and Hall-CRC PressBoca Raton2nd ednCarlin, B. P. and Louis, T. A. (2000) Bayes and Empirical Bayes Methods for Data Analysis, 2nd edn. Boca Raton: Chapman and Hall-CRC Press.\n\nAnalysis of multivariate probit models. S Chib, E Greenberg, Biometrika. 85Chib, S. and Greenberg, E. (1998) Analysis of multivariate probit models. Biometrika, 85, 347-361.\n\nEmpirical Bayes estimates of age-standardised relative risks for use in disease mapping. D G Clayton, J Kaldor, Biometrics. 43Clayton, D. G. and Kaldor, J. (1987) Empirical Bayes estimates of age-standardised relative risks for use in disease mapping. Biometrics, 43, 671-681.\n\nThe direct use of likelihood for significance testing. A P. ; O Dempster, P Barndorff-Nielsen, G Blaesild, Schou, Proc. Conf. Foundational Questions in Statistical Inference. Conf. Foundational Questions in Statistical Inference7Aarhus: University of Aarhus.Dempster, A. P. (1974) The direct use of likelihood for significance testing. In Proc. Conf. Foundational Questions in Statistical Inference (eds O. Barndorff-Nielsen, P. Blaesild and G. Schou), pp. 335-352. Aarhus: University of Aarhus. (1997a) The direct use of likelihood for significance testing. Statist. Comput., 7, 247-252. (1997b) Commentary on the paper by Murray Aitkin, and on discussion by Mervyn Stone. Statist. Comput., 7, 265-269.\n\nHow biased is the apparent error rate of a prediction rule?. B Efron, J. Am. Statist. Ass. 81Efron, B. (1986) How biased is the apparent error rate of a prediction rule? J. Am. Statist. Ass., 81, 461-470.\n\nBayesian analyses of longitudinal binary data using markov regression models of unknown order. A Erkanli, R Soyer, A Angold, Statist. Med. 20Erkanli, A., Soyer, R. and Angold, A. (2001) Bayesian analyses of longitudinal binary data using markov regression models of unknown order. Statist. Med., 20, 755-770.\n\nBayesian inference for prevalence in longitudinal two-phase studies. A Erkanli, R Soyer, E Costello, Biometrics. 55Erkanli, A., Soyer, R. and Costello, E. (1999) Bayesian inference for prevalence in longitudinal two-phase studies. Biometrics, 55, 1145-1150.\n\nDiagnostics for smoothing splines. R L Eubank, J. R. Statist. Soc. B. 47Eubank, R. L. (1985) Diagnostics for smoothing splines. J. R. Statist. Soc. B, 47, 332-341.\n\nDiagnostics for penalized least-squares estimators. R Eubank, R Gunst, Statist. Probab. Lett. 4Eubank, R. and Gunst, R. (1986) Diagnostics for penalized least-squares estimators. Statist. Probab. Lett., 4, 265-272.\n\nA likelihood-based method for analysing longitudinal binary responses. G Fitzmaurice, N Laird, Biometrika. 80Fitzmaurice, G. and Laird, N. (1993) A likelihood-based method for analysing longitudinal binary responses. Biometrika, 80, 141-151.\n\nBayesian model choice: asymptotics and exact calculations. A E Gelfand, D K Dey, J. R. Statist. Soc. B. 56Gelfand, A. E. and Dey, D. K. (1994) Bayesian model choice: asymptotics and exact calculations. J. R. Statist. Soc. B, 56, 501-514.\n\nConditional categorical response models with application to treatment of acute myocardial infarction. A E Gelfand, M D Ecker, C Christiansen, T J Mclaughlin, S B Soumerai, Appl. Statist. 49Gelfand, A. E., Ecker, M. D., Christiansen, C., McLaughlin, T. J. and Soumerai, S. B. (2000) Conditional categor- ical response models with application to treatment of acute myocardial infarction. Appl. Statist., 49, 171-186.\n\nModel choice: a minimum posterior predictive loss approach. A Gelfand, S Ghosh, Biometrika. 85Gelfand, A. and Ghosh, S. (1998) Model choice: a minimum posterior predictive loss approach. Biometrika, 85, 1-11.\n\nInequalities between expected marginal log likelihoods with implications for likelihood-based model comparison. A E Gelfand, M Trevisani, StorrsDepartment of Statistics, University of ConnecticutTechnical ReportGelfand, A. E. and Trevisani, M. (2002) Inequalities between expected marginal log likelihoods with implications for likelihood-based model comparison. Technical Report. Department of Statistics, University of Connecticut, Storrs.\n\nMarkov Chain Monte Carlo in Practice. W R Gilks, S Richardson, D J Spiegelhalter, Chapman and HallNew YorkGilks, W. R., Richardson, S. and Spiegelhalter, D. J. (eds) (1996) Markov Chain Monte Carlo in Practice. New York: Chapman and Hall.\n\nRandom-effects models for longitudinal data using Gibbs sampling. W R Gilks, C C Wang, P Coursaget, B Yvonnet, Biometrics. 49Gilks, W. R., Wang, C. C., Coursaget, P. and Yvonnet, B. (1993) Random-effects models for longitudinal data using Gibbs sampling. Biometrics, 49, 441-453.\n\nThe surprise index for the multivariate normal distribution. I J Good, Ann. Math. Statist. 27Good, I. J. (1956) The surprise index for the multivariate normal distribution. Ann. Math. Statist., 27, 1130-1135.\n\nHidden Markov models and disease mapping. P Green, S Richardson, J. Am. Statist. Ass. to be publishedGreen, P. and Richardson, S. (2002) Hidden Markov models and disease mapping. J. Am. Statist. Ass., to be published.\n\nMCMC methods for computing Bayes factors: a comparative review. C Han, B Carlin, J. Am. Statist. Ass. 96Han, C. and Carlin, B. (2001) MCMC methods for computing Bayes factors: a comparative review. J. Am. Statist. Ass., 96, 1122-1132.\n\nGeneralized Additive Models. T Hastie, R Tibshirani, Chapman and HallLondonHastie, T. and Tibshirani, R. (1990) Generalized Additive Models. London: Chapman and Hall.\n\nCounting degrees of freedom in hierarchical and other richly-parameterised models. J Hodges, D Sargent, Biometrika. 88Hodges, J. and Sargent, D. (2001) Counting degrees of freedom in hierarchical and other richly-parameterised models. Biometrika, 88, 367-379.\n\nThe behaviour of maximum likelihood estimates under non-standard conditions. P J Huber, Proc. 5th. 5thHuber, P. J. (1967) The behaviour of maximum likelihood estimates under non-standard conditions. In Proc. 5th\n\nBayes factors and model uncertainty. R Kass, A Raftery, J. Am. Statist. Ass. 90Kass, R. and Raftery, A. (1995) Bayes factors and model uncertainty. J. Am. Statist. Ass., 90, 773-795.\n\nBayesian model choice: what and why?. J T Key, L R Pericchi, A F M Smith, Bayesian Statistics 6. J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. SmithOxfordOxford University PressKey, J. T., Pericchi, L. R. and Smith, A. F. M. (1999) Bayesian model choice: what and why? In Bayesian Statistics 6 (eds J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith), pp. 343-370. Oxford: Oxford University Press.\n\nA correspondence between Bayesian estimation on stochastic processes and smoothing by splines. G Kimeldorf, G Wahba, Ann. Math. Statist. 41Kimeldorf, G. and Wahba, G. (1970) A correspondence between Bayesian estimation on stochastic processes and smoothing by splines. Ann. Math. Statist., 41, 495-502.\n\nOn information and sufficiency. S Kullback, R A Leibler, Ann. Math. Statist. 22Kullback, S. and Leibler, R. A. (1951) On information and sufficiency. Ann. Math. Statist., 22, 79-86.\n\nRandom effects models for longitudinal data. N M Laird, J H Ware, Biometrics. 38Laird, N. M. and Ware, J. H. (1982) Random effects models for longitudinal data. Biometrics, 38, 963-974.\n\nPredictive model selection. P W Laud, J G Ibrahim, J. R. Statist. Soc. B. 57Laud, P. W. and Ibrahim, J. G. (1995) Predictive model selection. J. R. Statist. Soc. B, 57, 247-262.\n\nHierarchical generalized linear models (with discussion). Y Lee, J A Nelder, J. Statist. Planng Inf. 58J. R. Statist. Soc. BLee, Y. and Nelder, J. A. (1996) Hierarchical generalized linear models (with discussion). J. R. Statist. Soc. B, 58, 619-678. van der Linde, A. (1995) Splines from a Bayesian point of view. Test, 4, 63-81. (2000) Reference priors for shrinkage and smoothing parameters. J. Statist. Planng Inf., 90, 245-274.\n\nBayes estimates for the linear model (with discussion). D V Lindley, A F M Smith, J. R. Statist. Soc. B. 34Lindley, D. V. and Smith, A. F. M. (1972) Bayes estimates for the linear model (with discussion). J. R. Statist. Soc. B, 34, 1-44.\n\nProbable networks and plausible predictions-a review of practical Bayesian methods for supervised neural networks. D J C Mackay, Netwrk Computn Neur. Syst. 4Neur. ComputnMacKay, D. J. C. (1992) Bayesian interpolation. Neur. Computn, 4, 415-447. (1995) Probable networks and plausible predictions-a review of practical Bayesian methods for super- vised neural networks. Netwrk Computn Neur. Syst., 6, 469-505.\n\nGeneralized Linear Models. P Mccullagh, J Nelder, Chapman and HallLondon2nd ednMcCullagh, P. and Nelder, J. (1989) Generalized Linear Models, 2nd edn. London: Chapman and Hall.\n\nPerforming likelihood ratio tests with multiply imputed data sets. X.-L Meng, D B Rubin, Biometrika. 79Meng, X.-L. and Rubin, D. B. (1992) Performing likelihood ratio tests with multiply imputed data sets. Bio- metrika, 79, 103-112.\n\nThe effective number of parameters: an analysis of generalization and regularization in nonlinear learning systems. J E Moody, Advances in Neural Information Processing Systems. J. E. Moody, S. J. Hanson and R. P. LippmannSan MateoMorgan Kaufmann4Moody, J. E. (1992) The effective number of parameters: an analysis of generalization and regularization in nonlinear learning systems. In Advances in Neural Information Processing Systems 4 (eds J. E. Moody, S. J. Hanson and R. P. Lippmann), pp. 847-854. San Mateo: Morgan Kaufmann.\n\nNetwork information criterion-determining the number of hidden units for artificial neural network models. N Murata, S Yoshizawa, S Amari, IEEE Trans. Neur. Netwrks. 5Murata, N., Yoshizawa, S. and Amari, S. (1994) Network information criterion-determining the number of hidden units for artificial neural network models. IEEE Trans. Neur. Netwrks, 5, 865-872.\n\nReference Bayesian methods for generalised linear mixed models. R Natarajan, R E Kass, J. Am. Statist. Ass. 95Natarajan, R. and Kass, R. E. (2000) Reference Bayesian methods for generalised linear mixed models. J. Am. Statist. Ass., 95, 227-237.\n\nA Bayesian model selection criterion. T E Raghunathan, SeattleUniversity of WashingtonTechnical ReportRaghunathan, T. E. (1988) A Bayesian model selection criterion. Technical Report. University of Washington, Seattle.\n\nThe Bayesian analysis of a pivotal pharmacokinetic study. N J Rahman, J C Wakefield, D A Stephens, C Falcoz, Statist. Meth. Med. Res. 8Rahman, N. J., Wakefield, J. C., Stephens, D. A. and Falcoz, C. (1999) The Bayesian analysis of a pivotal pharmacokinetic study. Statist. Meth. Med. Res., 8, 195-216.\n\nOn Bayesian analysis of mixtures with an unknown number of components (with discussion). S Richardson, P J Green, J. R. Statist. Soc. B. 59Richardson, S. and Green, P. J. (1997) On Bayesian analysis of mixtures with an unknown number of components (with discussion). J. R. Statist. Soc. B, 59, 731-792.\n\nB D Ripley, Pattern Recognition and Neural Networks. CambridgeCambridge University PressRipley, B. D. (1996) Pattern Recognition and Neural Networks. Cambridge: Cambridge University Press.\n\nInformation criteria for choice of regression models: a comment. T Sawa, Econometrica. 46Sawa, T. (1978) Information criteria for choice of regression models: a comment. Econometrica, 46, 1273-1291.\n\nEstimating the dimension of a model. G Schwarz, Ann. Statist. 6Schwarz, G. (1978) Estimating the dimension of a model. Ann. Statist., 6, 461-466.\n\nParameterizations for natural exponential-families with quadratic variance functions. E Slate, J. Am. Statist. Ass. 89Slate, E. (1994) Parameterizations for natural exponential-families with quadratic variance functions. J. Am. Statist. Ass., 89, 1471-1482.\n\nWinBUGS Version 1.3 User Manual. Cambridge: Medical Research Council Biostatistics Unit. D J Spiegelhalter, A Thomas, N G Best, Spiegelhalter, D. J., Thomas, A. and Best, N. G. (2000) WinBUGS Version 1.3 User Manual. Cambridge: Medical Research Council Biostatistics Unit. (Available from http://www.mrc-bsu.cam.ac.uk/bugs.)\n\nD J Spiegelhalter, A Thomas, N G Best, W R Gilks, Cambridge: Medical Research Council Biostatistics Unit. 1Version 0.5 (Version iiSpiegelhalter, D. J., Thomas, A., Best, N. G. and Gilks, W. R. (1996) BUGS Examples Volume 1, Version 0.5 (Version ii). Cambridge: Medical Research Council Biostatistics Unit.\n\nAn asymptotic equivalence of choice of model by cross-validation and Akaike's criterion. M Stone, J. R. Statist. Soc. B. 39Stone, M. (1977) An asymptotic equivalence of choice of model by cross-validation and Akaike's criterion. J. R. Statist. Soc. B, 39, 44-47.\n\nDistribution of informational statistics and a criterion for model fitting. K Takeuchi, Suri-Kagaku. 153in JapaneseTakeuchi, K. (1976) Distribution of informational statistics and a criterion for model fitting (in Japanese). Suri-Kagaku, 153, 12-18.\n\nBayesian neural networks with correlated residuals. A Vehtari, J Lampinen, IJCNN'99: Proc. Vehtari, A. and Lampinen, J. (1999) Bayesian neural networks with correlated residuals. In IJCNN'99: Proc.\n\nJoint Conf. Neural Networks. New York: Institute of Electrical and Electronic Engineers. Int. Joint Conf. Neural Networks. New York: Institute of Electrical and Electronic Engineers.\n\nImproper priors, spline smoothing and the problem of guarding against model errors in regressions. G Wahba, J. R. Statist. Soc. B. 40Wahba, G. (1978) Improper priors, spline smoothing and the problem of guarding against model errors in regressions. J. R. Statist. Soc. B, 40, 364-372.\n\nconfidence intervals\" for the cross-validated smoothing spline. Bayesian, J. R. Statist. Soc. B. 45Bayesian \"confidence intervals\" for the cross-validated smoothing spline. J. R. Statist. Soc. B, 45, 133-150.\n\nSpline Models for Observational Data. Philadelphia: Society for Industrial and Applied Mathematics. Spline Models for Observational Data. Philadelphia: Society for Industrial and Applied Mathe- matics.\n\nOn measuring and correcting the effects of data mining and model selection. J Ye, J. Am. Statist. Ass. 93Ye, J. (1998) On measuring and correcting the effects of data mining and model selection. J. Am. Statist. Ass., 93, 120-131.\n\nEvaluation of highly complex modeling procedures with binomial and Poisson data. J Ye, W Wong, ChicagoGraduate School of Business, University of ChicagoTechnical ReportYe, J. and Wong, W. (1998) Evaluation of highly complex modeling procedures with binomial and Poisson data. Technical Report. Graduate School of Business, University of Chicago, Chicago.\n\nGeneralised linear models with random effects; a Gibbs sampling approach. S L Zeger, M R Karim, J. Am. Statist. Ass. 86Zeger, S. L. and Karim, M. R. (1991) Generalised linear models with random effects; a Gibbs sampling approach. J. Am. Statist. Ass., 86, 79-86.\n\nComparing hierarchical models for spatio-temporally misaligned data using the deviance information criterion. L Zhu, B Carlin, Statist. Med. 19References in the discussionZhu, L. and Carlin, B. (2000) Comparing hierarchical models for spatio-temporally misaligned data using the deviance information criterion. Statist. Med., 19, 2265-2278. References in the discussion\n\nPosterior Bayes factors (with discussion). M Aitkin, J. R. Statist. Soc. B. 53Aitkin, M. (1991) Posterior Bayes factors (with discussion). J. R. Statist. Soc. B, 53, 111-142.\n\nInformation theory and an extension of the maximum likelihood principle. H Akaike, Proc. 2nd Int. Symp. Information Theory. B. N. Petrov and F. Cs\u00e1ki2nd Int. Symp. Information TheoryBudapestAkad\u00e9miai Kiad\u00f3Akaike, H. (1973) Information theory and an extension of the maximum likelihood principle. In Proc. 2nd Int. Symp. Information Theory (eds B. N. Petrov and F. Cs\u00e1ki), pp. 267-281. Budapest: Akad\u00e9miai Kiad\u00f3.\n\nA note on the generalized information criterion for choice of a model. A C Atkinson, Biometrika. 67Atkinson, A. C. (1980) A note on the generalized information criterion for choice of a model. Biometrika, 67, 413-418.\n\nForward search added variable t tests and the effect of masked outliers on model selection and transformation. A C Atkinson, M Riani, LSERR73London School of Economics and Political Science. J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith7Oxford University PressTechnical ReportIn Bayesian StatisticsAtkinson, A. C. and Riani, M. (2000) Robust Diagnostic Regression Analysis. New York: Springer. (2002) Forward search added variable t tests and the effect of masked outliers on model selection and transformation. Technical Report LSERR73. London School of Economics and Political Science, London. Bernardo, J. M. (1979) Expected information as expected utility. Ann. Statist., 7, 686-690. (1999) Nested hypothesis testing: the Bayesian reference criterion (with discussion). In Bayesian Statistics 6 (eds J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith), pp. 101-130. Oxford: Oxford University Press.\n\n. J M Bernardo, A F M Smith, WileyNew YorkBernardo, J. M. and Smith, A. F. M. (1994) Bayesian Theory. New York: Wiley.\n\nIntrinsic estimation. 7th Valencia Int. J M Bernardo, M Suarez, Meet. Bayesian Statistics. Bernardo, J. M. and Suarez, M. (2002) Intrinsic estimation. 7th Valencia Int. Meet. Bayesian Statistics, Tenerife, June.\n\nModel Selection and Inference: a Practical Information-theoretic Approach. K P Burnham, D R Anderson, SpringerNew YorkBurnham, K. P. and Anderson, D. R. (1998) Model Selection and Inference: a Practical Information-theoretic Approach. New York: Springer.\n\nModel Selection and Multimodel Inference: a Practical Information-theoretical Approach. SpringerNew York2nd ednModel Selection and Multimodel Inference: a Practical Information-theoretical Approach, 2nd edn. New York: Springer.\n\nMixture models, latent variables and partitioned importance sampling. G Casella, C P Robert, M T Wells, ParisTechnical ReportCasella, G., Robert, C. P. and Wells, M. T. (2000) Mixture models, latent variables and partitioned importance sampling. Technical Report. Paris.\n\nComputational and inferential difficulties with mixtures posterior distribution. G Celeux, M Hurn, C P Robert, J. Am. Statist. Ass. 95Celeux, G., Hurn, M. and Robert, C. P. (2000) Computational and inferential difficulties with mixtures posterior distribution. J. Am. Statist. Ass., 95, 957-979.\n\nExperts in Uncertainty. R M Cooke, Oxford University PressOxfordCooke, R. M. (1991) Experts in Uncertainty. Oxford: Oxford University Press.\n\nProbabilistic Networks and Expert Systems. R G Cowell, A P Dawid, S L Lauritzen, D J Spiegelhalter, SpringerNew YorkCowell, R. G., Dawid, A. P., Lauritzen, S. L. and Spiegelhalter, D. J. (1999) Probabilistic Networks and Expert Systems. New York: Springer.\n\nNonconjugate Bayesian estimation of covariance matrices and its use in hierarchical models. M J Daniels, R E Kass, J. Am. Statist. Ass. 94BiometricsDaniels, M. J. and Kass, R. E. (1999) Nonconjugate Bayesian estimation of covariance matrices and its use in hierarchical models. J. Am. Statist. Ass., 94, 1254-1263. (2001) Shrinkage estimators for covariance matrices. Biometrics, 57, 1173-1184.\n\nFisherian inference in likelihood and prequential frames of reference (with discussion). A P S Dawid, N L Kotz, C B Johnson, Read, Current Issues in Statistical Inference: Essays in. M. Ghosh and P. K. PathakNew York; Oxford; HaywardInstitute of Mathematical Statistics147Encyclopedia of Statistical SciencesDawid, A. P. (1984) Statistical theory: the prequential approach. J. R. Statist. Soc. A, 147, 278-292. (1986) Probability forecasting. In Encyclopedia of Statistical Sciences, vol. 7 (eds S. Kotz, N. L. Johnson and C. B. Read), pp. 210-218. New York: Wiley-Interscience. (1991) Fisherian inference in likelihood and prequential frames of reference (with discussion). J. R. Statist. Soc. B, 53, 79-109. (1992a) Prequential analysis, stochastic complexity and Bayesian inference (with discussion). In Bayesian Statistics 4 (eds J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith), pp. 109-125. Oxford: Oxford University Press. (1992b) Prequential data analysis. In Current Issues in Statistical Inference: Essays in Honor of D. Basu (eds M. Ghosh and P. K. Pathak), pp. 113-126. Hayward: Institute of Mathematical Statistics.\n\nDiscussion on 'Decision models in screening for breast cancer. D Draper, J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. SmithOxford University PressOxfordDraper, D. (1999) Discussion on 'Decision models in screening for breast cancer' (by G. Parmigiani). In Bayesian Statistics 6 (eds J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith), pp. 541-543. Oxford: Oxford University Press.\n\nA case study of stochastic optimization in health policy: problem formulation and preliminary results. D Draper, D Fouskakis, J. Global Optimzn. 18Draper, D. and Fouskakis, D. (2000) A case study of stochastic optimization in health policy: problem formulation and preliminary results. J. Global Optimzn, 18, 399-416.\n\nModel choice in qualitative regression models. J Dupuis, C P Robert, J. Statist. Planng Inf. to be publishedDupuis, J. and Robert, C. P. (2002) Model choice in qualitative regression models. J. Statist. Planng Inf., to be published.\n\nHow biased is the apparent error rate of a prediction rule?. B Efron, J. Am. Statist. Ass. 81Efron, B. (1986) How biased is the apparent error rate of a prediction rule? J. Am. Statist. Ass., 81, 461-470.\n\nStochastic optimization: a review. D Fouskakis, D Draper, Int. Statist. Rev. to be publishedFouskakis, D. and Draper, D. (2002) Stochastic optimization: a review. Int. Statist. Rev., to be published.\n\nCluster modelling for disease rate mapping. R Gangnon, M Clayton, Spatial Cluster Modelling. A. B. Lawson and D. DenisonNew YorkCRC PressGangnon, R. and Clayton, M. (2002) Cluster modelling for disease rate mapping. In Spatial Cluster Modelling (eds A. B. Lawson and D. Denison), ch. 8. New York: CRC Press.\n\nModel determination using sampling-based methods. A E Gelfand, W. R. Gilks, S. Richardson and D. J. SpiegelhalterChapman and HallLondonGelfand, A. E. (1996) Model determination using sampling-based methods. In Markov Chain Monte Carlo in Practice (eds W. R. Gilks, S. Richardson and D. J. Spiegelhalter), pp. 145-162. London: Chapman and Hall.\n\nModel determination using predictive distributions with implementation via sampling-based methods (with discussion). A E Gelfand, D K Dey, H Chang, In Bayesian Statistics. J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith4Oxford University PressGelfand, A. E., Dey, D. K. and Chang, H. (1992) Model determination using predictive distributions with implementation via sampling-based methods (with discussion). In Bayesian Statistics 4 (eds J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith), pp. 147-167. Oxford: Oxford University Press.\n\nPosterior predictive assessment of model fitness via realized discrepancies (with discussion). A Gelman, X.-L Meng, H Stern, Statist. Sin. 6Gelman, A., Meng, X.-L. and Stern, H. (1996) Posterior predictive assessment of model fitness via realized dis- crepancies (with discussion). Statist. Sin., 6, 733-807.\n\nRational decisions. I J Good, J. R. Statist. Soc. B. 14Good, I. J. (1952) Rational decisions. J. R. Statist. Soc. B, 14, 107-114.\n\nHidden Markov models and disease mapping. P Green, S Richardson, J. Am. Statist. Ass. to be publishedGreen, P. and Richardson, S. (2002) Hidden Markov models and disease mapping. J. Am. Statist. Ass., to be published.\n\nCounting degrees of freedom in hierarchical and other richly-parameterised models. J Hodges, D Sargent, Biometrika. 88Hodges, J. and Sargent, D. (2001) Counting degrees of freedom in hierarchical and other richly-parameterised models. Biometrika, 88, 367-379.\n\nBayesian wavelet analysis with a model complexity prior. C Holmes, D Denison, Bayesian Statistics 6. J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. SmithOxfordOxford University PressHolmes, C. and Denison, D. (1999) Bayesian wavelet analysis with a model complexity prior. In Bayesian Statistics 6 (eds J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith), pp. 769-776. Oxford: Oxford University Press.\n\nBayes factors and model uncertainty. R Kass, A Raftery, J. Am. Statist. Ass. 90Kass, R. and Raftery, A. (1995) Bayes factors and model uncertainty. J. Am. Statist. Ass., 90, 773-795.\n\nBayesian model choice: what and why?. J T Key, L R Pericchi, A F M Smith, Bayesian Statistics 6. J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. SmithOxfordOxford University PressKey, J. T., Pericchi, L. R. and Smith, A. F. M. (1999) Bayesian model choice: what and why? In Bayesian Statistics 6 (eds J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith), pp. 343-370. Oxford: Oxford University Press.\n\nBayesian model discrimination in the analysis of capture-recapture and related data. R King, BristolSchool of Mathematics, University of BristolPhD ThesisKing, R. (2001) Bayesian model discrimination in the analysis of capture-recapture and related data. PhD Thesis. School of Mathematics, University of Bristol, Bristol.\n\nBayesian estimation of census undercount. R King, S P Brooks, Biometrika. 88King, R. and Brooks, S. P. (2001) Bayesian estimation of census undercount. Biometrika, 88, 317-336.\n\nGeneralised information criteria in model selection. S Konishi, G Kitagawa, Biometrika. 83Konishi, S. and Kitagawa, G. (1996) Generalised information criteria in model selection. Biometrika, 83, 875-890.\n\nLocal computations with probabilities on graphical structures and their application to expert systems (with discussion). S L Lauritzen, D J Spiegelhalter, J. R. Statist. Soc. B. 50Lauritzen, S. L. and Spiegelhalter, D. J. (1988) Local computations with probabilities on graphical structures and their application to expert systems (with discussion). J. R. Statist. Soc. B, 50, 157-224.\n\nCluster modelling of disease incidence via rjmcmc methods: a comparative evaluation. A B Lawson, Statist. Med. 19Lawson, A. B. (2000) Cluster modelling of disease incidence via rjmcmc methods: a comparative evaluation. Statist. Med., 19, 2361-2376.\n\nHierarchical generalized linear models: a synthesis of generalized linear models, random effect models and structured dispersions. Y Lee, J A Nelder, J. R. Statist. Soc. B. 58Statist. ModlngLee, Y. and Nelder, J. A. (1996) Hierarchical generalized linear models (with discussion). J. R. Statist. Soc. B, 58, 619-678. (2001a) Hierarchical generalized linear models: a synthesis of generalized linear models, random effect models and structured dispersions. Biometrika, 88, 987-1006. (2001b) Modelling and analysing correlated non-normal data. Statist. Modlng, 1, 3-16.\n\nChoosing a model selection strategy. X De Luna, K Skouras, Scand. J. Statist. to be publishedde Luna, X. and Skouras, K. (2003) Choosing a model selection strategy. Scand. J. Statist., to be published.\n\nModel selection and accounting for model uncertainty in graphical models using Occam's window. D Madigan, A E Raftery, 213SeattleDepartment of Statistics, University of WashingtonTechnical ReportMadigan, D. and Raftery, A. E. (1991) Model selection and accounting for model uncertainty in graphical models using Occam's window. Technical Report 213. Department of Statistics, University of Washington, Seattle.\n\nPerfect sampling for point process cluster modelling. I Mckeague, M Loiseaux, Spatial Cluster Modelling. A. B. Lawson and D. DenisonNew YorkCRC PressMcKeague, I. and Loiseaux, M. (2002) Perfect sampling for point process cluster modelling. In Spatial Cluster Modelling (eds A. B. Lawson and D. Denison), ch. 5. New York: CRC Press.\n\nPerforming likelihood ratio tests with multiply imputed data sets. X.-L Meng, D B Rubin, Biometrika. 79Meng, X.-L. and Rubin, D. B. (1992) Performing likelihood ratio tests with multiply imputed data sets. Biometrika, 79, 103-112.\n\nA robust Bayesian look at the theory of precise measurement. In Decision Research from Bayesian Approaches to Normative Systems. E Moreno, L R Pericchi, J Kadane, J. Shantan et al.KluwerBostonMoreno, E., Pericchi, L. R. and Kadane, J. (1998) A robust Bayesian look at the theory of precise measure- ment. In Decision Research from Bayesian Approaches to Normative Systems (eds J. Shantan et al.). Boston: Kluwer.\n\nApplied Linear Statistical Models. J Neter, M H Kutner, C J Nachtsheim, W Wasserman, McGraw-HillNew York4th ednNeter, J., Kutner, M. H., Nachtsheim, C. J. and Wasserman, W. (1996) Applied Linear Statistical Models, 4th edn. New York: McGraw-Hill.\n\nRobust Bayesian credible intervals and prior ignorance. L R Pericchi, P Walley, Int. Statist. Rev. 58Pericchi, L. R. and Walley, P. (1991) Robust Bayesian credible intervals and prior ignorance. Int. Statist. Rev., 58, 1-23.\n\nSome criteria for Bayesian model choice. M Plummer, PreprintPlummer, M. (2002) Some criteria for Bayesian model choice. Preprint. (Available from http://calvin. iarc.fr/martyn/papers/.)\n\nM B Priestley, Spectral Analysis and Time Series. LondonAcademic PressPriestley, M. B. (1981) Spectral Analysis and Time Series. London: Academic Press.\n\nIntrinsic loss functions. C P Robert, Theory Decsn. 40Robert, C. P. (1996) Intrinsic loss functions. Theory Decsn, 40, 191-214.\n\nAn asymptotic theory for linear model selection. J Shao, Statist. Sin. 7Shao, J. (1997) An asymptotic theory for linear model selection. Statist. Sin., 7, 221-264.\n\nOn efficient probability forecasting systems. K Skouras, A P Dawid, 218Biometrika. 86Department of Statistical Science, University College LondonResearch ReportConsistency in misspecified modelsSkouras, K. and Dawid, A. P. (1999) On efficient probability forecasting systems. Biometrika, 86, 765-784. (2000) Consistency in misspecified models. Research Report 218. Department of Statistical Science, University College London, London. (Available from: http://www.ucl.ac.uk/Stats/research/ abs00.html#218.)\n\nPlausible Bayesian games. J Q Smith, Bayesian Statistics 5. J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. SmithOxfordOxford University PressSmith, J. Q. (1996) Plausible Bayesian games. In Bayesian Statistics 5 (eds J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith), pp. 387-406. Oxford: Oxford University Press.\n\nCross-validatory choice and assessment of statistical predictions (with discussion). M Stone, J. R. Statist. Soc. B. 36Stone, M. (1974) Cross-validatory choice and assessment of statistical predictions (with discussion). J. R. Statist. Soc. B, 36, 111-147.\n\nAn asymptotic equivalence of choice of model by cross-validation and Akaike's criterion. J. R. Statist. Soc. B. 36An asymptotic equivalence of choice of model by cross-validation and Akaike's criterion. J. R. Statist. Soc. B, 36, 44-47.\n\nBayesian model assessment and selection using expected utilities. DSc Dissertation. Helsinki University of Technology. A Vehtari, HelsinkiVehtari, A. (2001) Bayesian model assessment and selection using expected utilities. DSc Dissertation. Helsinki Uni- versity of Technology, Helsinki. (Available from http://lib.hut.fi/Diss/2001/isbn9512257653/.)\n\nCross-validation, information criteria, expected utilities and the effective number of parameters. A Vehtari, J Lampinen, Neur. Computn. 14Bayesian model assessment and comparison using cross-validation predictive densities. To be publishedVehtari, A. and Lampinen, J. (2002a) Bayesian model assessment and comparison using cross-validation predictive densities. Neur. Computn, 14, in the press. (2002b) Cross-validation, information criteria, expected utilities and the effective number of parameters. To be published.\n\nBayesian information criterion for censored survival models. C T Volinsky, A E Raftery, Biometrics. 56Volinsky, C. T. and Raftery, A. E. (2000) Bayesian information criterion for censored survival models. Biometrics, 56, 256-262.\n\nA statistic for allocating C p to individual cases. S Weisberg, Technometrics. 23Weisberg, S. (1981) A statistic for allocating C p to individual cases. Technometrics, 23, 27-31.\n\nOn measuring and correcting the effects of data mining and model selection. J Ye, J. Am. Statist. Ass. 93Ye, J. (1998) On measuring and correcting the effects of data mining and model selection. J. Am. Statist. Ass., 93, 120-131.\n\nComparing hierarchical models for spatio-temporally misaligned data using the deviance information criterion. L Zhu, B Carlin, Statist. Med. 19Zhu, L. and Carlin, B. (2000) Comparing hierarchical models for spatio-temporally misaligned data using the deviance information criterion. Statist. Med., 19, 2265-2278.\n", "annotations": {"author": "[{\"end\":75,\"start\":53},{\"end\":90,\"start\":76},{\"end\":108,\"start\":91},{\"end\":132,\"start\":109},{\"end\":280,\"start\":133},{\"end\":311,\"start\":281}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":61},{\"end\":89,\"start\":85},{\"end\":107,\"start\":101},{\"end\":131,\"start\":118}]", "author_first_name": "[{\"end\":58,\"start\":53},{\"end\":60,\"start\":59},{\"end\":82,\"start\":76},{\"end\":84,\"start\":83},{\"end\":98,\"start\":91},{\"end\":100,\"start\":99},{\"end\":117,\"start\":109}]", "author_affiliation": "[{\"end\":279,\"start\":134},{\"end\":310,\"start\":282}]", "title": "[{\"end\":46,\"start\":1},{\"end\":357,\"start\":312}]", "venue": "[{\"end\":380,\"start\":359}]", "abstract": "[{\"end\":2064,\"start\":735}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3141,\"start\":3128},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3433,\"start\":3410},{\"end\":3553,\"start\":3540},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3565,\"start\":3553},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3578,\"start\":3565},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3591,\"start\":3578},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6519,\"start\":6490},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7246,\"start\":7231},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7274,\"start\":7258},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7304,\"start\":7283},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8057,\"start\":8047},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8304,\"start\":8292},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8504,\"start\":8492},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8528,\"start\":8504},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8965,\"start\":8937},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9029,\"start\":9017},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9067,\"start\":9051},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9972,\"start\":9958},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":10183,\"start\":10167},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10349,\"start\":10336},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10781,\"start\":10768},{\"end\":10792,\"start\":10782},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11079,\"start\":11067},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11208,\"start\":11187},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11279,\"start\":11265},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":22080,\"start\":22056},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":23139,\"start\":23115},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24242,\"start\":24218},{\"end\":24518,\"start\":24505},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24590,\"start\":24561},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25646,\"start\":25625},{\"end\":26891,\"start\":26878},{\"end\":26911,\"start\":26891},{\"end\":27133,\"start\":27120},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":27378,\"start\":27364},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27400,\"start\":27378},{\"end\":27478,\"start\":27465},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27672,\"start\":27646},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":27688,\"start\":27677},{\"end\":27702,\"start\":27688},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28662,\"start\":28649},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":29227,\"start\":29213},{\"end\":29384,\"start\":29363},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":32107,\"start\":32080},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":37925,\"start\":37912},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":38328,\"start\":38301},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":38839,\"start\":38811},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":39395,\"start\":39368},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40068,\"start\":40053},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":40120,\"start\":40102},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":40144,\"start\":40122},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":40165,\"start\":40146},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":40197,\"start\":40170},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40371,\"start\":40355},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":42085,\"start\":42057},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":42528,\"start\":42501},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":46397,\"start\":46379},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":46826,\"start\":46813},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":47362,\"start\":47338},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":47390,\"start\":47367},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":48451,\"start\":48438},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":51194,\"start\":51181},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":52070,\"start\":52048},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":52150,\"start\":52128},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":52218,\"start\":52191},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":52314,\"start\":52292},{\"end\":52386,\"start\":52358},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":52435,\"start\":52414},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":52507,\"start\":52479},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":52774,\"start\":52748},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":52800,\"start\":52774},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":53954,\"start\":53941},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":54772,\"start\":54744},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":55231,\"start\":55203},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":55338,\"start\":55322},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":60435,\"start\":60408},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":60545,\"start\":60530},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":61525,\"start\":61498},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":61567,\"start\":61544},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":61878,\"start\":61851},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":63937,\"start\":63909},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":64461,\"start\":64436},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":66079,\"start\":66054},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":67684,\"start\":67672},{\"end\":67794,\"start\":67782},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":68403,\"start\":68380},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":68432,\"start\":68408},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":69664,\"start\":69649},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":70203,\"start\":70182},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":79617,\"start\":79600},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":81895,\"start\":81884},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":82172,\"start\":82150},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":85784,\"start\":85771},{\"attributes\":{\"ref_id\":\"b109\"},\"end\":85917,\"start\":85891},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":86091,\"start\":86071},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":86774,\"start\":86748},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":86966,\"start\":86953},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":90775,\"start\":90763},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":90869,\"start\":90853},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":91128,\"start\":91113},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":91396,\"start\":91381},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":91590,\"start\":91577},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":91782,\"start\":91768},{\"attributes\":{\"ref_id\":\"b119\"},\"end\":91815,\"start\":91787},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":91895,\"start\":91881},{\"attributes\":{\"ref_id\":\"b119\"},\"end\":91928,\"start\":91900},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":92527,\"start\":92505},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":92541,\"start\":92527},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":93109,\"start\":93094},{\"attributes\":{\"ref_id\":\"b119\"},\"end\":93137,\"start\":93109},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":96492,\"start\":96480},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":97875,\"start\":97862},{\"end\":98074,\"start\":98064},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":98779,\"start\":98766},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":98808,\"start\":98784},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":99513,\"start\":99493},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":101746,\"start\":101731},{\"attributes\":{\"ref_id\":\"b121\"},\"end\":102171,\"start\":102156},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":102284,\"start\":102259},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":102939,\"start\":102920},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":103263,\"start\":103238},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":104851,\"start\":104837},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":105494,\"start\":105481},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":105834,\"start\":105813},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":106200,\"start\":106186},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":106263,\"start\":106238},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":106367,\"start\":106354},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":106391,\"start\":106367},{\"attributes\":{\"ref_id\":\"b105\"},\"end\":107630,\"start\":107601},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":107656,\"start\":107630},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":108085,\"start\":108072},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":108933,\"start\":108918},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":109602,\"start\":109575},{\"end\":111343,\"start\":111327},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":119940,\"start\":119905},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":122952,\"start\":122938},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":123014,\"start\":122987},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":123046,\"start\":123019},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":126372,\"start\":126344},{\"end\":128649,\"start\":128624},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":129840,\"start\":129818},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":129864,\"start\":129842},{\"attributes\":{\"ref_id\":\"b116\"},\"end\":130977,\"start\":130964},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":131012,\"start\":130999},{\"attributes\":{\"ref_id\":\"b113\"},\"end\":131343,\"start\":131332},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":131566,\"start\":131552},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":132897,\"start\":132877},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":137343,\"start\":137317},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":139923,\"start\":139911},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":139939,\"start\":139923},{\"end\":144779,\"start\":144751},{\"end\":144897,\"start\":144881},{\"end\":145858,\"start\":145824},{\"end\":145917,\"start\":145881},{\"end\":148077,\"start\":148062},{\"attributes\":{\"ref_id\":\"b120\"},\"end\":149975,\"start\":149948},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":151177,\"start\":151150}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":152100,\"start\":151762},{\"attributes\":{\"id\":\"fig_1\"},\"end\":152323,\"start\":152101},{\"attributes\":{\"id\":\"fig_2\"},\"end\":152624,\"start\":152324},{\"attributes\":{\"id\":\"fig_3\"},\"end\":153116,\"start\":152625},{\"attributes\":{\"id\":\"fig_4\"},\"end\":153347,\"start\":153117},{\"attributes\":{\"id\":\"fig_5\"},\"end\":153508,\"start\":153348},{\"attributes\":{\"id\":\"fig_6\"},\"end\":153713,\"start\":153509},{\"attributes\":{\"id\":\"fig_7\"},\"end\":154300,\"start\":153714},{\"attributes\":{\"id\":\"fig_8\"},\"end\":154362,\"start\":154301},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":154550,\"start\":154363},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":155258,\"start\":154551},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":155530,\"start\":155259},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":155920,\"start\":155531},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":157425,\"start\":155921},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":157977,\"start\":157426},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":158211,\"start\":157978}]", "paragraph": "[{\"end\":2648,\"start\":2080},{\"end\":3715,\"start\":2650},{\"end\":4620,\"start\":3717},{\"end\":5359,\"start\":4622},{\"end\":5696,\"start\":5434},{\"end\":5976,\"start\":5698},{\"end\":6836,\"start\":5978},{\"end\":6931,\"start\":6838},{\"end\":8010,\"start\":6933},{\"end\":8774,\"start\":8037},{\"end\":9298,\"start\":8818},{\"end\":9301,\"start\":9300},{\"end\":9662,\"start\":9303},{\"end\":9973,\"start\":9709},{\"end\":10111,\"start\":10037},{\"end\":10227,\"start\":10113},{\"end\":10563,\"start\":10229},{\"end\":10585,\"start\":10565},{\"end\":11280,\"start\":10752},{\"end\":11456,\"start\":11282},{\"end\":11510,\"start\":11497},{\"end\":11552,\"start\":11526},{\"end\":11680,\"start\":11595},{\"end\":12239,\"start\":11682},{\"end\":12304,\"start\":12262},{\"end\":12941,\"start\":12306},{\"end\":13007,\"start\":12943},{\"end\":13331,\"start\":13038},{\"end\":13534,\"start\":13333},{\"end\":14367,\"start\":13569},{\"end\":14453,\"start\":14396},{\"end\":14942,\"start\":14455},{\"end\":15120,\"start\":14944},{\"end\":18102,\"start\":15122},{\"end\":18608,\"start\":18104},{\"end\":18951,\"start\":18657},{\"end\":19076,\"start\":18953},{\"end\":19442,\"start\":19198},{\"end\":19740,\"start\":19584},{\"end\":20021,\"start\":19763},{\"end\":20141,\"start\":20038},{\"end\":20232,\"start\":20173},{\"end\":20290,\"start\":20258},{\"end\":20558,\"start\":20368},{\"end\":20689,\"start\":20581},{\"end\":20974,\"start\":20744},{\"end\":21207,\"start\":21006},{\"end\":21331,\"start\":21209},{\"end\":21612,\"start\":21360},{\"end\":21965,\"start\":21643},{\"end\":22094,\"start\":21993},{\"end\":22519,\"start\":22138},{\"end\":22758,\"start\":22653},{\"end\":22938,\"start\":22792},{\"end\":23085,\"start\":22959},{\"end\":23140,\"start\":23087},{\"end\":23216,\"start\":23186},{\"end\":23490,\"start\":23242},{\"end\":23779,\"start\":23521},{\"end\":24062,\"start\":23814},{\"end\":24243,\"start\":24130},{\"end\":25215,\"start\":24340},{\"end\":25682,\"start\":25251},{\"end\":26331,\"start\":25753},{\"end\":26596,\"start\":26473},{\"end\":26729,\"start\":26683},{\"end\":27863,\"start\":26774},{\"end\":28089,\"start\":27909},{\"end\":28426,\"start\":28167},{\"end\":28908,\"start\":28467},{\"end\":29578,\"start\":28932},{\"end\":29982,\"start\":29628},{\"end\":30127,\"start\":30037},{\"end\":30241,\"start\":30168},{\"end\":30600,\"start\":30497},{\"end\":30707,\"start\":30602},{\"end\":30946,\"start\":30773},{\"end\":31122,\"start\":30948},{\"end\":31190,\"start\":31124},{\"end\":31461,\"start\":31254},{\"end\":31913,\"start\":31463},{\"end\":32197,\"start\":31948},{\"end\":32273,\"start\":32268},{\"end\":32564,\"start\":32344},{\"end\":32980,\"start\":32606},{\"end\":33533,\"start\":33025},{\"end\":34247,\"start\":33923},{\"end\":34731,\"start\":34414},{\"end\":35041,\"start\":34771},{\"end\":35457,\"start\":35064},{\"end\":36155,\"start\":35501},{\"end\":36341,\"start\":36178},{\"end\":36683,\"start\":36381},{\"end\":37032,\"start\":36706},{\"end\":37673,\"start\":37047},{\"end\":38289,\"start\":37733},{\"end\":38681,\"start\":38291},{\"end\":39194,\"start\":38765},{\"end\":39419,\"start\":39230},{\"end\":39505,\"start\":39421},{\"end\":39819,\"start\":39677},{\"end\":40675,\"start\":39930},{\"end\":40871,\"start\":40732},{\"end\":41080,\"start\":40953},{\"end\":41608,\"start\":41115},{\"end\":42233,\"start\":41610},{\"end\":42347,\"start\":42235},{\"end\":42592,\"start\":42366},{\"end\":42900,\"start\":42617},{\"end\":43253,\"start\":42957},{\"end\":44159,\"start\":43306},{\"end\":44634,\"start\":44161},{\"end\":44884,\"start\":44636},{\"end\":45308,\"start\":44953},{\"end\":45425,\"start\":45352},{\"end\":45562,\"start\":45466},{\"end\":45791,\"start\":45619},{\"end\":47056,\"start\":45833},{\"end\":47925,\"start\":47058},{\"end\":48543,\"start\":48003},{\"end\":48779,\"start\":48597},{\"end\":49047,\"start\":48890},{\"end\":49095,\"start\":49049},{\"end\":49313,\"start\":49240},{\"end\":49598,\"start\":49385},{\"end\":49734,\"start\":49643},{\"end\":49853,\"start\":49736},{\"end\":50016,\"start\":49946},{\"end\":50157,\"start\":50076},{\"end\":50459,\"start\":50288},{\"end\":51247,\"start\":50501},{\"end\":51543,\"start\":51249},{\"end\":51873,\"start\":51545},{\"end\":52621,\"start\":51886},{\"end\":53491,\"start\":52676},{\"end\":53963,\"start\":53612},{\"end\":54782,\"start\":54013},{\"end\":56826,\"start\":54849},{\"end\":57869,\"start\":56828},{\"end\":58179,\"start\":57871},{\"end\":60801,\"start\":58181},{\"end\":61096,\"start\":60891},{\"end\":61616,\"start\":61361},{\"end\":62157,\"start\":61682},{\"end\":62838,\"start\":62159},{\"end\":63549,\"start\":62840},{\"end\":64502,\"start\":63608},{\"end\":65147,\"start\":64619},{\"end\":65420,\"start\":65248},{\"end\":66266,\"start\":65480},{\"end\":66552,\"start\":66268},{\"end\":67432,\"start\":66554},{\"end\":67592,\"start\":67447},{\"end\":68619,\"start\":67594},{\"end\":69384,\"start\":68694},{\"end\":69470,\"start\":69386},{\"end\":69765,\"start\":69488},{\"end\":70830,\"start\":69767},{\"end\":71778,\"start\":70852},{\"end\":72506,\"start\":71802},{\"end\":72858,\"start\":72575},{\"end\":73904,\"start\":72873},{\"end\":74280,\"start\":73981},{\"end\":75430,\"start\":74282},{\"end\":76384,\"start\":75432},{\"end\":78207,\"start\":76386},{\"end\":78527,\"start\":78209},{\"end\":78666,\"start\":78551},{\"end\":80053,\"start\":78668},{\"end\":80856,\"start\":80055},{\"end\":81022,\"start\":80879},{\"end\":82746,\"start\":81024},{\"end\":83740,\"start\":82748},{\"end\":84841,\"start\":83742},{\"end\":84941,\"start\":84843},{\"end\":86967,\"start\":84943},{\"end\":87856,\"start\":86969},{\"end\":89896,\"start\":87858},{\"end\":90125,\"start\":89898},{\"end\":90172,\"start\":90127},{\"end\":91300,\"start\":90224},{\"end\":91397,\"start\":91348},{\"end\":93138,\"start\":91399},{\"end\":93338,\"start\":93209},{\"end\":93560,\"start\":93340},{\"end\":94331,\"start\":93624},{\"end\":94774,\"start\":94370},{\"end\":94941,\"start\":94817},{\"end\":95465,\"start\":94943},{\"end\":96098,\"start\":95510},{\"end\":96321,\"start\":96100},{\"end\":96468,\"start\":96352},{\"end\":97658,\"start\":96470},{\"end\":98218,\"start\":97757},{\"end\":98303,\"start\":98220},{\"end\":98382,\"start\":98305},{\"end\":98705,\"start\":98384},{\"end\":98857,\"start\":98753},{\"end\":99074,\"start\":98859},{\"end\":99486,\"start\":99170},{\"end\":99561,\"start\":99488},{\"end\":99861,\"start\":99641},{\"end\":101278,\"start\":99863},{\"end\":103635,\"start\":101350},{\"end\":104033,\"start\":103637},{\"end\":104463,\"start\":104035},{\"end\":105175,\"start\":104465},{\"end\":105630,\"start\":105177},{\"end\":106768,\"start\":105677},{\"end\":106890,\"start\":106827},{\"end\":108086,\"start\":106892},{\"end\":108460,\"start\":108088},{\"end\":108844,\"start\":108462},{\"end\":108934,\"start\":108846},{\"end\":109157,\"start\":108936},{\"end\":112008,\"start\":109159},{\"end\":112673,\"start\":112010},{\"end\":112864,\"start\":112675},{\"end\":113405,\"start\":112886},{\"end\":113936,\"start\":113407},{\"end\":114283,\"start\":113938},{\"end\":114360,\"start\":114325},{\"end\":115025,\"start\":114393},{\"end\":115981,\"start\":115027},{\"end\":116236,\"start\":115983},{\"end\":116941,\"start\":116238},{\"end\":117516,\"start\":116943},{\"end\":117691,\"start\":117518},{\"end\":117817,\"start\":117693},{\"end\":117989,\"start\":117819},{\"end\":118050,\"start\":117991},{\"end\":118165,\"start\":118061},{\"end\":118200,\"start\":118167},{\"end\":118483,\"start\":118334},{\"end\":118903,\"start\":118485},{\"end\":119001,\"start\":118956},{\"end\":120241,\"start\":119041},{\"end\":120995,\"start\":120243},{\"end\":121554,\"start\":121007},{\"end\":122981,\"start\":121610},{\"end\":124176,\"start\":122983},{\"end\":124970,\"start\":124178},{\"end\":125145,\"start\":124972},{\"end\":125387,\"start\":125147},{\"end\":125838,\"start\":125389},{\"end\":126195,\"start\":125918},{\"end\":126373,\"start\":126309},{\"end\":128079,\"start\":126427},{\"end\":128345,\"start\":128081},{\"end\":130031,\"start\":128392},{\"end\":130675,\"start\":130068},{\"end\":132254,\"start\":130677},{\"end\":132903,\"start\":132256},{\"end\":133827,\"start\":132949},{\"end\":134584,\"start\":133829},{\"end\":134856,\"start\":134586},{\"end\":136237,\"start\":134858},{\"end\":136583,\"start\":136239},{\"end\":137695,\"start\":136603},{\"end\":138604,\"start\":137697},{\"end\":139349,\"start\":138606},{\"end\":139891,\"start\":139351},{\"end\":140987,\"start\":139893},{\"end\":142744,\"start\":141009},{\"end\":145437,\"start\":142746},{\"end\":145714,\"start\":145439},{\"end\":147245,\"start\":145731},{\"end\":147514,\"start\":147247},{\"end\":148011,\"start\":147516},{\"end\":150441,\"start\":148013},{\"end\":151223,\"start\":150443},{\"end\":151761,\"start\":151225}]", "formula": "[{\"attributes\":{\"id\":\"formula_2\"},\"end\":10036,\"start\":9974},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10751,\"start\":10586},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11496,\"start\":11457},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11525,\"start\":11511},{\"attributes\":{\"id\":\"formula_9\"},\"end\":12261,\"start\":12240},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13037,\"start\":13008},{\"attributes\":{\"id\":\"formula_11\"},\"end\":13568,\"start\":13535},{\"attributes\":{\"id\":\"formula_13\"},\"end\":19197,\"start\":19077},{\"attributes\":{\"id\":\"formula_14\"},\"end\":19583,\"start\":19443},{\"attributes\":{\"id\":\"formula_15\"},\"end\":19762,\"start\":19741},{\"attributes\":{\"id\":\"formula_16\"},\"end\":20037,\"start\":20022},{\"attributes\":{\"id\":\"formula_17\"},\"end\":20172,\"start\":20142},{\"attributes\":{\"id\":\"formula_18\"},\"end\":20257,\"start\":20233},{\"attributes\":{\"id\":\"formula_19\"},\"end\":20580,\"start\":20559},{\"attributes\":{\"id\":\"formula_20\"},\"end\":20743,\"start\":20690},{\"attributes\":{\"id\":\"formula_21\"},\"end\":21005,\"start\":20975},{\"attributes\":{\"id\":\"formula_22\"},\"end\":21359,\"start\":21332},{\"attributes\":{\"id\":\"formula_23\"},\"end\":22137,\"start\":22095},{\"attributes\":{\"id\":\"formula_24\"},\"end\":22652,\"start\":22520},{\"attributes\":{\"id\":\"formula_25\"},\"end\":22791,\"start\":22759},{\"attributes\":{\"id\":\"formula_26\"},\"end\":22958,\"start\":22939},{\"attributes\":{\"id\":\"formula_27\"},\"end\":23185,\"start\":23141},{\"attributes\":{\"id\":\"formula_28\"},\"end\":23241,\"start\":23217},{\"attributes\":{\"id\":\"formula_29\"},\"end\":23520,\"start\":23491},{\"attributes\":{\"id\":\"formula_30\"},\"end\":24129,\"start\":24063},{\"attributes\":{\"id\":\"formula_31\"},\"end\":24339,\"start\":24244},{\"attributes\":{\"id\":\"formula_32\"},\"end\":25250,\"start\":25216},{\"attributes\":{\"id\":\"formula_33\"},\"end\":25752,\"start\":25718},{\"attributes\":{\"id\":\"formula_34\"},\"end\":26472,\"start\":26332},{\"attributes\":{\"id\":\"formula_35\"},\"end\":26773,\"start\":26730},{\"attributes\":{\"id\":\"formula_36\"},\"end\":27908,\"start\":27864},{\"attributes\":{\"id\":\"formula_37\"},\"end\":28166,\"start\":28090},{\"attributes\":{\"id\":\"formula_38\"},\"end\":28466,\"start\":28427},{\"attributes\":{\"id\":\"formula_39\"},\"end\":28931,\"start\":28909},{\"attributes\":{\"id\":\"formula_40\"},\"end\":30036,\"start\":29983},{\"attributes\":{\"id\":\"formula_41\"},\"end\":30167,\"start\":30128},{\"attributes\":{\"id\":\"formula_42\"},\"end\":30496,\"start\":30242},{\"attributes\":{\"id\":\"formula_43\"},\"end\":30772,\"start\":30708},{\"attributes\":{\"id\":\"formula_44\"},\"end\":31253,\"start\":31191},{\"attributes\":{\"id\":\"formula_45\"},\"end\":32267,\"start\":32198},{\"attributes\":{\"id\":\"formula_46\"},\"end\":32343,\"start\":32274},{\"attributes\":{\"id\":\"formula_47\"},\"end\":32605,\"start\":32565},{\"attributes\":{\"id\":\"formula_48\"},\"end\":33922,\"start\":33534},{\"attributes\":{\"id\":\"formula_49\"},\"end\":34413,\"start\":34248},{\"attributes\":{\"id\":\"formula_50\"},\"end\":38764,\"start\":38682},{\"attributes\":{\"id\":\"formula_51\"},\"end\":39676,\"start\":39506},{\"attributes\":{\"id\":\"formula_52\"},\"end\":40952,\"start\":40872},{\"attributes\":{\"id\":\"formula_53\"},\"end\":41114,\"start\":41081},{\"attributes\":{\"id\":\"formula_54\"},\"end\":42365,\"start\":42348},{\"attributes\":{\"id\":\"formula_55\"},\"end\":42956,\"start\":42901},{\"attributes\":{\"id\":\"formula_56\"},\"end\":44952,\"start\":44885},{\"attributes\":{\"id\":\"formula_57\"},\"end\":45465,\"start\":45426},{\"attributes\":{\"id\":\"formula_58\"},\"end\":45618,\"start\":45563},{\"attributes\":{\"id\":\"formula_59\"},\"end\":45832,\"start\":45792},{\"attributes\":{\"id\":\"formula_60\"},\"end\":48002,\"start\":47967},{\"attributes\":{\"id\":\"formula_61\"},\"end\":48596,\"start\":48544},{\"attributes\":{\"id\":\"formula_62\"},\"end\":48889,\"start\":48780},{\"attributes\":{\"id\":\"formula_63\"},\"end\":49239,\"start\":49096},{\"attributes\":{\"id\":\"formula_64\"},\"end\":49384,\"start\":49314},{\"attributes\":{\"id\":\"formula_65\"},\"end\":49642,\"start\":49599},{\"attributes\":{\"id\":\"formula_66\"},\"end\":49945,\"start\":49854},{\"attributes\":{\"id\":\"formula_67\"},\"end\":50075,\"start\":50017},{\"attributes\":{\"id\":\"formula_68\"},\"end\":50287,\"start\":50158},{\"attributes\":{\"id\":\"formula_69\"},\"end\":50500,\"start\":50460},{\"attributes\":{\"id\":\"formula_70\"},\"end\":53611,\"start\":53492},{\"attributes\":{\"id\":\"formula_71\"},\"end\":54012,\"start\":53964},{\"attributes\":{\"id\":\"formula_72\"},\"end\":54848,\"start\":54783},{\"attributes\":{\"id\":\"formula_73\"},\"end\":60890,\"start\":60848},{\"attributes\":{\"id\":\"formula_74\"},\"end\":61360,\"start\":61097},{\"attributes\":{\"id\":\"formula_75\"},\"end\":61681,\"start\":61617},{\"attributes\":{\"id\":\"formula_76\"},\"end\":64618,\"start\":64503},{\"attributes\":{\"id\":\"formula_77\"},\"end\":65247,\"start\":65148},{\"attributes\":{\"id\":\"formula_78\"},\"end\":65479,\"start\":65421},{\"attributes\":{\"id\":\"formula_79\"},\"end\":68693,\"start\":68654},{\"attributes\":{\"id\":\"formula_80\"},\"end\":80878,\"start\":80857},{\"attributes\":{\"id\":\"formula_81\"},\"end\":91347,\"start\":91301},{\"attributes\":{\"id\":\"formula_82\"},\"end\":93623,\"start\":93561},{\"attributes\":{\"id\":\"formula_83\"},\"end\":94369,\"start\":94332},{\"attributes\":{\"id\":\"formula_84\"},\"end\":94816,\"start\":94775},{\"attributes\":{\"id\":\"formula_85\"},\"end\":96351,\"start\":96322},{\"attributes\":{\"id\":\"formula_86\"},\"end\":98752,\"start\":98706},{\"attributes\":{\"id\":\"formula_87\"},\"end\":99169,\"start\":99075},{\"attributes\":{\"id\":\"formula_88\"},\"end\":105676,\"start\":105631},{\"attributes\":{\"id\":\"formula_89\"},\"end\":112885,\"start\":112865},{\"attributes\":{\"id\":\"formula_90\"},\"end\":114324,\"start\":114284},{\"attributes\":{\"id\":\"formula_91\"},\"end\":114392,\"start\":114361},{\"attributes\":{\"id\":\"formula_92\"},\"end\":118060,\"start\":118051},{\"attributes\":{\"id\":\"formula_93\"},\"end\":118226,\"start\":118201},{\"attributes\":{\"id\":\"formula_94\"},\"end\":118333,\"start\":118226},{\"attributes\":{\"id\":\"formula_95\"},\"end\":118955,\"start\":118904},{\"attributes\":{\"id\":\"formula_96\"},\"end\":119040,\"start\":119002},{\"attributes\":{\"id\":\"formula_97\"},\"end\":125917,\"start\":125839},{\"attributes\":{\"id\":\"formula_98\"},\"end\":126308,\"start\":126196},{\"attributes\":{\"id\":\"formula_99\"},\"end\":128391,\"start\":128346},{\"attributes\":{\"id\":\"formula_100\"},\"end\":132948,\"start\":132904}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":55430,\"start\":55423},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":57977,\"start\":57970},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":65686,\"start\":65679},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":77590,\"start\":77583},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":79389,\"start\":79382},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":80671,\"start\":80664},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":81128,\"start\":81121},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":81904,\"start\":81897},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":100245,\"start\":100238},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":100390,\"start\":100383},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":108568,\"start\":108561},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":116399,\"start\":116391},{\"end\":120322,\"start\":120315},{\"end\":121090,\"start\":121083},{\"end\":121136,\"start\":121129}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2078,\"start\":2066},{\"attributes\":{\"n\":\"2.\"},\"end\":5396,\"start\":5362},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5432,\"start\":5399},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8035,\"start\":8013},{\"attributes\":{\"n\":\"2.3.\"},\"end\":8816,\"start\":8777},{\"attributes\":{\"n\":\"2.4.\"},\"end\":9707,\"start\":9665},{\"attributes\":{\"n\":\"2.5.\"},\"end\":11593,\"start\":11555},{\"attributes\":{\"n\":\"2.6.\"},\"end\":14394,\"start\":14370},{\"attributes\":{\"n\":\"3.\"},\"end\":18655,\"start\":18611},{\"attributes\":{\"n\":\"3.2.\"},\"end\":20366,\"start\":20293},{\"attributes\":{\"n\":\"4.\"},\"end\":21641,\"start\":21615},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21991,\"start\":21968},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23812,\"start\":23782},{\"attributes\":{\"n\":\"4.3.\"},\"end\":25717,\"start\":25685},{\"attributes\":{\"n\":\"4.4.\"},\"end\":26681,\"start\":26599},{\"attributes\":{\"n\":\"4.5.\"},\"end\":29626,\"start\":29581},{\"attributes\":{\"n\":\"5.\"},\"end\":31946,\"start\":31916},{\"attributes\":{\"n\":\"5.1.\"},\"end\":33023,\"start\":32983},{\"attributes\":{\"n\":\"5.1.2.\"},\"end\":34769,\"start\":34734},{\"attributes\":{\"n\":\"5.1.3.\"},\"end\":35062,\"start\":35044},{\"attributes\":{\"n\":\"5.2.\"},\"end\":35499,\"start\":35460},{\"attributes\":{\"n\":\"5.2.1.\"},\"end\":36176,\"start\":36158},{\"attributes\":{\"n\":\"5.2.2.\"},\"end\":36379,\"start\":36344},{\"attributes\":{\"n\":\"5.2.3.\"},\"end\":36704,\"start\":36686},{\"attributes\":{\"n\":\"5.2.4.\"},\"end\":37045,\"start\":37035},{\"attributes\":{\"n\":\"5.3.\"},\"end\":37731,\"start\":37676},{\"attributes\":{\"n\":\"5.4.\"},\"end\":39228,\"start\":39197},{\"attributes\":{\"n\":\"6.\"},\"end\":39855,\"start\":39822},{\"attributes\":{\"n\":\"6.1.\"},\"end\":39928,\"start\":39858},{\"attributes\":{\"n\":\"6.2.\"},\"end\":40730,\"start\":40678},{\"attributes\":{\"n\":\"6.3.\"},\"end\":42615,\"start\":42595},{\"attributes\":{\"n\":\"7.\"},\"end\":43284,\"start\":43256},{\"attributes\":{\"n\":\"7.1.\"},\"end\":43304,\"start\":43287},{\"attributes\":{\"n\":\"7.2.\"},\"end\":45350,\"start\":45311},{\"attributes\":{\"n\":\"7.3.\"},\"end\":47966,\"start\":47928},{\"attributes\":{\"n\":\"8.\"},\"end\":51884,\"start\":51876},{\"attributes\":{\"n\":\"8.1.\"},\"end\":52674,\"start\":52624},{\"attributes\":{\"n\":\"8.2.\"},\"end\":60847,\"start\":60804},{\"attributes\":{\"n\":\"8.3.\"},\"end\":63606,\"start\":63552},{\"attributes\":{\"n\":\"9.\"},\"end\":67445,\"start\":67435},{\"attributes\":{\"n\":\"9.1.2.\"},\"end\":68653,\"start\":68622},{\"attributes\":{\"n\":\"9.1.3.\"},\"end\":69486,\"start\":69473},{\"attributes\":{\"n\":\"9.2.2.\"},\"end\":70850,\"start\":70833},{\"attributes\":{\"n\":\"9.2.3.\"},\"end\":71800,\"start\":71781},{\"attributes\":{\"n\":\"9.2.4.\"},\"end\":72548,\"start\":72509},{\"attributes\":{\"n\":\"9.2.5.\"},\"end\":72573,\"start\":72551},{\"attributes\":{\"n\":\"9.3.\"},\"end\":72871,\"start\":72861},{\"end\":73979,\"start\":73907},{\"end\":78549,\"start\":78530},{\"end\":90222,\"start\":90175},{\"end\":93207,\"start\":93141},{\"end\":95508,\"start\":95468},{\"end\":97755,\"start\":97661},{\"end\":99639,\"start\":99564},{\"end\":101348,\"start\":101281},{\"end\":106825,\"start\":106771},{\"end\":121005,\"start\":120998},{\"end\":121608,\"start\":121557},{\"end\":126425,\"start\":126376},{\"end\":130066,\"start\":130034},{\"end\":136601,\"start\":136586},{\"end\":141007,\"start\":140990},{\"end\":145729,\"start\":145717},{\"end\":151771,\"start\":151763},{\"end\":152333,\"start\":152325},{\"end\":152634,\"start\":152626},{\"end\":153126,\"start\":153118},{\"end\":153357,\"start\":153349},{\"end\":153518,\"start\":153510},{\"end\":154561,\"start\":154552},{\"end\":155269,\"start\":155260},{\"end\":155541,\"start\":155532},{\"end\":155931,\"start\":155922},{\"end\":157436,\"start\":157427}]", "table": "[{\"end\":155258,\"start\":154803},{\"end\":155530,\"start\":155311},{\"end\":155920,\"start\":155605},{\"end\":157425,\"start\":156159},{\"end\":157977,\"start\":157512},{\"end\":158211,\"start\":158005}]", "figure_caption": "[{\"end\":152100,\"start\":151773},{\"end\":152323,\"start\":152103},{\"end\":152624,\"start\":152335},{\"end\":153116,\"start\":152636},{\"end\":153347,\"start\":153128},{\"end\":153508,\"start\":153359},{\"end\":153713,\"start\":153520},{\"end\":154300,\"start\":153716},{\"end\":154362,\"start\":154303},{\"end\":154550,\"start\":154365},{\"end\":154803,\"start\":154563},{\"end\":155311,\"start\":155271},{\"end\":155605,\"start\":155543},{\"end\":156159,\"start\":155933},{\"end\":157512,\"start\":157438},{\"end\":158005,\"start\":157980}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34016,\"start\":34010},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":35902,\"start\":35896},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":58226,\"start\":58220},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":66811,\"start\":66805},{\"end\":76458,\"start\":76452},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":102017,\"start\":102011},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":102430,\"start\":102421},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":102553,\"start\":102547},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":103004,\"start\":102998},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":103359,\"start\":103350},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":103534,\"start\":103525},{\"end\":116736,\"start\":116728},{\"end\":120679,\"start\":120676},{\"end\":121127,\"start\":121121},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":140385,\"start\":140379},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":140986,\"start\":140980},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":145568,\"start\":145562}]", "bib_author_first_name": "[{\"end\":163539,\"start\":163538},{\"end\":163919,\"start\":163918},{\"end\":163921,\"start\":163920},{\"end\":163932,\"start\":163931},{\"end\":163934,\"start\":163933},{\"end\":164162,\"start\":164161},{\"end\":164164,\"start\":164163},{\"end\":164363,\"start\":164362},{\"end\":164365,\"start\":164364},{\"end\":164487,\"start\":164486},{\"end\":164489,\"start\":164488},{\"end\":164501,\"start\":164500},{\"end\":164505,\"start\":164502},{\"end\":164698,\"start\":164697},{\"end\":164944,\"start\":164943},{\"end\":164954,\"start\":164953},{\"end\":165122,\"start\":165121},{\"end\":165126,\"start\":165123},{\"end\":165319,\"start\":165318},{\"end\":165321,\"start\":165320},{\"end\":165332,\"start\":165331},{\"end\":165334,\"start\":165333},{\"end\":165563,\"start\":165562},{\"end\":165565,\"start\":165564},{\"end\":165767,\"start\":165766},{\"end\":165776,\"start\":165775},{\"end\":165969,\"start\":165968},{\"end\":165971,\"start\":165970},{\"end\":165982,\"start\":165981},{\"end\":165984,\"start\":165983},{\"end\":166159,\"start\":166158},{\"end\":166161,\"start\":166160},{\"end\":166171,\"start\":166170},{\"end\":166173,\"start\":166172},{\"end\":166406,\"start\":166405},{\"end\":166414,\"start\":166413},{\"end\":166630,\"start\":166629},{\"end\":166632,\"start\":166631},{\"end\":166643,\"start\":166642},{\"end\":166874,\"start\":166873},{\"end\":166881,\"start\":166875},{\"end\":166893,\"start\":166892},{\"end\":166914,\"start\":166913},{\"end\":167585,\"start\":167584},{\"end\":167825,\"start\":167824},{\"end\":167836,\"start\":167835},{\"end\":167845,\"start\":167844},{\"end\":168109,\"start\":168108},{\"end\":168120,\"start\":168119},{\"end\":168129,\"start\":168128},{\"end\":168334,\"start\":168333},{\"end\":168336,\"start\":168335},{\"end\":168516,\"start\":168515},{\"end\":168526,\"start\":168525},{\"end\":168751,\"start\":168750},{\"end\":168766,\"start\":168765},{\"end\":168982,\"start\":168981},{\"end\":168984,\"start\":168983},{\"end\":168995,\"start\":168994},{\"end\":168997,\"start\":168996},{\"end\":169264,\"start\":169263},{\"end\":169266,\"start\":169265},{\"end\":169277,\"start\":169276},{\"end\":169279,\"start\":169278},{\"end\":169288,\"start\":169287},{\"end\":169304,\"start\":169303},{\"end\":169306,\"start\":169305},{\"end\":169320,\"start\":169319},{\"end\":169322,\"start\":169321},{\"end\":169638,\"start\":169637},{\"end\":169649,\"start\":169648},{\"end\":169900,\"start\":169899},{\"end\":169902,\"start\":169901},{\"end\":169913,\"start\":169912},{\"end\":170269,\"start\":170268},{\"end\":170271,\"start\":170270},{\"end\":170280,\"start\":170279},{\"end\":170294,\"start\":170293},{\"end\":170296,\"start\":170295},{\"end\":170537,\"start\":170536},{\"end\":170539,\"start\":170538},{\"end\":170548,\"start\":170547},{\"end\":170550,\"start\":170549},{\"end\":170558,\"start\":170557},{\"end\":170571,\"start\":170570},{\"end\":170813,\"start\":170812},{\"end\":170815,\"start\":170814},{\"end\":171004,\"start\":171003},{\"end\":171013,\"start\":171012},{\"end\":171245,\"start\":171244},{\"end\":171252,\"start\":171251},{\"end\":171446,\"start\":171445},{\"end\":171456,\"start\":171455},{\"end\":171668,\"start\":171667},{\"end\":171678,\"start\":171677},{\"end\":171923,\"start\":171922},{\"end\":171925,\"start\":171924},{\"end\":172096,\"start\":172095},{\"end\":172104,\"start\":172103},{\"end\":172281,\"start\":172280},{\"end\":172283,\"start\":172282},{\"end\":172290,\"start\":172289},{\"end\":172292,\"start\":172291},{\"end\":172304,\"start\":172303},{\"end\":172308,\"start\":172305},{\"end\":172756,\"start\":172755},{\"end\":172769,\"start\":172768},{\"end\":172997,\"start\":172996},{\"end\":173009,\"start\":173008},{\"end\":173011,\"start\":173010},{\"end\":173193,\"start\":173192},{\"end\":173195,\"start\":173194},{\"end\":173204,\"start\":173203},{\"end\":173206,\"start\":173205},{\"end\":173363,\"start\":173362},{\"end\":173365,\"start\":173364},{\"end\":173373,\"start\":173372},{\"end\":173375,\"start\":173374},{\"end\":173572,\"start\":173571},{\"end\":173579,\"start\":173578},{\"end\":173581,\"start\":173580},{\"end\":174004,\"start\":174003},{\"end\":174006,\"start\":174005},{\"end\":174017,\"start\":174016},{\"end\":174021,\"start\":174018},{\"end\":174302,\"start\":174301},{\"end\":174306,\"start\":174303},{\"end\":174624,\"start\":174623},{\"end\":174637,\"start\":174636},{\"end\":174845,\"start\":174841},{\"end\":174853,\"start\":174852},{\"end\":174855,\"start\":174854},{\"end\":175125,\"start\":175124},{\"end\":175127,\"start\":175126},{\"end\":175648,\"start\":175647},{\"end\":175658,\"start\":175657},{\"end\":175671,\"start\":175670},{\"end\":175966,\"start\":175965},{\"end\":175979,\"start\":175978},{\"end\":175981,\"start\":175980},{\"end\":176187,\"start\":176186},{\"end\":176189,\"start\":176188},{\"end\":176427,\"start\":176426},{\"end\":176429,\"start\":176428},{\"end\":176439,\"start\":176438},{\"end\":176441,\"start\":176440},{\"end\":176454,\"start\":176453},{\"end\":176456,\"start\":176455},{\"end\":176468,\"start\":176467},{\"end\":176761,\"start\":176760},{\"end\":176775,\"start\":176774},{\"end\":176777,\"start\":176776},{\"end\":176976,\"start\":176975},{\"end\":176978,\"start\":176977},{\"end\":177231,\"start\":177230},{\"end\":177403,\"start\":177402},{\"end\":177599,\"start\":177598},{\"end\":177861,\"start\":177860},{\"end\":177863,\"start\":177862},{\"end\":177880,\"start\":177879},{\"end\":177890,\"start\":177889},{\"end\":177892,\"start\":177891},{\"end\":178098,\"start\":178097},{\"end\":178100,\"start\":178099},{\"end\":178117,\"start\":178116},{\"end\":178127,\"start\":178126},{\"end\":178129,\"start\":178128},{\"end\":178137,\"start\":178136},{\"end\":178139,\"start\":178138},{\"end\":178494,\"start\":178493},{\"end\":178745,\"start\":178744},{\"end\":178972,\"start\":178971},{\"end\":178983,\"start\":178982},{\"end\":179402,\"start\":179401},{\"end\":180078,\"start\":180077},{\"end\":180314,\"start\":180313},{\"end\":180320,\"start\":180319},{\"end\":180663,\"start\":180662},{\"end\":180665,\"start\":180664},{\"end\":180674,\"start\":180673},{\"end\":180676,\"start\":180675},{\"end\":180963,\"start\":180962},{\"end\":180970,\"start\":180969},{\"end\":181267,\"start\":181266},{\"end\":181473,\"start\":181472},{\"end\":181884,\"start\":181883},{\"end\":181886,\"start\":181885},{\"end\":182143,\"start\":182142},{\"end\":182145,\"start\":182144},{\"end\":182157,\"start\":182156},{\"end\":182963,\"start\":182962},{\"end\":182965,\"start\":182964},{\"end\":182977,\"start\":182976},{\"end\":182981,\"start\":182978},{\"end\":183121,\"start\":183120},{\"end\":183123,\"start\":183122},{\"end\":183135,\"start\":183134},{\"end\":183369,\"start\":183368},{\"end\":183371,\"start\":183370},{\"end\":183382,\"start\":183381},{\"end\":183384,\"start\":183383},{\"end\":183849,\"start\":183848},{\"end\":183860,\"start\":183859},{\"end\":183862,\"start\":183861},{\"end\":183872,\"start\":183871},{\"end\":183874,\"start\":183873},{\"end\":184132,\"start\":184131},{\"end\":184142,\"start\":184141},{\"end\":184150,\"start\":184149},{\"end\":184152,\"start\":184151},{\"end\":184372,\"start\":184371},{\"end\":184374,\"start\":184373},{\"end\":184533,\"start\":184532},{\"end\":184535,\"start\":184534},{\"end\":184545,\"start\":184544},{\"end\":184547,\"start\":184546},{\"end\":184556,\"start\":184555},{\"end\":184558,\"start\":184557},{\"end\":184571,\"start\":184570},{\"end\":184573,\"start\":184572},{\"end\":184840,\"start\":184839},{\"end\":184842,\"start\":184841},{\"end\":184853,\"start\":184852},{\"end\":184855,\"start\":184854},{\"end\":185233,\"start\":185232},{\"end\":185237,\"start\":185234},{\"end\":185246,\"start\":185245},{\"end\":185248,\"start\":185247},{\"end\":185256,\"start\":185255},{\"end\":185258,\"start\":185257},{\"end\":186350,\"start\":186349},{\"end\":186793,\"start\":186792},{\"end\":186803,\"start\":186802},{\"end\":187056,\"start\":187055},{\"end\":187066,\"start\":187065},{\"end\":187068,\"start\":187067},{\"end\":187304,\"start\":187303},{\"end\":187484,\"start\":187483},{\"end\":187497,\"start\":187496},{\"end\":187694,\"start\":187693},{\"end\":187705,\"start\":187704},{\"end\":188009,\"start\":188008},{\"end\":188011,\"start\":188010},{\"end\":188421,\"start\":188420},{\"end\":188423,\"start\":188422},{\"end\":188434,\"start\":188433},{\"end\":188436,\"start\":188435},{\"end\":188443,\"start\":188442},{\"end\":188960,\"start\":188959},{\"end\":188973,\"start\":188969},{\"end\":188981,\"start\":188980},{\"end\":189195,\"start\":189194},{\"end\":189197,\"start\":189196},{\"end\":189348,\"start\":189347},{\"end\":189357,\"start\":189356},{\"end\":189608,\"start\":189607},{\"end\":189618,\"start\":189617},{\"end\":189843,\"start\":189842},{\"end\":189853,\"start\":189852},{\"end\":190244,\"start\":190243},{\"end\":190252,\"start\":190251},{\"end\":190429,\"start\":190428},{\"end\":190431,\"start\":190430},{\"end\":190438,\"start\":190437},{\"end\":190440,\"start\":190439},{\"end\":190452,\"start\":190451},{\"end\":190456,\"start\":190453},{\"end\":190894,\"start\":190893},{\"end\":191174,\"start\":191173},{\"end\":191182,\"start\":191181},{\"end\":191184,\"start\":191183},{\"end\":191363,\"start\":191362},{\"end\":191374,\"start\":191373},{\"end\":191636,\"start\":191635},{\"end\":191638,\"start\":191637},{\"end\":191651,\"start\":191650},{\"end\":191653,\"start\":191652},{\"end\":191987,\"start\":191986},{\"end\":191989,\"start\":191988},{\"end\":192283,\"start\":192282},{\"end\":192290,\"start\":192289},{\"end\":192292,\"start\":192291},{\"end\":192758,\"start\":192757},{\"end\":192769,\"start\":192768},{\"end\":193019,\"start\":193018},{\"end\":193030,\"start\":193029},{\"end\":193032,\"start\":193031},{\"end\":193390,\"start\":193389},{\"end\":193402,\"start\":193401},{\"end\":193739,\"start\":193735},{\"end\":193747,\"start\":193746},{\"end\":193749,\"start\":193748},{\"end\":194030,\"start\":194029},{\"end\":194040,\"start\":194039},{\"end\":194042,\"start\":194041},{\"end\":194054,\"start\":194053},{\"end\":194350,\"start\":194349},{\"end\":194359,\"start\":194358},{\"end\":194361,\"start\":194360},{\"end\":194371,\"start\":194370},{\"end\":194373,\"start\":194372},{\"end\":194387,\"start\":194386},{\"end\":194619,\"start\":194618},{\"end\":194621,\"start\":194620},{\"end\":194633,\"start\":194632},{\"end\":194830,\"start\":194829},{\"end\":194976,\"start\":194975},{\"end\":194978,\"start\":194977},{\"end\":195156,\"start\":195155},{\"end\":195158,\"start\":195157},{\"end\":195308,\"start\":195307},{\"end\":195470,\"start\":195469},{\"end\":195481,\"start\":195480},{\"end\":195483,\"start\":195482},{\"end\":195957,\"start\":195956},{\"end\":195959,\"start\":195958},{\"end\":196351,\"start\":196350},{\"end\":196881,\"start\":196880},{\"end\":197212,\"start\":197211},{\"end\":197223,\"start\":197222},{\"end\":197695,\"start\":197694},{\"end\":197697,\"start\":197696},{\"end\":197709,\"start\":197708},{\"end\":197711,\"start\":197710},{\"end\":197917,\"start\":197916},{\"end\":198121,\"start\":198120},{\"end\":198386,\"start\":198385},{\"end\":198393,\"start\":198392}]", "bib_author_last_name": "[{\"end\":163546,\"start\":163540},{\"end\":163929,\"start\":163922},{\"end\":163942,\"start\":163935},{\"end\":164169,\"start\":164165},{\"end\":164374,\"start\":164366},{\"end\":164498,\"start\":164490},{\"end\":164511,\"start\":164506},{\"end\":164704,\"start\":164699},{\"end\":164951,\"start\":164945},{\"end\":164963,\"start\":164955},{\"end\":165130,\"start\":165127},{\"end\":165329,\"start\":165322},{\"end\":165342,\"start\":165335},{\"end\":165574,\"start\":165566},{\"end\":165773,\"start\":165768},{\"end\":165784,\"start\":165777},{\"end\":165979,\"start\":165972},{\"end\":165993,\"start\":165985},{\"end\":166168,\"start\":166162},{\"end\":166179,\"start\":166174},{\"end\":166411,\"start\":166407},{\"end\":166424,\"start\":166415},{\"end\":166640,\"start\":166633},{\"end\":166650,\"start\":166644},{\"end\":166890,\"start\":166882},{\"end\":166911,\"start\":166894},{\"end\":166923,\"start\":166915},{\"end\":166930,\"start\":166925},{\"end\":167591,\"start\":167586},{\"end\":167833,\"start\":167826},{\"end\":167842,\"start\":167837},{\"end\":167852,\"start\":167846},{\"end\":168117,\"start\":168110},{\"end\":168126,\"start\":168121},{\"end\":168138,\"start\":168130},{\"end\":168343,\"start\":168337},{\"end\":168523,\"start\":168517},{\"end\":168532,\"start\":168527},{\"end\":168763,\"start\":168752},{\"end\":168772,\"start\":168767},{\"end\":168992,\"start\":168985},{\"end\":169001,\"start\":168998},{\"end\":169274,\"start\":169267},{\"end\":169285,\"start\":169280},{\"end\":169301,\"start\":169289},{\"end\":169317,\"start\":169307},{\"end\":169331,\"start\":169323},{\"end\":169646,\"start\":169639},{\"end\":169655,\"start\":169650},{\"end\":169910,\"start\":169903},{\"end\":169923,\"start\":169914},{\"end\":170277,\"start\":170272},{\"end\":170291,\"start\":170281},{\"end\":170310,\"start\":170297},{\"end\":170545,\"start\":170540},{\"end\":170555,\"start\":170551},{\"end\":170568,\"start\":170559},{\"end\":170579,\"start\":170572},{\"end\":170820,\"start\":170816},{\"end\":171010,\"start\":171005},{\"end\":171024,\"start\":171014},{\"end\":171249,\"start\":171246},{\"end\":171259,\"start\":171253},{\"end\":171453,\"start\":171447},{\"end\":171467,\"start\":171457},{\"end\":171675,\"start\":171669},{\"end\":171686,\"start\":171679},{\"end\":171931,\"start\":171926},{\"end\":172101,\"start\":172097},{\"end\":172112,\"start\":172105},{\"end\":172287,\"start\":172284},{\"end\":172301,\"start\":172293},{\"end\":172314,\"start\":172309},{\"end\":172766,\"start\":172757},{\"end\":172775,\"start\":172770},{\"end\":173006,\"start\":172998},{\"end\":173019,\"start\":173012},{\"end\":173201,\"start\":173196},{\"end\":173211,\"start\":173207},{\"end\":173370,\"start\":173366},{\"end\":173383,\"start\":173376},{\"end\":173576,\"start\":173573},{\"end\":173588,\"start\":173582},{\"end\":174014,\"start\":174007},{\"end\":174027,\"start\":174022},{\"end\":174313,\"start\":174307},{\"end\":174634,\"start\":174625},{\"end\":174644,\"start\":174638},{\"end\":174850,\"start\":174846},{\"end\":174861,\"start\":174856},{\"end\":175133,\"start\":175128},{\"end\":175655,\"start\":175649},{\"end\":175668,\"start\":175659},{\"end\":175677,\"start\":175672},{\"end\":175976,\"start\":175967},{\"end\":175986,\"start\":175982},{\"end\":176201,\"start\":176190},{\"end\":176436,\"start\":176430},{\"end\":176451,\"start\":176442},{\"end\":176465,\"start\":176457},{\"end\":176475,\"start\":176469},{\"end\":176772,\"start\":176762},{\"end\":176783,\"start\":176778},{\"end\":176985,\"start\":176979},{\"end\":177236,\"start\":177232},{\"end\":177411,\"start\":177404},{\"end\":177605,\"start\":177600},{\"end\":177877,\"start\":177864},{\"end\":177887,\"start\":177881},{\"end\":177897,\"start\":177893},{\"end\":178114,\"start\":178101},{\"end\":178124,\"start\":178118},{\"end\":178134,\"start\":178130},{\"end\":178145,\"start\":178140},{\"end\":178500,\"start\":178495},{\"end\":178754,\"start\":178746},{\"end\":178980,\"start\":178973},{\"end\":178992,\"start\":178984},{\"end\":179408,\"start\":179403},{\"end\":179660,\"start\":179652},{\"end\":180081,\"start\":180079},{\"end\":180317,\"start\":180315},{\"end\":180325,\"start\":180321},{\"end\":180671,\"start\":180666},{\"end\":180682,\"start\":180677},{\"end\":180967,\"start\":180964},{\"end\":180977,\"start\":180971},{\"end\":181274,\"start\":181268},{\"end\":181480,\"start\":181474},{\"end\":181895,\"start\":181887},{\"end\":182154,\"start\":182146},{\"end\":182163,\"start\":182158},{\"end\":182974,\"start\":182966},{\"end\":182987,\"start\":182982},{\"end\":183132,\"start\":183124},{\"end\":183142,\"start\":183136},{\"end\":183379,\"start\":183372},{\"end\":183393,\"start\":183385},{\"end\":183857,\"start\":183850},{\"end\":183869,\"start\":183863},{\"end\":183880,\"start\":183875},{\"end\":184139,\"start\":184133},{\"end\":184147,\"start\":184143},{\"end\":184159,\"start\":184153},{\"end\":184380,\"start\":184375},{\"end\":184542,\"start\":184536},{\"end\":184553,\"start\":184548},{\"end\":184568,\"start\":184559},{\"end\":184587,\"start\":184574},{\"end\":184850,\"start\":184843},{\"end\":184860,\"start\":184856},{\"end\":185243,\"start\":185238},{\"end\":185253,\"start\":185249},{\"end\":185266,\"start\":185259},{\"end\":185272,\"start\":185268},{\"end\":186357,\"start\":186351},{\"end\":186800,\"start\":186794},{\"end\":186813,\"start\":186804},{\"end\":187063,\"start\":187057},{\"end\":187075,\"start\":187069},{\"end\":187310,\"start\":187305},{\"end\":187494,\"start\":187485},{\"end\":187504,\"start\":187498},{\"end\":187702,\"start\":187695},{\"end\":187713,\"start\":187706},{\"end\":188019,\"start\":188012},{\"end\":188431,\"start\":188424},{\"end\":188440,\"start\":188437},{\"end\":188449,\"start\":188444},{\"end\":188967,\"start\":188961},{\"end\":188978,\"start\":188974},{\"end\":188987,\"start\":188982},{\"end\":189202,\"start\":189198},{\"end\":189354,\"start\":189349},{\"end\":189368,\"start\":189358},{\"end\":189615,\"start\":189609},{\"end\":189626,\"start\":189619},{\"end\":189850,\"start\":189844},{\"end\":189861,\"start\":189854},{\"end\":190249,\"start\":190245},{\"end\":190260,\"start\":190253},{\"end\":190435,\"start\":190432},{\"end\":190449,\"start\":190441},{\"end\":190462,\"start\":190457},{\"end\":190899,\"start\":190895},{\"end\":191179,\"start\":191175},{\"end\":191191,\"start\":191185},{\"end\":191371,\"start\":191364},{\"end\":191383,\"start\":191375},{\"end\":191648,\"start\":191639},{\"end\":191667,\"start\":191654},{\"end\":191996,\"start\":191990},{\"end\":192287,\"start\":192284},{\"end\":192299,\"start\":192293},{\"end\":192766,\"start\":192759},{\"end\":192777,\"start\":192770},{\"end\":193027,\"start\":193020},{\"end\":193040,\"start\":193033},{\"end\":193399,\"start\":193391},{\"end\":193411,\"start\":193403},{\"end\":193744,\"start\":193740},{\"end\":193755,\"start\":193750},{\"end\":194037,\"start\":194031},{\"end\":194051,\"start\":194043},{\"end\":194061,\"start\":194055},{\"end\":194356,\"start\":194351},{\"end\":194368,\"start\":194362},{\"end\":194384,\"start\":194374},{\"end\":194397,\"start\":194388},{\"end\":194630,\"start\":194622},{\"end\":194640,\"start\":194634},{\"end\":194838,\"start\":194831},{\"end\":194988,\"start\":194979},{\"end\":195165,\"start\":195159},{\"end\":195313,\"start\":195309},{\"end\":195478,\"start\":195471},{\"end\":195489,\"start\":195484},{\"end\":195965,\"start\":195960},{\"end\":196357,\"start\":196352},{\"end\":196889,\"start\":196882},{\"end\":197220,\"start\":197213},{\"end\":197232,\"start\":197224},{\"end\":197706,\"start\":197698},{\"end\":197719,\"start\":197712},{\"end\":197926,\"start\":197918},{\"end\":198124,\"start\":198122},{\"end\":198390,\"start\":198387},{\"end\":198400,\"start\":198394}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":64903870},\"end\":163876,\"start\":163465},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":44372930},\"end\":164084,\"start\":163878},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":120760010},\"end\":164318,\"start\":164086},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":121507326},\"end\":164482,\"start\":164320},{\"attributes\":{\"id\":\"b4\"},\"end\":164608,\"start\":164484},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":42087677},\"end\":164870,\"start\":164610},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2027012},\"end\":165117,\"start\":164872},{\"attributes\":{\"id\":\"b7\"},\"end\":165258,\"start\":165119},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":17109035},\"end\":165497,\"start\":165260},{\"attributes\":{\"id\":\"b9\"},\"end\":165691,\"start\":165499},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":120452626},\"end\":165935,\"start\":165693},{\"attributes\":{\"id\":\"b11\"},\"end\":166103,\"start\":165937},{\"attributes\":{\"id\":\"b12\"},\"end\":166363,\"start\":166105},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":5312061},\"end\":166538,\"start\":166365},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":21068137},\"end\":166816,\"start\":166540},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":29853574},\"end\":167521,\"start\":166818},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":31129752},\"end\":167727,\"start\":167523},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":10844140},\"end\":168037,\"start\":167729},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":16843854},\"end\":168296,\"start\":168039},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":125752833},\"end\":168461,\"start\":168298},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":120796008},\"end\":168677,\"start\":168463},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":120357497},\"end\":168920,\"start\":168679},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":120688106},\"end\":169159,\"start\":168922},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":78952140},\"end\":169575,\"start\":169161},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":4342641},\"end\":169785,\"start\":169577},{\"attributes\":{\"id\":\"b25\"},\"end\":170228,\"start\":169787},{\"attributes\":{\"id\":\"b26\"},\"end\":170468,\"start\":170230},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":41350623},\"end\":170749,\"start\":170470},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":123348874},\"end\":170959,\"start\":170751},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10775641},\"end\":171178,\"start\":170961},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":436439},\"end\":171414,\"start\":171180},{\"attributes\":{\"id\":\"b31\"},\"end\":171582,\"start\":171416},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":123098948},\"end\":171843,\"start\":171584},{\"attributes\":{\"id\":\"b33\"},\"end\":172056,\"start\":171845},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":17191147},\"end\":172240,\"start\":172058},{\"attributes\":{\"id\":\"b35\"},\"end\":172658,\"start\":172242},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":120654716},\"end\":172962,\"start\":172660},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":116908168},\"end\":173145,\"start\":172964},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":15404916},\"end\":173332,\"start\":173147},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":15725649},\"end\":173511,\"start\":173334},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":123501507},\"end\":173945,\"start\":173513},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":116156932},\"end\":174184,\"start\":173947},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":14332165},\"end\":174594,\"start\":174186},{\"attributes\":{\"id\":\"b43\"},\"end\":174772,\"start\":174596},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":120497430},\"end\":175006,\"start\":174774},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":609306},\"end\":175538,\"start\":175008},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":9431299},\"end\":175899,\"start\":175540},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":120565888},\"end\":176146,\"start\":175901},{\"attributes\":{\"id\":\"b48\"},\"end\":176366,\"start\":176148},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":11315658},\"end\":176669,\"start\":176368},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":7980390},\"end\":176973,\"start\":176671},{\"attributes\":{\"id\":\"b51\"},\"end\":177163,\"start\":176975},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":153686379},\"end\":177363,\"start\":177165},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":123722079},\"end\":177510,\"start\":177365},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":121797474},\"end\":177769,\"start\":177512},{\"attributes\":{\"id\":\"b55\"},\"end\":178095,\"start\":177771},{\"attributes\":{\"id\":\"b56\"},\"end\":178402,\"start\":178097},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":116699031},\"end\":178666,\"start\":178404},{\"attributes\":{\"id\":\"b58\"},\"end\":178917,\"start\":178668},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":5322538},\"end\":179116,\"start\":178919},{\"attributes\":{\"id\":\"b60\"},\"end\":179300,\"start\":179118},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":116740162},\"end\":179586,\"start\":179302},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":115238658},\"end\":179796,\"start\":179588},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":121858740},\"end\":179999,\"start\":179798},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":42810613},\"end\":180230,\"start\":180001},{\"attributes\":{\"id\":\"b65\"},\"end\":180586,\"start\":180232},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":117443753},\"end\":180850,\"start\":180588},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":6531539},\"end\":181221,\"start\":180852},{\"attributes\":{\"id\":\"b68\"},\"end\":181397,\"start\":181223},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":64903870},\"end\":181810,\"start\":181399},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":120171442},\"end\":182029,\"start\":181812},{\"attributes\":{\"doi\":\"LSERR73\",\"id\":\"b71\",\"matched_paper_id\":121610543},\"end\":182958,\"start\":182031},{\"attributes\":{\"id\":\"b72\"},\"end\":183078,\"start\":182960},{\"attributes\":{\"id\":\"b73\"},\"end\":183291,\"start\":183080},{\"attributes\":{\"id\":\"b74\"},\"end\":183547,\"start\":183293},{\"attributes\":{\"id\":\"b75\"},\"end\":183776,\"start\":183549},{\"attributes\":{\"id\":\"b76\"},\"end\":184048,\"start\":183778},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":16330826},\"end\":184345,\"start\":184050},{\"attributes\":{\"id\":\"b78\"},\"end\":184487,\"start\":184347},{\"attributes\":{\"id\":\"b79\"},\"end\":184745,\"start\":184489},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":122704354},\"end\":185141,\"start\":184747},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":116100840},\"end\":186284,\"start\":185143},{\"attributes\":{\"id\":\"b82\"},\"end\":186687,\"start\":186286},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":16766936},\"end\":187006,\"start\":186689},{\"attributes\":{\"id\":\"b84\"},\"end\":187240,\"start\":187008},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":31129752},\"end\":187446,\"start\":187242},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":15250376},\"end\":187647,\"start\":187448},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":64436645},\"end\":187956,\"start\":187649},{\"attributes\":{\"id\":\"b88\"},\"end\":188301,\"start\":187958},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":61181247},\"end\":188862,\"start\":188303},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":17309196},\"end\":189172,\"start\":188864},{\"attributes\":{\"id\":\"b91\"},\"end\":189303,\"start\":189174},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":10775641},\"end\":189522,\"start\":189305},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":123098948},\"end\":189783,\"start\":189524},{\"attributes\":{\"id\":\"b94\",\"matched_paper_id\":7147711},\"end\":190204,\"start\":189785},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":17191147},\"end\":190388,\"start\":190206},{\"attributes\":{\"id\":\"b96\"},\"end\":190806,\"start\":190390},{\"attributes\":{\"id\":\"b97\"},\"end\":191129,\"start\":190808},{\"attributes\":{\"id\":\"b98\"},\"end\":191307,\"start\":191131},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":16742755},\"end\":191512,\"start\":191309},{\"attributes\":{\"id\":\"b100\",\"matched_paper_id\":58792451},\"end\":191899,\"start\":191514},{\"attributes\":{\"id\":\"b101\",\"matched_paper_id\":21865142},\"end\":192149,\"start\":191901},{\"attributes\":{\"id\":\"b102\",\"matched_paper_id\":121975514},\"end\":192718,\"start\":192151},{\"attributes\":{\"id\":\"b103\",\"matched_paper_id\":123009278},\"end\":192921,\"start\":192720},{\"attributes\":{\"doi\":\"213\",\"id\":\"b104\"},\"end\":193333,\"start\":192923},{\"attributes\":{\"id\":\"b105\",\"matched_paper_id\":18600320},\"end\":193666,\"start\":193335},{\"attributes\":{\"id\":\"b106\",\"matched_paper_id\":120497430},\"end\":193898,\"start\":193668},{\"attributes\":{\"id\":\"b107\"},\"end\":194312,\"start\":193900},{\"attributes\":{\"id\":\"b108\"},\"end\":194560,\"start\":194314},{\"attributes\":{\"id\":\"b109\",\"matched_paper_id\":120036967},\"end\":194786,\"start\":194562},{\"attributes\":{\"id\":\"b110\"},\"end\":194973,\"start\":194788},{\"attributes\":{\"id\":\"b111\"},\"end\":195127,\"start\":194975},{\"attributes\":{\"id\":\"b112\"},\"end\":195256,\"start\":195129},{\"attributes\":{\"id\":\"b113\",\"matched_paper_id\":42994238},\"end\":195421,\"start\":195258},{\"attributes\":{\"doi\":\"218\",\"id\":\"b114\",\"matched_paper_id\":122551586},\"end\":195928,\"start\":195423},{\"attributes\":{\"id\":\"b115\"},\"end\":196263,\"start\":195930},{\"attributes\":{\"id\":\"b116\",\"matched_paper_id\":116210394},\"end\":196521,\"start\":196265},{\"attributes\":{\"id\":\"b117\",\"matched_paper_id\":116699031},\"end\":196759,\"start\":196523},{\"attributes\":{\"id\":\"b118\"},\"end\":197110,\"start\":196761},{\"attributes\":{\"id\":\"b119\"},\"end\":197631,\"start\":197112},{\"attributes\":{\"id\":\"b120\",\"matched_paper_id\":793386},\"end\":197862,\"start\":197633},{\"attributes\":{\"id\":\"b121\",\"matched_paper_id\":121185262},\"end\":198042,\"start\":197864},{\"attributes\":{\"id\":\"b122\",\"matched_paper_id\":42810613},\"end\":198273,\"start\":198044},{\"attributes\":{\"id\":\"b123\",\"matched_paper_id\":6531539},\"end\":198587,\"start\":198275}]", "bib_title": "[{\"end\":163536,\"start\":163465},{\"end\":163916,\"start\":163878},{\"end\":164159,\"start\":164086},{\"end\":164360,\"start\":164320},{\"end\":164695,\"start\":164610},{\"end\":164941,\"start\":164872},{\"end\":165316,\"start\":165260},{\"end\":165764,\"start\":165693},{\"end\":166403,\"start\":166365},{\"end\":166627,\"start\":166540},{\"end\":166871,\"start\":166818},{\"end\":167582,\"start\":167523},{\"end\":167822,\"start\":167729},{\"end\":168106,\"start\":168039},{\"end\":168331,\"start\":168298},{\"end\":168513,\"start\":168463},{\"end\":168748,\"start\":168679},{\"end\":168979,\"start\":168922},{\"end\":169261,\"start\":169161},{\"end\":169635,\"start\":169577},{\"end\":170534,\"start\":170470},{\"end\":170810,\"start\":170751},{\"end\":171001,\"start\":170961},{\"end\":171242,\"start\":171180},{\"end\":171665,\"start\":171584},{\"end\":171920,\"start\":171845},{\"end\":172093,\"start\":172058},{\"end\":172278,\"start\":172242},{\"end\":172753,\"start\":172660},{\"end\":172994,\"start\":172964},{\"end\":173190,\"start\":173147},{\"end\":173360,\"start\":173334},{\"end\":173569,\"start\":173513},{\"end\":174001,\"start\":173947},{\"end\":174299,\"start\":174186},{\"end\":174839,\"start\":174774},{\"end\":175122,\"start\":175008},{\"end\":175645,\"start\":175540},{\"end\":175963,\"start\":175901},{\"end\":176424,\"start\":176368},{\"end\":176758,\"start\":176671},{\"end\":177228,\"start\":177165},{\"end\":177400,\"start\":177365},{\"end\":177596,\"start\":177512},{\"end\":178491,\"start\":178404},{\"end\":178742,\"start\":178668},{\"end\":178969,\"start\":178919},{\"end\":179399,\"start\":179302},{\"end\":179650,\"start\":179588},{\"end\":179834,\"start\":179798},{\"end\":180075,\"start\":180001},{\"end\":180660,\"start\":180588},{\"end\":180960,\"start\":180852},{\"end\":181264,\"start\":181223},{\"end\":181470,\"start\":181399},{\"end\":181881,\"start\":181812},{\"end\":182140,\"start\":182031},{\"end\":183118,\"start\":183080},{\"end\":184129,\"start\":184050},{\"end\":184837,\"start\":184747},{\"end\":185230,\"start\":185143},{\"end\":186790,\"start\":186689},{\"end\":187053,\"start\":187008},{\"end\":187301,\"start\":187242},{\"end\":187481,\"start\":187448},{\"end\":187691,\"start\":187649},{\"end\":188418,\"start\":188303},{\"end\":188957,\"start\":188864},{\"end\":189192,\"start\":189174},{\"end\":189345,\"start\":189305},{\"end\":189605,\"start\":189524},{\"end\":189840,\"start\":189785},{\"end\":190241,\"start\":190206},{\"end\":190426,\"start\":190390},{\"end\":191171,\"start\":191131},{\"end\":191360,\"start\":191309},{\"end\":191633,\"start\":191514},{\"end\":191984,\"start\":191901},{\"end\":192280,\"start\":192151},{\"end\":192755,\"start\":192720},{\"end\":193387,\"start\":193335},{\"end\":193733,\"start\":193668},{\"end\":194616,\"start\":194562},{\"end\":195153,\"start\":195129},{\"end\":195305,\"start\":195258},{\"end\":195467,\"start\":195423},{\"end\":195954,\"start\":195930},{\"end\":196348,\"start\":196265},{\"end\":196610,\"start\":196523},{\"end\":197209,\"start\":197112},{\"end\":197692,\"start\":197633},{\"end\":197914,\"start\":197864},{\"end\":198118,\"start\":198044},{\"end\":198383,\"start\":198275}]", "bib_author": "[{\"end\":163548,\"start\":163538},{\"end\":163931,\"start\":163918},{\"end\":163944,\"start\":163931},{\"end\":164171,\"start\":164161},{\"end\":164376,\"start\":164362},{\"end\":164500,\"start\":164486},{\"end\":164513,\"start\":164500},{\"end\":164706,\"start\":164697},{\"end\":164953,\"start\":164943},{\"end\":164965,\"start\":164953},{\"end\":165132,\"start\":165121},{\"end\":165331,\"start\":165318},{\"end\":165344,\"start\":165331},{\"end\":165576,\"start\":165562},{\"end\":165775,\"start\":165766},{\"end\":165786,\"start\":165775},{\"end\":165981,\"start\":165968},{\"end\":165995,\"start\":165981},{\"end\":166170,\"start\":166158},{\"end\":166181,\"start\":166170},{\"end\":166413,\"start\":166405},{\"end\":166426,\"start\":166413},{\"end\":166642,\"start\":166629},{\"end\":166652,\"start\":166642},{\"end\":166892,\"start\":166873},{\"end\":166913,\"start\":166892},{\"end\":166925,\"start\":166913},{\"end\":166932,\"start\":166925},{\"end\":167593,\"start\":167584},{\"end\":167835,\"start\":167824},{\"end\":167844,\"start\":167835},{\"end\":167854,\"start\":167844},{\"end\":168119,\"start\":168108},{\"end\":168128,\"start\":168119},{\"end\":168140,\"start\":168128},{\"end\":168345,\"start\":168333},{\"end\":168525,\"start\":168515},{\"end\":168534,\"start\":168525},{\"end\":168765,\"start\":168750},{\"end\":168774,\"start\":168765},{\"end\":168994,\"start\":168981},{\"end\":169003,\"start\":168994},{\"end\":169276,\"start\":169263},{\"end\":169287,\"start\":169276},{\"end\":169303,\"start\":169287},{\"end\":169319,\"start\":169303},{\"end\":169333,\"start\":169319},{\"end\":169648,\"start\":169637},{\"end\":169657,\"start\":169648},{\"end\":169912,\"start\":169899},{\"end\":169925,\"start\":169912},{\"end\":170279,\"start\":170268},{\"end\":170293,\"start\":170279},{\"end\":170312,\"start\":170293},{\"end\":170547,\"start\":170536},{\"end\":170557,\"start\":170547},{\"end\":170570,\"start\":170557},{\"end\":170581,\"start\":170570},{\"end\":170822,\"start\":170812},{\"end\":171012,\"start\":171003},{\"end\":171026,\"start\":171012},{\"end\":171251,\"start\":171244},{\"end\":171261,\"start\":171251},{\"end\":171455,\"start\":171445},{\"end\":171469,\"start\":171455},{\"end\":171677,\"start\":171667},{\"end\":171688,\"start\":171677},{\"end\":171933,\"start\":171922},{\"end\":172103,\"start\":172095},{\"end\":172114,\"start\":172103},{\"end\":172289,\"start\":172280},{\"end\":172303,\"start\":172289},{\"end\":172316,\"start\":172303},{\"end\":172768,\"start\":172755},{\"end\":172777,\"start\":172768},{\"end\":173008,\"start\":172996},{\"end\":173021,\"start\":173008},{\"end\":173203,\"start\":173192},{\"end\":173213,\"start\":173203},{\"end\":173372,\"start\":173362},{\"end\":173385,\"start\":173372},{\"end\":173578,\"start\":173571},{\"end\":173590,\"start\":173578},{\"end\":174016,\"start\":174003},{\"end\":174029,\"start\":174016},{\"end\":174315,\"start\":174301},{\"end\":174636,\"start\":174623},{\"end\":174646,\"start\":174636},{\"end\":174852,\"start\":174841},{\"end\":174863,\"start\":174852},{\"end\":175135,\"start\":175124},{\"end\":175657,\"start\":175647},{\"end\":175670,\"start\":175657},{\"end\":175679,\"start\":175670},{\"end\":175978,\"start\":175965},{\"end\":175988,\"start\":175978},{\"end\":176203,\"start\":176186},{\"end\":176438,\"start\":176426},{\"end\":176453,\"start\":176438},{\"end\":176467,\"start\":176453},{\"end\":176477,\"start\":176467},{\"end\":176774,\"start\":176760},{\"end\":176785,\"start\":176774},{\"end\":176987,\"start\":176975},{\"end\":177238,\"start\":177230},{\"end\":177413,\"start\":177402},{\"end\":177607,\"start\":177598},{\"end\":177879,\"start\":177860},{\"end\":177889,\"start\":177879},{\"end\":177899,\"start\":177889},{\"end\":178116,\"start\":178097},{\"end\":178126,\"start\":178116},{\"end\":178136,\"start\":178126},{\"end\":178147,\"start\":178136},{\"end\":178502,\"start\":178493},{\"end\":178756,\"start\":178744},{\"end\":178982,\"start\":178971},{\"end\":178994,\"start\":178982},{\"end\":179410,\"start\":179401},{\"end\":179662,\"start\":179652},{\"end\":180083,\"start\":180077},{\"end\":180319,\"start\":180313},{\"end\":180327,\"start\":180319},{\"end\":180673,\"start\":180662},{\"end\":180684,\"start\":180673},{\"end\":180969,\"start\":180962},{\"end\":180979,\"start\":180969},{\"end\":181276,\"start\":181266},{\"end\":181482,\"start\":181472},{\"end\":181897,\"start\":181883},{\"end\":182156,\"start\":182142},{\"end\":182165,\"start\":182156},{\"end\":182976,\"start\":182962},{\"end\":182989,\"start\":182976},{\"end\":183134,\"start\":183120},{\"end\":183144,\"start\":183134},{\"end\":183381,\"start\":183368},{\"end\":183395,\"start\":183381},{\"end\":183859,\"start\":183848},{\"end\":183871,\"start\":183859},{\"end\":183882,\"start\":183871},{\"end\":184141,\"start\":184131},{\"end\":184149,\"start\":184141},{\"end\":184161,\"start\":184149},{\"end\":184382,\"start\":184371},{\"end\":184544,\"start\":184532},{\"end\":184555,\"start\":184544},{\"end\":184570,\"start\":184555},{\"end\":184589,\"start\":184570},{\"end\":184852,\"start\":184839},{\"end\":184862,\"start\":184852},{\"end\":185245,\"start\":185232},{\"end\":185255,\"start\":185245},{\"end\":185268,\"start\":185255},{\"end\":185274,\"start\":185268},{\"end\":186359,\"start\":186349},{\"end\":186802,\"start\":186792},{\"end\":186815,\"start\":186802},{\"end\":187065,\"start\":187055},{\"end\":187077,\"start\":187065},{\"end\":187312,\"start\":187303},{\"end\":187496,\"start\":187483},{\"end\":187506,\"start\":187496},{\"end\":187704,\"start\":187693},{\"end\":187715,\"start\":187704},{\"end\":188021,\"start\":188008},{\"end\":188433,\"start\":188420},{\"end\":188442,\"start\":188433},{\"end\":188451,\"start\":188442},{\"end\":188969,\"start\":188959},{\"end\":188980,\"start\":188969},{\"end\":188989,\"start\":188980},{\"end\":189204,\"start\":189194},{\"end\":189356,\"start\":189347},{\"end\":189370,\"start\":189356},{\"end\":189617,\"start\":189607},{\"end\":189628,\"start\":189617},{\"end\":189852,\"start\":189842},{\"end\":189863,\"start\":189852},{\"end\":190251,\"start\":190243},{\"end\":190262,\"start\":190251},{\"end\":190437,\"start\":190428},{\"end\":190451,\"start\":190437},{\"end\":190464,\"start\":190451},{\"end\":190901,\"start\":190893},{\"end\":191181,\"start\":191173},{\"end\":191193,\"start\":191181},{\"end\":191373,\"start\":191362},{\"end\":191385,\"start\":191373},{\"end\":191650,\"start\":191635},{\"end\":191669,\"start\":191650},{\"end\":191998,\"start\":191986},{\"end\":192289,\"start\":192282},{\"end\":192301,\"start\":192289},{\"end\":192768,\"start\":192757},{\"end\":192779,\"start\":192768},{\"end\":193029,\"start\":193018},{\"end\":193042,\"start\":193029},{\"end\":193401,\"start\":193389},{\"end\":193413,\"start\":193401},{\"end\":193746,\"start\":193735},{\"end\":193757,\"start\":193746},{\"end\":194039,\"start\":194029},{\"end\":194053,\"start\":194039},{\"end\":194063,\"start\":194053},{\"end\":194358,\"start\":194349},{\"end\":194370,\"start\":194358},{\"end\":194386,\"start\":194370},{\"end\":194399,\"start\":194386},{\"end\":194632,\"start\":194618},{\"end\":194642,\"start\":194632},{\"end\":194840,\"start\":194829},{\"end\":194990,\"start\":194975},{\"end\":195167,\"start\":195155},{\"end\":195315,\"start\":195307},{\"end\":195480,\"start\":195469},{\"end\":195491,\"start\":195480},{\"end\":195967,\"start\":195956},{\"end\":196359,\"start\":196350},{\"end\":196891,\"start\":196880},{\"end\":197222,\"start\":197211},{\"end\":197234,\"start\":197222},{\"end\":197708,\"start\":197694},{\"end\":197721,\"start\":197708},{\"end\":197928,\"start\":197916},{\"end\":198126,\"start\":198120},{\"end\":198392,\"start\":198385},{\"end\":198402,\"start\":198392}]", "bib_venue": "[{\"end\":163587,\"start\":163548},{\"end\":163965,\"start\":163944},{\"end\":164189,\"start\":164171},{\"end\":164388,\"start\":164376},{\"end\":164727,\"start\":164706},{\"end\":164980,\"start\":164965},{\"end\":165175,\"start\":165132},{\"end\":165363,\"start\":165344},{\"end\":165560,\"start\":165499},{\"end\":165798,\"start\":165786},{\"end\":165966,\"start\":165937},{\"end\":166156,\"start\":166105},{\"end\":166436,\"start\":166426},{\"end\":166662,\"start\":166652},{\"end\":166991,\"start\":166932},{\"end\":167612,\"start\":167593},{\"end\":167866,\"start\":167854},{\"end\":168150,\"start\":168140},{\"end\":168366,\"start\":168345},{\"end\":168555,\"start\":168534},{\"end\":168784,\"start\":168774},{\"end\":169024,\"start\":169003},{\"end\":169346,\"start\":169333},{\"end\":169667,\"start\":169657},{\"end\":169897,\"start\":169787},{\"end\":170266,\"start\":170230},{\"end\":170591,\"start\":170581},{\"end\":170840,\"start\":170822},{\"end\":171045,\"start\":171026},{\"end\":171280,\"start\":171261},{\"end\":171443,\"start\":171416},{\"end\":171698,\"start\":171688},{\"end\":171942,\"start\":171933},{\"end\":172133,\"start\":172114},{\"end\":172337,\"start\":172316},{\"end\":172795,\"start\":172777},{\"end\":173039,\"start\":173021},{\"end\":173223,\"start\":173213},{\"end\":173406,\"start\":173385},{\"end\":173612,\"start\":173590},{\"end\":174050,\"start\":174029},{\"end\":174340,\"start\":174315},{\"end\":174621,\"start\":174596},{\"end\":174873,\"start\":174863},{\"end\":175184,\"start\":175135},{\"end\":175704,\"start\":175679},{\"end\":176007,\"start\":175988},{\"end\":176184,\"start\":176148},{\"end\":176500,\"start\":176477},{\"end\":176806,\"start\":176785},{\"end\":177026,\"start\":176987},{\"end\":177250,\"start\":177238},{\"end\":177425,\"start\":177413},{\"end\":177626,\"start\":177607},{\"end\":177858,\"start\":177771},{\"end\":178201,\"start\":178147},{\"end\":178523,\"start\":178502},{\"end\":178767,\"start\":178756},{\"end\":179008,\"start\":178994},{\"end\":179205,\"start\":179118},{\"end\":179431,\"start\":179410},{\"end\":179683,\"start\":179662},{\"end\":179896,\"start\":179836},{\"end\":180102,\"start\":180083},{\"end\":180311,\"start\":180232},{\"end\":180703,\"start\":180684},{\"end\":180991,\"start\":180979},{\"end\":181297,\"start\":181276},{\"end\":181521,\"start\":181482},{\"end\":181907,\"start\":181897},{\"end\":182220,\"start\":182172},{\"end\":183169,\"start\":183144},{\"end\":183366,\"start\":183293},{\"end\":183635,\"start\":183549},{\"end\":183846,\"start\":183778},{\"end\":184180,\"start\":184161},{\"end\":184369,\"start\":184347},{\"end\":184530,\"start\":184489},{\"end\":184881,\"start\":184862},{\"end\":185324,\"start\":185274},{\"end\":186347,\"start\":186286},{\"end\":186832,\"start\":186815},{\"end\":187099,\"start\":187077},{\"end\":187331,\"start\":187312},{\"end\":187523,\"start\":187506},{\"end\":187740,\"start\":187715},{\"end\":188006,\"start\":187958},{\"end\":188473,\"start\":188451},{\"end\":189001,\"start\":188989},{\"end\":189225,\"start\":189204},{\"end\":189389,\"start\":189370},{\"end\":189638,\"start\":189628},{\"end\":189884,\"start\":189863},{\"end\":190281,\"start\":190262},{\"end\":190485,\"start\":190464},{\"end\":190891,\"start\":190808},{\"end\":191203,\"start\":191193},{\"end\":191395,\"start\":191385},{\"end\":191690,\"start\":191669},{\"end\":192010,\"start\":191998},{\"end\":192322,\"start\":192301},{\"end\":192796,\"start\":192779},{\"end\":193016,\"start\":192923},{\"end\":193438,\"start\":193413},{\"end\":193767,\"start\":193757},{\"end\":194027,\"start\":193900},{\"end\":194347,\"start\":194314},{\"end\":194659,\"start\":194642},{\"end\":194827,\"start\":194788},{\"end\":195023,\"start\":194990},{\"end\":195179,\"start\":195167},{\"end\":195327,\"start\":195315},{\"end\":195504,\"start\":195494},{\"end\":195988,\"start\":195967},{\"end\":196380,\"start\":196359},{\"end\":196633,\"start\":196612},{\"end\":196878,\"start\":196761},{\"end\":197247,\"start\":197234},{\"end\":197731,\"start\":197721},{\"end\":197941,\"start\":197928},{\"end\":198145,\"start\":198126},{\"end\":198414,\"start\":198402},{\"end\":163655,\"start\":163614},{\"end\":167046,\"start\":166993},{\"end\":171947,\"start\":171944},{\"end\":172405,\"start\":172399},{\"end\":175239,\"start\":175230},{\"end\":177037,\"start\":177028},{\"end\":181589,\"start\":181548},{\"end\":185376,\"start\":185351},{\"end\":187777,\"start\":187769},{\"end\":189952,\"start\":189946},{\"end\":190553,\"start\":190547},{\"end\":193475,\"start\":193467},{\"end\":195031,\"start\":195025},{\"end\":196056,\"start\":196050}]"}}}, "year": 2023, "month": 12, "day": 17}