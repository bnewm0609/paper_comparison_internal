{"id": 9314967, "updated": "2023-09-28 15:17:22.615", "metadata": {"title": "Detailed, accurate, human shape estimation from clothed 3D scan sequences", "authors": "[{\"first\":\"Chao\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Sergi\",\"last\":\"Pujades\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Black\",\"middle\":[]},{\"first\":\"Gerard\",\"last\":\"Pons-Moll\",\"middle\":[]}]", "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2017, "month": 3, "day": 13}, "abstract": "We address the problem of estimating human body shape from 3D scans over time. Reliable estimation of 3D body shape is necessary for many applications including virtual try-on, health monitoring, and avatar creation for virtual reality. Scanning bodies in minimal clothing, however, presents a practical barrier to these applications. We address this problem by estimating body shape under clothing from a sequence of 3D scans. Previous methods that have exploited statistical models of body shape produce overly smooth shapes lacking personalized details. In this paper we contribute a new approach to recover not only an approximate shape of the person, but also their detailed shape. Our approach allows the estimated shape to deviate from a parametric model to fit the 3D scans. We demonstrate the method using high quality 4D data as well as sequences of visual hulls extracted from multi-view images. We also make available a new high quality 4D dataset that enables quantitative evaluation. Our method outperforms the previous state of the art, both qualitatively and quantitatively.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1703.04454", "mag": "2950684141", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ZhangPBP17", "doi": "10.1109/cvpr.2017.582"}}, "content": {"source": {"pdf_hash": "507589abde8d95e96e00a1dc594d39916ad69688", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1703.04454v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1703.04454", "status": "GREEN"}}, "grobid": {"id": "60047cd7b6b98c2227adb67d4fb09e66c868f453", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/507589abde8d95e96e00a1dc594d39916ad69688.txt", "contents": "\nDetailed, accurate, human shape estimation from clothed 3D scan sequences\n\n\nChao Zhang 1chao.zhang@tuebingen.mpg.de \nMPI for Intelligent Systems\nT\u00fcbingenGermany\n\nDept. of Computer Science\nThe University of York\nUK\n\nSergi Pujades spujades@tuebingen.mpg.de \nMPI for Intelligent Systems\nT\u00fcbingenGermany\n\nMichael Black black@tuebingen.mpg.de \nMPI for Intelligent Systems\nT\u00fcbingenGermany\n\nGerard Pons-Moll gerard.pons.moll@tuebingen.mpg.de \nMPI for Intelligent Systems\nT\u00fcbingenGermany\n\nDetailed, accurate, human shape estimation from clothed 3D scan sequences\n\nFigure 1: Given static 3D scans or 3D scan sequences (in pink), we estimate the naked shape under clothing (beige). Our method obtains accurate results by minimizing an objective function that captures the visible details of the skin, while being robust to clothing. We show several pairs of clothed scan sequences and the estimated body shape underneath.AbstractWe address the problem of estimating human body shape from 3D scans over time. Reliable estimation of 3D body shape is necessary for many applications including virtual try-on, health monitoring, and avatar creation for virtual reality. Scanning bodies in minimal clothing, however, presents a practical barrier to these applications. We address this problem by estimating body shape under clothing from a sequence of 3D scans. Previous methods that have exploited statistical models of body shape produce overly smooth shapes lacking personalized details. In this paper we contribute a new approach to recover not only an approximate shape of the person, but also their detailed shape. Our approach allows the estimated shape to deviate from a parametric model to fit the 3D scans. We demonstrate the method using high quality 4D data as well as sequences of visual hulls extracted from multi-view images. We also make available a new high quality 4D dataset that enables quantitative evaluation. Our method outperforms the previous state of the art, both qualitatively and quantitatively.\n\nIntroduction\n\nWe address the problem of estimating the body shape of a person wearing clothing from 3D scan sequences or visual hulls computed from multi-view images. Reliably estimating the shape under clothing is useful for many applications including virtual try-on, biometrics, and fitness. It is also a key component for virtual clothing and cloth simulation where garments need to be synthesized on top of the minimally-clothed body. Furthermore, most digital recordings of humans are done wearing clothes and therefore automatic methods to extract biometric information from such data are needed. While clothes occlude the minimally-clothed shape (MCS) of the human and make the task challenging, different poses of the person provide different constraints on the shape under the clothes. Previous work [3,38] exploits this fact by optimizing shape using several different poses. To do this they use the statistical shape model SCAPE [2] that factorizes human shape into subject identity and pose. The main limitation of such approaches is that only the parameters of the statistical model are optimized and so the solutions are constrained to lie on the model space. While statistical models provide powerful constraints on the human shape, they are typically overlysmooth and important identity details such as face features are lost. More importantly, constraints such as \"the cloth garment should lie outside the body shape surface\" are difficult to satisfy when optimizing model parameters. This is because shape deformations in most statistical body models are global, so a step in model space that, for example, shrinks the belly might have the \"side effect\" of making the person shorter.\n\nTherefore, we propose a novel method to estimate the MCS, that recovers accurate global body shape as well as important local shape identity details as can be seen in Figure 1. Our hypothesis is that several poses of the person wearing the same clothes provide enough constraints for detailed body shape capture. Moreover, if some identity details are visible, e.g. the face, the method should capture them.\n\nTo do so, we propose to minimize a single-frame objective function that (i) enforces the scan cloth vertices to remain outside of the MCS, (ii) makes the MCS tightly fits the visible skin parts, and (iii) uses a robust function that snaps MCS to close-by cloth vertices and ignores far away cloth points.\n\nIn contrast to previous work, where only model shape parameters are optimized, we directly optimize the vertices of a template with N = 6890 vertices in a canonical \"T\" pose (unposed template). This allows us to capture local shape details by satisfying the objective constraints. To satisfy anthropometric constraints, we regularize the optimisation vertices to remain close to a statistical body model. Here we use SMPL [20], a publicly available vertex-based model that is compatible with standard graphics pipelines. While this formulation leads to a larger number of variables to optimise, we show that it leads to more accurate and more detailed results.\n\nWhile simple, the single-frame objective is very powerful, as it can be adapted to different tasks. To leverage the temporal information one would like to optimize all scans in the sequence at once. At the high resolution of our scans this is computationally very expensive and memory intensive. Hence, we first register/align all scans by deforming one template to explain both, skin and cloth scan points. These cloth alignments are obtained by minimizing a special case of the single-frame objective with all vertices treated as skin. Since the model factors pose and shape, all cloth alignment templates live in a common unposed space; we call the union of these unposed alignments the fusion scan. Since the cloth should lie outside the body for all frames we minimize the single-frame objective using the fusion scan as input and obtain an accurate shape template (fusion shape) for the person. Finally, to obtain the pose and the time varying shape details, we optimize again the single objective function using the fusion shape as a regularizer. The overview of the method is described in Figure 2. The result is a numerically and visually accurate estimation of the body shape under clothing and pose that fits the clothed scans (see Figure 1).\n\nTo validate our approach we use an existing dataset [38] and collected a new dataset (BUFF : Bodies Under Flowing Fashion) that includes high resolution 3D scan sequences of 3 males and 3 females in different clothing styles. To foster research in this direction we will make BUFF publicly available for research purposes. BUFF contains in total 9.827 high resolution clothed scans with ground truth naked shape for each subject. Qualitative as well as quantitative results demonstrate that our method outperforms previous state of the art methods.\n\n\nRelated Work\n\nBody Models. A key ingredient for robust human pose and shape estimation is the body model. Early body models in computer vision were based on simple primitives [5,13,28,32]. More recent body models [2,20,42] encode shape and pose deformations separately and are learned from thousands of scans of real people. Some works model shape and pose deformations jointly as in [16] where they perform PCA on a rotation-invariant encoding of triangles. A popular body model is SCAPE [2], which factors triangle deformations into pose and shape. Recent work has proposed to make SCAPE more efficient by approximating the pose dependent deformations with Linear Blend Skinning (LBS) [18,27]. To increase the expressiveness of the shape space, [8] combines SCAPE with localized multilinear models for each body part. SMPL [20] models variations due to pose and shape using a linear function. Some models [26,29] incorporate also dynamic soft-tissue deformations; inferring soft-tissue deformations under clothing is an interesting future direction.\n\nPose and Shape Estimation. A large number of works estimate the body pose and shape from people wearing minimal clothing. Methods [36,39,40] to estimate pose and shape from a depth sensor typically combine silhouette, depth data or color terms. In [6] they estimate body shape from a depth sequence but they focus on people wearing minimal clothing.\n\nIn [17] they propose a real-time full body tracker based on Kinect but they first acquire the shape of the subject in a fixed posture and then keep shape parameters fixed. A large number of methods track the human pose/shape from images or multi-view images ignoring clothing or treating it as noise [4,12,34]. The advent of human part detectors trained from large amounts of annotated data using Convolutional Neural Networks [1,35,23] have made human shape and pose estimation possible in challenging scenarios [7,30,10]. In [7] they fit a SMPL model to joint detections to estimate pose and shape. However the estimated shape is a simplification since bone lengths alone can not determine the full body shape. Recently, [30] uses a sum-of-Gaussians body model [34] and estimates pose and shape in outdoor sequences but the alignment energy does not consider clothing.\n\nShape under clothing. Estimating the underlying shape occluded by cloth is a highly under-constrained problem. To cope with this, most existing methods exploit statistical body models, like SCAPE or variants of it. In [15] they estimate shape from a single 3D scan. Their rotation-invariant body representation does not separate and pose parameters and thus it can not be trivially extended to sequences. In [21] they propose a layered model of cloth and estimate the body shape by detecting areas where the cloth is close to the body. Wuhrer et al. [37] estimate shape under clothing on single or multiple 3D scans. The pose and shape is estimated at every frame and the final shape is obtained as the average over multiple frames. Stoll et al. [33] estimate the naked shape under a clothed template but require manual input and their focus is estimating an approximate shape to use as a proxy for collision detection. All these approaches require manual input to initialize the pose [15,21,37].\n\nThe work of [31] incorporates a model of clothing for more robust tracking under clothing but only results for the lower leg are shown and shape is given as input to the method. Following the same principle [14] propose to learn the statistics of how cloth deviates from the body for robust inference but they do so in 2D.\n\nAuthors of [3] estimate the body shape under clothing from multi-view images and similarly to us they exploit temporal information. However, they only optimize model parameters and hence shape details are not captured. Numerical evaluation is only provided using biometric shape features. The work of [38] proposes a similar approach to estimate shape and pose under clothing in motion but they do it from scans and only optimize model parameters. The pose deformation model used in [38] is too simple to track complex poses such as shrugging or lifting arms.\n\nThe previous work is restricted to optimize model parameters and hence results lack detail because they are restricted to the model space. We go beyond state of the art and efficiently estimate jointly model parameters and a subject specific free form shape. Other work is purely model free and attempts to estimate non-rigid 3D shape over time [41,22,11,25]. While this work can capture people in clothing, it does not use a body model and cannot estimate shape under clothing. Our method combines the strong constraints of a body model with the freedom to deform like the model-free methods.\n\n\nBody Model\n\nSMPL [20] is a body model that uses a learned rigged template T with V = 6890 vertices. The vertex positions of SMPL are adapted according to identity-dependent shape parameters and the skeleton pose. The skeletal structure of the human body is modeled with a kinematic chain consisting of rigid bone segments linked by n = 24 joints. Each joint is modeled as a ball joint with 3 rotational Degrees of Freedom (DoF), parametrized with exponential coordinates \u03c9. Including translation, the pose \u03b8 is determined by a pose vector of 3 \u00d7 23 + 3 = 72 parameters.\n\nTo model shape and pose dependent deformations SMPL modifies the template in an additive way and predicts the joint locations from the deformed template. The model,\nM (\u03b2, \u03b8) is then M (\u03b2, \u03b8) = W (T (\u03b2, \u03b8), J(\u03b2), \u03b8, W) (1) T (\u03b2, \u03b8) = T \u00b5 + B s (\u03b2) + B p (\u03b8) (2) where W (T \u00b5 , \u03b8, J) : R 3N \u00d7 R |\u03b8| \u00d7 R 3K \u2192 R 3N\nis a linear blend skinning function that takes vertices in the rest pose T \u00b5 , joint locations J, a pose \u03b8, and the blend weights W, and returns the posed vertices. The parameters B s (\u03b2) and B p (\u03b8) are vectors of vertex offsets from the template. We refer to these as shape and pose blend shapes respectively. We use M to refer to the mesh produces by SMPL. Note that this is different from M , which only refers to the vertices. See [20] for more details.\n\n\nMethod\n\nOur goal is to estimate the naked shape and pose of a subject from a sequence of clothed scans {S} k . If the scans have color information, we use it to split the scan vertices into two sets: the skin (S skin ) and the cloth (S cloth ), otherwise we consider all vertices as cloth (S cloth = S). The outputs of our method are: a personalized static template shape T Fu , a detailed template T Est , as well as the per frame poses \u03b8 k , and the per frame detailed template shapes T k Est . Ideally, pose dependent shape changes should be explained by T Est and the pose deformation model; however, in practice models deviate from real data. Therefore, we allow our result T k Est to slightly vary over time. This allows us to capture time changing details, e.g. facial details, present in the data, which the model can not represent.\n\nGiven a single scan we obtain the shape by minimizing a single-frame objective function (Section 4.1) that constrains the scan cloth points to be outside of the body, and penalizes deviations from the body to skin parts. However, estimating the shape from a single scan is an under-constrained problem. Fortunately, when all the information in a sequence is considered, the underlying shape is more constrained, as different poses will make the cloth tight to the body in different parts. In order to exploit such rich temporal information we first bring all input scans into correspondence. As a result we obtain a set of posed registrations and unposed template registrations (see Figure 2 a and b). The union of the unposed templates creates the fusion scan (Figure 2c). We use it to estimate a single shape, that we call the fusion shape (Figure 2 d). Since all temporal information is fused into a single fusion scan, we can estimate the fusion shape using the same single-frame objective function. Using the fusion shape template as a prior, we can accurately estimate the pose and shape of the sequence. In Figure 2 we show the different steps of the method. The results of each stage are obtained using variants of the same single-frame objective.\n\n\nSingle-Frame Objective\n\nWe define the single-frame objective function as:\nE(T Est , M (\u03b2, 0), \u03b8; S) = \u03bb skin E skin + \u03bb cloth E cloth +\u03bb cpl E cpl + \u03bb prior E prior ,\nwhere E skin is the skin term, E cloth is the cloth term, E cpl is the model coupling term and E prior includes prior terms for pose, shape, and translation. M (\u03b2, 0) = T \u00b5 + B s (\u03b2); T \u00b5 is the default template of the SMPL model, and \u03b2 are the coefficients of the shape space, see Eq.\n\n(2). Next we describe each of the terms.\n\nSkin term: We penalize deviations from the model to scan points labeled as skin s i \u2208 S skin (see Figure 3). A straightforward penalty applied to only skin points creates a discontinuity at the boundaries, which leads to poor results Figure 3(c). In order to make the cost function smooth, we first compute the geodesic distance of a point in the alignment to the closest cloth point, and we apply a logistic function to map geodesic distance values between [0 and 1] Figure 3(b). We name this function g(x) :\nR 3 \u2192 [0, 1].\nThe resulting value is propagated to the scan points by nearest distance, and used to weight each scan residual. This way, points close to skin-cloth boundaries have a smooth decreasing weight. This effectively makes the function smooth and robust to inaccurate segmentations Figure 3(d).\nE skin (T Est , \u03b8; S) = s i \u2208S skin g(s i )\u03c1(dist(s i , M(T Est , \u03b8))),(3)\nwhere dist is a point to surface distance, and \u03c1(\u00b7) is Geman-McClure penalty function. Note that dist() is computing the closest primitive on the mesh M(T Est , \u03b8), triangle, edge or point; analytic derivatives are computed accordingly in each case.\n\nCloth term: The cloth objective consists of two terms: E cloth = E outside + E inside . The outside term penalizes cloth points penetrating the mesh and the fit term encourages the mesh to remain close to the cloth surface. This is in contrast to previous work [38] that assumes a closed scan and pushes the model inside. Since scans are not closed surfaces we just penalize cloth points penetrating our closed registration surface. Therefore, the approach is general to point clouds. The outside term is mathematically the sum of penalties for every scan point labeled as cloth s \u2208 S cloth Figure 4: Left: Cloth term. The x-axis is the signed distance between s \u2208 S cloth and M(T Est , \u03b8). Points inside (negative) have a quadratic penalty, while points outside are penalized using a robust Geman-McClure function. Right: Root mean squared error and standard deviation between single-frame estimations and the ground truth. Results have significant dispersion depending on pose. Results for subject 00005, motion \"hips\" and clothing style \"soccer\". that penetrates the shape mesh:\nE outside (T Est , \u03b8; S) = si\u2208S cloth \u03b4 i dist(s i , M(T Est , \u03b8)),(4)\nwhere \u03b4 i is an indicator function returning 1 if the scan point s i lies inside the mesh and 0 otherwise. The activation \u03b4 i is easily obtained by computing the angle between the mesh surface normal and the vector connecting the scan vertex and the closest point in the mesh.\n\nMinimization of the outside term alone can make the shape excessively thin. Hence, the inside term E inside is used to maintain the volume of the naked model. Every cloth scan vertex pays a penalty if it deviates from the body. Since we want to be robust to wide clothing, we define E inside as a Geman-McClure cost function. With this robust cost function, points far away (e.g. points in skirt or wide jacket) pay a small nearly-constant penalty. The resulting cloth term is illustrated in the left part of Figure 4.\n\nCoupling term: Optimizing only E skin and E cloth results in very unstable results because no human anthropometric constraints are enforced. Therefore, we constrain the template T Est to remain close to the statistical shape body model E cpl (T Est , M (0, \u03b2)) = diag(w)(T Est \u2212 M (0, \u03b2)) 2 (5) where the diagonal matrix diag(w) simply increases the coupling strength for parts like hands and feet where the scans are noisier. Since we are jointly optimizing T Est , and \u03b2, the model representation of the shape is pulled towards T Est and vice versa. The result of the optimization is a detailed estimation T Est and a model representation of the shape \u03b2.\n\nPrior term: The pose is regularized with a Gaussian prior computed from the pose training set of [20]. Specifically we enforce a Mahalanobis distance prior on the pose:\nE prior (\u03b8) = (\u03b8 \u2212 \u00b5 \u03b8 ) T \u03a3 \u22121 \u03b8 (\u03b8 \u2212 \u00b5 \u03b8 )(6)\nwhere the mean \u00b5 \u03b8 and covariance \u03a3 \u22121 \u03b8 are computed from the pose training set. A similar prior can be enforced on the shape space coefficients \u03b2 but we found it did not make a significant difference.\n\nTo optimize the single-frame objective we compute the derivatives using the autodifferentiation tool Chumpy [19]. We use the \"dogleg\" gradient-based descent minimization method [24].\n\n\nFusion Shape Estimation\n\nThe problem with the single-frame objective is two fold: the temporal information is disregarded and the frame wise shape changes over time depending on the pose. This can be seen in the right part of Figure 4. The straightforward approach is to extend the single-frame objective to multiple frames and optimize jointly a single T Est , \u03b2 and the N frames poses {\u03b8 k } N frames k=1 . Unfortunately, our scans have around 150, 000 points, and optimizing all poses jointly makes the optimization highly inefficient and memory intensive. Furthermore, slight miss-alignments in pose make the shape shrink too much. Hence we propose an effective and more efficient solution. We first sequentially register all the scans to a single clothed template. For registration we use the single-frame objective function with \u03bb cloth = 0. From this we obtain a template clothed per frame T k cloth . The interesting thing is that the set of T k cloth templates contain the non-rigid cloth motion with the motion due to pose factored out, see Figure 2. The naked shape should lie inside all the clothed templates. Hence we gather all templates and treat them as a single point cloud that we call the fusion scan\nS Fu = {T k cloth } N frames k=1\n. Hence, we can easily obtain a single shape estimate by using again the single-frame objective\nT Fu = arg min TEst,\u03b2 E(T Est , M (\u03b2, 0), 0; S Fu ).(7)\nThe obtained fusion shapes are already quite accurate because the fusion scan carves the volume where the naked shape should lie.\n\n\nPose and Shape Tracking\n\nFinally we use the fusion shape to perform tracking regularizing the estimated shapes to remain close to the fusion shape. We achieve that by coupling the estimations towards the fusion shape instead of the SMPL model shape space. So the coupling term is now E cpl (T Est , M (0, \u03b2)) \u2192 E cpl (T Est , T Fu ) \nT k Est = arg min TEst,\u03b8 E(T Est , T Fu , \u03b8; S k ).(8)\n\nDatasets\n\nIn this section we present our new BUFF dataset. We start by introducing the previous dataset and discussing its strengths and weaknesses.\n\n\nExisting dataset\n\nThe INRIA dataset [38] consists of sequences of meshes obtained by applying a visual hull reconstruction to a 68color-camera (4M pixels) system at 30fps. The dataset includes sparse motion capture (MoCap) data of 6 subjects (3 female and 3 male) captured in 3 different motions and 3 clothing styles each. The texture information of the scans is not available. Figure 5 a) and b) show frames from the dataset. The \"ground truth shape\" of a subject is estimated by fitting the S-SCAPE [18] model to the \"tight cloths\" sequence.\n\nAs shown in Figure 5 c) and d), their statistical body model does not capture the individual details of the human shape. The main drawback this \"ground truth shape\" is that it biases the evaluation to the model space. All recovered details, that fall outside the model, will be penalized in the quantitative evaluation. Alternatively, one could compare the obtained shape directly with the visual hull. Unfortunately, visual hulls are not very accurate, sometimes over estimating the over-estimating, some times under-estimating the true shape. While relevant for qualitative evaluation of the shape estimates, we believe that this dataset is limited for quantitative evaluation.\n\nThis motivated us to create BUFF, which preserves details and allows quantitative evaluation of the shape estimation.\n\n\nBUFF\n\nTo create BUFF, we use a custom-built multi-camera active stereo system (3dMD LLC, Atlanta, GA) to capture temporal sequences of full-body 3D scans at 60 frames per second. The system uses 22 pairs of stereo cameras, 22 color cameras, 34 speckle projectors and arrays of whitelight LED panels. The projectors and LEDs flash at 120fps to alternate between stereo capture and color capture. The  projected texture pattern makes stereo matching more accurate, dense, and reliable compared with passive stereo methods. The stereo pairs are arranged to give full body capture for a range of activities, enabling us to capture people in motion. The system outputs 3D meshes with approximately 150K vertices on average. BUFF consists of 6 subjects, 3 male and 3 female wearing 2 clothing styles: a) t-shirt and long pants and b) a soccer outfit, see Figure 6. The sequence lengths range between 4 to 9 seconds (200-500 frames) with a total of 12,089 3D scans.\n\nAs shown by previous state of the art methods [3], skin color is a rich source of information. We thus include texture data in our dataset. All subjects gave informed written consent before participating in the study. One subject did not give permission to release their data for research purposes. Consequently, the public BUFF dataset consists of 9,827 scans. \n\n\nComputing the ground truth shapes\n\nIn order to estimate the \"ground truth\" shapes in our dataset we capture the subjects in a \"minimal clothing\" (tight fitting sports underwear). Participants performed an \"A-T-U-Squat\" motion (first row of Figure 7). For all frames, we use our method to fit the data considering all vertices as \"skin\" (see Section 4.1). We obtain N template meshes T i \u00b5 , which do not perfectly match, because the pose and the shape are not perfectly factorized in the SMPL model [20]. We define the T GT as the mean of the estimates of all frames.\n\nWe quantitatively estimated the accuracy of our \"ground truth\" MCS estimations. More than half of the scan points are within 1.5mm distance of T GT and 80% closer than 3mm. Because the scan point cloud has some noise (e.g. points of the scanning platform, poorly reconstructed hands, hair,...), we believe the computed T GT provides an accurate explanation of the subjects \"minimally clothed shape\". In the bottom row of Figure 7 we qualitatively show the visual accuracy of the computed ground truth MCS.\n\n\nExperiments\n\nIn this section we present the evaluation measures and the obtained qualitative and quantitative results. \n\n\nEvaluation on previous datasets\n\nWe evaluate our results quantitatively on pose estimation, and qualitatively on shape estimation in the INRIA dataset [38]. We estimated the body shape for all tight clothes sequences considering all vertices as skin. To initialize the pose we use the automatically computed landmarks of the Stitched Puppet [42]. We compare the MoCap marker locations to the corresponding vertices of our model. In [38], landmarks were manually defined. To transfer the landmark locations from one model to another, we fit the SMPL model to S-SCAPE model. In Figure 10 we report errors; our method achieves state of the art results in pose estimation. In the first row of Figure 11 we present qualitative results for the INRIA dataset. Our results are plausible estimates of minimally-clothed shape. In the second row of Figure 11 we qualitatively compare our results to previous  work on the dancer sequence from [9]. Our results visually outperform previous state of the art. Additional results are presented in the supplemental material.\n\n\nEvaluation on BUFF\n\nTo quantitatively evaluate the results in BUFF, we compare the estimated body shapes with the computed ground truth meshes (Section 5.2.1). We define the \"registration error\" of the estimated body shape in terms of the scan-tomodel distance with respect to the groundtruth MCS. Given a result mesh S, we optimize for pose \u03b8 so that the posed T GT best fits S. Then the error between S and the posed T GT is computed as the Euclidean distance between each vertex in T GT and its closest point on the surface S.\n\nIn Table 1 we show the numerical results obtained by [38], our fusion mesh, and our detailed mesh. The results obtained with our method systematically outperform the best state of the art method. In Figure 8 we show qualitative results on the pose estimations. Our method properly recovers the scan pose, and visually outperforms [38], especially in elbow and shoulder estimations. In Figure 9 we show qualitative results on the shape estimations. The proposed fusion shape accurately recovers the body shape, while the detailed shape is capable of capturing the missing details. While the detailed shape is visually closer to the ground truth, quantitatively, both results are very similar Table 1. Additional results are presented in the Sup. Mat.\n\n\nConclusion\n\nWe introduced a novel method to estimate a detailed body shape under clothing from a sequence of 3D scans. Our method exploits the information in a sequence by fusing all clothed registrations into a single frame. This results in very accurate shape estimates. We also contribute a new dataset (BUFF) of high resolution 3D scan sequences of clothed people as well as ground truth minimally-clothed shapes for each subject. BUFF is the first dataset of high quality 4D scans of clothed people; it will enable accurate quantitative evaluation of body shape estimation. Results on BUFF reveal a clear improvement of with respect to state of the art. One of the limitations of the presented approach is the systematic underestimation of female breast shape; this appears to be a limitation of SMPL. SMPL does not take into account soft tissue deformations of the body; future work will incorporate knowledge of soft tissue deformation to obtain even more accurate results. In addition, using the obtained minimally-clothed shapes and cloth alignments we plan to learn a model of cloth deviations from the body.\n\nFigure 2 Figure 3 :\n23: a) Cloth alignments b) Unposed alignments c) Fusion Scan d) Fusion shape e) Posed and tracked shape. Overview: three example frames are shown. Notice the match in the cloth wrinkles between posed a) and unposed b) alignments. Different time frames provide different constraints in the unposed space. The fusion scan is the union of the frame wise unposed alingments. Color code indicates variance for that region. From the fusion scan c) we obtain the fusion shape d)Skin term weights. a) alignment segmentation (red: skin, blue: cloth) b) geodesic distance to the closest cloth vertex on the alignment c) broken result with unsmooth neck and arms d) smooth result final.\n\nFigure 5 :\n5INRIA dataset: a) and b) scan examples; c) estimated ground truth shape for b); d) overlay of b) and c).\n\nFigure 6 :\n6Examples of BUFF: To validate our method we captured a new dataset including 6 subjects wearing different clothing styles and different motion patterns.\n\nFigure 7 :\n7Top row: Subject 03223 performing the \"A-T-U-Squat\" motion in \"minimal clothing\". These scans are used to compute the groundtruth MCS T GT . Bottom row: Triplet of scan, estimated ground truth model and both overlayed (frame 000150). The proposed ground truth shape captures the details present in the scan point cloud.\n\nFigure 8 :\n8Qualitative pose comparison on BUFF. From left to right: scan, Yang et al. [38], our result.\n\nFigure 9 :Figure 10 :\n910Qualitative shape comparison on BUFF. From left to right: ground truth shape, Yang et al.[38], fusion shape, detailed shape. Comparison in accuracy of pose estimation on INRIA dataset. Left: cumulative landmark errors. Right: per frame average landmark error. EHBS is[38].\n\n\nt-shirt, long pantssoccer outfit \nAvrg. \ntilt twist left \n00005 00096 00032 00057 03223 00114 \n00005 00032 00057 03223 00114 Avrg. \nYang et al. [38] \n17.29 \n18.68 \n13.76 \n17.94 \n17.90 \n15.42 \n16.77 \n16.96 \n18.52 \n20.41 \n16.40 \n17.27 \nfusion mesh \n2.58 \n2.89 \n2.39 \n2.53 \n2.43 \n2.38 \n2.50 \n2.63 \n2.37 \n2.28 \n2.28 \n2.47 \ndetailed \n2.52 \n2.83 \n2.36 \n2.44 \n2.27 \n2.31 \n2.44 \n2.59 \n2.28 \n2.17 \n2.23 \n2.40 \n\nhips \n00005 00096 00032 00057 03223 00114 \n00005 00032 00057 03223 00114 Avrg. \nYang et al. [38] \n21.02 \n21.66 \n15.77 \n17.87 \n21.84 \n18.05 \n22.52 \n16.81 \n19.55 \n22.03 \n17.54 \n19.51 \nfusion mesh \n2.81 \n2.71 \n2.66 \n2.66 \n2.54 \n2.65 \n2.65 \n2.63 \n2.58 \n2.50 \n2.57 \n2.63 \ndetailed \n2.75 \n2.64 \n2.63 \n2.55 \n2.40 \n2.56 \n2.58 \n2.59 \n2.50 \n2.38 \n2.51 \n2.55 \n\nshoulders mill \n00005 00096 00032 00057 03223 00114 \n00005 00032 00057 03223 00114 Avrg. \nYang et al. [38] \n18.77 \n19.02 \n18.02 \n16.50 \n18.15 \n14.78 \n18.74 \n17.88 \n15.80 \n19.47 \n16.37 \n17.59 \nfusion mesh \n2.56 \n2.92 \n2.74 \n2.46 \n2.42 \n2.69 \n2.89 \n2.87 \n2.37 \n2.44 \n2.58 \n2.63 \ndetailed \n2.49 \n2.85 \n2.72 \n2.37 \n2.26 \n2.59 \n2.83 \n2.82 \n2.28 \n2.33 \n2.51 \n2.55 \n\n\n\nTable 1 :\n1Numerical results for the estimated naked shapes. We report the root mean squared error in millimeters of point to surface distance between the posed GT mesh and the method result. The best value is highlighted in bold.Figure 11: Top: Qualitative results on the INRIA dataset; scan (pink), our result. Bottom: Qualitative comparison on dancer sequence[9]. From left to right: scan, Wuhrer et al.[37], Yang et al.[38], proposed.\nAcknowledgmentsWe thank the authors of[21]and[37]for providing their results for comparison. We especially thank the authors of[38]for running their method on BUFF.\nDeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation. DeepCut: Joint Subset Partition and Labeling for Multi Per- son Pose Estimation., June 2016. 3\n\nScape: shape completion and animation of people. D Anguelov, P Srinivasan, D Koller, S Thrun, J Rodgers, J Davis, In ACM Transactions on Graphics. 242ACMD. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers, and J. Davis. Scape: shape completion and animation of peo- ple. In ACM Transactions on Graphics (TOG), volume 24, pages 408-416. ACM, 2005. 2\n\nThe naked truth: Estimating body shape under clothing. A O B\u0203lan, M J Black, European Conference on Computer Vision. Springer26A. O. B\u0203lan and M. J. Black. The naked truth: Estimat- ing body shape under clothing. In European Conference on Computer Vision, pages 15-29. Springer, 2008. 2, 3, 6\n\nDetailed human shape and pose from images. A O Balan, L Sigal, M J Black, J E Davis, H W Haussecker, 2007 IEEE Conference on Computer Vision and Pattern Recognition. IEEEA. O. Balan, L. Sigal, M. J. Black, J. E. Davis, and H. W. Haussecker. Detailed human shape and pose from images. In 2007 IEEE Conference on Computer Vision and Pattern Recognition, pages 1-8. IEEE, 2007. 3\n\nEstimating anthropometry and pose from a single image. C Barron, I A Kakadiaris, Computer Vision and Pattern Recognition. 1Proceedings. IEEE Conference onC. Barron and I. A. Kakadiaris. Estimating anthropometry and pose from a single image. In Computer Vision and Pat- tern Recognition, 2000. Proceedings. IEEE Conference on, volume 1, pages 669-676. IEEE, 2000. 2\n\nDetailed full-body reconstructions of moving people from monocular RGB-D sequences. F Bogo, M J Black, M Loper, J Romero, International Conference on Computer Vision (ICCV). F. Bogo, M. J. Black, M. Loper, and J. Romero. Detailed full-body reconstructions of moving people from monocular RGB-D sequences. In International Conference on Com- puter Vision (ICCV), pages 2300-2308, Dec. 2015. 2\n\nKeep it SMPL: Automatic estimation of 3D human pose and shape from a single image. F Bogo, A Kanazawa, C Lassner, P Gehler, J Romero, M J Black, Computer Vision -ECCV 2016. Springer International PublishingF. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero, and M. J. Black. Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image. In Computer Vision -ECCV 2016, Lecture Notes in Computer Science. Springer International Publishing, Oct. 2016. 3\n\nTensor-based human body modeling. Y Chen, Z Liu, Z Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionY. Chen, Z. Liu, and Z. Zhang. Tensor-based human body modeling. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, pages 105-112, 2013. 2\n\nPerformance capture from sparse multi-view video. E De Aguiar, C Stoll, C Theobalt, N Ahmed, H.-P Seidel, S Thrun, In ACM Transactions on Graphics. 27898ACME. De Aguiar, C. Stoll, C. Theobalt, N. Ahmed, H.-P. Seidel, and S. Thrun. Performance capture from sparse multi-view video. In ACM Transactions on Graphics (TOG), volume 27, page 98. ACM, 2008. 8\n\nShape from selfies: Human body shape estimation using cca regression forests. E Dibra, C \u00d6ztireli, R Ziegler, M Gross, European Conference on Computer Vision. SpringerE. Dibra, C.\u00d6ztireli, R. Ziegler, and M. Gross. Shape from selfies: Human body shape estimation using cca regression forests. In European Conference on Computer Vision, pages 88-104. Springer, 2016. 3\n\nFusion4d: Real-time performance capture of challenging scenes. M Dou, S Khamis, Y Degtyarev, P Davidson, S R Fanello, A Kowdle, S O Escolano, C Rhemann, D Kim, J Taylor, ACM Transactions on Graphics (TOG). 354114M. Dou, S. Khamis, Y. Degtyarev, P. Davidson, S. R. Fanello, A. Kowdle, S. O. Escolano, C. Rhemann, D. Kim, J. Taylor, et al. Fusion4d: Real-time performance capture of chal- lenging scenes. ACM Transactions on Graphics (TOG), 35(4):114, 2016. 3\n\nOptimization and filtering for human motion capture. J Gall, B Rosenhahn, T Brox, H.-P Seidel, International journal of computer vision. 871-23J. Gall, B. Rosenhahn, T. Brox, and H.-P. Seidel. Optimiza- tion and filtering for human motion capture. International journal of computer vision, 87(1-2):75-92, 2010. 3\n\n3-d model-based tracking of humans in action: a multi-view approach. D M Gavrila, L S Davis, Computer Vision and Pattern Recognition. IEEED. M. Gavrila and L. S. Davis. 3-d model-based tracking of humans in action: a multi-view approach. In Computer Vi- sion and Pattern Recognition, 1996. Proceedings CVPR'96, 1996 IEEE Computer Society Conference on, pages 73-80. IEEE, 1996. 2\n\nA 2d human body model dressed in eigen clothing. P Guan, O Freifeld, M J Black, European conference on computer vision. Springer3P. Guan, O. Freifeld, and M. J. Black. A 2d human body model dressed in eigen clothing. In European conference on computer vision, pages 285-298. Springer, 2010. 3\n\nEstimating body shape of dressed humans. N Hasler, C Stoll, B Rosenhahn, T Thorm\u00e4hlen, H.-P Seidel, Computers & Graphics. 333N. Hasler, C. Stoll, B. Rosenhahn, T. Thorm\u00e4hlen, and H.-P. Seidel. Estimating body shape of dressed humans. Comput- ers & Graphics, 33(3):211-216, 2009. 3\n\nA statistical model of human pose and body shape. N Hasler, C Stoll, M Sunkel, B Rosenhahn, H.-P Seidel, Computer Graphics Forum. Wiley Online Library28N. Hasler, C. Stoll, M. Sunkel, B. Rosenhahn, and H.-P. Sei- del. A statistical model of human pose and body shape. In Computer Graphics Forum, volume 28, pages 337-346. Wi- ley Online Library, 2009. 2\n\nPersonalization and evaluation of a real-time depth-based full body tracker. T Helten, A Baak, G Bharaj, M M\u00fcller, H.-P Seidel, C Theobalt, 2013 International Conference on 3D Vision-3DV 2013. T. Helten, A. Baak, G. Bharaj, M. M\u00fcller, H.-P. Seidel, and C. Theobalt. Personalization and evaluation of a real-time depth-based full body tracker. In 2013 International Confer- ence on 3D Vision-3DV 2013, pages 279-286. IEEE, 2013. 2\n\nMoviereshape: Tracking and reshaping of humans in videos. A Jain, T Thorm\u00e4hlen, H.-P Seidel, C Theobalt, ACM Transactions on Graphics. 296ACMA. Jain, T. Thorm\u00e4hlen, H.-P. Seidel, and C. Theobalt. Moviereshape: Tracking and reshaping of humans in videos. In ACM Transactions on Graphics (TOG), volume 29, page 148. ACM, 2010. 2, 6\n\nChumpy is a python-based framework designed to handle the auto-differentiation problem. M Loper, M. Loper. Chumpy is a python-based framework designed to handle the auto-differentiation problem. htttps:// pypi.python.org/pypi/chumpy, 2015. 5\n\nSMPL: A skinned multi-person linear model. M Loper, N Mahmood, J Romero, G Pons-Moll, M J Black, Proc. SIGGRAPH Asia). SIGGRAPH Asia)347M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black. SMPL: A skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia), 34(6):248:1- 248:16, Oct. 2015. 2, 3, 5, 7\n\nA layered model of human body and garment deformation. A Neophytou, A Hilton, 2nd International Conference on 3D Vision. IEEE19A. Neophytou and A. Hilton. A layered model of human body and garment deformation. In 2014 2nd International Conference on 3D Vision, volume 1, pages 171-178. IEEE, 2014. 3, 9\n\nDynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time. R A Newcombe, D Fox, S M Seitz, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionR. A. Newcombe, D. Fox, and S. M. Seitz. Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 343-352, 2015. 3\n\nStacked hourglass networks for human pose estimation. A Newell, K Yang, J Deng, arXiv:1603.06937arXiv preprintA. Newell, K. Yang, and J. Deng. Stacked hourglass networks for human pose estimation. arXiv preprint arXiv:1603.06937, 2016. 3\n\nNumerical Optimization. J Nocedal, S J Wright, SpringerNew York2nd editionJ. Nocedal and S. J. Wright. Numerical Optimization. Springer, New York, 2nd edition, 2006. 5\n\nHoloportation: Virtual 3d teleportation in real-time. S Orts-Escolano, C Rhemann, S Fanello, W Chang, A Kowdle, Y Degtyarev, D Kim, P L Davidson, S Khamis, M Dou, Proceedings of the 29th Annual Symposium on User Interface Software and Technology. the 29th Annual Symposium on User Interface Software and TechnologyACMS. Orts-Escolano, C. Rhemann, S. Fanello, W. Chang, A. Kowdle, Y. Degtyarev, D. Kim, P. L. Davidson, S. Khamis, M. Dou, et al. Holoportation: Virtual 3d telepor- tation in real-time. In Proceedings of the 29th Annual Sym- posium on User Interface Software and Technology, pages 741-754. ACM, 2016. 3\n\nData-driven modeling of skin and muscle deformation. S I Park, J K Hodgins, ACM Trans. Graph. 273S. I. Park and J. K. Hodgins. Data-driven modeling of skin and muscle deformation. ACM Trans. Graph., 27(3):96:1- 96:6, Aug. 2008. 2\n\nBuilding statistical shape spaces for 3d human modeling. L Pishchulin, S Wuhrer, T Helten, C Theobalt, B Schiele, L. Pishchulin, S. Wuhrer, T. Helten, C. Theobalt, and B. Schiele. Building statistical shape spaces for 3d human modeling. [arXiv], March 2015. 2\n\nArticulated soft objects for videobased body modeling. R Plankers, P Fua, CVLAB-CONF- 2001-005International Conference on Computer Vision. Vancouver, CanadaR. Plankers and P. Fua. Articulated soft objects for video- based body modeling. In International Conference on Com- puter Vision, Vancouver, Canada, number CVLAB-CONF- 2001-005, pages 394-401, 2001. 2\n\nDyna: a model of dynamic human shape in motion. G Pons-Moll, J Romero, N Mahmood, M J Black, ACM Transactions on Graphics (TOG). 342120G. Pons-Moll, J. Romero, N. Mahmood, and M. J. Black. Dyna: a model of dynamic human shape in motion. ACM Transactions on Graphics (TOG), 34:120, 2015. 2\n\nGeneral automatic human shape and motion capture using volumetric contour cues. H Rhodin, N Robertini, D Casas, C Richardt, H.-P Seidel, C Theobalt, European Conference on Computer Vision. SpringerH. Rhodin, N. Robertini, D. Casas, C. Richardt, H.-P. Sei- del, and C. Theobalt. General automatic human shape and motion capture using volumetric contour cues. In European Conference on Computer Vision, pages 509-526. Springer, 2016. 3\n\nA system for articulated tracking incorporating a clothing model. Machine Vision and Applications. B Rosenhahn, U Kersting, K Powell, R Klette, G Klette, H.-P Seidel, 18B. Rosenhahn, U. Kersting, K. Powell, R. Klette, G. Klette, and H.-P. Seidel. A system for articulated tracking incorpo- rating a clothing model. Machine Vision and Applications, 18(1):25-40, 2007. 3\n\nTracking loose-limbed people. L Sigal, S Bhatia, S Roth, M J Black, M Isard, Proceedings of the 2004 IEEE Computer Society Conference on. the 2004 IEEE Computer Society Conference onIEEE1421Computer Vision and Pattern RecognitionL. Sigal, S. Bhatia, S. Roth, M. J. Black, and M. Isard. Track- ing loose-limbed people. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, volume 1, pages I- 421. IEEE, 2004. 2\n\nVideo-based reconstruction of animatable human characters. C Stoll, J Gall, E Aguiar, S Thrun, C Theobalt, ACM Transactions on Graphics (TOG). 2963C. Stoll, J. Gall, E. De Aguiar, S. Thrun, and C. Theobalt. Video-based reconstruction of animatable human characters. ACM Transactions on Graphics (TOG), 29(6):139, 2010. 3\n\nFast articulated motion tracking using a sums of gaussians body model. C Stoll, N Hasler, J Gall, H.-P Seidel, C Theobalt, 2011 International Conference on Computer Vision. IEEEC. Stoll, N. Hasler, J. Gall, H.-P. Seidel, and C. Theobalt. Fast articulated motion tracking using a sums of gaussians body model. In 2011 International Conference on Computer Vision, pages 951-958. IEEE, 2011. 3\n\nConvolutional pose machines. S.-E Wei, V Ramakrishna, T Kanade, Y Sheikh, S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh. Con- volutional pose machines. 2016. 3\n\nHome 3d body scans from noisy image and range data. A Weiss, D Hirshberg, M J Black, 2011 International Conference on Computer Vision. A. Weiss, D. Hirshberg, and M. J. Black. Home 3d body scans from noisy image and range data. In 2011 Interna- tional Conference on Computer Vision, pages 1951-1958. IEEE, 2011. 2\n\nEstimation of human body shape and posture under clothing. S Wuhrer, L Pishchulin, A Brunton, C Shu, J Lang, Computer Vision and Image Understanding. 12739S. Wuhrer, L. Pishchulin, A. Brunton, C. Shu, and J. Lang. Estimation of human body shape and posture under cloth- ing. Computer Vision and Image Understanding, 127:31-42, 2014. 3, 8, 9\n\nEstimation of Human Body Shape in Motion with Wide Clothing. J Yang, J.-S Franco, F H\u00e9troy-Wheeler, S Wuhrer, European Conference on Computer Vision. Netherlands89J. Yang, J.-S. Franco, F. H\u00e9troy-Wheeler, and S. Wuhrer. Es- timation of Human Body Shape in Motion with Wide Cloth- ing. In European Conference on Computer Vision 2016, Am- sterdam, Netherlands, Oct. 2016. 2, 3, 4, 6, 7, 8, 9\n\nReal-time simultaneous pose and shape estimation for articulated objects using a single depth camera. M Ye, R Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionM. Ye and R. Yang. Real-time simultaneous pose and shape estimation for articulated objects using a single depth cam- era. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2345-2352, 2014. 2\n\nQuality dynamic human body modeling using a single low-cost depth camera. Q Zhang, B Fu, M Ye, R Yang, 2014 IEEE Conference on Computer Vision and Pattern Recognition. Q. Zhang, B. Fu, M. Ye, and R. Yang. Quality dynamic human body modeling using a single low-cost depth camera. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 676-683. IEEE, 2014. 2\n\nReal-time non-rigid reconstruction using an rgb-d camera. M Zollh\u00f6fer, M Nie\u00dfner, S Izadi, C Rehmann, C Zach, M Fisher, C Wu, A Fitzgibbon, C Loop, C Theobalt, ACM Transactions on Graphics (TOG). 334156M. Zollh\u00f6fer, M. Nie\u00dfner, S. Izadi, C. Rehmann, C. Zach, M. Fisher, C. Wu, A. Fitzgibbon, C. Loop, C. Theobalt, et al. Real-time non-rigid reconstruction using an rgb-d camera. ACM Transactions on Graphics (TOG), 33(4):156, 2014. 3\n\nThe stitched puppet: A graphical model of 3d human shape and pose. S Zuffi, M J Black, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 27S. Zuffi and M. J. Black. The stitched puppet: A graphical model of 3d human shape and pose. In 2015 IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), pages 3537-3546. IEEE, 2015. 2, 7\n", "annotations": {"author": "[{\"end\":215,\"start\":77},{\"end\":301,\"start\":216},{\"end\":384,\"start\":302},{\"end\":481,\"start\":385}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":82},{\"end\":229,\"start\":222},{\"end\":315,\"start\":310},{\"end\":401,\"start\":392}]", "author_first_name": "[{\"end\":81,\"start\":77},{\"end\":221,\"start\":216},{\"end\":309,\"start\":302},{\"end\":391,\"start\":385}]", "author_affiliation": "[{\"end\":161,\"start\":118},{\"end\":214,\"start\":163},{\"end\":300,\"start\":257},{\"end\":383,\"start\":340},{\"end\":480,\"start\":437}]", "title": "[{\"end\":74,\"start\":1},{\"end\":555,\"start\":482}]", "venue": null, "abstract": "[{\"end\":2010,\"start\":557}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2825,\"start\":2822},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2828,\"start\":2825},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2956,\"start\":2953},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4857,\"start\":4853},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6404,\"start\":6400},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7077,\"start\":7074},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7080,\"start\":7077},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7083,\"start\":7080},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7086,\"start\":7083},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7115,\"start\":7112},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7118,\"start\":7115},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7121,\"start\":7118},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7287,\"start\":7283},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7391,\"start\":7388},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7590,\"start\":7586},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7593,\"start\":7590},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7649,\"start\":7646},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7728,\"start\":7724},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7810,\"start\":7806},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7813,\"start\":7810},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8086,\"start\":8082},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8089,\"start\":8086},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8092,\"start\":8089},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8203,\"start\":8200},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8310,\"start\":8306},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8606,\"start\":8603},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8609,\"start\":8606},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8612,\"start\":8609},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8733,\"start\":8730},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8736,\"start\":8733},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8739,\"start\":8736},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8819,\"start\":8816},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8822,\"start\":8819},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8825,\"start\":8822},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8833,\"start\":8830},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9030,\"start\":9026},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9070,\"start\":9066},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9397,\"start\":9393},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9587,\"start\":9583},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9729,\"start\":9725},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9925,\"start\":9921},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10164,\"start\":10160},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10167,\"start\":10164},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10170,\"start\":10167},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10189,\"start\":10185},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10384,\"start\":10380},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10511,\"start\":10508},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10802,\"start\":10798},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10984,\"start\":10980},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11407,\"start\":11403},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11410,\"start\":11407},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11413,\"start\":11410},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11416,\"start\":11413},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11675,\"start\":11671},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12976,\"start\":12972},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16997,\"start\":16993},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19442,\"start\":19438},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19874,\"start\":19870},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19943,\"start\":19939},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22065,\"start\":22061},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22531,\"start\":22527},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24381,\"start\":24378},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25200,\"start\":25196},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26051,\"start\":26047},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26241,\"start\":26237},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26332,\"start\":26328},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26830,\"start\":26827},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":27544,\"start\":27540},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":27821,\"start\":27817},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":30897,\"start\":30893},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":31075,\"start\":31071},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32558,\"start\":32555},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":32603,\"start\":32599},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":32620,\"start\":32616}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30054,\"start\":29358},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30172,\"start\":30055},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30338,\"start\":30173},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30671,\"start\":30339},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30777,\"start\":30672},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31076,\"start\":30778},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32191,\"start\":31077},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32631,\"start\":32192}]", "paragraph": "[{\"end\":3714,\"start\":2026},{\"end\":4123,\"start\":3716},{\"end\":4429,\"start\":4125},{\"end\":5091,\"start\":4431},{\"end\":6346,\"start\":5093},{\"end\":6896,\"start\":6348},{\"end\":7950,\"start\":6913},{\"end\":8301,\"start\":7952},{\"end\":9173,\"start\":8303},{\"end\":10171,\"start\":9175},{\"end\":10495,\"start\":10173},{\"end\":11056,\"start\":10497},{\"end\":11651,\"start\":11058},{\"end\":12223,\"start\":11666},{\"end\":12389,\"start\":12225},{\"end\":12994,\"start\":12536},{\"end\":13837,\"start\":13005},{\"end\":15094,\"start\":13839},{\"end\":15170,\"start\":15121},{\"end\":15549,\"start\":15264},{\"end\":15591,\"start\":15551},{\"end\":16102,\"start\":15593},{\"end\":16405,\"start\":16117},{\"end\":16730,\"start\":16481},{\"end\":17813,\"start\":16732},{\"end\":18161,\"start\":17885},{\"end\":18681,\"start\":18163},{\"end\":19339,\"start\":18683},{\"end\":19509,\"start\":19341},{\"end\":19760,\"start\":19558},{\"end\":19944,\"start\":19762},{\"end\":21166,\"start\":19972},{\"end\":21295,\"start\":21200},{\"end\":21481,\"start\":21352},{\"end\":21817,\"start\":21509},{\"end\":22022,\"start\":21884},{\"end\":22569,\"start\":22043},{\"end\":23250,\"start\":22571},{\"end\":23369,\"start\":23252},{\"end\":24330,\"start\":23378},{\"end\":24694,\"start\":24332},{\"end\":25264,\"start\":24732},{\"end\":25771,\"start\":25266},{\"end\":25893,\"start\":25787},{\"end\":26953,\"start\":25929},{\"end\":27485,\"start\":26976},{\"end\":28236,\"start\":27487},{\"end\":29357,\"start\":28251}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12535,\"start\":12390},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15263,\"start\":15171},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16116,\"start\":16103},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16480,\"start\":16406},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17884,\"start\":17814},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19557,\"start\":19510},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21199,\"start\":21167},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21351,\"start\":21296},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21872,\"start\":21818}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27497,\"start\":27490},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28185,\"start\":28178}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2024,\"start\":2012},{\"attributes\":{\"n\":\"2.\"},\"end\":6911,\"start\":6899},{\"attributes\":{\"n\":\"3.\"},\"end\":11664,\"start\":11654},{\"attributes\":{\"n\":\"4.\"},\"end\":13003,\"start\":12997},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15119,\"start\":15097},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19970,\"start\":19947},{\"attributes\":{\"n\":\"4.3.\"},\"end\":21507,\"start\":21484},{\"attributes\":{\"n\":\"5.\"},\"end\":21882,\"start\":21874},{\"attributes\":{\"n\":\"5.1.\"},\"end\":22041,\"start\":22025},{\"attributes\":{\"n\":\"5.2.\"},\"end\":23376,\"start\":23372},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":24730,\"start\":24697},{\"attributes\":{\"n\":\"6.\"},\"end\":25785,\"start\":25774},{\"attributes\":{\"n\":\"6.1.\"},\"end\":25927,\"start\":25896},{\"attributes\":{\"n\":\"6.2.\"},\"end\":26974,\"start\":26956},{\"attributes\":{\"n\":\"7.\"},\"end\":28249,\"start\":28239},{\"end\":29378,\"start\":29359},{\"end\":30066,\"start\":30056},{\"end\":30184,\"start\":30174},{\"end\":30350,\"start\":30340},{\"end\":30683,\"start\":30673},{\"end\":30800,\"start\":30779},{\"end\":32202,\"start\":32193}]", "table": "[{\"end\":32191,\"start\":31098}]", "figure_caption": "[{\"end\":30054,\"start\":29381},{\"end\":30172,\"start\":30068},{\"end\":30338,\"start\":30186},{\"end\":30671,\"start\":30352},{\"end\":30777,\"start\":30685},{\"end\":31076,\"start\":30804},{\"end\":31098,\"start\":31079},{\"end\":32631,\"start\":32204}]", "figure_ref": "[{\"end\":6198,\"start\":6190},{\"end\":6344,\"start\":6336},{\"end\":14530,\"start\":14522},{\"end\":14610,\"start\":14600},{\"end\":14693,\"start\":14681},{\"end\":14961,\"start\":14953},{\"end\":15700,\"start\":15691},{\"end\":15835,\"start\":15827},{\"end\":16069,\"start\":16061},{\"end\":16401,\"start\":16393},{\"end\":17331,\"start\":17323},{\"end\":18680,\"start\":18672},{\"end\":20181,\"start\":20173},{\"end\":21006,\"start\":20998},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22415,\"start\":22404},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22591,\"start\":22583},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24229,\"start\":24221},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24945,\"start\":24937},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25695,\"start\":25687},{\"end\":26481,\"start\":26472},{\"end\":26594,\"start\":26585},{\"end\":26743,\"start\":26734},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27694,\"start\":27686},{\"end\":27880,\"start\":27872}]", "bib_author_first_name": "[{\"end\":33022,\"start\":33021},{\"end\":33034,\"start\":33033},{\"end\":33048,\"start\":33047},{\"end\":33058,\"start\":33057},{\"end\":33067,\"start\":33066},{\"end\":33078,\"start\":33077},{\"end\":33386,\"start\":33385},{\"end\":33388,\"start\":33387},{\"end\":33397,\"start\":33396},{\"end\":33399,\"start\":33398},{\"end\":33668,\"start\":33667},{\"end\":33670,\"start\":33669},{\"end\":33679,\"start\":33678},{\"end\":33688,\"start\":33687},{\"end\":33690,\"start\":33689},{\"end\":33699,\"start\":33698},{\"end\":33701,\"start\":33700},{\"end\":33710,\"start\":33709},{\"end\":33712,\"start\":33711},{\"end\":34058,\"start\":34057},{\"end\":34068,\"start\":34067},{\"end\":34070,\"start\":34069},{\"end\":34453,\"start\":34452},{\"end\":34461,\"start\":34460},{\"end\":34463,\"start\":34462},{\"end\":34472,\"start\":34471},{\"end\":34481,\"start\":34480},{\"end\":34845,\"start\":34844},{\"end\":34853,\"start\":34852},{\"end\":34865,\"start\":34864},{\"end\":34876,\"start\":34875},{\"end\":34886,\"start\":34885},{\"end\":34896,\"start\":34895},{\"end\":34898,\"start\":34897},{\"end\":35273,\"start\":35272},{\"end\":35281,\"start\":35280},{\"end\":35288,\"start\":35287},{\"end\":35661,\"start\":35660},{\"end\":35664,\"start\":35662},{\"end\":35674,\"start\":35673},{\"end\":35683,\"start\":35682},{\"end\":35695,\"start\":35694},{\"end\":35707,\"start\":35703},{\"end\":35717,\"start\":35716},{\"end\":36043,\"start\":36042},{\"end\":36052,\"start\":36051},{\"end\":36064,\"start\":36063},{\"end\":36075,\"start\":36074},{\"end\":36397,\"start\":36396},{\"end\":36404,\"start\":36403},{\"end\":36414,\"start\":36413},{\"end\":36427,\"start\":36426},{\"end\":36439,\"start\":36438},{\"end\":36441,\"start\":36440},{\"end\":36452,\"start\":36451},{\"end\":36462,\"start\":36461},{\"end\":36464,\"start\":36463},{\"end\":36476,\"start\":36475},{\"end\":36487,\"start\":36486},{\"end\":36494,\"start\":36493},{\"end\":36846,\"start\":36845},{\"end\":36854,\"start\":36853},{\"end\":36867,\"start\":36866},{\"end\":36878,\"start\":36874},{\"end\":37176,\"start\":37175},{\"end\":37178,\"start\":37177},{\"end\":37189,\"start\":37188},{\"end\":37191,\"start\":37190},{\"end\":37537,\"start\":37536},{\"end\":37545,\"start\":37544},{\"end\":37557,\"start\":37556},{\"end\":37559,\"start\":37558},{\"end\":37823,\"start\":37822},{\"end\":37833,\"start\":37832},{\"end\":37842,\"start\":37841},{\"end\":37855,\"start\":37854},{\"end\":37872,\"start\":37868},{\"end\":38114,\"start\":38113},{\"end\":38124,\"start\":38123},{\"end\":38133,\"start\":38132},{\"end\":38143,\"start\":38142},{\"end\":38159,\"start\":38155},{\"end\":38496,\"start\":38495},{\"end\":38506,\"start\":38505},{\"end\":38514,\"start\":38513},{\"end\":38524,\"start\":38523},{\"end\":38537,\"start\":38533},{\"end\":38547,\"start\":38546},{\"end\":38908,\"start\":38907},{\"end\":38916,\"start\":38915},{\"end\":38933,\"start\":38929},{\"end\":38943,\"start\":38942},{\"end\":39269,\"start\":39268},{\"end\":39467,\"start\":39466},{\"end\":39476,\"start\":39475},{\"end\":39487,\"start\":39486},{\"end\":39497,\"start\":39496},{\"end\":39510,\"start\":39509},{\"end\":39512,\"start\":39511},{\"end\":39809,\"start\":39808},{\"end\":39822,\"start\":39821},{\"end\":40135,\"start\":40134},{\"end\":40137,\"start\":40136},{\"end\":40149,\"start\":40148},{\"end\":40156,\"start\":40155},{\"end\":40158,\"start\":40157},{\"end\":40586,\"start\":40585},{\"end\":40596,\"start\":40595},{\"end\":40604,\"start\":40603},{\"end\":40795,\"start\":40794},{\"end\":40806,\"start\":40805},{\"end\":40808,\"start\":40807},{\"end\":40994,\"start\":40993},{\"end\":41011,\"start\":41010},{\"end\":41022,\"start\":41021},{\"end\":41033,\"start\":41032},{\"end\":41042,\"start\":41041},{\"end\":41052,\"start\":41051},{\"end\":41065,\"start\":41064},{\"end\":41072,\"start\":41071},{\"end\":41074,\"start\":41073},{\"end\":41086,\"start\":41085},{\"end\":41096,\"start\":41095},{\"end\":41611,\"start\":41610},{\"end\":41613,\"start\":41612},{\"end\":41621,\"start\":41620},{\"end\":41623,\"start\":41622},{\"end\":41846,\"start\":41845},{\"end\":41860,\"start\":41859},{\"end\":41870,\"start\":41869},{\"end\":41880,\"start\":41879},{\"end\":41892,\"start\":41891},{\"end\":42105,\"start\":42104},{\"end\":42117,\"start\":42116},{\"end\":42457,\"start\":42456},{\"end\":42470,\"start\":42469},{\"end\":42480,\"start\":42479},{\"end\":42491,\"start\":42490},{\"end\":42493,\"start\":42492},{\"end\":42779,\"start\":42778},{\"end\":42789,\"start\":42788},{\"end\":42802,\"start\":42801},{\"end\":42811,\"start\":42810},{\"end\":42826,\"start\":42822},{\"end\":42836,\"start\":42835},{\"end\":43233,\"start\":43232},{\"end\":43246,\"start\":43245},{\"end\":43258,\"start\":43257},{\"end\":43268,\"start\":43267},{\"end\":43278,\"start\":43277},{\"end\":43291,\"start\":43287},{\"end\":43534,\"start\":43533},{\"end\":43543,\"start\":43542},{\"end\":43553,\"start\":43552},{\"end\":43561,\"start\":43560},{\"end\":43563,\"start\":43562},{\"end\":43572,\"start\":43571},{\"end\":44042,\"start\":44041},{\"end\":44051,\"start\":44050},{\"end\":44059,\"start\":44058},{\"end\":44069,\"start\":44068},{\"end\":44078,\"start\":44077},{\"end\":44376,\"start\":44375},{\"end\":44385,\"start\":44384},{\"end\":44395,\"start\":44394},{\"end\":44406,\"start\":44402},{\"end\":44416,\"start\":44415},{\"end\":44729,\"start\":44725},{\"end\":44736,\"start\":44735},{\"end\":44751,\"start\":44750},{\"end\":44761,\"start\":44760},{\"end\":44916,\"start\":44915},{\"end\":44925,\"start\":44924},{\"end\":44938,\"start\":44937},{\"end\":44940,\"start\":44939},{\"end\":45238,\"start\":45237},{\"end\":45248,\"start\":45247},{\"end\":45262,\"start\":45261},{\"end\":45273,\"start\":45272},{\"end\":45280,\"start\":45279},{\"end\":45582,\"start\":45581},{\"end\":45593,\"start\":45589},{\"end\":45603,\"start\":45602},{\"end\":45621,\"start\":45620},{\"end\":46014,\"start\":46013},{\"end\":46020,\"start\":46019},{\"end\":46474,\"start\":46473},{\"end\":46483,\"start\":46482},{\"end\":46489,\"start\":46488},{\"end\":46495,\"start\":46494},{\"end\":46835,\"start\":46834},{\"end\":46848,\"start\":46847},{\"end\":46859,\"start\":46858},{\"end\":46868,\"start\":46867},{\"end\":46879,\"start\":46878},{\"end\":46887,\"start\":46886},{\"end\":46897,\"start\":46896},{\"end\":46903,\"start\":46902},{\"end\":46917,\"start\":46916},{\"end\":46925,\"start\":46924},{\"end\":47279,\"start\":47278},{\"end\":47288,\"start\":47287},{\"end\":47290,\"start\":47289}]", "bib_author_last_name": "[{\"end\":33031,\"start\":33023},{\"end\":33045,\"start\":33035},{\"end\":33055,\"start\":33049},{\"end\":33064,\"start\":33059},{\"end\":33075,\"start\":33068},{\"end\":33084,\"start\":33079},{\"end\":33394,\"start\":33389},{\"end\":33405,\"start\":33400},{\"end\":33676,\"start\":33671},{\"end\":33685,\"start\":33680},{\"end\":33696,\"start\":33691},{\"end\":33707,\"start\":33702},{\"end\":33723,\"start\":33713},{\"end\":34065,\"start\":34059},{\"end\":34081,\"start\":34071},{\"end\":34458,\"start\":34454},{\"end\":34469,\"start\":34464},{\"end\":34478,\"start\":34473},{\"end\":34488,\"start\":34482},{\"end\":34850,\"start\":34846},{\"end\":34862,\"start\":34854},{\"end\":34873,\"start\":34866},{\"end\":34883,\"start\":34877},{\"end\":34893,\"start\":34887},{\"end\":34904,\"start\":34899},{\"end\":35278,\"start\":35274},{\"end\":35285,\"start\":35282},{\"end\":35294,\"start\":35289},{\"end\":35671,\"start\":35665},{\"end\":35680,\"start\":35675},{\"end\":35692,\"start\":35684},{\"end\":35701,\"start\":35696},{\"end\":35714,\"start\":35708},{\"end\":35723,\"start\":35718},{\"end\":36049,\"start\":36044},{\"end\":36061,\"start\":36053},{\"end\":36072,\"start\":36065},{\"end\":36081,\"start\":36076},{\"end\":36401,\"start\":36398},{\"end\":36411,\"start\":36405},{\"end\":36424,\"start\":36415},{\"end\":36436,\"start\":36428},{\"end\":36449,\"start\":36442},{\"end\":36459,\"start\":36453},{\"end\":36473,\"start\":36465},{\"end\":36484,\"start\":36477},{\"end\":36491,\"start\":36488},{\"end\":36501,\"start\":36495},{\"end\":36851,\"start\":36847},{\"end\":36864,\"start\":36855},{\"end\":36872,\"start\":36868},{\"end\":36885,\"start\":36879},{\"end\":37186,\"start\":37179},{\"end\":37197,\"start\":37192},{\"end\":37542,\"start\":37538},{\"end\":37554,\"start\":37546},{\"end\":37565,\"start\":37560},{\"end\":37830,\"start\":37824},{\"end\":37839,\"start\":37834},{\"end\":37852,\"start\":37843},{\"end\":37866,\"start\":37856},{\"end\":37879,\"start\":37873},{\"end\":38121,\"start\":38115},{\"end\":38130,\"start\":38125},{\"end\":38140,\"start\":38134},{\"end\":38153,\"start\":38144},{\"end\":38166,\"start\":38160},{\"end\":38503,\"start\":38497},{\"end\":38511,\"start\":38507},{\"end\":38521,\"start\":38515},{\"end\":38531,\"start\":38525},{\"end\":38544,\"start\":38538},{\"end\":38556,\"start\":38548},{\"end\":38913,\"start\":38909},{\"end\":38927,\"start\":38917},{\"end\":38940,\"start\":38934},{\"end\":38952,\"start\":38944},{\"end\":39275,\"start\":39270},{\"end\":39473,\"start\":39468},{\"end\":39484,\"start\":39477},{\"end\":39494,\"start\":39488},{\"end\":39507,\"start\":39498},{\"end\":39518,\"start\":39513},{\"end\":39819,\"start\":39810},{\"end\":39829,\"start\":39823},{\"end\":40146,\"start\":40138},{\"end\":40153,\"start\":40150},{\"end\":40164,\"start\":40159},{\"end\":40593,\"start\":40587},{\"end\":40601,\"start\":40597},{\"end\":40609,\"start\":40605},{\"end\":40803,\"start\":40796},{\"end\":40815,\"start\":40809},{\"end\":41008,\"start\":40995},{\"end\":41019,\"start\":41012},{\"end\":41030,\"start\":41023},{\"end\":41039,\"start\":41034},{\"end\":41049,\"start\":41043},{\"end\":41062,\"start\":41053},{\"end\":41069,\"start\":41066},{\"end\":41083,\"start\":41075},{\"end\":41093,\"start\":41087},{\"end\":41100,\"start\":41097},{\"end\":41618,\"start\":41614},{\"end\":41631,\"start\":41624},{\"end\":41857,\"start\":41847},{\"end\":41867,\"start\":41861},{\"end\":41877,\"start\":41871},{\"end\":41889,\"start\":41881},{\"end\":41900,\"start\":41893},{\"end\":42114,\"start\":42106},{\"end\":42121,\"start\":42118},{\"end\":42467,\"start\":42458},{\"end\":42477,\"start\":42471},{\"end\":42488,\"start\":42481},{\"end\":42499,\"start\":42494},{\"end\":42786,\"start\":42780},{\"end\":42799,\"start\":42790},{\"end\":42808,\"start\":42803},{\"end\":42820,\"start\":42812},{\"end\":42833,\"start\":42827},{\"end\":42845,\"start\":42837},{\"end\":43243,\"start\":43234},{\"end\":43255,\"start\":43247},{\"end\":43265,\"start\":43259},{\"end\":43275,\"start\":43269},{\"end\":43285,\"start\":43279},{\"end\":43298,\"start\":43292},{\"end\":43540,\"start\":43535},{\"end\":43550,\"start\":43544},{\"end\":43558,\"start\":43554},{\"end\":43569,\"start\":43564},{\"end\":43578,\"start\":43573},{\"end\":44048,\"start\":44043},{\"end\":44056,\"start\":44052},{\"end\":44066,\"start\":44060},{\"end\":44075,\"start\":44070},{\"end\":44087,\"start\":44079},{\"end\":44382,\"start\":44377},{\"end\":44392,\"start\":44386},{\"end\":44400,\"start\":44396},{\"end\":44413,\"start\":44407},{\"end\":44425,\"start\":44417},{\"end\":44733,\"start\":44730},{\"end\":44748,\"start\":44737},{\"end\":44758,\"start\":44752},{\"end\":44768,\"start\":44762},{\"end\":44922,\"start\":44917},{\"end\":44935,\"start\":44926},{\"end\":44946,\"start\":44941},{\"end\":45245,\"start\":45239},{\"end\":45259,\"start\":45249},{\"end\":45270,\"start\":45263},{\"end\":45277,\"start\":45274},{\"end\":45285,\"start\":45281},{\"end\":45587,\"start\":45583},{\"end\":45600,\"start\":45594},{\"end\":45618,\"start\":45604},{\"end\":45628,\"start\":45622},{\"end\":46017,\"start\":46015},{\"end\":46025,\"start\":46021},{\"end\":46480,\"start\":46475},{\"end\":46486,\"start\":46484},{\"end\":46492,\"start\":46490},{\"end\":46500,\"start\":46496},{\"end\":46845,\"start\":46836},{\"end\":46856,\"start\":46849},{\"end\":46865,\"start\":46860},{\"end\":46876,\"start\":46869},{\"end\":46884,\"start\":46880},{\"end\":46894,\"start\":46888},{\"end\":46900,\"start\":46898},{\"end\":46914,\"start\":46904},{\"end\":46922,\"start\":46918},{\"end\":46934,\"start\":46926},{\"end\":47285,\"start\":47280},{\"end\":47296,\"start\":47291}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":32970,\"start\":32797},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3423879},\"end\":33328,\"start\":32972},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":15163269},\"end\":33622,\"start\":33330},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":13181540},\"end\":34000,\"start\":33624},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":11118973},\"end\":34366,\"start\":34002},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1578762},\"end\":34759,\"start\":34368},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":13438951},\"end\":35236,\"start\":34761},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3052384},\"end\":35608,\"start\":35238},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":10591691},\"end\":35962,\"start\":35610},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":13047686},\"end\":36331,\"start\":35964},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":207236141},\"end\":36790,\"start\":36333},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":26252463},\"end\":37104,\"start\":36792},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5697345},\"end\":37485,\"start\":37106},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9958232},\"end\":37779,\"start\":37487},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":16885120},\"end\":38061,\"start\":37781},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7245910},\"end\":38416,\"start\":38063},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8413233},\"end\":38847,\"start\":38418},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":13953980},\"end\":39178,\"start\":38849},{\"attributes\":{\"id\":\"b18\"},\"end\":39421,\"start\":39180},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5328073},\"end\":39751,\"start\":39423},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1822757},\"end\":40055,\"start\":39753},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":206592546},\"end\":40529,\"start\":40057},{\"attributes\":{\"doi\":\"arXiv:1603.06937\",\"id\":\"b22\"},\"end\":40768,\"start\":40531},{\"attributes\":{\"id\":\"b23\"},\"end\":40937,\"start\":40770},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1459429},\"end\":41555,\"start\":40939},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":11462594},\"end\":41786,\"start\":41557},{\"attributes\":{\"id\":\"b26\"},\"end\":42047,\"start\":41788},{\"attributes\":{\"doi\":\"CVLAB-CONF- 2001-005\",\"id\":\"b27\",\"matched_paper_id\":15433078},\"end\":42406,\"start\":42049},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5551454},\"end\":42696,\"start\":42408},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":9506814},\"end\":43131,\"start\":42698},{\"attributes\":{\"id\":\"b30\"},\"end\":43501,\"start\":43133},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14806670},\"end\":43980,\"start\":43503},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":17687873},\"end\":44302,\"start\":43982},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":14989649},\"end\":44694,\"start\":44304},{\"attributes\":{\"id\":\"b34\"},\"end\":44861,\"start\":44696},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":11868277},\"end\":45176,\"start\":44863},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":14049421},\"end\":45518,\"start\":45178},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":39241981},\"end\":45909,\"start\":45520},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":606458},\"end\":46397,\"start\":45911},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":15451489},\"end\":46774,\"start\":46399},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":9616070},\"end\":47209,\"start\":46776},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":5758477},\"end\":47570,\"start\":47211}]", "bib_title": "[{\"end\":33019,\"start\":32972},{\"end\":33383,\"start\":33330},{\"end\":33665,\"start\":33624},{\"end\":34055,\"start\":34002},{\"end\":34450,\"start\":34368},{\"end\":34842,\"start\":34761},{\"end\":35270,\"start\":35238},{\"end\":35658,\"start\":35610},{\"end\":36040,\"start\":35964},{\"end\":36394,\"start\":36333},{\"end\":36843,\"start\":36792},{\"end\":37173,\"start\":37106},{\"end\":37534,\"start\":37487},{\"end\":37820,\"start\":37781},{\"end\":38111,\"start\":38063},{\"end\":38493,\"start\":38418},{\"end\":38905,\"start\":38849},{\"end\":39464,\"start\":39423},{\"end\":39806,\"start\":39753},{\"end\":40132,\"start\":40057},{\"end\":40991,\"start\":40939},{\"end\":41608,\"start\":41557},{\"end\":42102,\"start\":42049},{\"end\":42454,\"start\":42408},{\"end\":42776,\"start\":42698},{\"end\":43531,\"start\":43503},{\"end\":44039,\"start\":43982},{\"end\":44373,\"start\":44304},{\"end\":44913,\"start\":44863},{\"end\":45235,\"start\":45178},{\"end\":45579,\"start\":45520},{\"end\":46011,\"start\":45911},{\"end\":46471,\"start\":46399},{\"end\":46832,\"start\":46776},{\"end\":47276,\"start\":47211}]", "bib_author": "[{\"end\":33033,\"start\":33021},{\"end\":33047,\"start\":33033},{\"end\":33057,\"start\":33047},{\"end\":33066,\"start\":33057},{\"end\":33077,\"start\":33066},{\"end\":33086,\"start\":33077},{\"end\":33396,\"start\":33385},{\"end\":33407,\"start\":33396},{\"end\":33678,\"start\":33667},{\"end\":33687,\"start\":33678},{\"end\":33698,\"start\":33687},{\"end\":33709,\"start\":33698},{\"end\":33725,\"start\":33709},{\"end\":34067,\"start\":34057},{\"end\":34083,\"start\":34067},{\"end\":34460,\"start\":34452},{\"end\":34471,\"start\":34460},{\"end\":34480,\"start\":34471},{\"end\":34490,\"start\":34480},{\"end\":34852,\"start\":34844},{\"end\":34864,\"start\":34852},{\"end\":34875,\"start\":34864},{\"end\":34885,\"start\":34875},{\"end\":34895,\"start\":34885},{\"end\":34906,\"start\":34895},{\"end\":35280,\"start\":35272},{\"end\":35287,\"start\":35280},{\"end\":35296,\"start\":35287},{\"end\":35673,\"start\":35660},{\"end\":35682,\"start\":35673},{\"end\":35694,\"start\":35682},{\"end\":35703,\"start\":35694},{\"end\":35716,\"start\":35703},{\"end\":35725,\"start\":35716},{\"end\":36051,\"start\":36042},{\"end\":36063,\"start\":36051},{\"end\":36074,\"start\":36063},{\"end\":36083,\"start\":36074},{\"end\":36403,\"start\":36396},{\"end\":36413,\"start\":36403},{\"end\":36426,\"start\":36413},{\"end\":36438,\"start\":36426},{\"end\":36451,\"start\":36438},{\"end\":36461,\"start\":36451},{\"end\":36475,\"start\":36461},{\"end\":36486,\"start\":36475},{\"end\":36493,\"start\":36486},{\"end\":36503,\"start\":36493},{\"end\":36853,\"start\":36845},{\"end\":36866,\"start\":36853},{\"end\":36874,\"start\":36866},{\"end\":36887,\"start\":36874},{\"end\":37188,\"start\":37175},{\"end\":37199,\"start\":37188},{\"end\":37544,\"start\":37536},{\"end\":37556,\"start\":37544},{\"end\":37567,\"start\":37556},{\"end\":37832,\"start\":37822},{\"end\":37841,\"start\":37832},{\"end\":37854,\"start\":37841},{\"end\":37868,\"start\":37854},{\"end\":37881,\"start\":37868},{\"end\":38123,\"start\":38113},{\"end\":38132,\"start\":38123},{\"end\":38142,\"start\":38132},{\"end\":38155,\"start\":38142},{\"end\":38168,\"start\":38155},{\"end\":38505,\"start\":38495},{\"end\":38513,\"start\":38505},{\"end\":38523,\"start\":38513},{\"end\":38533,\"start\":38523},{\"end\":38546,\"start\":38533},{\"end\":38558,\"start\":38546},{\"end\":38915,\"start\":38907},{\"end\":38929,\"start\":38915},{\"end\":38942,\"start\":38929},{\"end\":38954,\"start\":38942},{\"end\":39277,\"start\":39268},{\"end\":39475,\"start\":39466},{\"end\":39486,\"start\":39475},{\"end\":39496,\"start\":39486},{\"end\":39509,\"start\":39496},{\"end\":39520,\"start\":39509},{\"end\":39821,\"start\":39808},{\"end\":39831,\"start\":39821},{\"end\":40148,\"start\":40134},{\"end\":40155,\"start\":40148},{\"end\":40166,\"start\":40155},{\"end\":40595,\"start\":40585},{\"end\":40603,\"start\":40595},{\"end\":40611,\"start\":40603},{\"end\":40805,\"start\":40794},{\"end\":40817,\"start\":40805},{\"end\":41010,\"start\":40993},{\"end\":41021,\"start\":41010},{\"end\":41032,\"start\":41021},{\"end\":41041,\"start\":41032},{\"end\":41051,\"start\":41041},{\"end\":41064,\"start\":41051},{\"end\":41071,\"start\":41064},{\"end\":41085,\"start\":41071},{\"end\":41095,\"start\":41085},{\"end\":41102,\"start\":41095},{\"end\":41620,\"start\":41610},{\"end\":41633,\"start\":41620},{\"end\":41859,\"start\":41845},{\"end\":41869,\"start\":41859},{\"end\":41879,\"start\":41869},{\"end\":41891,\"start\":41879},{\"end\":41902,\"start\":41891},{\"end\":42116,\"start\":42104},{\"end\":42123,\"start\":42116},{\"end\":42469,\"start\":42456},{\"end\":42479,\"start\":42469},{\"end\":42490,\"start\":42479},{\"end\":42501,\"start\":42490},{\"end\":42788,\"start\":42778},{\"end\":42801,\"start\":42788},{\"end\":42810,\"start\":42801},{\"end\":42822,\"start\":42810},{\"end\":42835,\"start\":42822},{\"end\":42847,\"start\":42835},{\"end\":43245,\"start\":43232},{\"end\":43257,\"start\":43245},{\"end\":43267,\"start\":43257},{\"end\":43277,\"start\":43267},{\"end\":43287,\"start\":43277},{\"end\":43300,\"start\":43287},{\"end\":43542,\"start\":43533},{\"end\":43552,\"start\":43542},{\"end\":43560,\"start\":43552},{\"end\":43571,\"start\":43560},{\"end\":43580,\"start\":43571},{\"end\":44050,\"start\":44041},{\"end\":44058,\"start\":44050},{\"end\":44068,\"start\":44058},{\"end\":44077,\"start\":44068},{\"end\":44089,\"start\":44077},{\"end\":44384,\"start\":44375},{\"end\":44394,\"start\":44384},{\"end\":44402,\"start\":44394},{\"end\":44415,\"start\":44402},{\"end\":44427,\"start\":44415},{\"end\":44735,\"start\":44725},{\"end\":44750,\"start\":44735},{\"end\":44760,\"start\":44750},{\"end\":44770,\"start\":44760},{\"end\":44924,\"start\":44915},{\"end\":44937,\"start\":44924},{\"end\":44948,\"start\":44937},{\"end\":45247,\"start\":45237},{\"end\":45261,\"start\":45247},{\"end\":45272,\"start\":45261},{\"end\":45279,\"start\":45272},{\"end\":45287,\"start\":45279},{\"end\":45589,\"start\":45581},{\"end\":45602,\"start\":45589},{\"end\":45620,\"start\":45602},{\"end\":45630,\"start\":45620},{\"end\":46019,\"start\":46013},{\"end\":46027,\"start\":46019},{\"end\":46482,\"start\":46473},{\"end\":46488,\"start\":46482},{\"end\":46494,\"start\":46488},{\"end\":46502,\"start\":46494},{\"end\":46847,\"start\":46834},{\"end\":46858,\"start\":46847},{\"end\":46867,\"start\":46858},{\"end\":46878,\"start\":46867},{\"end\":46886,\"start\":46878},{\"end\":46896,\"start\":46886},{\"end\":46902,\"start\":46896},{\"end\":46916,\"start\":46902},{\"end\":46924,\"start\":46916},{\"end\":46936,\"start\":46924},{\"end\":47287,\"start\":47278},{\"end\":47298,\"start\":47287}]", "bib_venue": "[{\"end\":32874,\"start\":32797},{\"end\":33117,\"start\":33086},{\"end\":33445,\"start\":33407},{\"end\":33788,\"start\":33725},{\"end\":34122,\"start\":34083},{\"end\":34540,\"start\":34490},{\"end\":34932,\"start\":34906},{\"end\":35373,\"start\":35296},{\"end\":35756,\"start\":35725},{\"end\":36121,\"start\":36083},{\"end\":36537,\"start\":36503},{\"end\":36927,\"start\":36887},{\"end\":37238,\"start\":37199},{\"end\":37605,\"start\":37567},{\"end\":37901,\"start\":37881},{\"end\":38191,\"start\":38168},{\"end\":38609,\"start\":38558},{\"end\":38982,\"start\":38954},{\"end\":39266,\"start\":39180},{\"end\":39540,\"start\":39520},{\"end\":39872,\"start\":39831},{\"end\":40243,\"start\":40166},{\"end\":40583,\"start\":40531},{\"end\":40792,\"start\":40770},{\"end\":41184,\"start\":41102},{\"end\":41649,\"start\":41633},{\"end\":41843,\"start\":41788},{\"end\":42186,\"start\":42143},{\"end\":42535,\"start\":42501},{\"end\":42885,\"start\":42847},{\"end\":43230,\"start\":43133},{\"end\":43639,\"start\":43580},{\"end\":44123,\"start\":44089},{\"end\":44475,\"start\":44427},{\"end\":44723,\"start\":44696},{\"end\":44996,\"start\":44948},{\"end\":45326,\"start\":45287},{\"end\":45668,\"start\":45630},{\"end\":46104,\"start\":46027},{\"end\":46565,\"start\":46502},{\"end\":46970,\"start\":46936},{\"end\":47363,\"start\":47298},{\"end\":35437,\"start\":35375},{\"end\":39556,\"start\":39542},{\"end\":40307,\"start\":40245},{\"end\":41253,\"start\":41186},{\"end\":42205,\"start\":42188},{\"end\":43685,\"start\":43641},{\"end\":45681,\"start\":45670},{\"end\":46168,\"start\":46106}]"}}}, "year": 2023, "month": 12, "day": 17}