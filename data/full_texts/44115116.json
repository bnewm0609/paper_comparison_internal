{"id": 44115116, "updated": "2023-09-30 21:58:19.763", "metadata": {"title": "Detecting Deceptive Reviews using Generative Adversarial Networks", "authors": "[{\"first\":\"Hojjat\",\"last\":\"Aghakhani\",\"middle\":[]},{\"first\":\"Aravind\",\"last\":\"Machiry\",\"middle\":[]},{\"first\":\"Shirin\",\"last\":\"Nilizadeh\",\"middle\":[]},{\"first\":\"Christopher\",\"last\":\"Kruegel\",\"middle\":[]},{\"first\":\"Giovanni\",\"last\":\"Vigna\",\"middle\":[]}]", "venue": "2018 IEEE Security and Privacy Workshops (SPW)", "journal": "2018 IEEE Security and Privacy Workshops (SPW)", "publication_date": {"year": 2018, "month": 5, "day": 25}, "abstract": "In the past few years, consumer review sites have become the main target of deceptive opinion spam, where fictitious opinions or reviews are deliberately written to sound authentic. Most of the existing work to detect the deceptive reviews focus on building supervised classifiers based on syntactic and lexical patterns of an opinion. With the successful use of Neural Networks on various classification applications, in this paper, we propose FakeGAN a system that for the first time augments and adopts Generative Adversarial Networks (GANs) for a text classification task, in particular, detecting deceptive reviews. Unlike standard GAN models which have a single Generator and Discriminator model, FakeGAN uses two discriminator models and one generative model. The generator is modeled as a stochastic policy agent in reinforcement learning (RL), and the discriminators use Monte Carlo search algorithm to estimate and pass the intermediate action-value as the RL reward to the generator. Providing the generator model with two discriminator models avoids the mod collapse issue by learning from both distributions of truthful and deceptive reviews. Indeed, our experiments show that using two discriminators provides FakeGAN high stability, which is a known issue for GAN architectures. While FakeGAN is built upon a semi-supervised classifier, known for less accuracy, our evaluation results on a dataset of TripAdvisor hotel reviews show the same performance in terms of accuracy as of the state-of-the-art approaches that apply supervised machine learning. These results indicate that GANs can be effective for text classification tasks. Specifically, FakeGAN is effective at detecting deceptive reviews.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1805.10364", "mag": "2963115556", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/sp/AghakhaniMNKV18", "doi": "10.1109/spw.2018.00022"}}, "content": {"source": {"pdf_hash": "2ee54b057bc37692027c7ae60b4084a011ffc1d9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1805.10364v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://ieeexplore.ieee.org/ielx7/8420091/8424589/08424638.pdf", "status": "BRONZE"}}, "grobid": {"id": "081b6fd313667ed2c03d9f3086e6c3a169fd8a1f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2ee54b057bc37692027c7ae60b4084a011ffc1d9.txt", "contents": "\nDetecting Deceptive Reviews using Generative Adversarial Networks\n\n\nHojjat Aghakhani \nUniversity of California\nSanta Barbara\n\nAravind Machiry machiry@cs.ucsb.edu \nUniversity of California\nSanta Barbara\n\nShirin Nilizadeh 2shirin.nilizadeh@sv.cmu.edu \nCarnegie Mellon University Silicon Valley\n\n\nChristopher Kruegel \nUniversity of California\nSanta Barbara\n\nGiovanni Vigna vigna@cs.ucsb.edu \nUniversity of California\nSanta Barbara\n\nDetecting Deceptive Reviews using Generative Adversarial Networks\n\nIn the past few years, consumer review sites have become the main target of deceptive opinion spam, where fictitious opinions or reviews are deliberately written to sound authentic. Most of the existing work to detect the deceptive reviews focus on building supervised classifiers based on syntactic and lexical patterns of an opinion. With the successful use of Neural Networks on various classification applications, in this paper, we propose FakeGAN a system that for the first time augments and adopts Generative Adversarial Networks (GANs) for a text classification task, in particular, detecting deceptive reviews.Unlike standard GAN models which have a single Generator and Discriminator model, FakeGAN uses two discriminator models and one generative model. The generator is modeled as a stochastic policy agent in reinforcement learning (RL), and the discriminators use Monte Carlo search algorithm to estimate and pass the intermediate action-value as the RL reward to the generator. Providing the generator model with two discriminator models avoids the mod collapse issue by learning from both distributions of truthful and deceptive reviews. Indeed, our experiments show that using two discriminators provides FakeGAN high stability, which is a known issue for GAN architectures. While FakeGAN is built upon a semi-supervised classifier, known for less accuracy, our evaluation results on a dataset of TripAdvisor hotel reviews show the same performance in terms of accuracy as of the state-of-the-art approaches that apply supervised machine learning. These results indicate that GANs can be effective for text classification tasks. Specifically, FakeGAN is effective at detecting deceptive reviews.\n\nI. INTRODUCTION\n\nIn the current world, we habitually turn to the wisdom of our peers, and often complete strangers, for advice, instead of merely taking the word of an advertiser or business owner. A 2015 study by marketing research company Mintel [1] found nearly 70 percent of Americans seek out others' opinions online before making a purchase. Many platforms such as Yelp.com and TripAdvisor.com have sprung up to facilitate this sharing of ideas amongst users. The heavy reliance on review information by the users has dramatic effects on business owners. It has been shown that an extra half-star rating on Yelp helps restaurants to sell out 19 percentage points more frequently [2].\n\nThis phenomenon has also lead to a market for various kinds of fraud. In simple cases, this could be a business rewarding its customers with a discount, or outright paying them, to write a favorable review. In more complex cases, this could involve astroturfing, opinion spamming [3] or deceptive opinion spamming [4], where fictitious reviews are deliberately written to sound authentic. Figure 1 shows an example of a truthful and deceptive review written for the same hotel. It is estimated that up to 25% of Yelp reviews are fraudulent [5], [6].\n\nDetecting deceptive reviews is a text classification problem. In recent years, deep learning techniques based on natural language processing have been shown to be successful for text classification tasks. Recursive Neural Network (Recur-siveNN) [7], [8], [9] has shown good performance classifying texts, while Recurrent Neural Network (RecurrentNN) [10] better captures the contextual information and is ideal for realizing semantics of long texts. However, RecurrentNN is a biased model, where later words in a text have more influence than earlier words [11]. This is not suitable for tasks such as detection of deceptive reviews that depend on an unbiased semantics of the entire document (review). Recently, techniques based on Convolutional Neural Network (CNN) [12], [13] were shown to be effective for text classification. However, the effectiveness of these techniques depends on careful selection of the window size [11], which controls the parameter space.\n\nMoreover, in general, the main problem with applying classification methods for detecting deceptive reviews is the lack of substantial ground truth datasets required for most of the supervised machine learning techniques. This problem worsens for neural networks based methods, whose complexity requires much bigger dataset to reach a reasonable performance.\n\nTo address the limitations of the existing techniques, we propose FakeGAN, which is a technique based on Generative Adversarial Network (GAN) [14]. GANs are a class of artificial intelligence algorithms used in unsupervised machine learning, implemented by a system of two neural networks contesting with each other in a zero-sum game framework. GANs have been used mostly for image-based applications [14], [15], [16], [17]. In this paper, for the first time, we propose the use of GANs for a text classification task, i.e., detecting deceptive reviews. Moreover, the use of a semi-supervised learning method like GAN can eliminate the problem of ground truth scarcity that in general hinders the detection success [4], [18], [19].\n\nWe augment GAN models for our application in such a way that unlike standard GAN models which have a single Generator and Discriminator model, FakeGAN uses two discriminator models D, D and one generative model G. The discriminator model D tries to distinguish between truthful and deceptive reviews whereas D tries to distinguish between reviews generated by the generative model G and samples from deceptive reviews distribution. The discriminator model D helps G to generate reviews close to the deceptive reviews distribution, while D helps G to generate reviews which are classified by D as truthful.\n\nOur intuition behind using two discriminators is to create a stronger generator model. If in the adversarial learning phase, the generator gets rewards only from D, the GAN may face the mod collapse issue [20], as it tries to learn two different distributions (truthful and deceptive reviews). The combination of D and D trains G to generate better deceptive reviews which in turn train D to be a better discriminator.\n\nIndeed, our evaluation using the TripAdvisor 1 hotel reviews dataset shows that the discriminator D generated by FakeGAN performs on par with the state-of-the-art methods that apply supervised machine learning, with an accuracy of 89.1%. These results indicate that GANs can be effective for text classification tasks, specifically, FakeGAN is effective at detecting deceptive reviews. To the best of our knowledge, FakeGAN is the first work that use GAN to generate better discriminator model (i.e., D) in contrast to the common GAN applications which aim to improve the generator model.\n\nIn summary, following are our contributions: 1) We propose FakeGAN, a deceptive review detection system based on a double discriminator GAN. 2) We believe that FakeGAN demonstrates a good first step towards using GANs for text classification tasks. 3) To the best of our knowledge, FakeGAN is the first system using semi-supervised neural network-based learning methods for detecting deceptive fraudulent reviews. 4) Our evaluation results demonstrate that FakeGAN is as effective as the state-of-the-art methods that apply supervised machine learning for detecting deceptive reviews.\n\n\nII. APPROACH\n\nGenerative Adversarial Network (GAN) [14] is a promising framework for generating high-quality samples with the same distribution as the target dataset. FakeGAN leverages GAN to learn the distributions of truthful and deceptive reviews and to build a semi-supervised classifier using the corresponding distributions.\n\nA GAN consists of two models: a generative model G which tries to capture the data distribution, and a discriminative model D that distinguishes between samples coming from the training data or the generator G. These two models are trained simultaneously, where G is trying to fool the discriminator D, while D is maximizing its probability estimation that whether a sample comes from the training data or is produced by the generator. In a nutshell, this framework corresponds to a minimax two-player game. 1 Tripadvisor.com The feedback or the gradient update from discriminator model plays a vital role in the effectiveness of a GAN. In the case of text generation, it is difficult to pass the gradient update because the generative model produces discrete tokens (words), but the discriminative model makes a decision for complete sequence or sentence. Inspired by SeqGAN [21] that uses GAN model for Chinese poem generation, in this work, we model the generator as a stochastic policy in reinforcement learning (RL), where the gradient update or RL reward signal is provided by the discriminator using Monte Carlo search. Monte Carlo is a heuristic search algorithm for identifying the most promising moves in a game. In summary, in each state of the game, it plays out the game to the very end for a fixed number of times according to a given policy. To find the most promising move, it must be provided by reward signals for a complete sequence of moves.\n\nAll the existing applications use GAN to create a strong generator, where the main issue is the convergence of generator model [22], [23], [20]. Mode collapse in particular is a known problem in GANs, where complexity and multimodality of the input distribution cause the generator to produce samples from a single mode. The generator may switch between modes during the learning phase, and this cat-and-mouse game may never end [24], [20]. Although no formal proof exists for convergence, in Section III we show that the FakeGAN's discriminator converges in practice.\n\nUnlike the typical applications of GANs, where the ultimate goal is to have a strong generator, FakeGAN leverages GAN to create a well-trained discriminator, so that it can successfully distinguish truthful and deceptive reviews. However, to avoid the stability issues inherent to GANs we augment our network to have two discriminator models though we use only one of them as our intended classifier. Note that leveraging samples generated by the generator makes our classifier a semi-supervised classifier.\n\n\nDefinitions\n\nWe start with defining certain symbols which will be used throughout this section to define various steps of our approach. The training dataset, X = X D \u222a X T , consists of two parts, deceptive reviews X D and truthful reviews X T . We use \u03c7 to denote the vocabulary of all tokens (i.e., words) which are available in X.\n\nOur generator model G \u03b1 parametrized by \u03b1 produces each review S 1:L as a sequence of tokens of length L where S 1:L \u2208 \u03c7 L . We use Z G to indicate all the reviews generated by our generator model G \u03b1 .\n\nWe use two discriminator models D and D . The discriminator D distinguishes between truthful and deceptive reviews, as such D(S 1:L ) is the probability that the sequence of tokens comes from X T or X D \u222a Z G . Similarly, D distinguishes between deceptive samples in the dataset and samples generated by G \u03b1 consequently D (S 1:L ) is a probability indicating how likely the sequence of tokens comes from X D or Z G .\n\nThe discriminator D guides the generator G \u03b1 to produce samples similar to X D whereas D guides G \u03b1 to generate  samples which seems truthful to D. So in each round of training, by using the feedback from D and D , the generator G \u03b1 tries to fool D and D by generating reviews that seems deceptive (not generated by G \u03b1 ) to D , and truthful (not generated by G \u03b1 or comes from X D ) to D. Figure 2 shows an overview of FakeGAN. During pretraining, we use the Maximum Likelihood Estimation (MLE) to train the generator G \u03b1 on deceptive reviews X D from the training dataset. We also use minimizing the cross-entropy technique to pre-train the discriminators.\n\nThe generator G \u03b1 is defined as a policy model in reinforcement learning. In timestep t, the state s is the sequence of produced tokens, and the action a is the next token. The policy model G \u03b1 (S t |S 1:t\u22121 ) is stochastic. Furthermore, the generator G \u03b1 is trained by using a policy gradient and Monte Carlo (MC) search on the expected end reward from the discriminative models D and D . Similar to [21], we consider the estimated probability D(S 1:L ) + D (S 1:L ) as the reward. Formally, the corresponding action-value function is: As mentioned before, G \u03b1 produces a review token by token. However, the discriminators provide the reward for a complete sequence. Moreover, G \u03b1 should care about the long-term reward, similar to playing Chess where players sometimes prefer to give up immediate good moves for a long-term goal of victory [25]. Therefore, to estimate the action-value function in every timestep t, we apply the Monte Carlo search N times with a roll-out policy G \u03b3 to sample the undetermined last L \u2212 t tokens. We define an N -time Monte Carlo search as\nA G\u03b1,{S 1 1:L , S 2 1:L , ..., S N 1:L } = M C G \u03b3 (S 1:t , N )(2)\nwhere\nfor 1 \u2264 i \u2264 N S i 1:t = (S 1 , ..., S t )(3)\nand S i t+1:L is sampled via roll-out policy G \u03b3 based on the current state S i 1:t\u22121 . The complexity of action-value estimation function mainly depends on the roll-out policy. While one might use a simple version (e.g., random sampling or sampling based on n-gram features) as the policy to train the GAN fast, to be more efficient, we use the same generative model (G \u03b3 = G \u03b1 at time t). Note that, a higher value of N results in less variance and more accurate evaluation of the actionvalue function. We can now define the action-value estimation function at t as\nA G\u03b1,D,D (a = S t , s = S 1:t\u22121 ) = 1 N N i=1 (D(S i 1:L ) + D (S i 1:L )) if t \u2264 L D(S 1:L ) + D (S 1:L ) if t = L(4)\nwhere S i 1:L s are created according to the Equation 2. As there is no intermediate reward for the generator, we define the the objective function for the generator G \u03b1 (based on [26]) to produce a sequence from the start state S 0 to maximize its final reward:\nJ(\u03b1) = S1\u2208\u03c7 G \u03b1 (S 1 |S 0 ) . A G\u03b1,D,D (a = S 1 , s = S 0 ) (5)\nConseqently, the gradient of the objective function J(\u03b1) is: We update the generator's parameters (\u03b1) as:\n\u2207\u03b1J(\u03b1) =\u03b1 \u2190 \u03b1 + \u03bb\u2207 \u03b1 J(\u03b1)(7)\nwhere \u03bb is the learning rate. By dynamically updating the discriminative models, we can further improve the generator. So, after generating g samples, we will re-train the discriminative models D and D for d steps using the following objective functions respectively:\nmin(\u2212E S\u223cX T [log D(S)] \u2212 E S\u223cX D \u2228G\u03b1 [1 \u2212 log D(S)]) (8) min(\u2212E S\u223cX D [log D (S)] \u2212 E S\u223cG\u03b1 [1 \u2212 log D (S)]) (9)\nIn each of the d steps, we use G \u03b1 to generate the same number of samples as number of truthful reviews i.e., |X G | = |X T |. The updated discriminators will be used to update the generator, and this cycle continues until FakeGAN converges. Algorithm 1 formally defines all the above steps.\n\n\nAlgorithm 1 FakeGAN\n\nRequire: discriminators D and D , generator G \u03b1 , roll-out policy G \u03b3 , dataset X Initialize \u03b1 with random weight. Load word2vec vector embeddings into G \u03b1 , D and D models Pre-train G \u03b1 using MLE on X D Pre-train D by minimizing the cross entropy Generate negative examples by G \u03b1 for training D Pre-train D by minimizing the cross entropy \u03b3 \u2190 \u03b1 repeat for g-steps do Generate a sequence of tokens S 1:L = (S 1 , ..., S L ) \u223c G \u03b1 for t in 1 : L do Compute A G\u03b1,D \u03b2 ,D \u03b8 (a = S t , s = S 1:t\u22121 ) by Eq. 4 end for Update \u03b1 via policy gradient Eq. 7 end for for d-steps do Use G \u03b1 to generate X G . Train discriminator D by Eq. 8 Train discriminator D by Eq. 9 end for \u03b3 \u2190 \u03b1 until D reaches a stable accuracy.\n\n\nThe Generative Model\n\nWe use RecurrentNNs (RNNs) to construct the generator. An RNN maps the input embedding representations s 1 , ..., s L of the input sequence of tokens S 1 , ..., S L into hidden states h 1 , ..., h L by using the following recursive function.\nh t = g(h t\u22121 , s t )(10)\nFinally, a softmax output layer z with bias vector c and weight matrix V maps the hidden layer neurons into the output token distribution as\np(s|s 1 , ..., s t ) = z(h t ) = softmax(c + V.h t )(11)\nTo deal with the common vanishing and exploding gradient problem [27] of the backpropagation through time, we exploit the Long Short-Term Memory (LSTM) cells [28].\n\n\nThe Discriminator Model\n\nFor the discriminators, we select the CNN because of their effectiveness for text classification tasks [29]. First, we construct the matrix of the sequence by concatenating the input embedding representations of the sequence of tokens s 1 , ..., s L as:\n\u03b6 1:L = s 1 \u2295 ... \u2295 s L(12)\nThen a kernel w computes a convolutional operation to a window size of l by using a non-linear function \u03c0, which results in a feature map:\nf i = \u03c0(w \u2297 \u03b6 i:i+l\u22121 + b)(13)\nWhere \u2297 is the inner product of two vectors, and b is a bias term. Usually, various numbers of kernels with different window sizes are used in CNN. We hyper-tune size of kernels by trying kernels which have been successfully used in text classification tasks by community [13], [30], [11]. Then we apply a maxover-time pooling operation over the feature maps to allow us to combine the outputs of different kernels. Based on [31] we add the highway architecture to improve the performance.\n\nIn the end, a fully connected layer with sigmoid activation functions is used to output the class probability of the input sequence.\n\nIII. EVALUATION We implemented FakeGAN using the TensorFlow [32] framework. We chose the dataset from [4] which has 800 reviews of 20 Chicago hotels with positive sentiment. The dataset consists of 400 truthful reviews provided by high profile users on TripAdvisor and 400 deceptive reviews written by Amazon Mechanical Workers. To the best of our knowledge, this is the biggest available dataset of labeled reviews and has been used by many related works [4], [18], [33]. Similar to SeqGAN [21], the generator in FakeGAN only creates fixed length sentences. Since the majority of reviews in this dataset has a length less than 200 words, we set the sequence length of FakeGAN (L) to 200. For sentences whose length is less than 200, we pad them with a fixed token <END> to reach the size of 200 resulting in 332 truthful and 353 deceptive reviews. Note that, having a larger dataset results in a less training time. Although larger dataset makes each adversarial step slower, it provides G a richer distribution of samples, thus reduces the number of adversarial steps resulting in less training time.\n\nWe used the k-fold cross-validation with k=5 to evaluate FakeGAN. We leveraged GloVe vectors 2 for word representation [34]. Similar to SeqGAN [21], the convergence of FakeGAN varies with the training parameters g and d of generator and discriminative models respectively. After experimenting with different values, we observed that following values g = 1 and d = 6 are optimal. For pre-training phase, we trained the generator and the discriminators until convergence, which took 120 and 50 steps respectively. The adversarial learning starts after the pre-training phase. All our experiments were run on a 40-core machine, where the pre-training took \u223cone hour, and the adversarial training took \u223c11 hours with a total of \u223c12 hours.\n\nA. Accuracy of Discriminator D As mentioned before, the goal of FakeGAN is to generate a highly accurate discriminator model, D, that can distinguish deceptive and truthful reviews. Figure 3a shows the accuracy trend for this model; for simplicity, the trend is shown only for the first iteration of k-fold cross-validation. During the pre-training phase, the accuracy of D stabilized at 50 th step. We set the adversarial learning to begin at step 51. After a little decrease in accuracy at the beginning, the accuracy increases and converges to 89.2%, which is on-par with the accuracy of state-of-the-art approach [4] that applied supervised machine learning on the same dataset (\u223c 89.8%). The accuracy, precision and recall for k-fold cross-validation are 89.1%, 98% and 81% all with a standard deviation of 0.5. This supports our hypothesis that adversarial training can be used for detecting deceptive reviews. Interestingly even though FakeGAN relies on semi-supervised learning, it yields similar performance as of a fully-supervised classification algorithm.\n\nB. Accuracy of Discriminator D Figure 3b shows the accuracy trend for the discriminator D . Similar to D, D converges after 450 steps with an accuracy of \u223c 99% accuracy. It means that at this point, the generator G will not be able to make any progress trying to fool D , and the output distribution of G will stay almost same. Thus, continuing adversarial learning does not result in any improvement of the accuracy of our main discriminator, D.\n\n\nC. Comparing FakeGAN with the original GAN approach\n\nTo justify the use of two discriminators in FakeGAN, we tried using just one discriminator (only D) in two different settings. In the first case, the generator G is pre-trained to learn only truthful reviews distribution. Here the discriminator D reached 83% accuracy in pre-training, and the accuracy of adversarial learning, i.e., the classifier, reduces to about 65%. In the second case, the generator G is pre-trained to learn only deceptive reviews distribution. Unlike the first case, adversarial learning improved the performance of D by converging at 84%, however, still, the performance is lower than that of FakeGAN.\n\nThese results demonstrate that using two discriminators is necessary to improve the accuracy of FakeGAN.\n\n\nD. Scalability Discussion\n\nWe argue that the time complexity of our proposed augmented GAN with two discriminators is the same as of original GANs because their bottleneck is the MC search, where using the rollout policy (which is G until the time) generates 16 complete sequences, to help the generator G for just outputting the most promising token as its current action. This happens for every token of a sequence which is generated by G. However, compared to MC search, discriminators D and D are efficient and not time-consuming.\n\n\nE. Stability Discussion\n\nAs we discussed in Section II, the stability of GANs is a known issue. We observed that the parameters g and d have a large effect on the convergence and performance of FakeGAN as illustrated in the Figure 4, when d and g are both equal to one. We believe that the stability of GAN makes hypertuning of FakeGAN a challenging task thus prevents it from outperforming the state-of-the-art methods based on supervised machine learning. However, with the following values d = 6 and g = 1, FakeGAN converges and performs on par with the state-of-the-art approach.\n\n\nIV. RELATED WORK\n\nText classification has been used extensively in email spam [35] detection and link spam detection in web pages [36], [37], [38]. Over the last decade, researchers have been working on deceptive opinion spam.\n\nJindal et al. [3] first introduced deceptive opinion spam problem as a widespread phenomenon and showed that it is different from other traditional spam activities. They built their ground truth dataset by considering the duplicate reviews as spam reviews and the rest as nonspam reviews. They extracted features related to review, product and reviewer, and trained a Logistic Regression model on these features to find fraudulent reviews on Amazon. Wu et al. [39] claimed that deleting dishonest reviews will distort the popularity significantly. They leveraged this idea to detect deceptive opinion spam in the absence of ground truth data. Both of these heuristic evaluation approaches are not necessarily true and thorough.\n\nYoo et al. [19] instructed a group of tourism marketing students to write a hotel review from the perspective of a hotel manager. They gathered 40 truthful and 42 deceptive hotel reviews and found that truthful and deceptive reviews have different lexical complexity. Ott et al. [4] created a much larger dataset of 800 opinions by crowdsourcing 3 the job of writing fraudulent reviews for existing businesses. They combined work from psychology and computational linguistics to develop and compare three 4 approaches for detecting deceptive opinion spam. On a similar dataset, Feng et al. [33] trained Support Vector Machine model based on syntactic stylometry features for deception detection. Li et al. [18] also combined ground truth dataset created by Ott et al. [4] with their employee (domain-expert) generated deceptive reviews to build a feature-based additive model for exploring the general rule for deceptive opinion spam detection. Rahman (a) Accuracy of FakeGAN (Discriminator D) at each step by feeding the testing dataset to D. While minimizing cross entropy method for pre-training D converges and reaches accuracy at \u223c 82%, adversarial training phase boosts the accuracy to \u223c 89%.\n\n(b) Accuracy of D at each step by feeding the testing dataset and generated samples by G to D . Similar to figure 3a, this plot shows that D converged after 450 steps resulting in the convergence of FakeGAN.  et al. [40] developed a system to detect venues that are targets of deceptive opinions. Although, this easies the identification of deceptive reviews considerable effort is still involved in identifying the actual deceptive reviews. In almost all these works, the size of the dataset limits the proposed model to reach its real capacity.\n\nTo alleviate these issues with the ground truth, we use a Generative adversarial network, which is more an unsupervised learning method rather than supervised. We start with an existing dataset and use the generator model to create necessary reviews to strengthen the classifier (discriminator).\n\n\nV. FUTURE WORK\n\nContrary to the popular belief that supervised learning techniques are superior to unsupervised techniques, the accuracy of FakeGAN, a semi-supervised learning technique is comparable to the state-of-the-art supervised techniques on the same dataset. We believe that this is a preliminary step which we plan to extend by trying different architectures like Conditional GAN [41] and better hyper-tuning.\n\n\nVI. CONCLUSION\n\nIn this paper, we propose FakeGAN, a technique to detect deceptive reviews using Generative Adversarial Networks (GAN). To the best of our knowledge, this is the first work to leverage GANs and semi-supervised learning methods to identify deceptive reviews. Our evaluation using a dataset of 800 reviews from 20 Chicago hotels of TripAdvisor shows that FakeGAN with an accuracy of 89.1% performed on par with the state-of-the-art models. We believe that FakeGAN demonstrates a good first step towards using GAN for text classification tasks, specifically those requiring very large ground truth datasets.\n\n\nreview provided by a high profile user on TripAdvisor (b) A deceptive review written by an Amazon Mechanical worker Fig. 1: A truthful review versus a deceptive review, both written for the same hotel.\n\nFig. 2 :\n2The overview of FakeGAN. The symbols + and \u2212 indicates positive and negative samples respectively. Note that, these are different from truthful and deceptive reviews.\n\n\nD,D (a = S L , s = S 1:L\u22121 ) = D(S 1:L ) + D (S 1:L ) (1)\n\n\n:t\u22121 \u223cG\u03b1 [ St\u2208\u03c7 \u2207\u03b1G\u03b1(St|S1:t\u22121) . AG \u03b1,D,D (a = St, s = S1:t\u22121)] (6)\n\nFig. 3 :\n3The accuracy of D and D on the test dataset over epochs. The vertical dashed line shows the beginning of adversarial training. (a) The accuracy of D fluctuates around 77% in constrast to the stabilization at 89.1% in Figure 3a (with values g=1 and d=6) (b) Accuracy of D . Unlike inFigure 3b, this plot shows that D is not stable.\n\nFig. 4 :\n4The accuracy of D and D on the test dataset over epochs while both g and d are one.\nCheck \"glove.6B.200d.txt\" from https://nlp.stanford.edu/projects/glove/\nThey used Amazon Mechanical Turk 4 Genre identification, psycholinguistic deception detection, and text categorization.\nACKNOWLEDGEMENTSWe would like to thank the anonymous reviewers for their valuable comments. This material is based on research sponsored by the Office of Naval Research under grant numbers N00014-15-1-2948, N00014-17-1-2011 and by DARPA under agreement number FA8750-15-2-0084. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. This work is also sponsored by a gift from Google's Anti-Abuse group. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA or the U.S. Government.\nSeven in 10 americans seek out opinion before making purchases. M Marketing Research, Company, M. marketing research company, \"Seven in 10 amer- icans seek out opinion before making purchases,\" http://www.mintel.com/press-centre/social-and-lifestyle/ seven-in-10-americans-seek-out-opinions-before-making-purchases, 2015.\n\nLearning from the crowd: Regression discontinuity estimates of the effects of an online review database. M Anderson, J Magruder, The Economic Journal. 122563M. Anderson and J. Magruder, \"Learning from the crowd: Regression discontinuity estimates of the effects of an online review database,\" The Economic Journal, vol. 122, no. 563, pp. 957-989, 2012.\n\nOpinion spam and analysis. N Jindal, B Liu, Proceedings of the 2008 International Conference on Web Search and Data Mining, ser. WSDM '08. the 2008 International Conference on Web Search and Data Mining, ser. WSDM '08New York, NY, USAACMN. Jindal and B. Liu, \"Opinion spam and analysis,\" in Proceedings of the 2008 International Conference on Web Search and Data Mining, ser. WSDM '08. New York, NY, USA: ACM, 2008, pp. 219-230. [Online].\n\n. http:/doi.acm.org/10.1145/1341531.1341560Available: http://doi.acm.org/10.1145/1341531.1341560\n\nFinding deceptive opinion spam by any stretch of the imagination. M Ott, Y Choi, C Cardie, J T Hancock, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics1M. Ott, Y. Choi, C. Cardie, and J. T. Hancock, \"Finding deceptive opinion spam by any stretch of the imagination,\" in Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Computational Linguistics, 2011, pp. 309-319.\n\nYelp admits a quarter of submitted reviews could be fake. B Technology, B. Technology, \"Yelp admits a quarter of submitted reviews could be fake,\" September 2013, http://www.bbc.com/news/technology-24299742.\n\nFake it till you make it: Reputation, competition, and yelp review fraud. M Luca, G Zervas, Management Science. M. Luca and G. Zervas, \"Fake it till you make it: Reputation, competition, and yelp review fraud,\" Management Science, 2016.\n\nSemi-supervised recursive autoencoders for predicting sentiment distributions. R Socher, J Pennington, E H Huang, A Y Ng, C D Manning, Proceedings of the conference on empirical methods in natural language processing. the conference on empirical methods in natural language processingAssociation for Computational LinguisticsR. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning, \"Semi-supervised recursive autoencoders for predicting sentiment distribu- tions,\" in Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, 2011, pp. 151-161.\n\nDynamic pooling and unfolding recursive autoencoders for paraphrase detection. R Socher, E H Huang, J Pennin, C D Manning, A Y Ng, Advances in neural information processing systems. R. Socher, E. H. Huang, J. Pennin, C. D. Manning, and A. Y. Ng, \"Dynamic pooling and unfolding recursive autoencoders for paraphrase detection,\" in Advances in neural information processing systems, 2011, pp. 801-809.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. R Socher, A Perelygin, J Y Wu, J Chuang, C D Manning, A Y Ng, C Potts, Proceedings of the conference on empirical methods in natural language processing (EMNLP). the conference on empirical methods in natural language processing (EMNLP)16311642R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D. Manning, A. Y. Ng, C. Potts et al., \"Recursive deep models for semantic compositionality over a sentiment treebank,\" in Proceedings of the conference on empirical methods in natural language processing (EMNLP), vol. 1631, 2013, p. 1642.\n\nFinding structure in time. J L Elman, Cognitive science. 142J. L. Elman, \"Finding structure in time,\" Cognitive science, vol. 14, no. 2, pp. 179-211, 1990.\n\nRecurrent convolutional neural networks for text classification. S Lai, L Xu, K Liu, J Zhao, AAAI. 333S. Lai, L. Xu, K. Liu, and J. Zhao, \"Recurrent convolutional neural networks for text classification.\" in AAAI, vol. 333, 2015, pp. 2267- 2273.\n\nConvolutional neural networks for sentence classification. Y Kim, arXiv:1408.5882arXiv preprintY. Kim, \"Convolutional neural networks for sentence classification,\" arXiv preprint arXiv:1408.5882, 2014.\n\nCharacter-level convolutional networks for text classification. X Zhang, J Zhao, Y Lecun, Advances in neural information processing systems. X. Zhang, J. Zhao, and Y. LeCun, \"Character-level convolutional networks for text classification,\" in Advances in neural information processing systems, 2015, pp. 649-657.\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, \"Generative adversarial nets,\" in Advances in neural information processing systems, 2014, pp. 2672-2680.\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. A Radford, L Metz, S Chintala, arXiv:1511.06434arXiv preprintA. Radford, L. Metz, and S. Chintala, \"Unsupervised representation learning with deep convolutional generative adversarial networks,\" arXiv preprint arXiv:1511.06434, 2015.\n\nSegan: Segmenting and generating the invisible. K Ehsani, R Mottaghi, A Farhadi, arXiv:1703.10239arXiv preprintK. Ehsani, R. Mottaghi, and A. Farhadi, \"Segan: Segmenting and generating the invisible,\" arXiv preprint arXiv:1703.10239, 2017.\n\nDeep generative image models using a laplacian pyramid of adversarial networks. E L Denton, S Chintala, R Fergus, Advances in neural information processing systems. E. L. Denton, S. Chintala, R. Fergus et al., \"Deep generative image models using a laplacian pyramid of adversarial networks,\" in Advances in neural information processing systems, 2015, pp. 1486-1494.\n\nTowards a general rule for identifying deceptive opinion spam. J Li, M Ott, C Cardie, E H Hovy, ACL. CiteseerJ. Li, M. Ott, C. Cardie, and E. H. Hovy, \"Towards a general rule for identifying deceptive opinion spam.\" in ACL (1). Citeseer, 2014, pp. 1566-1576.\n\nComparison of deceptive and truthful travel reviews. K.-H Yoo, U Gretzel, K.-H. Yoo and U. Gretzel, \"Comparison of deceptive and truthful travel reviews,\" Information and communication technologies in tourism 2009, pp. 37-47, 2009.\n\nUnrolled generative adversarial networks. L Metz, B Poole, D Pfau, J Sohl-Dickstein, arXiv:1611.02163arXiv preprintL. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein, \"Unrolled generative adversarial networks,\" arXiv preprint arXiv:1611.02163, 2016.\n\nSeqgan: sequence generative adversarial nets with policy gradient. L Yu, W Zhang, J Wang, Y Yu, Thirty-First AAAI Conference on Artificial Intelligence. L. Yu, W. Zhang, J. Wang, and Y. Yu, \"Seqgan: sequence generative adversarial nets with policy gradient,\" in Thirty-First AAAI Conference on Artificial Intelligence, 2017.\n\nWasserstein gan. M Arjovsky, S Chintala, L Bottou, arXiv:1701.07875arXiv preprintM. Arjovsky, S. Chintala, and L. Bottou, \"Wasserstein gan,\" arXiv preprint arXiv:1701.07875, 2017.\n\nImproved training of wasserstein gans. I Gulrajani, F Ahmed, M Arjovsky, V Dumoulin, A Courville, arXiv:1704.00028arXiv preprintI. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville, \"Improved training of wasserstein gans,\" arXiv preprint arXiv:1704.00028, 2017.\n\nI Goodfellow, arXiv:1701.00160Nips 2016 tutorial: Generative adversarial networks. arXiv preprintI. Goodfellow, \"Nips 2016 tutorial: Generative adversarial networks,\" arXiv preprint arXiv:1701.00160, 2016.\n\nMastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, Nature. 5297587D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot et al., \"Mastering the game of go with deep neural networks and tree search,\" Nature, vol. 529, no. 7587, pp. 484-489, 2016.\n\nPolicy gradient methods for reinforcement learning with function approximation. R S Sutton, D A Mcallester, S P Singh, Y Mansour, Advances in neural information processing systems. R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour, \"Policy gradient methods for reinforcement learning with function approximation,\" in Advances in neural information processing systems, 2000, pp. 1057- 1063.\n\nDeep learning. I Goodfellow, Y Bengio, A Courville, MIT PressI. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT Press, 2016.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 98S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\" Neural computation, vol. 9, no. 8, pp. 1735-1780, 1997.\n\nText understanding from scratch. X Zhang, Y Lecun, arXiv:1502.01710arXiv preprintX. Zhang and Y. LeCun, \"Text understanding from scratch,\" arXiv preprint arXiv:1502.01710, 2015.\n\nW Y Wang, arXiv:1705.00648liar, liar pants on fire\": A new benchmark dataset for fake news detection. arXiv preprintW. Y. Wang, \"\" liar, liar pants on fire\": A new benchmark dataset for fake news detection,\" arXiv preprint arXiv:1705.00648, 2017.\n\nHighway networks. R K Srivastava, K Greff, J Schmidhuber, arXiv:1505.00387arXiv preprintR. K. Srivastava, K. Greff, and J. Schmidhuber, \"Highway networks,\" arXiv preprint arXiv:1505.00387, 2015.\n\nTensorflow: Large-scale machine learning on heterogeneous distributed systems. M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen, C Citro, G S Corrado, A Davis, J Dean, M Devin, arXiv:1603.04467arXiv preprintM. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin et al., \"Tensorflow: Large-scale machine learning on heterogeneous distributed systems,\" arXiv preprint arXiv:1603.04467, 2016.\n\nSyntactic stylometry for deception detection. S Feng, R Banerjee, Y Choi, Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers. the 50th Annual Meeting of the Association for Computational Linguistics: Short PapersAssociation for Computational Linguistics2S. Feng, R. Banerjee, and Y. Choi, \"Syntactic stylometry for deception detection,\" in Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2. Association for Computational Linguistics, 2012, pp. 171-175.\n\nGlove: Global vectors for word representation. J Pennington, R Socher, C D Manning, Empirical Methods in Natural Language Processing. J. Pennington, R. Socher, and C. D. Manning, \"Glove: Global vectors for word representation,\" in Empirical Methods in Natural Language Processing (EMNLP), 2014, pp. 1532-1543. [Online]. Available: http://www.aclweb.org/anthology/D14-1162\n\nSupport vector machines for spam categorization. H Drucker, D Wu, V N Vapnik, IEEE Transactions on Neural networks. 105H. Drucker, D. Wu, and V. N. Vapnik, \"Support vector machines for spam categorization,\" IEEE Transactions on Neural networks, vol. 10, no. 5, pp. 1048-1054, 1999.\n\nCombating web spam with trustrank. Z Gy\u00f6ngyi, H Garcia-Molina, J Pedersen, Proceedings of the Thirtieth international conference on Very large data bases. the Thirtieth international conference on Very large data bases30Z. Gy\u00f6ngyi, H. Garcia-Molina, and J. Pedersen, \"Combating web spam with trustrank,\" in Proceedings of the Thirtieth international conference on Very large data bases-Volume 30. VLDB Endowment, 2004, pp. 576-587.\n\nDetecting spam web pages through content analysis. A Ntoulas, M Najork, M Manasse, D Fetterly, Proceedings of the 15th international conference on World Wide Web. the 15th international conference on World Wide WebACMA. Ntoulas, M. Najork, M. Manasse, and D. Fetterly, \"Detecting spam web pages through content analysis,\" in Proceedings of the 15th international conference on World Wide Web. ACM, 2006, pp. 83-92.\n\nWeb spam taxonomy. Z Gyongyi, H Garcia-Molina, First international workshop on adversarial information retrieval on the web. Z. Gyongyi and H. Garcia-Molina, \"Web spam taxonomy,\" in First international workshop on adversarial information retrieval on the web (AIRWeb 2005), 2005.\n\nDistortion as a validation criterion in the identification of suspicious reviews. G Wu, D Greene, B Smyth, P Cunningham, Proceedings of the First Workshop on Social Media Analytics. the First Workshop on Social Media AnalyticsACMG. Wu, D. Greene, B. Smyth, and P. Cunningham, \"Distortion as a validation criterion in the identification of suspicious reviews,\" in Proceedings of the First Workshop on Social Media Analytics. ACM, 2010, pp. 10-13.\n\nTurning the tide: Curbing deceptive yelp behaviors. M Rahman, B Carbunar, J Ballesteros, G Burri, D Horng, SDM. SIAM. M. Rahman, B. Carbunar, J. Ballesteros, G. Burri, D. Horng et al., \"Turning the tide: Curbing deceptive yelp behaviors.\" in SDM. SIAM, 2014, pp. 244-252.\n\nConditional generative adversarial nets. M Mirza, S Osindero, abs/1411.1784CoRR. M. Mirza and S. Osindero, \"Conditional generative adversarial nets,\" CoRR, vol. abs/1411.1784, 2014. [Online]. Available: http: //arxiv.org/abs/1411.1784\n", "annotations": {"author": "[{\"end\":126,\"start\":69},{\"end\":203,\"start\":127},{\"end\":294,\"start\":204},{\"end\":355,\"start\":295},{\"end\":429,\"start\":356}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":76},{\"end\":142,\"start\":135},{\"end\":220,\"start\":211},{\"end\":314,\"start\":307},{\"end\":370,\"start\":365}]", "author_first_name": "[{\"end\":75,\"start\":69},{\"end\":134,\"start\":127},{\"end\":210,\"start\":204},{\"end\":306,\"start\":295},{\"end\":364,\"start\":356}]", "author_affiliation": "[{\"end\":125,\"start\":87},{\"end\":202,\"start\":164},{\"end\":293,\"start\":251},{\"end\":354,\"start\":316},{\"end\":428,\"start\":390}]", "title": "[{\"end\":66,\"start\":1},{\"end\":495,\"start\":430}]", "venue": null, "abstract": "[{\"end\":2210,\"start\":497}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2463,\"start\":2460},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2900,\"start\":2897},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3186,\"start\":3183},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3220,\"start\":3217},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3446,\"start\":3443},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3451,\"start\":3448},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3702,\"start\":3699},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3707,\"start\":3704},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3712,\"start\":3709},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3808,\"start\":3804},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4015,\"start\":4011},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4226,\"start\":4222},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4232,\"start\":4228},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4384,\"start\":4380},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4929,\"start\":4925},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5189,\"start\":5185},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5195,\"start\":5191},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5201,\"start\":5197},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5207,\"start\":5203},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5502,\"start\":5499},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5508,\"start\":5504},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5514,\"start\":5510},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6333,\"start\":6329},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7776,\"start\":7772},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8562,\"start\":8561},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8933,\"start\":8929},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9647,\"start\":9643},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9653,\"start\":9649},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9659,\"start\":9655},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9949,\"start\":9945},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9955,\"start\":9951},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12619,\"start\":12615},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13060,\"start\":13056},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14277,\"start\":14273},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16518,\"start\":16514},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16611,\"start\":16607},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16747,\"start\":16743},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17368,\"start\":17364},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17374,\"start\":17370},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17380,\"start\":17376},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17521,\"start\":17517},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17781,\"start\":17777},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17822,\"start\":17819},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18176,\"start\":18173},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18182,\"start\":18178},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18188,\"start\":18184},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18212,\"start\":18208},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18944,\"start\":18940},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18968,\"start\":18964},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20177,\"start\":20174},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23068,\"start\":23064},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23120,\"start\":23116},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23126,\"start\":23122},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":23132,\"start\":23128},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23231,\"start\":23228},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23678,\"start\":23674},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23958,\"start\":23954},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24225,\"start\":24222},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24537,\"start\":24533},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24653,\"start\":24649},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24714,\"start\":24711},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25363,\"start\":25359},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26382,\"start\":26378}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27234,\"start\":27031},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27412,\"start\":27235},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27472,\"start\":27413},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27543,\"start\":27473},{\"attributes\":{\"id\":\"fig_4\"},\"end\":27885,\"start\":27544},{\"attributes\":{\"id\":\"fig_5\"},\"end\":27980,\"start\":27886}]", "paragraph": "[{\"end\":2901,\"start\":2229},{\"end\":3452,\"start\":2903},{\"end\":4421,\"start\":3454},{\"end\":4781,\"start\":4423},{\"end\":5515,\"start\":4783},{\"end\":6122,\"start\":5517},{\"end\":6542,\"start\":6124},{\"end\":7132,\"start\":6544},{\"end\":7718,\"start\":7134},{\"end\":8051,\"start\":7735},{\"end\":9514,\"start\":8053},{\"end\":10084,\"start\":9516},{\"end\":10593,\"start\":10086},{\"end\":10929,\"start\":10609},{\"end\":11133,\"start\":10931},{\"end\":11552,\"start\":11135},{\"end\":12212,\"start\":11554},{\"end\":13287,\"start\":12214},{\"end\":13360,\"start\":13355},{\"end\":13973,\"start\":13406},{\"end\":14355,\"start\":14093},{\"end\":14525,\"start\":14420},{\"end\":14822,\"start\":14555},{\"end\":15227,\"start\":14936},{\"end\":15958,\"start\":15251},{\"end\":16224,\"start\":15983},{\"end\":16391,\"start\":16251},{\"end\":16612,\"start\":16449},{\"end\":16893,\"start\":16640},{\"end\":17060,\"start\":16922},{\"end\":17581,\"start\":17092},{\"end\":17715,\"start\":17583},{\"end\":18819,\"start\":17717},{\"end\":19555,\"start\":18821},{\"end\":20624,\"start\":19557},{\"end\":21072,\"start\":20626},{\"end\":21754,\"start\":21128},{\"end\":21860,\"start\":21756},{\"end\":22397,\"start\":21890},{\"end\":22983,\"start\":22425},{\"end\":23212,\"start\":23004},{\"end\":23941,\"start\":23214},{\"end\":25141,\"start\":23943},{\"end\":25689,\"start\":25143},{\"end\":25986,\"start\":25691},{\"end\":26407,\"start\":26005},{\"end\":27030,\"start\":26426}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13293,\"start\":13288},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13354,\"start\":13293},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13405,\"start\":13361},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14092,\"start\":13974},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14419,\"start\":14356},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14534,\"start\":14526},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14554,\"start\":14534},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14935,\"start\":14823},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16250,\"start\":16225},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16448,\"start\":16392},{\"attributes\":{\"id\":\"formula_10\"},\"end\":16921,\"start\":16894},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17091,\"start\":17061}]", "table_ref": null, "section_header": "[{\"end\":2227,\"start\":2212},{\"end\":7733,\"start\":7721},{\"end\":10607,\"start\":10596},{\"end\":15249,\"start\":15230},{\"end\":15981,\"start\":15961},{\"end\":16638,\"start\":16615},{\"end\":21126,\"start\":21075},{\"end\":21888,\"start\":21863},{\"end\":22423,\"start\":22400},{\"end\":23002,\"start\":22986},{\"end\":26003,\"start\":25989},{\"end\":26424,\"start\":26410},{\"end\":27244,\"start\":27236},{\"end\":27553,\"start\":27545},{\"end\":27895,\"start\":27887}]", "table": null, "figure_caption": "[{\"end\":27234,\"start\":27033},{\"end\":27412,\"start\":27246},{\"end\":27472,\"start\":27415},{\"end\":27543,\"start\":27475},{\"end\":27885,\"start\":27555},{\"end\":27980,\"start\":27897}]", "figure_ref": "[{\"end\":3300,\"start\":3292},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11952,\"start\":11944},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19748,\"start\":19739},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20666,\"start\":20657},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22632,\"start\":22624}]", "bib_author_first_name": "[{\"end\":28958,\"start\":28957},{\"end\":29322,\"start\":29321},{\"end\":29334,\"start\":29333},{\"end\":29598,\"start\":29597},{\"end\":29608,\"start\":29607},{\"end\":30175,\"start\":30174},{\"end\":30182,\"start\":30181},{\"end\":30190,\"start\":30189},{\"end\":30200,\"start\":30199},{\"end\":30202,\"start\":30201},{\"end\":30840,\"start\":30839},{\"end\":31065,\"start\":31064},{\"end\":31073,\"start\":31072},{\"end\":31308,\"start\":31307},{\"end\":31318,\"start\":31317},{\"end\":31332,\"start\":31331},{\"end\":31334,\"start\":31333},{\"end\":31343,\"start\":31342},{\"end\":31345,\"start\":31344},{\"end\":31351,\"start\":31350},{\"end\":31353,\"start\":31352},{\"end\":31933,\"start\":31932},{\"end\":31943,\"start\":31942},{\"end\":31945,\"start\":31944},{\"end\":31954,\"start\":31953},{\"end\":31964,\"start\":31963},{\"end\":31966,\"start\":31965},{\"end\":31977,\"start\":31976},{\"end\":31979,\"start\":31978},{\"end\":32334,\"start\":32333},{\"end\":32344,\"start\":32343},{\"end\":32357,\"start\":32356},{\"end\":32359,\"start\":32358},{\"end\":32365,\"start\":32364},{\"end\":32375,\"start\":32374},{\"end\":32377,\"start\":32376},{\"end\":32388,\"start\":32387},{\"end\":32390,\"start\":32389},{\"end\":32396,\"start\":32395},{\"end\":32895,\"start\":32894},{\"end\":32897,\"start\":32896},{\"end\":33090,\"start\":33089},{\"end\":33097,\"start\":33096},{\"end\":33103,\"start\":33102},{\"end\":33110,\"start\":33109},{\"end\":33331,\"start\":33330},{\"end\":33539,\"start\":33538},{\"end\":33548,\"start\":33547},{\"end\":33556,\"start\":33555},{\"end\":33818,\"start\":33817},{\"end\":33832,\"start\":33831},{\"end\":33849,\"start\":33848},{\"end\":33858,\"start\":33857},{\"end\":33864,\"start\":33863},{\"end\":33880,\"start\":33879},{\"end\":33889,\"start\":33888},{\"end\":33902,\"start\":33901},{\"end\":34270,\"start\":34269},{\"end\":34281,\"start\":34280},{\"end\":34289,\"start\":34288},{\"end\":34553,\"start\":34552},{\"end\":34563,\"start\":34562},{\"end\":34575,\"start\":34574},{\"end\":34826,\"start\":34825},{\"end\":34828,\"start\":34827},{\"end\":34838,\"start\":34837},{\"end\":34850,\"start\":34849},{\"end\":35177,\"start\":35176},{\"end\":35183,\"start\":35182},{\"end\":35190,\"start\":35189},{\"end\":35200,\"start\":35199},{\"end\":35202,\"start\":35201},{\"end\":35430,\"start\":35426},{\"end\":35437,\"start\":35436},{\"end\":35649,\"start\":35648},{\"end\":35657,\"start\":35656},{\"end\":35666,\"start\":35665},{\"end\":35674,\"start\":35673},{\"end\":35924,\"start\":35923},{\"end\":35930,\"start\":35929},{\"end\":35939,\"start\":35938},{\"end\":35947,\"start\":35946},{\"end\":36200,\"start\":36199},{\"end\":36212,\"start\":36211},{\"end\":36224,\"start\":36223},{\"end\":36403,\"start\":36402},{\"end\":36416,\"start\":36415},{\"end\":36425,\"start\":36424},{\"end\":36437,\"start\":36436},{\"end\":36449,\"start\":36448},{\"end\":36641,\"start\":36640},{\"end\":36916,\"start\":36915},{\"end\":36926,\"start\":36925},{\"end\":36935,\"start\":36934},{\"end\":36937,\"start\":36936},{\"end\":36949,\"start\":36948},{\"end\":36957,\"start\":36956},{\"end\":36966,\"start\":36965},{\"end\":36987,\"start\":36986},{\"end\":37004,\"start\":37003},{\"end\":37018,\"start\":37017},{\"end\":37036,\"start\":37035},{\"end\":37409,\"start\":37408},{\"end\":37411,\"start\":37410},{\"end\":37421,\"start\":37420},{\"end\":37423,\"start\":37422},{\"end\":37437,\"start\":37436},{\"end\":37439,\"start\":37438},{\"end\":37448,\"start\":37447},{\"end\":37745,\"start\":37744},{\"end\":37759,\"start\":37758},{\"end\":37769,\"start\":37768},{\"end\":37892,\"start\":37891},{\"end\":37906,\"start\":37905},{\"end\":38093,\"start\":38092},{\"end\":38102,\"start\":38101},{\"end\":38239,\"start\":38238},{\"end\":38241,\"start\":38240},{\"end\":38505,\"start\":38504},{\"end\":38507,\"start\":38506},{\"end\":38521,\"start\":38520},{\"end\":38530,\"start\":38529},{\"end\":38762,\"start\":38761},{\"end\":38771,\"start\":38770},{\"end\":38782,\"start\":38781},{\"end\":38792,\"start\":38791},{\"end\":38802,\"start\":38801},{\"end\":38810,\"start\":38809},{\"end\":38819,\"start\":38818},{\"end\":38821,\"start\":38820},{\"end\":38832,\"start\":38831},{\"end\":38841,\"start\":38840},{\"end\":38849,\"start\":38848},{\"end\":39169,\"start\":39168},{\"end\":39177,\"start\":39176},{\"end\":39189,\"start\":39188},{\"end\":39736,\"start\":39735},{\"end\":39750,\"start\":39749},{\"end\":39760,\"start\":39759},{\"end\":39762,\"start\":39761},{\"end\":40111,\"start\":40110},{\"end\":40122,\"start\":40121},{\"end\":40128,\"start\":40127},{\"end\":40130,\"start\":40129},{\"end\":40380,\"start\":40379},{\"end\":40391,\"start\":40390},{\"end\":40408,\"start\":40407},{\"end\":40829,\"start\":40828},{\"end\":40840,\"start\":40839},{\"end\":40850,\"start\":40849},{\"end\":40861,\"start\":40860},{\"end\":41213,\"start\":41212},{\"end\":41224,\"start\":41223},{\"end\":41557,\"start\":41556},{\"end\":41563,\"start\":41562},{\"end\":41573,\"start\":41572},{\"end\":41582,\"start\":41581},{\"end\":41974,\"start\":41973},{\"end\":41984,\"start\":41983},{\"end\":41996,\"start\":41995},{\"end\":42011,\"start\":42010},{\"end\":42020,\"start\":42019},{\"end\":42236,\"start\":42235},{\"end\":42245,\"start\":42244}]", "bib_author_last_name": "[{\"end\":28977,\"start\":28959},{\"end\":28986,\"start\":28979},{\"end\":29331,\"start\":29323},{\"end\":29343,\"start\":29335},{\"end\":29605,\"start\":29599},{\"end\":29612,\"start\":29609},{\"end\":30179,\"start\":30176},{\"end\":30187,\"start\":30183},{\"end\":30197,\"start\":30191},{\"end\":30210,\"start\":30203},{\"end\":30851,\"start\":30841},{\"end\":31070,\"start\":31066},{\"end\":31080,\"start\":31074},{\"end\":31315,\"start\":31309},{\"end\":31329,\"start\":31319},{\"end\":31340,\"start\":31335},{\"end\":31348,\"start\":31346},{\"end\":31361,\"start\":31354},{\"end\":31940,\"start\":31934},{\"end\":31951,\"start\":31946},{\"end\":31961,\"start\":31955},{\"end\":31974,\"start\":31967},{\"end\":31982,\"start\":31980},{\"end\":32341,\"start\":32335},{\"end\":32354,\"start\":32345},{\"end\":32362,\"start\":32360},{\"end\":32372,\"start\":32366},{\"end\":32385,\"start\":32378},{\"end\":32393,\"start\":32391},{\"end\":32402,\"start\":32397},{\"end\":32903,\"start\":32898},{\"end\":33094,\"start\":33091},{\"end\":33100,\"start\":33098},{\"end\":33107,\"start\":33104},{\"end\":33115,\"start\":33111},{\"end\":33335,\"start\":33332},{\"end\":33545,\"start\":33540},{\"end\":33553,\"start\":33549},{\"end\":33562,\"start\":33557},{\"end\":33829,\"start\":33819},{\"end\":33846,\"start\":33833},{\"end\":33855,\"start\":33850},{\"end\":33861,\"start\":33859},{\"end\":33877,\"start\":33865},{\"end\":33886,\"start\":33881},{\"end\":33899,\"start\":33890},{\"end\":33909,\"start\":33903},{\"end\":34278,\"start\":34271},{\"end\":34286,\"start\":34282},{\"end\":34298,\"start\":34290},{\"end\":34560,\"start\":34554},{\"end\":34572,\"start\":34564},{\"end\":34583,\"start\":34576},{\"end\":34835,\"start\":34829},{\"end\":34847,\"start\":34839},{\"end\":34857,\"start\":34851},{\"end\":35180,\"start\":35178},{\"end\":35187,\"start\":35184},{\"end\":35197,\"start\":35191},{\"end\":35207,\"start\":35203},{\"end\":35434,\"start\":35431},{\"end\":35445,\"start\":35438},{\"end\":35654,\"start\":35650},{\"end\":35663,\"start\":35658},{\"end\":35671,\"start\":35667},{\"end\":35689,\"start\":35675},{\"end\":35927,\"start\":35925},{\"end\":35936,\"start\":35931},{\"end\":35944,\"start\":35940},{\"end\":35950,\"start\":35948},{\"end\":36209,\"start\":36201},{\"end\":36221,\"start\":36213},{\"end\":36231,\"start\":36225},{\"end\":36413,\"start\":36404},{\"end\":36422,\"start\":36417},{\"end\":36434,\"start\":36426},{\"end\":36446,\"start\":36438},{\"end\":36459,\"start\":36450},{\"end\":36652,\"start\":36642},{\"end\":36923,\"start\":36917},{\"end\":36932,\"start\":36927},{\"end\":36946,\"start\":36938},{\"end\":36954,\"start\":36950},{\"end\":36963,\"start\":36958},{\"end\":36984,\"start\":36967},{\"end\":37001,\"start\":36988},{\"end\":37015,\"start\":37005},{\"end\":37033,\"start\":37019},{\"end\":37044,\"start\":37037},{\"end\":37418,\"start\":37412},{\"end\":37434,\"start\":37424},{\"end\":37445,\"start\":37440},{\"end\":37456,\"start\":37449},{\"end\":37756,\"start\":37746},{\"end\":37766,\"start\":37760},{\"end\":37779,\"start\":37770},{\"end\":37903,\"start\":37893},{\"end\":37918,\"start\":37907},{\"end\":38099,\"start\":38094},{\"end\":38108,\"start\":38103},{\"end\":38246,\"start\":38242},{\"end\":38518,\"start\":38508},{\"end\":38527,\"start\":38522},{\"end\":38542,\"start\":38531},{\"end\":38768,\"start\":38763},{\"end\":38779,\"start\":38772},{\"end\":38789,\"start\":38783},{\"end\":38799,\"start\":38793},{\"end\":38807,\"start\":38803},{\"end\":38816,\"start\":38811},{\"end\":38829,\"start\":38822},{\"end\":38838,\"start\":38833},{\"end\":38846,\"start\":38842},{\"end\":38855,\"start\":38850},{\"end\":39174,\"start\":39170},{\"end\":39186,\"start\":39178},{\"end\":39194,\"start\":39190},{\"end\":39747,\"start\":39737},{\"end\":39757,\"start\":39751},{\"end\":39770,\"start\":39763},{\"end\":40119,\"start\":40112},{\"end\":40125,\"start\":40123},{\"end\":40137,\"start\":40131},{\"end\":40388,\"start\":40381},{\"end\":40405,\"start\":40392},{\"end\":40417,\"start\":40409},{\"end\":40837,\"start\":40830},{\"end\":40847,\"start\":40841},{\"end\":40858,\"start\":40851},{\"end\":40870,\"start\":40862},{\"end\":41221,\"start\":41214},{\"end\":41238,\"start\":41225},{\"end\":41560,\"start\":41558},{\"end\":41570,\"start\":41564},{\"end\":41579,\"start\":41574},{\"end\":41593,\"start\":41583},{\"end\":41981,\"start\":41975},{\"end\":41993,\"start\":41985},{\"end\":42008,\"start\":41997},{\"end\":42017,\"start\":42012},{\"end\":42026,\"start\":42021},{\"end\":42242,\"start\":42237},{\"end\":42254,\"start\":42246}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":29214,\"start\":28893},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":17001943},\"end\":29568,\"start\":29216},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3219406},\"end\":30008,\"start\":29570},{\"attributes\":{\"doi\":\"http:/doi.acm.org/10.1145/1341531.1341560\",\"id\":\"b3\"},\"end\":30106,\"start\":30010},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2510724},\"end\":30779,\"start\":30108},{\"attributes\":{\"id\":\"b5\"},\"end\":30988,\"start\":30781},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1496449},\"end\":31226,\"start\":30990},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3116311},\"end\":31851,\"start\":31228},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6979578},\"end\":32252,\"start\":31853},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":990233},\"end\":32865,\"start\":32254},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2763403},\"end\":33022,\"start\":32867},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":16756501},\"end\":33269,\"start\":33024},{\"attributes\":{\"doi\":\"arXiv:1408.5882\",\"id\":\"b12\"},\"end\":33472,\"start\":33271},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":368182},\"end\":33786,\"start\":33474},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1033682},\"end\":34173,\"start\":33788},{\"attributes\":{\"doi\":\"arXiv:1511.06434\",\"id\":\"b15\"},\"end\":34502,\"start\":34175},{\"attributes\":{\"doi\":\"arXiv:1703.10239\",\"id\":\"b16\"},\"end\":34743,\"start\":34504},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1282515},\"end\":35111,\"start\":34745},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":142368},\"end\":35371,\"start\":35113},{\"attributes\":{\"id\":\"b19\"},\"end\":35604,\"start\":35373},{\"attributes\":{\"doi\":\"arXiv:1611.02163\",\"id\":\"b20\"},\"end\":35854,\"start\":35606},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3439214},\"end\":36180,\"start\":35856},{\"attributes\":{\"doi\":\"arXiv:1701.07875\",\"id\":\"b22\"},\"end\":36361,\"start\":36182},{\"attributes\":{\"doi\":\"arXiv:1704.00028\",\"id\":\"b23\"},\"end\":36638,\"start\":36363},{\"attributes\":{\"doi\":\"arXiv:1701.00160\",\"id\":\"b24\"},\"end\":36845,\"start\":36640},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":515925},\"end\":37326,\"start\":36847},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1211821},\"end\":37727,\"start\":37328},{\"attributes\":{\"id\":\"b27\"},\"end\":37865,\"start\":37729},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1915014},\"end\":38057,\"start\":37867},{\"attributes\":{\"doi\":\"arXiv:1502.01710\",\"id\":\"b29\"},\"end\":38236,\"start\":38059},{\"attributes\":{\"doi\":\"arXiv:1705.00648\",\"id\":\"b30\"},\"end\":38484,\"start\":38238},{\"attributes\":{\"doi\":\"arXiv:1505.00387\",\"id\":\"b31\"},\"end\":38680,\"start\":38486},{\"attributes\":{\"doi\":\"arXiv:1603.04467\",\"id\":\"b32\"},\"end\":39120,\"start\":38682},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":7842466},\"end\":39686,\"start\":39122},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1957433},\"end\":40059,\"start\":39688},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":14895712},\"end\":40342,\"start\":40061},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":409062},\"end\":40775,\"start\":40344},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":9068476},\"end\":41191,\"start\":40777},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":11222009},\"end\":41472,\"start\":41193},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1720899},\"end\":41919,\"start\":41474},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":11974261},\"end\":42192,\"start\":41921},{\"attributes\":{\"doi\":\"abs/1411.1784\",\"id\":\"b41\",\"matched_paper_id\":12803511},\"end\":42428,\"start\":42194}]", "bib_title": "[{\"end\":29319,\"start\":29216},{\"end\":29595,\"start\":29570},{\"end\":30172,\"start\":30108},{\"end\":31062,\"start\":30990},{\"end\":31305,\"start\":31228},{\"end\":31930,\"start\":31853},{\"end\":32331,\"start\":32254},{\"end\":32892,\"start\":32867},{\"end\":33087,\"start\":33024},{\"end\":33536,\"start\":33474},{\"end\":33815,\"start\":33788},{\"end\":34823,\"start\":34745},{\"end\":35174,\"start\":35113},{\"end\":35921,\"start\":35856},{\"end\":36913,\"start\":36847},{\"end\":37406,\"start\":37328},{\"end\":37889,\"start\":37867},{\"end\":39166,\"start\":39122},{\"end\":39733,\"start\":39688},{\"end\":40108,\"start\":40061},{\"end\":40377,\"start\":40344},{\"end\":40826,\"start\":40777},{\"end\":41210,\"start\":41193},{\"end\":41554,\"start\":41474},{\"end\":41971,\"start\":41921},{\"end\":42233,\"start\":42194}]", "bib_author": "[{\"end\":28979,\"start\":28957},{\"end\":28988,\"start\":28979},{\"end\":29333,\"start\":29321},{\"end\":29345,\"start\":29333},{\"end\":29607,\"start\":29597},{\"end\":29614,\"start\":29607},{\"end\":30181,\"start\":30174},{\"end\":30189,\"start\":30181},{\"end\":30199,\"start\":30189},{\"end\":30212,\"start\":30199},{\"end\":30853,\"start\":30839},{\"end\":31072,\"start\":31064},{\"end\":31082,\"start\":31072},{\"end\":31317,\"start\":31307},{\"end\":31331,\"start\":31317},{\"end\":31342,\"start\":31331},{\"end\":31350,\"start\":31342},{\"end\":31363,\"start\":31350},{\"end\":31942,\"start\":31932},{\"end\":31953,\"start\":31942},{\"end\":31963,\"start\":31953},{\"end\":31976,\"start\":31963},{\"end\":31984,\"start\":31976},{\"end\":32343,\"start\":32333},{\"end\":32356,\"start\":32343},{\"end\":32364,\"start\":32356},{\"end\":32374,\"start\":32364},{\"end\":32387,\"start\":32374},{\"end\":32395,\"start\":32387},{\"end\":32404,\"start\":32395},{\"end\":32905,\"start\":32894},{\"end\":33096,\"start\":33089},{\"end\":33102,\"start\":33096},{\"end\":33109,\"start\":33102},{\"end\":33117,\"start\":33109},{\"end\":33337,\"start\":33330},{\"end\":33547,\"start\":33538},{\"end\":33555,\"start\":33547},{\"end\":33564,\"start\":33555},{\"end\":33831,\"start\":33817},{\"end\":33848,\"start\":33831},{\"end\":33857,\"start\":33848},{\"end\":33863,\"start\":33857},{\"end\":33879,\"start\":33863},{\"end\":33888,\"start\":33879},{\"end\":33901,\"start\":33888},{\"end\":33911,\"start\":33901},{\"end\":34280,\"start\":34269},{\"end\":34288,\"start\":34280},{\"end\":34300,\"start\":34288},{\"end\":34562,\"start\":34552},{\"end\":34574,\"start\":34562},{\"end\":34585,\"start\":34574},{\"end\":34837,\"start\":34825},{\"end\":34849,\"start\":34837},{\"end\":34859,\"start\":34849},{\"end\":35182,\"start\":35176},{\"end\":35189,\"start\":35182},{\"end\":35199,\"start\":35189},{\"end\":35209,\"start\":35199},{\"end\":35436,\"start\":35426},{\"end\":35447,\"start\":35436},{\"end\":35656,\"start\":35648},{\"end\":35665,\"start\":35656},{\"end\":35673,\"start\":35665},{\"end\":35691,\"start\":35673},{\"end\":35929,\"start\":35923},{\"end\":35938,\"start\":35929},{\"end\":35946,\"start\":35938},{\"end\":35952,\"start\":35946},{\"end\":36211,\"start\":36199},{\"end\":36223,\"start\":36211},{\"end\":36233,\"start\":36223},{\"end\":36415,\"start\":36402},{\"end\":36424,\"start\":36415},{\"end\":36436,\"start\":36424},{\"end\":36448,\"start\":36436},{\"end\":36461,\"start\":36448},{\"end\":36654,\"start\":36640},{\"end\":36925,\"start\":36915},{\"end\":36934,\"start\":36925},{\"end\":36948,\"start\":36934},{\"end\":36956,\"start\":36948},{\"end\":36965,\"start\":36956},{\"end\":36986,\"start\":36965},{\"end\":37003,\"start\":36986},{\"end\":37017,\"start\":37003},{\"end\":37035,\"start\":37017},{\"end\":37046,\"start\":37035},{\"end\":37420,\"start\":37408},{\"end\":37436,\"start\":37420},{\"end\":37447,\"start\":37436},{\"end\":37458,\"start\":37447},{\"end\":37758,\"start\":37744},{\"end\":37768,\"start\":37758},{\"end\":37781,\"start\":37768},{\"end\":37905,\"start\":37891},{\"end\":37920,\"start\":37905},{\"end\":38101,\"start\":38092},{\"end\":38110,\"start\":38101},{\"end\":38248,\"start\":38238},{\"end\":38520,\"start\":38504},{\"end\":38529,\"start\":38520},{\"end\":38544,\"start\":38529},{\"end\":38770,\"start\":38761},{\"end\":38781,\"start\":38770},{\"end\":38791,\"start\":38781},{\"end\":38801,\"start\":38791},{\"end\":38809,\"start\":38801},{\"end\":38818,\"start\":38809},{\"end\":38831,\"start\":38818},{\"end\":38840,\"start\":38831},{\"end\":38848,\"start\":38840},{\"end\":38857,\"start\":38848},{\"end\":39176,\"start\":39168},{\"end\":39188,\"start\":39176},{\"end\":39196,\"start\":39188},{\"end\":39749,\"start\":39735},{\"end\":39759,\"start\":39749},{\"end\":39772,\"start\":39759},{\"end\":40121,\"start\":40110},{\"end\":40127,\"start\":40121},{\"end\":40139,\"start\":40127},{\"end\":40390,\"start\":40379},{\"end\":40407,\"start\":40390},{\"end\":40419,\"start\":40407},{\"end\":40839,\"start\":40828},{\"end\":40849,\"start\":40839},{\"end\":40860,\"start\":40849},{\"end\":40872,\"start\":40860},{\"end\":41223,\"start\":41212},{\"end\":41240,\"start\":41223},{\"end\":41562,\"start\":41556},{\"end\":41572,\"start\":41562},{\"end\":41581,\"start\":41572},{\"end\":41595,\"start\":41581},{\"end\":41983,\"start\":41973},{\"end\":41995,\"start\":41983},{\"end\":42010,\"start\":41995},{\"end\":42019,\"start\":42010},{\"end\":42028,\"start\":42019},{\"end\":42244,\"start\":42235},{\"end\":42256,\"start\":42244}]", "bib_venue": "[{\"end\":29804,\"start\":29709},{\"end\":30431,\"start\":30330},{\"end\":31512,\"start\":31446},{\"end\":32569,\"start\":32495},{\"end\":39385,\"start\":39299},{\"end\":40562,\"start\":40499},{\"end\":40991,\"start\":40940},{\"end\":41700,\"start\":41656},{\"end\":28955,\"start\":28893},{\"end\":29365,\"start\":29345},{\"end\":29707,\"start\":29614},{\"end\":30328,\"start\":30212},{\"end\":30837,\"start\":30781},{\"end\":31100,\"start\":31082},{\"end\":31444,\"start\":31363},{\"end\":32033,\"start\":31984},{\"end\":32493,\"start\":32404},{\"end\":32922,\"start\":32905},{\"end\":33121,\"start\":33117},{\"end\":33328,\"start\":33271},{\"end\":33613,\"start\":33564},{\"end\":33960,\"start\":33911},{\"end\":34267,\"start\":34175},{\"end\":34550,\"start\":34504},{\"end\":34908,\"start\":34859},{\"end\":35212,\"start\":35209},{\"end\":35424,\"start\":35373},{\"end\":35646,\"start\":35606},{\"end\":36007,\"start\":35952},{\"end\":36197,\"start\":36182},{\"end\":36400,\"start\":36363},{\"end\":36721,\"start\":36670},{\"end\":37052,\"start\":37046},{\"end\":37507,\"start\":37458},{\"end\":37742,\"start\":37729},{\"end\":37938,\"start\":37920},{\"end\":38090,\"start\":38059},{\"end\":38338,\"start\":38264},{\"end\":38502,\"start\":38486},{\"end\":38759,\"start\":38682},{\"end\":39297,\"start\":39196},{\"end\":39820,\"start\":39772},{\"end\":40175,\"start\":40139},{\"end\":40497,\"start\":40419},{\"end\":40938,\"start\":40872},{\"end\":41316,\"start\":41240},{\"end\":41654,\"start\":41595},{\"end\":42037,\"start\":42028},{\"end\":42273,\"start\":42269}]"}}}, "year": 2023, "month": 12, "day": 17}