{"id": 235097487, "updated": "2023-10-06 02:35:09.305", "metadata": {"title": "Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation", "authors": "[{\"first\":\"Samuel\",\"last\":\"Kiegeland\",\"middle\":[]},{\"first\":\"Julia\",\"last\":\"Kreutzer\",\"middle\":[]}]", "venue": "NAACL", "journal": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Policy gradient algorithms have found wide adoption in NLP, but have recently become subject to criticism, doubting their suitability for NMT. Choshen et al. (2020) identify multiple weaknesses and suspect that their success is determined by the shape of output distributions rather than the reward. In this paper, we revisit these claims and study them under a wider range of configurations. Our experiments on in-domain and cross-domain adaptation reveal the importance of exploration and reward scaling, and provide empirical counter-evidence to these claims.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.08942", "mag": null, "acl": "2021.naacl-main.133", "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2106-08942", "doi": "10.18653/v1/2021.naacl-main.133"}}, "content": {"source": {"pdf_hash": "a109b995dfeb444417f66545c67bce210bd11650", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/2021.naacl-main.133.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/2021.naacl-main.133.pdf", "status": "HYBRID"}}, "grobid": {"id": "c635db97106ce90c6a3fe5437ad79628a04a563a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a109b995dfeb444417f66545c67bce210bd11650.txt", "contents": "\nRevisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation\nJune 6-11, 2021\n\nSamuel Kiegeland kiegeland@cl.uni-heidelberg.de \nHeidelberg University\n\n\nJulia Kreutzer jkreutzer@google.com \nHeidelberg University\n\n\nGoogle Research \nHeidelberg University\n\n\nRevisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation\n\nProceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\nthe 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesJune 6-11, 20211673\nPolicy gradient algorithms have found wide adoption in NLP, but have recently become subject to criticism, doubting their suitability for NMT. Choshen et al. (2020) identify multiple weaknesses and suspect that their success is determined by the shape of output distributions rather than the reward. In this paper, we revisit these claims and study them under a wider range of configurations. Our experiments on in-domain and cross-domain adaptation reveal the importance of exploration and reward scaling, and provide empirical counterevidence to these claims.\n\nIntroduction\n\nIn neural sequence-to-sequence learning, in particular Neural Machine Translation (NMT), Reinforcement Learning (RL) has gained attraction due to the suitability of Policy Gradient (PG) methods for the end-to-end training paradigm (Ranzato et al., 2016;Li et al., 2016;Yu et al., 2017;Li et al., 2018;Flachs et al., 2019;Sankar and Ravi, 2019). The idea is to let the model explore the output space beyond the reference output that is used for standard cross-entropy minimization, by reinforcing model outputs according to their quality, effectively increasing the likelihood of higher-quality samples. The classic exploration-exploitation dilemma from RL is addressed by sampling from a pretrained model's softmax distribution over output tokens, such that the model entropy steers exploration.\n\nFor the application of NMT, it was firstly utilized to bridge the mismatch between the optimization for token-level likelihoods during training and the corpus-level held-out set evaluations with nondifferentiable/decomposable metrics like BLEU (Ranzato et al., 2016;Edunov et al., 2018), and secondly to reduce exposure bias in autoregressive sequence generators (Ranzato et al., 2016;Wang and Sennrich, 2020). It has furthermore been identified as a promising tool to adapt pretrained models to new domains or user preferences by replacing reward functions with human feedback in humanin-the-loop learning (Sokolov et al., 2016;Nguyen et al., 2017).\n\nRecently, the effectiveness of these methods has been questioned: Choshen et al. (2020) identify multiple theoretical and empirical weaknesses, leading to the suspicion that performance gains with RL in NMT are not due to the reward signal. The most surprising result is that the replacement of a meaningful reward function (giving higher rewards to higher-quality translations) by a constant reward (reinforcing all model samples equally) yields similar improvements in BLEU. To explain this counter-intuitive result, Choshen et al. (2020) conclude that a phenomenon called the peakiness effect must be responsible for performance gains instead of the reward. This means that the most likely tokens in the beginning gain probability mass regardless of the rewards they receive during RL training. If this hypothesis was true, then the perspectives for using methods of RL for encoding real-world preferences into the model would be quite dire, as models would essentially be stuck with whatever they learned during supervised pretraining and not reflect the feedback they obtain later on.\n\nHowever, the analysis by Choshen et al. (2020) missed a few crucial aspects of RL that have led to empirical success in previous works: First, variance reduction techniques such as the average reward baseline were already proposed with the original Policy Gradient by Williams (1992), and proved effective for NMT (Kreutzer et al., 2017;Nguyen et al., 2017). Second, the exploration-exploitation trade-off can be controlled by modifying the sampling function (Sharaf and Daum\u00e9 III, 2017), which in turn influences the peakiness.\n\nWe therefore revisit the previous findings with NMT experiments differentiating model behavior between in-domain and out-of-domain adap-tation, controlling exploration, reducing variance, and isolating the effect of reward scaling. This allows us to establish a more holistic view of the previously identified weaknesses of RL. In fact, our experiments reveal that improvements in BLEU can not solely be explained by increased peakiness, and that simple methods encouraging stronger exploration can successfully move previously lower-ranked token into higher ranks. We observe generally low empirical gains in indomain adaptation, which might explain the surprising success of constant rewards in (Choshen et al., 2020). However, we find that rewards and their scaling do matter for domain adaptation. Furthermore, our results corroborate the auspicious findings of Wang and Sennrich (2020) that RL mitigates exposure bias. Our paper thus reinstates the potential of RL for model adaptation in NMT, and puts previous pessimistic findings into perspective. The code for our experiments is publicly available. 1\n\n\nRL for NMT\n\nThe objective of RL in NMT is to maximize the expected reward for the model's outputs with respect to the parameters \u03b8:\narg max \u03b8 E p \u03b8 (y|x) [\u2206(y, y )].\nwhere y denotes a reference translation, y is the generated translation and \u2206 is a metric (e.g. BLEU (Papineni et al., 2002)), rewarding similarities to the reference. Applying the log derivative trick, the following gradient can be derived:\n\u2207 \u03b8 = E p \u03b8 (y|x) [\u2206(y, y )\u2207 \u03b8 log p \u03b8 (y | x)]. (1)\nThe benefit of Eq. 1 is that it does not require differentiation of \u2206 which allows for direct optimization of the BLEU score or human feedback. 2\n\n\nPolicy Gradient\n\nHowever, computing the gradient requires the summation over all y \u2208 V m trg , which is computationally infeasible for large sequence lengths m and vocabulary sizes V trg as they are common in NMT. Therefore, Eq. 1 is usually approximated through Monte Carlo sampling (Williams, 1992) resulting in unbiased estimators of the full gradient.\n\nWe draw one sample from the multinomial distribution defined by the model's softmax to approximate Eq. 1 (Ranzato et al., 2016;Kreutzer et al., 1 https://github.com/samuki/ reinforce-joey 2 Rewards may be obtained without reference translations y , hence \u2206(y) can replace \u2206(y, y ) in the following equations. 2017; Choshen et al., 2020), which results in the following update rule with learning rate \u03b1:\nu k = \u2207 \u03b8 log p \u03b8 (y | x)\u2206(y, y ) (2) \u03b8 t+1 = \u03b8 t + \u03b1u k (3)\n\nSoftmax Temperature\n\nThe temperature \u03c4 of the softmax distribution exp(y i /\u03c4 )/ j exp(y j /\u03c4 ) can be used to control the amount of exploration during learning. Setting 0 < \u03c4 < 1 results in less diverse samples while setting \u03c4 > 1 increases the diversity and also the entropy of the distribution. Lowering the temperature (i.e. making the distribution peakier) may be used to make policies more deterministic towards the end of training (Sutton and Barto, 1998;Rose, 1998;Sokolov et al., 2017), while we aim to reduce peakiness by increasing the temperature.\n\n\nModified Rewards\n\nVariance reduction techniques were already suggested by Williams (1992) and found to improve generalization for NMT (Kreutzer et al., 2017). The simplest option is the baseline reward, which in practice is realized by subtracting a running average of historic rewards from the current reward \u2206 in Eq. 2. It represents an expected reward, so that model outputs get more strongly reinforced or penalized if they diverge from it. In addition to variance reduction, subtracting baseline rewards also change the scale of rewards (e.g. \u2206 \u2208 [0, 1] for BLEU becomes \u2206 \u2208 [\u22120.5, 0.5]), allowing updates towards or away from samples by switching the sign of u k (Eq. 2). The same range of rewards can be obtained by rescaling them, e.g., to \u2206(y,y )\u2212min max \u2212 min \u2212 0.5 with the minimum (min) and maximum (max) \u2206 within each batch.\n\n\nMinimum Risk Training\n\nMinimum Risk Training (MRT) (Shen et al., 2016) aims to minimize the empirical risk of task loss over a larger set of n = |S|, n > 1 output samples\nS(x) \u2282 Y (x): arg min \u03b8 (x,y )\u2208D y\u2208S(x) Q \u03b8,\u03b1 (y | x)[\u2212\u2206(y, y )], Q \u03b8,\u03b1 (y | x (s) ) = p \u03b8 (y | x) \u03b1 y \u2208S(x) p \u03b8 (y | x) \u03b1 .\nAs pointed out by Choshen et al. (2020), MRT learns with biased stochastic estimates of the RL objective due to the renormalization of model scores, but that has not hindered its empirical success (Shen et al., 2016;Edunov et al., 2018;Wieting et al., 2019;Wang and Sennrich, 2020). Interestingly, the resulting gradient update includes a renormalization of sampled rewards, yielding a similar effect to the baseline reward (Shen et al., 2016). It also allows for more exploration thanks to learning from multiple samples per input, but it is therefore less attractive for human-in-the-loop learning and efficient training.\n\n\nExposure Bias\n\nThe exposure bias in NMT arises from the model only being exposed to the ground truth during training, and receiving its own previous predictions during inference-while it might be overly reliant on perfect context, which in turn lets errors accumulate rapidly over long sequences (Ranzato et al., 2016). Wang and Sennrich (2020) hypothesize that exposure bias increases the prevalence of hallucinations in domain adaptation and causes the beam search curse (Koehn and Knowles, 2017; Yang et al., 2018), which describes the problem that the model's performance worsens with large beams. Wang and Sennrich (2020) find that MRT with multiple samples can mitigate this problem thanks to being exposed to model predictions during training. We will extend this finding to other PG variants with single samples.\n\n\nExperiments\n\nWe implement PG and MRT (without enforcing gold tokens in S; n = 5) in Joey NMT ( Remaining experimental details can be found in the Appendix. The goal is not to find the best model in a supervised domain adaptation setup (\"Fine-tuning\" in Table 2), but to investigate if/how scalar rewards expressing translation preferences can guide learning, mimicking a human-in-the-loop learning scenario.\n\n\nPeakiness\n\nChoshen et al. (2020) suspect that PG improvements are due to an increase in peakiness. Increased peakiness is indicated by a disproportionate rise of p top10 and p mode , the average token probability of the 10 most likely tokens, and the mode, respectively. To test the influence of peakiness on performance, we deliberately increase and decrease the peakiness of the output distribution by adjusting the parameter \u03c4 . In Tables 1 and 2 we can see that all PG variants generally increase peakiness (p top10 and p mode ), but that those with higher temperature \u03c4 > 1 show a lower increase. Comparing the peakiness with the BLEU scores, we find that BLEU gains are not tied to increasing peakiness in in-domain and cross-domain adaptation experiments. This is exemplified by reward scaling (\"PG+scaled\"), which improves the BLEU but does not lead to an increase in peakiness com-pared to PG.These results show that improvements in BLEU can not just be explained by the peakiness effect, contradicting the hypothesis of Choshen et al. (2020). However, in cross-domain adaptation exploration plays a major role: Since the model has lower entropy on the new data, reducing exploration (lower \u03c4 ) helps to improve translation quality.\n\n\nUpwards Mobility\n\nOne disadvantage of high peakiness is that previously likely tokens accumulate even more probability mass during RL. Choshen et al. (2020) therefore fear that it might be close to impossible to transport lower-ranking tokens to higher ranks with RL. We test this hypothesis under different exploration settings by counting the number of gold tokens in each rank of the output distribution. That number is divided by the number of all gold tokens to obtain the probability of gold tokens appearing in each rank. We then compare the probability before and after RL. Fig. 1 illustrates that training with an increased temperature pushes more gold tokens out of the lowest rank. The baseline reward has a beneficial effect to that aim, since it allows down-weighing samples as well. This shows that upwards mobility is feasible and not a principled problem for PG.\n\n\nMeaningful Rewards\n\nChoshen et al. (2020) observe an increase in peakiness when all rewards are set to 1, and BLEU improvements even comparable to BLEU rewards. While our results with a constant reward of 1 (\"PG+constant\") also show an increase in peakiness for cross-domain adaptation (Table 2), we do not observe any improvements over the pretrained model, which contradicts the results of Choshen et al. (2020). Similarly, domain adaptation via self-training does not show improvements over the baseline, which confirms that gains do not come from being exposed to new inputs alone. While the effects in-domain are generally weak with a maximum gain of 0.5 BLEU over the baseline (with beam size k = 5, Table 1), the results for domain adaptation (Table 2) show a clear advantage of using informative rewards with up to +4.7 BLEU for PG and +6.7 BLEU for MRT (with beam size k = 5). We conclude that rewards do matter for PG for NMT. Figure 1: Change in probability for gold tokens to belong to each rank before and after RL on in-domain data.\n\n\nAllowing Negative Rewards\n\nAs described in Section 2.3, scaling the reward (\"PG+scaled\"), subtracting a baseline (\"PG+average bl\"), or normalizing it over multiple samples for MRT, introduces negative rewards, which enables updates away from sampled outputs. BLEU under domain shift (Table 2) shows a significant improvement when allowing negative rewards. The scaled reward increases the score by almost 1 BLEU, the average reward baseline by almost 2 BLEU and MRT leads to a gain of about 4.5 BLEU over plain PG.\n\n\nThe Beam Curse\n\nThe results show that improvements of RL over the baseline are higher with lower beam sizes, since RL reduces the need for exploration (through search) during inference thanks to the exploration during training. These findings are in line with (Bahdanau et al., 2017). For RL models, BLEU reductions caused by larger beams are weaker than for the baseline model in both settings, which confirms that PG methods are effective at mitigating the beam search problem, and according to Wang and Sennrich (2020) might also reduce hallucinations.\n\n\nDiscussion\n\nDespite the promising empirical gains over a pretrained baseline, all above methods would fail if trained from scratch, as there are no non-zeroreward translation outputs sampled when starting from a random policy. Empirical improvements over a strong pretrained model vanish when there is little to learn from the new feedback, e.g. when  it is given on the same data which the model was already trained on, as we have shown above, relating to the \"failure\" cases in (Choshen et al., 2020). RL methods for MT can be effective at adapting a model to new custom preferences if these preferences can be reflected in an appropriate reward function, which we simulated with in-domain data.\n\nIn Table 2, we observed this effect and gained several BLEU points without revealing reference translations to the model. Being exposed to new sources alone (without rewards) is not sufficient to obtain improvements, which we tested by self-training (Table 2). Ultimately, the potential to improve MT models with RL methods lies in situations where there are no reference translations but reward signals, and models can be pretrained on existing data.\n\n\nConclusion\n\nWe provided empirical counter-evidence for some of the claimed weaknesses of RL in NMT by untying BLEU gains from peakiness, showcasing the upwards mobility of low-ranking tokens, and re-confirming the importance of reward functions. The affirmed gains of PG variants in adaptation scenarios and their responsiveness to reward functions, combined with exposure bias repair and avoidance of the beam curse, rekindle the potential to utilize them for adapting models to human preferences.   Table 3 lists the sizes of data split for the parallel datasets from WMT15 (Bojar et al., 2015) and IWSLT14 3 used in the experiments. The two datasets are preprocessed using scripts from the Moses toolkit. 4 The preprocessing pipeline contains the following steps:\n\n\u2022 Tokenization with tokenizer.perl\n\n\u2022 Lowercasing with lowercase.perl\n\n\u2022 Filtering using clean-corpus-n.perl.\n\nSentences with more than 80 words are removed from the dataset Additionally, we applied Byte-Pair-Encoding (Sennrich et al., 2016) using subword-nmt 5 to create subword units. Table 8 contains the hyperparameters as Joey NMT configurations for the pretrained models, Table 9 the modified hyperparameters for PG, and  \n\n\nB Model Configurations\n\n\nD Additional Considerations\n\nLearned Baseline A reward baseline can also be learned by formulating it as a regression problem, but like Wu et al. (2018) we found no empirical gains, thus excluded it from the experiments reported in this paper.\n\nScaling Rewards We found that selecting max and min over all previous rewards led to deteriorating BLEU scores. This is why we recompute them for each batch.\n\nGold Tokens in MRT Shen et al. (2016) add the gold sequence to the sample space. However, Edunov et al. (2018) find that this destabilizes training, so Choshen et al. (2020) and Wang and Sennrich (2020) choose to omit it, and so do we.\n\n\nE Development Results\n\nTables 4 and 5 report results on the development set that were used for tuning the models. They show stable results across different held-out sets.\n\n\nF Absolute Peakiness\n\nTables 7 and 6 contain the absolute values for the change peakiness that were used to compute percentages for the main paper results.      \n\n\nKreutzer  et al., 2019)  for Transformers(Vaswani et al., 2017).We simulate rewards for training samples from IWSLT14 de-en with sacreBLEU (Post, 2018), and test on IWSLT14 held-out sets. We consider two different domains for pretraining, WMT15 and IWSLT14. This allows us to distinguish the effects of RL in in-domain learning vs domain adaptation scenarios. RL experiments are repeated three times and we report mean and standard deviation.\n\nTable 2 :\n2Cross-domain adaptation: Peakiness indicators (%), and IWSLT14 test set results for beam size k.\n\nTable 3 :\n3Training, dev, and test sizes for WMT15 and IWSLT14 de-en data in number of sentences.\n\nTable 10 the\n10modified hyperparameters for MRT. Random seeds for the three runs were set to 42, 8 and 64.Fig. 2shows that both MRT and Policy Gradient need a comparable amount of steps (with a batch size of 256 tokens) to reach their optimum. However, the performance of MRT is more stable over the course of training, while Policy Gradient shows higher variance. MRT learns from n = 5 outputs and rewards per step (compared to Policy Gradient with n = 1), which stabilizes the updates.Figure 2: Dev BLEU/steps for cross-domain.C Sample Efficiency \n\n\n\nTable 4 :\n4PG variants in-domain adaptation (IWSLT14), beam size=5.\n\n\nModelDev BLEU Test BLEU PG 21.63 \u00b1 0.39 22.62 \u00b1 0.56 PG + average bl 23.31 \u00b1 0.32 24.53 \u00b1 0.26 PG + \u03c4 = 1.2 20.45 \u00b1 0.18 21.58 \u00b1 0.21 PG + \u03c4 = 0.8Pretraining \n18.86 \n20.35 \n\n22.16 \u00b1 0.08 23.71 \u00b1 0.27 \nPG + constant \n18.86 \u00b1 0 \n20.35 \u00b1 0 \nPG + scaled \n22.40 \u00b1 0.10 23.50 \u00b1 0.15 \n\n\n\nTable 5 :\n5PG variants cross-domain adaptation (WMT15 to IWSLT14), beam size=5\n\nTable 7 :\n7Absolute changes in peakiness for in-domain (IWSLT14) adaptation.Model \nIWSLT14 \nWMT15 \n\nParameter \nSetting \nSetting \n\ninitializer \n\"xavier\" \n\"xavier\" \nembed initializer \n\"xavier\" \n\"xavier\" \nembed init gain \n1.0 \n1.0 \ninit gain \n1.0 \n1.0 \nbias initializer \n\"zeros\" \n\"zeros\" \ntied embeddings \nTrue \nTrue \ntied softmax \nTrue \nTrue \nencoder type \nTransformer \nTransfomer \nencoder embeddings dim 256 \n128 \nencoder hidden size \n256 \n128 \nencoder dropout \n0.3 \n0.3 \nencoder num layers \n6 \n6 \nencoder num heads \n4 \n4 \nencoder ff_size \n1024 \n512 \n\ndecoder type \nTransformer \nTransformer \ndecoder embeddings dim 256 \n128 \ndecoder hidden size \n256 \n128 \ndecoder dropout \n0.3 \n0.3 \ndecoder num layers \n6 \n6 \ndecoder num heads \n4 \n4 \ndecoder ff_size \n1024 \n512 \n\noptimizer \n\"adam\" \n\"adam\" \nnormalization \n\"tokens\" \n\"tokens\" \nadam_betas \n[0.9, 0.999] \n[0.9, 0.999] \nscheduling \n\"plateau\" \n\"plateau\" \npatience \n5 \n5 \ndecrease_factor \n0.7 \n0.7 \nloss \n\"crossentropy\" \"crossentropy\" \nlearning rate \n0.0003 \n0.0003 \nlearning rate_min \n0.00000002 \n0.00000002 \nweight decay \n0.0 \n0.0 \nlabel smoothing \n0.1 \n0.1 \nbatch size \n2048 \n4096 \nbatch type \n\"token\" \n\"token\" \nepochs \n100 \n100 \n\n\n\nTable 8 :\n8Pretraining model parametersParameter \nIn-domain \nCross-domain \n\nlearning rate \n0.00001 \n0.0001 \nbatch size \n128 \n256 \n\n\n\nTable 9 :\n9Policy Gradient parametersParameter \nIn-domain \nCross-domain \n\nlearning rate \n0.00001 \n0.0001 \nbatch size \n32 \n64 \nbatch multiplier 4 \n4 \neval batch size \n128 \n128 \nsamples \n5 \n5 \nalpha \n0.005 \n0.005 \n\n\n\nTable 10 :\n10MRT parameters\nhttps://sites.google.com/site/ iwsltevaluation2014/mt-track 4 https://github.com/moses-smt/ mosesdecoder/tree/master/scripts 5 https://github.com/rsennrich/ subword-nmt\nAcknowledgementsWe acknowledge the support by the state of Baden-W\u00fcrttemberg through bwHPC compute resources.\n. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron CDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C.\n\nAn actor-critic algorithm for sequence prediction. Yoshua Courville, Bengio, 5th International Conference on Learning Representations. Toulon, FranceConference Track ProceedingsCourville, and Yoshua Bengio. 2017. An actor-critic algorithm for sequence prediction. In 5th Inter- national Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Con- ference Track Proceedings.\n\nOnd\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina Scarton, Lucia Specia, Marco Turchi, Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine TranslationLisbon, PortugalAssociation for Computational LinguisticsFindings of the 2015 workshop on statistical machine translationOnd\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina Scarton, Lucia Specia, and Marco Turchi. 2015. Findings of the 2015 workshop on statistical machine translation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 1-46, Lisbon, Portugal. Association for Computational Linguistics.\n\nOn the weaknesses of reinforcement learning for neural machine translation. Leshem Choshen, Lior Fox, Zohar Aizenbud, Omri Abend, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. 2020. On the weaknesses of reinforcement learning for neural machine translation. In 8th Inter- national Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\n\nClassical structured prediction losses for sequence to sequence learning. Sergey Edunov, Myle Ott, Michael Auli, David Grangier, Marc&apos;aurelio Ranzato, 10.18653/v1/N18-1033Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics1Sergey Edunov, Myle Ott, Michael Auli, David Grang- ier, and Marc'Aurelio Ranzato. 2018. Classical structured prediction losses for sequence to se- quence learning. In Proceedings of the 2018 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pages 355-364, New Orleans, Louisiana. Association for Computational Linguistics.\n\nHistorical text normalization with delayed rewards. Simon Flachs, Marcel Bollmann, Anders S\u00f8gaard, 10.18653/v1/P19-1157Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-4344-4355. the 57th Annual Meeting of the Association for Computational Linguis-4344-4355Florence, ItalyAssociation for Computational LinguisticsSimon Flachs, Marcel Bollmann, and Anders S\u00f8gaard. 2019. Historical text normalization with delayed rewards. In Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguis- 4344-4355, Florence, Italy. Association for Compu- tational Linguistics.\n\nSimple statistical gradientfollowing algorithms for connectionist reinforcement learning. Ronald J Williams, 10.1007/BF00992696Mach. Learn. 83-4Ronald J. Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. Mach. Learn., 8(3-4):229-256.\n\nA study of reinforcement learning for neural machine translation. Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, Tie-Yan Liu, 10.18653/v1/D18-1397Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsLijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie- Yan Liu. 2018. A study of reinforcement learning for neural machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Nat- ural Language Processing, pages 3612-3621, Brus- sels, Belgium. Association for Computational Lin- guistics.\n\nBreaking the beam search curse: A study of (re-)scoring methods and stopping criteria for neural machine translation. Yilin Yang, Liang Huang, Mingbo Ma, 10.18653/v1/D18-1342Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsYilin Yang, Liang Huang, and Mingbo Ma. 2018. Breaking the beam search curse: A study of (re- )scoring methods and stopping criteria for neural machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 3054-3059, Brussels, Bel- gium. Association for Computational Linguistics.\n\nLearning to skim text. Adams Wei Yu, Hongrae Lee, Quoc Le, 10.18653/v1/P17-1172Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaLong Papers1Association for Computational LinguisticsAdams Wei Yu, Hongrae Lee, and Quoc Le. 2017. Learning to skim text. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1880- 1890, Vancouver, Canada. Association for Computa- tional Linguistics.\n", "annotations": {"author": "[{\"end\":173,\"start\":101},{\"end\":234,\"start\":174},{\"end\":275,\"start\":235}]", "publisher": null, "author_last_name": "[{\"end\":117,\"start\":108},{\"end\":188,\"start\":180},{\"end\":250,\"start\":242}]", "author_first_name": "[{\"end\":107,\"start\":101},{\"end\":179,\"start\":174},{\"end\":241,\"start\":235}]", "author_affiliation": "[{\"end\":172,\"start\":150},{\"end\":233,\"start\":211},{\"end\":274,\"start\":252}]", "title": "[{\"end\":83,\"start\":1},{\"end\":358,\"start\":276}]", "venue": "[{\"end\":502,\"start\":360}]", "abstract": "[{\"end\":1211,\"start\":650}]", "bib_ref": "[{\"end\":1480,\"start\":1458},{\"end\":1496,\"start\":1480},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1512,\"start\":1496},{\"end\":1528,\"start\":1512},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1548,\"start\":1528},{\"end\":1570,\"start\":1548},{\"end\":2290,\"start\":2268},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2310,\"start\":2290},{\"end\":2409,\"start\":2387},{\"end\":2433,\"start\":2409},{\"end\":2653,\"start\":2631},{\"end\":2673,\"start\":2653},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2763,\"start\":2742},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3216,\"start\":3195},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4050,\"start\":4035},{\"end\":4104,\"start\":4081},{\"end\":4124,\"start\":4104},{\"end\":4254,\"start\":4226},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5016,\"start\":4994},{\"end\":5699,\"start\":5676},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6318,\"start\":6302},{\"end\":6502,\"start\":6480},{\"end\":6518,\"start\":6502},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6711,\"start\":6690},{\"end\":7302,\"start\":7290},{\"end\":7313,\"start\":7302},{\"end\":7334,\"start\":7313},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7491,\"start\":7476},{\"end\":7559,\"start\":7536},{\"end\":8312,\"start\":8293},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8577,\"start\":8556},{\"end\":8754,\"start\":8735},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8774,\"start\":8754},{\"end\":8795,\"start\":8774},{\"end\":8819,\"start\":8795},{\"end\":8981,\"start\":8962},{\"end\":9482,\"start\":9460},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9681,\"start\":9663},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11448,\"start\":11427},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11797,\"start\":11776},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12935,\"start\":12914},{\"end\":14371,\"start\":14348},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15148,\"start\":15126},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16395,\"start\":16375},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17175,\"start\":17159},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17537,\"start\":17517},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17600,\"start\":17579}]", "figure": "[{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":18444,\"start\":18000},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":18553,\"start\":18445},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":18652,\"start\":18554},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":19205,\"start\":18653},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":19274,\"start\":19206},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":19556,\"start\":19275},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":19636,\"start\":19557},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":20814,\"start\":19637},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":20947,\"start\":20815},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":21162,\"start\":20948},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":21191,\"start\":21163}]", "paragraph": "[{\"end\":2022,\"start\":1227},{\"end\":2674,\"start\":2024},{\"end\":3765,\"start\":2676},{\"end\":4295,\"start\":3767},{\"end\":5406,\"start\":4297},{\"end\":5540,\"start\":5421},{\"end\":5816,\"start\":5575},{\"end\":6015,\"start\":5870},{\"end\":6373,\"start\":6035},{\"end\":6777,\"start\":6375},{\"end\":7399,\"start\":6861},{\"end\":8239,\"start\":7420},{\"end\":8412,\"start\":8265},{\"end\":9161,\"start\":8538},{\"end\":9984,\"start\":9179},{\"end\":10394,\"start\":10000},{\"end\":11638,\"start\":10408},{\"end\":12519,\"start\":11659},{\"end\":13568,\"start\":12542},{\"end\":14085,\"start\":13598},{\"end\":14643,\"start\":14104},{\"end\":15343,\"start\":14658},{\"end\":15796,\"start\":15345},{\"end\":16565,\"start\":15811},{\"end\":16601,\"start\":16567},{\"end\":16636,\"start\":16603},{\"end\":16676,\"start\":16638},{\"end\":16995,\"start\":16678},{\"end\":17266,\"start\":17052},{\"end\":17425,\"start\":17268},{\"end\":17662,\"start\":17427},{\"end\":17835,\"start\":17688},{\"end\":17999,\"start\":17860}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5574,\"start\":5541},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5869,\"start\":5817},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6838,\"start\":6778},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8537,\"start\":8413}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":10247,\"start\":10240},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":10846,\"start\":10832},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":12817,\"start\":12808},{\"end\":13235,\"start\":13228},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":13281,\"start\":13272},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":13863,\"start\":13854},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":15355,\"start\":15348},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":15604,\"start\":15595},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":16307,\"start\":16300},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":16861,\"start\":16854},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":16952,\"start\":16945}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1225,\"start\":1213},{\"attributes\":{\"n\":\"2\"},\"end\":5419,\"start\":5409},{\"attributes\":{\"n\":\"2.1\"},\"end\":6033,\"start\":6018},{\"attributes\":{\"n\":\"2.2\"},\"end\":6859,\"start\":6840},{\"attributes\":{\"n\":\"2.3\"},\"end\":7418,\"start\":7402},{\"attributes\":{\"n\":\"2.4\"},\"end\":8263,\"start\":8242},{\"attributes\":{\"n\":\"2.5\"},\"end\":9177,\"start\":9164},{\"attributes\":{\"n\":\"3\"},\"end\":9998,\"start\":9987},{\"attributes\":{\"n\":\"3.1\"},\"end\":10406,\"start\":10397},{\"attributes\":{\"n\":\"3.2\"},\"end\":11657,\"start\":11641},{\"attributes\":{\"n\":\"3.3\"},\"end\":12540,\"start\":12522},{\"attributes\":{\"n\":\"3.4\"},\"end\":13596,\"start\":13571},{\"attributes\":{\"n\":\"3.5\"},\"end\":14102,\"start\":14088},{\"attributes\":{\"n\":\"3.6\"},\"end\":14656,\"start\":14646},{\"attributes\":{\"n\":\"4\"},\"end\":15809,\"start\":15799},{\"end\":17020,\"start\":16998},{\"end\":17050,\"start\":17023},{\"end\":17686,\"start\":17665},{\"end\":17858,\"start\":17838},{\"end\":18455,\"start\":18446},{\"end\":18564,\"start\":18555},{\"end\":18666,\"start\":18654},{\"end\":19216,\"start\":19207},{\"end\":19567,\"start\":19558},{\"end\":19647,\"start\":19638},{\"end\":20825,\"start\":20816},{\"end\":20958,\"start\":20949},{\"end\":21174,\"start\":21164}]", "table": "[{\"end\":19205,\"start\":19183},{\"end\":19556,\"start\":19423},{\"end\":20814,\"start\":19714},{\"end\":20947,\"start\":20855},{\"end\":21162,\"start\":20986}]", "figure_caption": "[{\"end\":18444,\"start\":18002},{\"end\":18553,\"start\":18457},{\"end\":18652,\"start\":18566},{\"end\":19183,\"start\":18669},{\"end\":19274,\"start\":19218},{\"end\":19423,\"start\":19277},{\"end\":19636,\"start\":19569},{\"end\":19714,\"start\":19649},{\"end\":20855,\"start\":20827},{\"end\":20986,\"start\":20960},{\"end\":21191,\"start\":21177}]", "figure_ref": "[{\"end\":12229,\"start\":12223},{\"end\":13467,\"start\":13459}]", "bib_author_first_name": "[{\"end\":21480,\"start\":21473},{\"end\":21499,\"start\":21491},{\"end\":21514,\"start\":21508},{\"end\":21526,\"start\":21519},{\"end\":21538,\"start\":21534},{\"end\":21551,\"start\":21545},{\"end\":21721,\"start\":21715},{\"end\":22076,\"start\":22070},{\"end\":22089,\"start\":22084},{\"end\":22111,\"start\":22102},{\"end\":22128,\"start\":22123},{\"end\":22145,\"start\":22137},{\"end\":22157,\"start\":22152},{\"end\":22173,\"start\":22166},{\"end\":22188,\"start\":22181},{\"end\":22208,\"start\":22200},{\"end\":22221,\"start\":22215},{\"end\":22233,\"start\":22229},{\"end\":22248,\"start\":22240},{\"end\":22263,\"start\":22258},{\"end\":22277,\"start\":22272},{\"end\":23051,\"start\":23045},{\"end\":23065,\"start\":23061},{\"end\":23076,\"start\":23071},{\"end\":23091,\"start\":23087},{\"end\":23519,\"start\":23513},{\"end\":23532,\"start\":23528},{\"end\":23545,\"start\":23538},{\"end\":23557,\"start\":23552},{\"end\":23585,\"start\":23568},{\"end\":24432,\"start\":24427},{\"end\":24447,\"start\":24441},{\"end\":24464,\"start\":24458},{\"end\":25092,\"start\":25086},{\"end\":25094,\"start\":25093},{\"end\":25362,\"start\":25357},{\"end\":25370,\"start\":25367},{\"end\":25380,\"start\":25377},{\"end\":25395,\"start\":25386},{\"end\":25408,\"start\":25401},{\"end\":26085,\"start\":26080},{\"end\":26097,\"start\":26092},{\"end\":26111,\"start\":26105},{\"end\":26721,\"start\":26716},{\"end\":26725,\"start\":26722},{\"end\":26737,\"start\":26730},{\"end\":26747,\"start\":26743}]", "bib_author_last_name": "[{\"end\":21489,\"start\":21481},{\"end\":21506,\"start\":21500},{\"end\":21517,\"start\":21515},{\"end\":21532,\"start\":21527},{\"end\":21543,\"start\":21539},{\"end\":21558,\"start\":21552},{\"end\":21731,\"start\":21722},{\"end\":21739,\"start\":21733},{\"end\":22082,\"start\":22077},{\"end\":22100,\"start\":22090},{\"end\":22121,\"start\":22112},{\"end\":22135,\"start\":22129},{\"end\":22150,\"start\":22146},{\"end\":22164,\"start\":22158},{\"end\":22179,\"start\":22174},{\"end\":22198,\"start\":22189},{\"end\":22213,\"start\":22209},{\"end\":22227,\"start\":22222},{\"end\":22238,\"start\":22234},{\"end\":22256,\"start\":22249},{\"end\":22270,\"start\":22264},{\"end\":22284,\"start\":22278},{\"end\":23059,\"start\":23052},{\"end\":23069,\"start\":23066},{\"end\":23085,\"start\":23077},{\"end\":23097,\"start\":23092},{\"end\":23526,\"start\":23520},{\"end\":23536,\"start\":23533},{\"end\":23550,\"start\":23546},{\"end\":23566,\"start\":23558},{\"end\":23593,\"start\":23586},{\"end\":24439,\"start\":24433},{\"end\":24456,\"start\":24448},{\"end\":24472,\"start\":24465},{\"end\":25103,\"start\":25095},{\"end\":25365,\"start\":25363},{\"end\":25375,\"start\":25371},{\"end\":25384,\"start\":25381},{\"end\":25399,\"start\":25396},{\"end\":25412,\"start\":25409},{\"end\":26090,\"start\":26086},{\"end\":26103,\"start\":26098},{\"end\":26114,\"start\":26112},{\"end\":26728,\"start\":26726},{\"end\":26741,\"start\":26738},{\"end\":26750,\"start\":26748}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":21662,\"start\":21471},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14096841},\"end\":22068,\"start\":21664},{\"attributes\":{\"id\":\"b2\"},\"end\":22967,\"start\":22070},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":195791459},\"end\":23437,\"start\":22969},{\"attributes\":{\"doi\":\"10.18653/v1/N18-1033\",\"id\":\"b4\",\"matched_paper_id\":3718988},\"end\":24373,\"start\":23439},{\"attributes\":{\"doi\":\"10.18653/v1/P19-1157\",\"id\":\"b5\",\"matched_paper_id\":196210175},\"end\":24994,\"start\":24375},{\"attributes\":{\"doi\":\"10.1007/BF00992696\",\"id\":\"b6\",\"matched_paper_id\":2332513},\"end\":25289,\"start\":24996},{\"attributes\":{\"doi\":\"10.18653/v1/D18-1397\",\"id\":\"b7\",\"matched_paper_id\":52100616},\"end\":25960,\"start\":25291},{\"attributes\":{\"doi\":\"10.18653/v1/D18-1342\",\"id\":\"b8\",\"matched_paper_id\":52118537},\"end\":26691,\"start\":25962},{\"attributes\":{\"doi\":\"10.18653/v1/P17-1172\",\"id\":\"b9\",\"matched_paper_id\":1762731},\"end\":27271,\"start\":26693}]", "bib_title": "[{\"end\":21713,\"start\":21664},{\"end\":23043,\"start\":22969},{\"end\":23511,\"start\":23439},{\"end\":24425,\"start\":24375},{\"end\":25084,\"start\":24996},{\"end\":25355,\"start\":25291},{\"end\":26078,\"start\":25962},{\"end\":26714,\"start\":26693}]", "bib_author": "[{\"end\":21491,\"start\":21473},{\"end\":21508,\"start\":21491},{\"end\":21519,\"start\":21508},{\"end\":21534,\"start\":21519},{\"end\":21545,\"start\":21534},{\"end\":21560,\"start\":21545},{\"end\":21733,\"start\":21715},{\"end\":21741,\"start\":21733},{\"end\":22084,\"start\":22070},{\"end\":22102,\"start\":22084},{\"end\":22123,\"start\":22102},{\"end\":22137,\"start\":22123},{\"end\":22152,\"start\":22137},{\"end\":22166,\"start\":22152},{\"end\":22181,\"start\":22166},{\"end\":22200,\"start\":22181},{\"end\":22215,\"start\":22200},{\"end\":22229,\"start\":22215},{\"end\":22240,\"start\":22229},{\"end\":22258,\"start\":22240},{\"end\":22272,\"start\":22258},{\"end\":22286,\"start\":22272},{\"end\":23061,\"start\":23045},{\"end\":23071,\"start\":23061},{\"end\":23087,\"start\":23071},{\"end\":23099,\"start\":23087},{\"end\":23528,\"start\":23513},{\"end\":23538,\"start\":23528},{\"end\":23552,\"start\":23538},{\"end\":23568,\"start\":23552},{\"end\":23595,\"start\":23568},{\"end\":24441,\"start\":24427},{\"end\":24458,\"start\":24441},{\"end\":24474,\"start\":24458},{\"end\":25105,\"start\":25086},{\"end\":25367,\"start\":25357},{\"end\":25377,\"start\":25367},{\"end\":25386,\"start\":25377},{\"end\":25401,\"start\":25386},{\"end\":25414,\"start\":25401},{\"end\":26092,\"start\":26080},{\"end\":26105,\"start\":26092},{\"end\":26116,\"start\":26105},{\"end\":26730,\"start\":26716},{\"end\":26743,\"start\":26730},{\"end\":26752,\"start\":26743}]", "bib_venue": "[{\"end\":21813,\"start\":21799},{\"end\":22425,\"start\":22356},{\"end\":23178,\"start\":23157},{\"end\":23908,\"start\":23759},{\"end\":24682,\"start\":24589},{\"end\":25610,\"start\":25522},{\"end\":26312,\"start\":26224},{\"end\":26950,\"start\":26861},{\"end\":21797,\"start\":21741},{\"end\":22354,\"start\":22286},{\"end\":23155,\"start\":23099},{\"end\":23757,\"start\":23615},{\"end\":24587,\"start\":24494},{\"end\":25134,\"start\":25123},{\"end\":25520,\"start\":25434},{\"end\":26222,\"start\":26136},{\"end\":26859,\"start\":26772}]"}}}, "year": 2023, "month": 12, "day": 17}