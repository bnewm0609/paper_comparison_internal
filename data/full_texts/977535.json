{"id": 977535, "updated": "2023-06-27 15:54:43.002", "metadata": {"title": "Pixelwise View Selection for Unstructured Multi-View Stereo", "authors": "[{\"first\":\"Johannes\",\"last\":\"Sch\u00f6nberger\",\"middle\":[\"L.\"]},{\"first\":\"Enliang\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Jan-Michael\",\"last\":\"Frahm\",\"middle\":[]},{\"first\":\"Marc\",\"last\":\"Pollefeys\",\"middle\":[]}]", "venue": "ECCV", "journal": "501-518", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": ". This work presents a Multi-View Stereo system for robust and e\ufb03cient dense modeling from unstructured image collections. Our core contributions are the joint estimation of depth and normal information, pixelwise view selection using photometric and geometric priors, and a multi-view geometric consistency term for the simultaneous re\ufb01nement and image-based depth and normal fusion. Experiments on benchmarks and large-scale Internet photo collections demonstrate state-of-the-art performance in terms of accuracy, completeness, and e\ufb03ciency.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2519683295", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/SchonbergerZFP16", "doi": "10.1007/978-3-319-46487-9_31"}}, "content": {"source": {"pdf_hash": "02677b913f430f419ee8a8f2a8860d1af5e86b63", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5898c96424460f835680d8b20ea5d84f4db214f0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/02677b913f430f419ee8a8f2a8860d1af5e86b63.txt", "contents": "\nPixelwise View Selection for Unstructured Multi-View Stereo\n\n\nJohannes L Sch\u00f6nberger \nETH Z\u00fcrich\n\n\nEnliang Zheng \nUNC Chapel Hill\n3 Microsoft\n\nMarc Pollefeys \nETH Z\u00fcrich\n\n\nJan-Michael Frahm \nUNC Chapel Hill\n3 Microsoft\n\nPixelwise View Selection for Unstructured Multi-View Stereo\nFig. 1. Reconstructions for Louvre, Todai-ji, Paris Opera, and Astronomical Clock.\nThis work presents a Multi-View Stereo system for robust and efficient dense modeling from unstructured image collections. Our core contributions are the joint estimation of depth and normal information, pixelwise view selection using photometric and geometric priors, and a multi-view geometric consistency term for the simultaneous refinement and image-based depth and normal fusion. Experiments on benchmarks and large-scale Internet photo collections demonstrate stateof-the-art performance in terms of accuracy, completeness, and efficiency. Fig. 1. Reconstructions for Louvre, Todai-ji, Paris Opera, and Astronomical Clock.\n\nIntroduction\n\nLarge-scale 3D reconstruction from Internet photos has seen a tremendous evolution in sparse modeling using Structure-from-Motion (SfM) [1][2][3][4][5][6][7][8] and in dense modeling using Multi-View Stereo (MVS) [9][10][11][12][13][14][15]. Many applications benefit from a dense scene representation, e.g., classification [16], image-based rendering [17], localization [18], etc. Despite the widespread use of MVS, the efficient and robust estimation of accurate, complete, and aesthetically pleasing dense models in uncontrolled environments remains a challenging task. Dense pixelwise correspondence search is the core problem of stereo methods. Recovering correct correspondence is challenging even in controlled environments with known viewing geometry and illumination. In uncontrolled settings, e.g., where the input consists of crowd-sourced images, it is crucial to account for various factors, such as heterogeneous resolution and illumination, scene variability, unstructured viewing geometry, and mis-registered views.\n\nOur proposed approach improves the state of the art in dense reconstruction for unstructured images. This work leverages the optimization framework by Zheng et al. [14] to propose the following core contributions: (1) Pixelwise normal estimation embedded into an improved PatchMatch sampling scheme.\n\n(2) Pixelwise view selection using triangulation angle, incident angle, and image resolution-based geometric priors. (3) Integration of a \"temporal\" view selection smoothness term. (4) Adaptive window support through bilateral photometric consistency for improved occlusion boundary behavior. (5) Introduction of a multi-view geometric consistency term for simultaneous depth/normal estimation and image-based fusion. (6) Reliable depth/normal filtering and fusion. Outlier-free and accurate depth/normal estimates further allow for direct meshing of the resulting point cloud. We achieve state-of-the-art results on benchmarks (Middlebury [19], Strecha [20]). To demonstrate the advantages of our method in a more challenging setting, we process SfM models of a world-scale Internet dataset [5]. The entire algorithm is released to the public as an opensource implementation as part of [8] at https://github.com/colmap/colmap.\n\n\nRelated Work\n\nStereo methods have advanced in terms of accuracy, completeness, scalability, and benchmarking -from the minimal stereo setup with two views [21][22][23][24] to multi-view methods [9,10,14,15,[25][26][27][28]. Furthermore, the joint estimation of semantics [29], dynamic scene reconstruction [30][31][32][33][34], and benchmarking [12,19,20,23]. Our method performs MVS with pixelwise view selection for depth/normal estimation and fusion. Here, we only review the most related approaches, within the large body of research in multi-view and two-view stereo.\n\nMVS leverages multiple views to overcome the inherent occlusion problems of two-view approaches [35][36][37]. Accordingly, view selection plays a crucial role in the effectiveness of MVS. Kang et al. [38] heuristically select the best views with minimal cost (usually 50%) for computing the depth of each pixel. Strecha et al. [39,40] probabilistically model scene visibility combined with a local depth smoothness assumption [39] in a Markov Random Field for pixelwise view selection. Different from our approach, their method is prohibitive in memory usage and does neither include normal estimation nor photometric and geometric priors for view selection. Gallup et al. [41] select different views and resolutions on a per-pixel basis to achieve a constant depth error. In contrast, our method simultaneously considers a variety of photometric and geometric priors improving upon the robustness and accuracy of the recently proposed depth estimation framework by Zheng et al. [14]. Their method is most closely related to our approach and is reviewed in more detail in Section 3.\n\nMVS methods commonly use a fronto-parallel scene structure assumption. Gallup et al. [42] observed the distortion of the cost function caused by structure that deviates from this prior and combats it by using multiple sweeping directions deduced from the sparse reconstruction. Earlier approaches [43][44][45] similarly account for the surface normal in stereo matching. Recently, Bleyer et al. [46] use PatchMatch to estimate per-pixel normals to compensate for the distortion of the cost function. In contrast to these approaches, we propose to estimate normals not in isolation but also considering the photometric and geometric constraints guiding the matchabilty of surface texture and its accuracy. By probabilistically modeling the contribution of individual viewing rays towards reliable surface recovery, we achieve significantly improved depth and normal estimates.\n\nDepth map fusion integrates multiple depth maps into a unified and augmented scene representation while mitigating any inconsistencies among individual estimates. Jancoseck et al. [28] fuses multiple depth estimates into a surface and, by evaluating visibility in 3D space, they also attempt to reconstruct parts that are not directly supported by depth measurements. In contrast, our method aims at directly maximizing the estimated surface support in the depth maps and achieves higher completeness and accuracy (see Section 5). Goesele et al. [47] propose a method that explicitly targets at the reconstruction from crowd-sourced images. They first select camera clusters for each surface and adjust their resolution to the smallest common resolution. For depth estimation, they then use the four most suitable images for each pixel. As already noted in Zheng et al. [14], this early pre-selection of reduced camera clusters may lead to less complete results and is sensitive to noise. Our method avoids this restrictive selection scheme by allowing dataset-wide, pixelwise sampling for view selection. Zach et al. [48] proposed a variational depth map formulation that enabled parallelized computation on the GPU. However, their volumetric approach imposes substantial memory requirements and is prohibitive for the large-scale scenes targeted by our method. Beyond these methods, there are several large-scale dense reconstruction and fusion methods for crowd-sourced images, e.g., Furukawa et al. [10] and Gallup et al. [49,50], who all perform heuristic pre-selection of views, which leads to reduced completeness and accuracy as compared to our method.\n\n\nReview of Joint View Selection and Depth Estimation\n\nThis section reviews the framework by Zheng et al. [14] to introduce notation and context for our contributions. Since their method processes each row/column independently, we limit the description to a single image row with l as the column index. Their method estimates the depth \u03b8 l for a pixel in the reference image \nP (X m l |Z m l , \u03b8 l ) = 1 N A exp \u2212 (1\u2212\u03c1 m l (\u03b8 l )) 2 2\u03c3 2 \u03c1 if Z m l = 1 1 N U if Z m l = 0,(1)where A = 1 \u22121 exp{\u2212 (1\u2212\u03c1) 2 2\u03c3 2 \u03c1\n}d\u03c1 and N is a constant canceling out in the inference. In the case of occlusion, the color distributions of the two patches are unrelated and follow the uniform distribution U in the range [\u22121, 1] with probability density 0.5. Otherwise, \u03c1 m l describes the color similarity between the reference and source patch based on normalized cross-correlation (NCC) using frontoparallel homography warping. The variable \u03c3 \u03c1 determines a soft threshold for \u03c1 m l on the reference patch being visible in the source image. The state-transition matrix from the preceding pixel l \u2212 1 to the current pixel l is P (Z m l |Z m l\u22121 ) = \nP (X, Z, \u03b8) = L l=1 M m=1 [P (Z m l |Z m l\u22121 )P (X m l |Z m l , \u03b8 l )](2)\nand then normalizing over P (X), Zheng et al. use variational inference theory to develop a framework that is a variant of the generalized expectationmaximization (GEM) algorithm [51]. For the inference of Z in the hidden Markov-Chain, the forward-backward algorithm is used in the E step of GEM. PatchMatchinspired [46] sampling serves as an efficient scheme for the inference of \u03b8 in the M step of GEM. Their method iteratively solves for Z with fixed \u03b8 and vice versa using interleaved row-/columnwise propagation. Full depth inference\n\u03b8 opt l = argmin \u03b8 * l M m=1 P l (m)(1 \u2212 \u03c1 m l (\u03b8 * l ))(3)\nhas high computational cost if M is large as PatchMatch requires the NCC to be computed many times. The value P l (m) = q(Z m l =1) \n\u03b8 opt l = argmin \u03b8 * l 1 |S| m\u2208S (1 \u2212 \u03c1 m l (\u03b8 * l ))(4)\nby sampling a subset of images S \u2282 {1 . . . M } from the distribution P l (m) and hence only computing the NCC for the most similar source images.\n\n\nAlgorithm\n\nIn this section, we describe our novel algorithm that leverages the optimization framework reviewed in the previous section. We first present the individual terms of the proposed likelihood function, while Section 4.6 explains their integration into the overall optimization framework.\n\n\nNormal Estimation\n\nZheng et al. [14] map between the reference and source images using frontoparallel homographies leading to artifacts for oblique structures [42]. In contrast, we estimate per-pixel depth \u03b8 l and normals n l \u2208 R 3 , n l = 1. A patch at x l \u2208 P 2 in the reference image warps to a source patch at x m l \u2208 P 2 using (3) and t m \u2208 R 3 define the relative transformation from the reference to the source camera frame. K and K m denote the calibration of the reference and source images, respectively, and d l = n T l p l is the orthogonal distance from the reference image to the plane at the point\nx m l = H l x l with H l = K m (R m \u2212 d \u22121 l t m n T l )K \u22121 . Here, R m \u2208 SOp l = \u03b8 l K \u22121 x l .\nGiven no knowledge of the scene, we assume a uniform prior P (N ) in the inference of the normals N = {n l | l = 1 . . . L}. Estimating N requires to change the terms P (X m l |Z m l , \u03b8 l ) and P l (m) from Eqs. (1) and (4) to also depend on N , as the color similarity \u03c1 m l is now based on slanted rather than fronto-parallel homographies. Consequently, the optimal depth and normal are chosen as\n(\u03b8 opt l ,n opt l ) = argmin \u03b8 * l ,n * l 1 |S| m\u2208S (1 \u2212 \u03c1 m l (\u03b8 * l , n * l )).(5)\nTo sample unbiased random normals in PatchMatch, we follow the approach by Galliani et al. [15]. With the additional two unknown normal parameters, the number of unknowns per pixel in the M step of GEM increases from one to three. While this in theory requires PatchMatch to generate many more samples, we propose an efficient propagation scheme that maintains the convergence rate of depth-only inference. Since depth \u03b8 l and normal n l define a local planar surface in 3D, we propagate the depth \u03b8 prp l\u22121 of the intersection of the ray of the current pixel x l with the local surface of the previous pixel (\u03b8 l\u22121 , n l\u22121 ). This exploits first-order smoothness of the surface (cf. [52]) and thereby drastically speeds up the optimization since correct depths propagate more quickly along the surface. Moreover, different from the typical iterative refinement of normals using bisection as an intermediate step between full sweeps of propagations (cf. [15,46]), we generate a small set of additional plane hypotheses at each propagation step. We observe that the current best depth and normal parameters can have the following states: neither of them, one of them, or both of them have the optimal solution or are close to it. By combining random and perturbed depths with current best normals and vice versa, we increase the chance of sampling the correct solution. More formally, at each step in PatchMatch, we choose the current best estimate for pixel l according to Eq. (4) from the set of hypotheses {(\u03b8 l , n l ), (\u03b8 prp l\u22121 , n l\u22121 ), (\u03b8 rnd l , n l ), (\u03b8 l , n rnd l ), (\u03b8 rnd l , n rnd l ), (\u03b8 prt l , n l ), (\u03b8 l , n prt l )},\n\nwhere \u03b8 rnd l and n rnd l denote randomly generated samples. To refine the current parameters when they are close to the optimal solution, we perturb the current estimate as \u03b8 prt l = (1 \u00b1 )\u03b8 l and n prt l = R n l . The variable describes a small depth perturbation, and the rotation matrix R \u2208 SO(3) perturbs the normal direction by a small angle subject to p T l n prt l < 0. Normal estimation improves both the reconstruction completeness and accuracy, while the new sampling scheme leads to both fast convergence and more accurate estimates (Section 5).\n\n\nGeometric Priors for View Selection\n\nThis section describes how to incorporate geometric priors in the pixelwise view selection for improved robustness in particular for unstructured imagery. On a high level, the proposed priors encourage the sampling of source images with sufficient baseline (Triangulation Prior ), similar resolution (Resolution Prior ), and non-oblique viewing direction (Incident Prior ). In contrast to prior work (e.g. [10,47,49]), which decouples inference and per-image geometric priors by pre-selecting source images, we integrate geometric priors on a per-pixel basis into the inference. The motivation for per-pixel geometric priors is similar to inferring per-pixel occlusion indicators Z. Since the pre-selection of source images is based on a sparse and therefore incomplete scene representation, the selected source views are often sub-optimal. Occlusion boundaries, triangulation angles, relative image resolution, and incident angle can vary significantly between a single pair of reference and source images (Fig. 2). Incorporating geometric priors in addition to the photometric occlusion indicators Z leads to a more comprehensive and robust pixelwise view selection. In the following, we detail the proposed priors and explain their integration into the optimization framework.\n\nTriangulation Prior. Zheng et al. [14] sample source images purely based on color similarity. Consequently, the more similar the reference patch is to the source patch, the higher the selection probability in the view sampling. Naturally, image pairs with small viewpoint change, which coincides with small baseline, have high color similarity. However, image pairs with zero baseline do not carry information for depth inference, because reconstructed points can arbitrarily move along the viewing ray without changing the color similarity. Pure photometric view selection favors to sample these uninformative views.\n\nTo eliminate this degenerate case, we calculate the triangulation angle \u03b1 m\nl = cos \u22121 (p l \u2212c m ) T p l p l \u2212c m p l with c m = \u2212(R m ) T t m and \u03b1 m l \u2208 [0, \u03c0) between two intersect-\ning viewing rays as a measure of the stability of the reconstructed point p l . Empirically, we choose the following likelihood function P (\u03b1 m l ) = 1\u2212 (min(\u1fb1,\u03b1 m l )\u2212\u1fb1) 2 \u03b1 2 to describe how informative a source image is for reconstructing the correct point. Intuitively, this function assigns low likelihood to source images for which the triangulation angle is below an a priori threshold\u1fb1. Otherwise, no additional view selection preference is imposed (see Fig. 2).\n\nResolution Prior. Unstructured datasets usually contain images captured by a multitude of camera types under diverse viewing geometry. As a consequence, images capture scene objects in a wide range of resolutions. To avoid under-and oversampling in computing \u03c1 m l , the patches in the reference and source image should have similar size and shape [47]. Similar size is favorable as it avoids comparing images captured at vastly different resolutions, e.g., due to different zoom factors or distance to the object. Similar shape avoids significantly distorted source patches caused by different viewing directions. In the case of different shape, areas within the same source patch have different sampling rates. An approximate measure of the relative size and shape between the reference and source patch is \u03b2 m l = b l b m l \u2208 R + , where b l and b m l denote the areas covered by the corresponding patches. In our implementation, the reference patch is always square. If the size and shape of the patches is similar, \u03b2 m l is close to the value 1. To quantify the similarity in resolution between two images, we propose the likelihood function P (\u03b2 m l ) = min(\u03b2 m l , (\u03b2 m l ) \u22121 ) and integrate it into P l (m). Note that, at increased computational cost, undersampling could alternatively be handled by adaptive resampling of the source image patch.\n\nIncident Prior. The inferred per-pixel normals provide geometric constraints on the solution space that we encode in the form of a prior. The estimated plane restricts the possible space of source camera locations and orientations. By construction, the camera location can only lie in the positive half-space defined by the plane (\u03b8 l , n m l ), while the camera viewing direction must face towards the opposite normal direction. Otherwise, it is geometrically impossible for the camera to observe the surface. To satisfy this geometric visibility constraint, the incident angle of the source camera \u03ba m l = cos \u22121 (p l \u2212c m ) T n m l p l \u2212c m n m l with \u03ba m l \u2208 [0, \u03c0) must be in the interval 0 \u2264 \u03ba m l < \u03c0 2 . In our method, the likelihood function\nP (\u03ba m l ) = exp(\u2212 \u03ba m l 2 2\u03c3 2 \u03ba\n) encodes the belief in whether this geometric constraint is satisfied. This associates some belief with a view even in the case where \u03ba m l \u2265 \u03c0 2 . The reason for this is, that in the initial inference stage, the variables \u03b8 l and n m l are unknown and hence the geometric constraints are likely not yet correct.\n\nIntegration. Fig. 2 visualizes the geometric priors, and Fig. 4 shows examples of specific priors over all reference image pixels. We integrate the priors into the inference as additional terms in the Monte-Carlo view sampling distribution\nP l (m) = q(Z m l = 1)q(\u03b1 m l )q(\u03b2 m l )q(\u03ba m l ) M m=1 q(Z m l = 1)q(\u03b1 m l )q(\u03b2 m l )q(\u03ba m l ) ,(7)\nwhere q(\u03b1 m l ), q(\u03b2 m l ), q(\u03ba m l ) are approximations during the variational inference, in the sense that they minimize the KL-divergence to the real posterior [53]. The distributions need no normalization in the inference because we solely use them as modulators for the sampling distribution P l (m). This formulation assumes statistical independence of the individual priors as a simplifying approximation, which makes the optimization feasible using relatively simple models for wellunderstood geometric relations. Intuitively, non-occluded images with sufficient baseline, similar resolution, and non-oblique viewing direction are favored in the view selection. Section 5 evaluates the priors in detail and shows how they improve the reconstruction robustness especially for unstructured datasets.\n\n\nView Selection Smoothness\n\nThe graphical model associated with the likelihood function in Eq. (2) uses statetransition probabilities to model spatial view selection smoothness for neighboring pixels in the propagation direction. Due to the interleaved inference using alternating propagation directions, Z m l suffers from oscillation, leading to striping effects as shown in Figure 5. To reduce the oscillation effect of Z m l,t in iteration t, we insert an additional \"temporal\" smoothness factor into the graphical model. In this new model, the state of Z m l,t depends not only on the state of its neighboring pixel l \u2212 1 but also on its own state in the previous iteration t \u2212 1. The temporal state-transition is defined as P (Z m l,t |Z m l,t\u22121 ) = \u03bbt 1\u2212\u03bbt\n1\u2212\u03bbt \u03bbt\n, where a larger \u03bb t enforces greater temporal smoothness during the optimization. In fact, as the optimization progresses from t = 1 . . . T , the value of the estimated Z m l,t\u22121 should stabilize around the optimal solution. Therefore, we adaptively increase the state-transition probability as \u03bb t = t 2T + 0.5, i.e., the inferred Z m \n\nFig . 5 shows the evolution of Z m l,t during the optimization and demonstrates the reduced oscillation, which effectively also leads to less noisy view sampling.\n\n\nPhotometric Consistency\n\nZheng et al. [14] employ NCC to compute the color similarity \u03c1 m l . NCC is statistically optimal for Gaussian noise but is especially vulnerable to producing blurred depth discontinuities [54]. Inspired by [46,55], we diminish these artifacts by using a bilaterally weighted adaption of NCC. We compute \u03c1 m l between a reference patch w l at x l with a corresponding source patch w m l at x m l as\n\u03c1 m l = cov w (w l , w m l ) cov w (w l , w l ) cov w (w m l , w m l )(9)\nwhere cov w (x, y) = E w (x \u2212 E w (x)) E w (y \u2212 E w (y)) is the weighted covariance and E w (x) = i w i x i / i w i is the weighted average. The per-pixel weight\nw i = exp(\u2212 \u2206gi 2\u03c3 2 g \u2212 \u2206xi 2\u03c3 2 x )\nindicates the likelihood that a pixel i in the local patch belongs to the same plane as its center pixel at l. It is a function of the grayscale color distance \u2206g i = |g i \u2212 g l | and the spatial distance \u2206x i = x i \u2212 x l , whose importance is relatively scaled by the Gaussian dispersion \u03c3 g and \u03c3 x . By integrating the bilaterally weighted NCC into the term P (X m l |Z m l , \u03b8 l , n l ), our method achieves more accurate results at occlusion boundaries, as shown in Section 5.\n\n\nGeometric Consistency\n\nMVS typically suffers from gross outliers due to noise, ambiguities, occlusions, etc. In these cases, the photometric consistency for different hypotheses is ambiguous as large depth variations induce only small cost changes. Spatial smoothness constraints can often reduce but not fully eliminate the resulting artifacts. A popular approach to filter these outliers is to enforce multi-view depth coherence through left-right consistency checks as a post-processing step [15,46].\n\nIn contrast to most approaches, we integrate multi-view geometric consistency constraints into the inference to increase both the completeness and the accuracy. Similar to Zhang et al. [56], we infer the best depth and normal based on both photometric and geometric consistency in multiple views. Since photometric ambiguities are usually unique to individual views (except textureless surfaces), exploiting the information from multiple views can often help to pinpoint the right solution. We compute the geometric consistency between two views as the forward-backward reprojection error \u03c8 m l = x l \u2212 H m l H l x l , where H m l denotes the projective backward transformation from the source to the reference image. It is composed from the source image estimates (\u03b8 m l , n m l ) interpolated at the forward projection x m l = H l x l . Intuitively, the estimated depths and normals are consistent if the reprojection error \u03c8 m l is small. Due to computational constraints, we cannot consider the occlusion indicators in the source image for the backward projection. Hence, to handle occlusion in the source image, we employ a robustified geometric cost in \u03be m l = 1\u2212\u03c1 m l +\u03b7 min (\u03c8 m l , \u03c8 max ) using \u03b7 = 0.5 as a constant regularizer and \u03c8 max = 3px as the maximum forward-backward reprojection error. Then, the optimal depth and normal is chosen as\n(\u03b8 opt l ,n opt l ) = argmin \u03b8 * l ,n * l 1 |S| m\u2208S \u03be m l (\u03b8 * l , n * l ).(10)\nThe geometric consistency term is modeled as P (\u03b8 l , n l |\u03b8 m l , n m l ) in the likelihood function, and Section 4.6 shows how to integrate its inference into the overall optimization framework. Experiments in Section 5 demonstrate how this formulation improves both the accuracy and the completeness of the results.\n\n\nIntegration\n\nThis section contextualizes the individual terms of the proposed algorithm by explaining their integration into the overall optimization framework [14]. The joint likelihood function P (X, Z, \u03b8, N ) of our proposed algorithm is defined as\nL l=1 M m=1 [P (Z m l,t |Z m l\u22121,t , Z m l,t\u22121 )P (X m l |Z m l , \u03b8 l , n l )P (\u03b8 l , n l |\u03b8 m l , n m l )]\nover the input images X, the occlusion indicators Z, the depths \u03b8, the normals N , and is composed of several individual terms. First, the spatial and temporal smoothness term P (Z m l,t |Z m l\u22121,t , Z m l,t\u22121 ) (Section 4.3) enforces spatially smooth occlusion maps with reduced temporal oscillation during the optimization. Second, the photometric consistency term P (X m l |Z m l , \u03b8 l , n l ) uses bilateral NCC (Section 4.4) and a slanted plane-induced homography (Section 4.1) to compute the color similarity \u03c1 m l between the reference and source images. Third, the geometric consistency term P (\u03b8 l , n l |\u03b8 m l , n m l ) to enforce multi-view consistent depth and normal estimates. The photometric and geometric consistency terms are computed using Monte-Carlo view sampling from the distribution P l (m) in Eq. (7). The distribution encourages the sampling of non-occluded source images with informative and non-degenerate viewing geometry (Section 4.2).\n\nAnalog to Zheng et al. [14], we factorize the real posterior P (Z, \u03b8, N |X) in its approximation q(Z, \u03b8, N ) = q(Z)q(\u03b8, N ) [53]. Furthermore, for tractability, we constrain q(\u03b8, N ) to the family of Kronecker delta functions q(\u03b8 l , n l ) = \u03b4(\u03b8 l =\u03b8 * l , n l =n * l ). Variational inference then aims to infer the optimal member of the family of approximate posteriors to find the optimal Z, \u03b8, N . The validity of using GEM for this type of problem has already been shown in [14,51]. To infer q(Z m l,t ) in iteration t of the E step of GEM, we employ the forward-backward algorithm as\nq(Z m l,t ) = 1 A \u2212 \u2192 m(Z m l,t ) \u2190 \u2212 m(Z m l,t )(11)\nwith \u2212 \u2192 m(Z m l,t ) and \u2190 \u2212 m(Z m l,t ) being the recursive forward and backward messages \u2212 \u2192 m(Z m l ) = P (X m l |Z m l , \u03b8 l , n l )\nZ m l\u22121 \u2212 \u2192 m(Z m l\u22121 )P (Z m l,t |Z m l\u22121,t , Z m l,t\u22121 ) (12) \u2190 \u2212 m(Z m l ) = Z m l+1 \u2190 \u2212 m(Z m l+1 )P (X m l+1 |Z m l+1 , \u03b8 l+1 , n l+1 )P (Z m l,t |Z m l+1,t , Z m l,t\u22121 ) (13)\nusing an uninformative prior \u2212 \u2192 m(Z m 0 ) = \u2212 \u2192 m(Z m L+1 ) = 0.5. The variable q(Z m l,t ) together with q(\u03b1 m l ), q(\u03b2 m l ), q(\u03ba m l ) determine the view sampling distribution P l (m) used in the M step of GEM as defined in Eq. (7). The M step uses PatchMatch propagation and sampling (Section 4.1) for choosing the optimal depth and normal parameters over q(\u03b8 l , n l ). Since geometrically consistent depth and normal inference is not feasible for all images simultaneously due to memory constraints, we decompose the inference in two stages. In the first stage, we estimate initial depths and normals for each image in the input set X according to Eq. (5). In the second stage, we use coordinate descent optimization to infer geometrically consistent depths and normals according to Eq. (10) by keeping all images but the current reference image as constant. We interleave the E and M step in both stages using row-and column-wise propagation. Four propagations in all directions denote a sweep. In the second stage, a single sweep defines a coordinate descent step, i.e., we alternate between different reference images after propagating through the four directions. Typically, the first stage converges after I 1 = 3 sweeps, while the second stage requires another I 2 = 2 sweeps through the entire image collection to reach a stable state. We refer the reader to the supplementary material for an overview of the steps of our algorithm.  Fig. 3. Reconstruction results for South Building [29] and Fountain [20]. From left to right: Depth map by Zheng et al. [14], then ours only with the photometric term, with the photometric and geometric terms, and the final filtered depth and normal maps.\n\n\nFiltering and Fusion\n\nAfter describing the depth and normal inference, this section proposes a robust method to filter any remaining outliers, e.g., in textureless sky regions.\n\nIn addition to the benefits described previously, the photometric and geometric consistency terms provide us with measures to robustly detect outliers at negligible computational cost. An inlier observation should be both photometrically and geometrically stable with support from multiple views. The sets\nS pho l = {x m l | q(Z m l ) >q Z } (14) S geo l = {x m l | q(\u03b1 m l ) \u2265q \u03b1 , q(\u03b2 m l ) \u2265q \u03b2 , q(\u03ba m l ) >q \u03ba , \u03c8 m l < \u03c8 max }(15)\ndetermine the photometric and geometric support of a reference image pixel x l . To satisfy both constraints, we define the effective support of an observation as S l = {x m l | x m l \u2208 S pho l , x m l \u2208 S geo l } and filter any observations with |S l | < s. In all our experiments, we set s = 3,q Z = 0.5,q \u03b1 = 1,q \u03b2 = 0.5, andq \u03ba = P (\u03ba = 90 \u2022 ). Figs. 3 and 6 show examples of filtered depth and normal maps.\n\nThe collection of support sets S over the observations in all input images defines a directed graph of consistent pixels. In this graph, pixels with sufficient support are nodes, and directed edges point from a reference to a source image pixel. Nodes are associated with depth and normal estimates and, together with the intrinsic and extrinsic calibration, edges define a projective transformation from the reference to the source pixel. Our fusion finds clusters of consistent pixels in this graph by initializing a new cluster using the node with maximum support |S| and recursively collecting connected nodes that satisfy three constraints. Towards this goal, we project the first node into 3D to obtain the location p 0 and normal n 0 . For the first constraint, the projected depth\u03b8 0 of the first node into the image of any other node in the cluster must be consistent with the estimated depth \u03b8 i of the other node such that |\u03b80\u2212\u03b8i| \u03b80 < \u03b8 (cf. [57]). Second, the normals of the two must be consistent such that 1 \u2212 n T 0 n i < n . Third, the reprojection error \u03c8 i of p 0 w.r.t. the other node must be smaller than\u03c8. Note that the graph can have loops, and therefore we only collect nodes once. In addition, multiple pixels in the same image can belong to the same cluster and, by choosing\u03c8, we can control the resolution of the fused point cloud. When there is no remaining node that satisfies the three constraints, we fuse the cluster's elements, if it has at least three elements. The fused point has median location p j and mean normal n j over all cluster elements. The median location is used to avoid artifacts when averaging over multiple neighboring pixels at large depth discontinuities. Finally, we remove the fused nodes from the graph and initialize a new cluster with maximum support |S| until the graph is empty. The resulting point cloud can then be colored (e.g. [58]) for visualization purposes and, since the points already have normals, we can directly apply meshing algorithms (e.g. Poisson reconstruction [59]) as an optional step.\n\n\nExperiments\n\nThis section first demonstrates the benefits of the proposed contributions in isolation. Following that, we compare to other methods and show state-of-the-art results on both low-and high-resolution benchmark datasets. Finally, we evaluate the performance of our algorithm in the challenging setting of large-scale Internet photo collections. The algorithm lends itself for massive parallelization on the row-and column-wise propagation and the view level. In all our experiments, we use a CUDA implementation of our algorithm on a Nvidia Titan X GPU. We set \u03b3 = 0.999, leading to an average of one occlusion indicator state change per 1000 pixels. Empirically, we choose \u03c3 \u03c1 = 0.6,\u1fb1 = 1 \u2022 , and \u03c3 k = 45 \u2022 .\n\nComponents. This paragraph shows the benefits of the individual components in isolation based on the South Building dataset [29], which consists of 128 unstructured images with a resolution of 7MP. We obtain sparse reconstructions using SfM [5]. For each reference view, we use all 127 images as source views with an average runtime of 50s per sweep. Normal Estimation: Fig. 3 shows depth maps using fronto-parallel homographies (1st column) and with normal estimation (2nd to 5th columns), which leads to increased completeness and accuracy for depth inference of oblique scene elements, such as the ground. In addition, our method estimates more accurate normals than standard PatchMatch (Fig. 5(b)). Due to the proposed PatchMatch sampling scheme, our algorithm requires the  [14] with our proposed spatial and temporal smoothness term for the occlusion variables Z. Algorithm starts from the left with the first sweep and is followed by consecutive sweeps to the right. (b) Estimated depths and normals using standard PatchMatch propagation (cf. Fig. 3 for ours). (c) Reference image with filtered depths and normals for crowd-sourced images same number sweeps to converge and only \u2248 25% more runtime due to more hypotheses as compared to Zheng et al. [14], who only estimate per-pixel depths. Geometric Priors: Fig. 4 demonstrates the benefit of each geometric prior. We show the likelihood functions for the reference view against one representative source image. For all priors, we observe varying likelihood within the same source image, underlining the benefit of pixel-wise view selection. The priors correctly downweigh the influence of source images with small triangulation angle, low resolution, or occluded views. Selection Smoothness Fig. 5(a) shows that our temporal smoothness term effectively mitigates the oscillation of the pure spatial smoothness term. While the occlusion variables in the formulation by Zheng et al. [14] oscillate depending on the propagation direction, in our method they quickly converge in a stable state leading to more stable view sampling. Geometric Consistency: Fig. 3 demonstrates improved completeness when incorporating the geometric consistency term, and it also allows to reliably detect outliers for practically outlier-free filtered results. To measure the quantitative impact of our contributions, we obtain benchmark results by omitting a single component or combinations of components from the formulation (Table 1). We observe that each component is important to achieve the overall accuracy and completeness of our method. For further evaluations and impressions of the benefits of our method, we strongly encourage the reader to view the supplementary material.\n\nBenchmarks. The Middlebury benchmark [23] consists of the Dino and Temple models captured at 640x480 under varying settings (Full, Ring, Sparse). For each reference image, we use all views as source images at a runtime of \u2248 40s per view for the Full models with \u2248 300 images. We achieve excellent accuracy and completeness on both models 1 . Specifically, using the standard settings, we rank 1st for Dino Full (tied) and Dino Sparse, while achieving competitive scores for the Temple (4th for Full, 8th for Ring). Note that our method performs best for higher resolutions, as normal estimation needs large patch sizes. Also, we use basic Poisson meshing [59], underlining the highly accurate and outlier-free depth/normal estimates produced by our method. The Strecha benchmark [20] consists of high-resolution images with ground-truth, and we follow the evaluation protocol of Hu and Mordohai [60]. Fig. 3 shows outputs for the Fountain  [20] with reported values from [60]. Ratio of pixels with error less than 2cm and 10cm. Ours w/o normals (\\N), geom. priors (\\P), temp. smoothness (\\S), geom. consistency (\\G), bilateral NCC (\\B), and with all components. [14] [60] [  dataset and, Table 1 lists the results quantifying both the accuracy and completeness. To maintain comparability against Zheng et al. [14], we evaluate our raw depth maps against the ground-truth. We produce significantly more accurate and complete results than Zheng et al., and we outperform the other methods in 3 of 4 categories, even though the results of [28,60,61] are evaluated based on the projection of a 3D surface obtained through depth map fusion. Internet Photos. We densely reconstruct models of 100M Internet photos released by Heinly et al. [5,8] using a single machine with 4 Nvidia Titan X. We process the 41K images at a rate of 70s per view using 2 threads per GPU and finish after 4.2 days in addition to the 6 days needed for sparse modeling using SfM. Whenever we reach the GPU memory limits, we select the most connected source images ranked by the number of shared sparse points. Usually, this limit is reached for \u2248 200 images, while image sizes vary from 0.01MP to 9MP. The fusion and filtering steps consume negligible runtime. Fig. 1 shows fused point clouds, Figs. 6 and 5(c) show depth/normal maps, and the supplementary material provides more results and comparisons against [9,10,47].\n\n\nConclusion\n\nThis work proposes a novel algorithm for robust and efficient dense reconstruction from unstructured image collections. Our method estimates accurate depth and normal information using photometric and geometric information for pixelwise view selection and for image-based fusion and filtering. We achieve stateof-the-art results on benchmarks and crowd-sourced data.\n\n\nX ref from a set of unstructured source images X src = {X m | m = 1 . . . M }. The estimate \u03b8 l maximizes the color similarity between a patch X ref l in the reference image and homography-warped patches X m l in non-occluded source images. The binary indicator variable Z m l \u2208 {0, 1} defines the set of non-occluded source images asX src l = {X m | Z m l = 1}. To sampleX src l , they infer the probability that the reference patch X ref l at depth \u03b8 l is visible at the source patch X m l using\n\n\nspatially smooth occlusion indicators, where a larger \u03b3 enforces neighboring pixels to have more similar indicators. Given reference and source images X = {X ref , X src }, the inference problem then boils down to recover, for all L pixels in the reference image, the depths \u03b8 = {\u03b8 l | l = 1 . . . L} and the occlusion indicators Z = {Z m l | l = 1 . . . L, m = 1 . . . M } from the posterior distribution P (Z, \u03b8|X) with a uniform prior P (\u03b8). To solve the computationally infeasible Bayesian approach of first computing the joint probability\n\nl\n=1) denotes the probability of the patch in source image m being similar to the reference patch, while q(Z) is an approximation of the real posterior P (Z). Source images with small P l (m) are non-informative for the depth inference, hence Zheng et al. propose a Monte Carlo based approximation of \u03b8 opt l for view selection\n\nFig. 2 .\n2Left: Illustration of geometric priors for reference view (R) and three source views(1)(2)(3). View 1 has similar resolution (red), and good triangulation (green) and incident angle (blue), while view 2 is oblique and has lower resolution. View 3 cannot see the patch. Right: Geometric prior likelihood functions with different parameters.\n\n\nt = 1 and t = T \u2212 1 have maximal and minimal influence on the final value Z m l,T , respectively. The two state-transitions are jointly modeled as P (Z m l,t |Z m l\u22121,t , Z m l,t\u22121 ) = P (Z m l,t |Z m l\u22121,t )P (Z m l,t |Z m l,t\u22121 ).\n\nFig. 4 .\n4Photometric and geometric priors for South Building dataset[29] between reference image (R) and each two selected source images(1)(2)(3)(4)(5).\n\nFig. 5 .\n5(a) Comparison of spatial smoothness term\n\nFig. 6 .\n6Reference image with filtered depths and normals for crowd-sourced images.\n\nTable 1 .\n1Strecha benchmark  \nFull results online at http://vision.middlebury.edu/mview/eval/.\n\nMulti-view matching for unordered image sets, or How do I organize my holiday snaps? In: ECCV. F Schaffalitzky, A Zisserman, Schaffalitzky, F., Zisserman, A.: Multi-view matching for unordered image sets, or How do I organize my holiday snaps? In: ECCV. (2002)\n\nPhoto tourism: exploring photo collections in 3d. N Snavely, S Seitz, R Szeliski, ACM Trans. Graphics. Snavely, N., Seitz, S., Szeliski, R.: Photo tourism: exploring photo collections in 3d. ACM Trans. Graphics (2006)\n\nBuilding rome in a day. S Agarwal, Y Furukawa, N Snavely, I Simon, B Curless, S Seitz, R Szeliski, Agarwal, S., Furukawa, Y., Snavely, N., Simon, I., Curless, B., Seitz, S., Szeliski, R.: Building rome in a day. In: ICCV. (2009)\n\nBuilding Rome on a Cloudless Day. J M Frahm, P Fite-Georgel, D Gallup, T Johnson, R Raguram, C Wu, Y H Jen, E Dunn, B Clipp, S Lazebnik, M Pollefeys, ECCV. Frahm, J.M., Fite-Georgel, P., Gallup, D., Johnson, T., Raguram, R., Wu, C., Jen, Y.H., Dunn, E., Clipp, B., Lazebnik, S., Pollefeys, M.: Building Rome on a Cloudless Day. In: ECCV. (2010)\n\nReconstructing the World* in Six Days *(As Captured by the Yahoo 100 Million Image Dataset). J Heinly, J L Sch\u00f6nberger, E Dunn, J M Frahm, In: CVPR. Heinly, J., Sch\u00f6nberger, J.L., Dunn, E., Frahm, J.M.: Reconstructing the World* in Six Days *(As Captured by the Yahoo 100 Million Image Dataset). In: CVPR. (2015)\n\nStructure from motion using structure-less resection. E Zheng, C Wu, ICCV. Zheng, E., Wu, C.: Structure from motion using structure-less resection. In: ICCV. (2015)\n\nFrom single image query to detailed 3d reconstruction. J L Sch\u00f6nberger, F Radenovi\u0107, O Chum, J M Frahm, CVPR.Sch\u00f6nberger, J.L., Radenovi\u0107, F., Chum, O., Frahm, J.M.: From single image query to detailed 3d reconstruction. In: CVPR. (2015)\n\nStructure-from-motion revisited. J L Sch\u00f6nberger, J M Frahm, CVPR. Sch\u00f6nberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: CVPR. (2016)\n\nAccurate, dense, and robust multiview stereopsis. Y Furukawa, J Ponce, CVPR. Furukawa, Y., Ponce, J.: Accurate, dense, and robust multiview stereopsis. In: CVPR. (2007)\n\nTowards internet-scale multiview stereo. Y Furukawa, B Curless, S M Seitz, R Szeliski, CVPR.Furukawa, Y., Curless, B., Seitz, S.M., Szeliski, R.: Towards internet-scale multi- view stereo. In: CVPR. (2010)\n\nScale robust multi view stereo. C Bailer, M Finckh, H P Lensch, ECCV. Bailer, C., Finckh, M., Lensch, H.P.: Scale robust multi view stereo. In: ECCV. (2012)\n\nThe visual turing test for scene reconstruction. Q Shan, R Adams, B Curless, Y Furukawa, S M Seitz, 3Shan, Q., Adams, R., Curless, B., Furukawa, Y., Seitz, S.M.: The visual turing test for scene reconstruction. In: 3DV. (2013)\n\nOccluding contours for multi-view stereo. Q Shan, B Curless, Y Furukawa, C Hernandez, S M Seitz, CVPR.Shan, Q., Curless, B., Furukawa, Y., Hernandez, C., Seitz, S.M.: Occluding con- tours for multi-view stereo. In: CVPR. (2014)\n\nPatchmatch based joint view selection and depthmap estimation. E Zheng, E Dunn, V Jojic, J M Frahm, CVPR.Zheng, E., Dunn, E., Jojic, V., Frahm, J.M.: Patchmatch based joint view selection and depthmap estimation. In: CVPR. (2014)\n\nMassively parallel multiview stereopsis by surface normal diffusion. S Galliani, K Lasinger, K Schindler, ICCV.Galliani, S., Lasinger, K., Schindler, K.: Massively parallel multiview stereopsis by surface normal diffusion. In: ICCV. (2015)\n\nReal-time human pose recognition in parts from single depth images. J Shotton, T Sharp, A Kipman, A Fitzgibbon, M Finocchio, A Blake, M Cook, R Moore, Comm. ACM. Shotton, J., Sharp, T., Kipman, A., Fitzgibbon, A., Finocchio, M., Blake, A., Cook, M., Moore, R.: Real-time human pose recognition in parts from single depth images. In: Comm. ACM. (2013)\n\nView interpolation for image synthesis. S E Chen, L Williams, Conference on Computer graphics and interactive techniques. Chen, S.E., Williams, L.: View interpolation for image synthesis. In: Conference on Computer graphics and interactive techniques. (1993)\n\nAir-ground localization and map augmentation using monocular dense reconstruction. C Forster, M Pizzoli, D Scaramuzza, In: IROS. Forster, C., Pizzoli, M., Scaramuzza, D.: Air-ground localization and map aug- mentation using monocular dense reconstruction. In: IROS. (2013)\n\nA comparison and evaluation of multi-view stereo reconstruction algorithms. S M Seitz, B Curless, J Diebel, D Scharstein, R Szeliski, CVPR.Seitz, S.M., Curless, B., Diebel, J., Scharstein, D., Szeliski, R.: A comparison and evaluation of multi-view stereo reconstruction algorithms. In: CVPR. (2006)\n\nOn benchmarking camera calibration and multi-view stereo for high resolution imagery. C Strecha, W Von Hansen, L V Gool, P Fua, U Thoennessen, CVPR.Strecha, C., von Hansen, W., Gool, L.V., Fua, P., Thoennessen, U.: On bench- marking camera calibration and multi-view stereo for high resolution imagery. In: CVPR. (2008)\n\nDisparity-space images and large occlusion stereo. S S Intille, A F Bobick, Intille, S.S., Bobick, A.F.: Disparity-space images and large occlusion stereo. (1994)\n\nA stereo matching algorithm with an adaptive window: Theory and experiment. T Kanade, M Okutomi, PAMIKanade, T., Okutomi, M.: A stereo matching algorithm with an adaptive window: Theory and experiment. PAMI (1994)\n\nA taxonomy and evaluation of dense two-frame stereo correspondence algorithms. D Scharstein, R Szeliski, IJCV.Scharstein, D., Szeliski, R.: A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. In: IJCV. (2002)\n\nFast cost-volume filtering for visual correspondence and beyond. C Rhemann, A Hosni, M Bleyer, C Rother, M Gelautz, CVPR.Rhemann, C., Hosni, A., Bleyer, M., Rother, C., Gelautz, M.: Fast cost-volume filtering for visual correspondence and beyond. In: CVPR. (2011)\n\nUsing multiple hypotheses to improve depth-maps for multi-view stereo. N Campbell, G Vogiatzis, C Hern\u00e1ndez, R Cipolla, ECCV.Campbell, N., Vogiatzis, G., Hern\u00e1ndez, C., Cipolla, R.: Using multiple hypotheses to improve depth-maps for multi-view stereo. In: ECCV. (2008)\n\nManhattan-world stereo. Y Furukawa, B Curless, S M Seitz, R Szeliski, Furukawa, Y., Curless, B., Seitz, S.M., Szeliski, R.: Manhattan-world stereo. In: CVPR. (2009)\n\nReconstructing building interiors from images. Y Furukawa, B Curless, S M Seitz, R Szeliski, Furukawa, Y., Curless, B., Seitz, S.M., Szeliski, R.: Reconstructing building inte- riors from images. In: CVPR. (2009)\n\nMulti-view reconstruction preserving weakly-supported surfaces. M Jancosek, T Pajdla, CVPR. Jancosek, M., Pajdla, T.: Multi-view reconstruction preserving weakly-supported surfaces. In: CVPR. (2011)\n\nJoint 3d scene reconstruction and class segmentation. C Hane, C Zach, A Cohen, R Angst, M Pollefeys, CVPR.Hane, C., Zach, C., Cohen, A., Angst, R., Pollefeys, M.: Joint 3d scene reconstruc- tion and class segmentation. In: CVPR. (2013)\n\nComplete multi-view reconstruction of dynamic scenes from probabilistic fusion of narrow and wide baseline stereo. T Tung, S Nobuhara, T Matsuyama, Tung, T., Nobuhara, S., Matsuyama, T.: Complete multi-view reconstruction of dynamic scenes from probabilistic fusion of narrow and wide baseline stereo. In: ICCV. (2009)\n\n3d reconstruction of dynamic textures in crowd sourced data. D Ji, E Dunn, J M Frahm, ECCV.Ji, D., Dunn, E., Frahm, J.M.: 3d reconstruction of dynamic textures in crowd sourced data. In: ECCV. (2014)\n\nA convex relaxation approach to space time multi-view 3d reconstruction. M Oswald, D Cremers, ICCV Workshops. Oswald, M., Cremers, D.: A convex relaxation approach to space time multi-view 3d reconstruction. In: ICCV Workshops. (2013)\n\nR Martin-Brualla, D Gallup, S M Seitz, 3d time-lapse reconstruction from internet photos. In: ICCV. Martin-Brualla, R., Gallup, D., Seitz, S.M.: 3d time-lapse reconstruction from internet photos. In: ICCV. (2015)\n\nFrom dusk till dawn: Modeling in the dark. F Radenovi\u0107, J L Sch\u00f6nberger, D Ji, J M Frahm, O Chum, J Matas, CVPR.Radenovi\u0107, F., Sch\u00f6nberger, J.L., Ji, D., Frahm, J.M., Chum, O., Matas, J.: From dusk till dawn: Modeling in the dark. In: CVPR. (2016)\n\nStereo matching with color-weighted correlation, hierarchical belief propagation, and occlusion handling. Q Yang, L Wang, R Yang, H Stew\u00e9nius, D Nist\u00e9r, PAMIYang, Q., Wang, L., Yang, R., Stew\u00e9nius, H., Nist\u00e9r, D.: Stereo matching with color-weighted correlation, hierarchical belief propagation, and occlusion handling. PAMI (2009)\n\nSymmetric stereo matching for occlusion handling. J Sun, Y Li, S B Kang, H Y Shum, CVPR.Sun, J., Li, Y., Kang, S.B., Shum, H.Y.: Symmetric stereo matching for occlusion handling. In: CVPR. (2005)\n\nA cooperative algorithm for stereo matching and occlusion detection. C L Zitnick, T Kanade, PAMIZitnick, C.L., Kanade, T.: A cooperative algorithm for stereo matching and occlu- sion detection. PAMI (2000)\n\nHandling occlusions in dense multi-view stereo. S B Kang, R Szeliski, J Chai, CVPR.Kang, S.B., Szeliski, R., Chai, J.: Handling occlusions in dense multi-view stereo. In: CVPR. (2001)\n\nWide-baseline stereo from multiple views: a probabilistic account. C Strecha, R Fransens, L Van Gool, CVPR.Strecha, C., Fransens, R., Van Gool, L.: Wide-baseline stereo from multiple views: a probabilistic account. In: CVPR. (2004)\n\nCombined depth and outlier estimation in multi-view stereo. C Strecha, R Fransens, L Van Gool, CVPR.Strecha, C., Fransens, R., Van Gool, L.: Combined depth and outlier estimation in multi-view stereo. In: CVPR. (2006)\n\nVariable baseline/resolution stereo. D Gallup, J M Frahm, P Mordohai, M Pollefeys, CVPR.Gallup, D., Frahm, J.M., Mordohai, P., Pollefeys, M.: Variable baseline/resolution stereo. In: CVPR. (2008)\n\nReal-time planesweeping stereo with multiple sweeping directions. D Gallup, J M Frahm, P Mordohai, Q Yang, M Pollefeys, CVPR.Gallup, D., Frahm, J.M., Mordohai, P., Yang, Q., Pollefeys, M.: Real-time plane- sweeping stereo with multiple sweeping directions. In: CVPR. (2007)\n\nElectronically directed focal stereo. P Burt, L Wixson, G Salgian, ICCV. Burt, P., Wixson, L., Salgian, G.: Electronically directed focal stereo. In: ICCV. (1995)\n\nMultiway cut for stereo and motion with slanted surfaces. S Birchfield, C Tomasi, ICCV. Birchfield, S., Tomasi, C.: Multiway cut for stereo and motion with slanted surfaces. In: ICCV. (1999)\n\nMulti-camera reconstruction based on surface normal estimation and best viewpoint selection. X Zabulis, K Daniilidis, 3Zabulis, X., Daniilidis, K.: Multi-camera reconstruction based on surface normal estimation and best viewpoint selection. In: 3DPVT. (2004)\n\nPatchmatch stereo-stereo matching with slanted support windows. M Bleyer, C Rhemann, C Rother, BMVC.Bleyer, M., Rhemann, C., Rother, C.: Patchmatch stereo-stereo matching with slanted support windows. In: BMVC. (2011)\n\nMulti-view stereo for community photo collections. M Goesele, N Snavely, B Curless, H Hoppe, S M Seitz, CVPR.Goesele, M., Snavely, N., Curless, B., Hoppe, H., Seitz, S.M.: Multi-view stereo for community photo collections. In: CVPR. (2007)\n\nFast and high quality fusion of depth maps. C Zach, 3Zach, C.: Fast and high quality fusion of depth maps. In: 3DPVT. (2008)\n\n3d reconstruction using an n-layer heightmap. D Gallup, M Pollefeys, J M Frahm, Pattern Recognition. Gallup, D., Pollefeys, M., Frahm, J.M.: 3d reconstruction using an n-layer heightmap. In: Pattern Recognition. (2010)\n\nEfficient and scalable depthmap fusion. E Zheng, E Dunn, R Raguram, J M Frahm, BMVC.Zheng, E., Dunn, E., Raguram, R., Frahm, J.M.: Efficient and scalable depthmap fusion. In: BMVC. (2012)\n\nA view of the em algorithm that justifies incremental, sparse, and other variants. In: Learning in graphical models. R M Neal, G E Hinton, Neal, R.M., Hinton, G.E.: A view of the em algorithm that justifies incremental, sparse, and other variants. In: Learning in graphical models. (1998)\n\nVariational patchmatch multiview reconstruction and refinement. P Heise, B Jensen, S Klose, A Knoll, CVPR.Heise, P., Jensen, B., Klose, S., Knoll, A.: Variational patchmatch multiview re- construction and refinement. In: CVPR. (2015)\n\nC M Bishop, Pattern Recognition and Machine Learning. Springer VerlagBishop, C.M.: Pattern Recognition and Machine Learning. Springer Verlag (2006)\n\nEvaluation of stereo matching costs on images with radiometric differences. H Hirschm\u00fcller, D Scharstein, PAMIHirschm\u00fcller, H., Scharstein, D.: Evaluation of stereo matching costs on images with radiometric differences. PAMI (2009)\n\nLocally adaptive support-weight approach for visual correspondence search. K J Yoon, I S Kweon, CVPR. Yoon, K.J., Kweon, I.S.: Locally adaptive support-weight approach for visual correspondence search. In: CVPR. (2005)\n\nRecovering consistent video depth maps via bundle optimization. G Zhang, J Jia, T T Wong, H Bao, CVPR.Zhang, G., Jia, J., Wong, T.T., Bao, H.: Recovering consistent video depth maps via bundle optimization. In: CVPR. (2008)\n\nReal-time visibility-based fusion of depth maps. P Merrell, A Akbarzadeh, L Wang, P Mordohai, J M Frahm, R Yang, D Nist\u00e9r, M Pollefeys, CVPR.Merrell, P., Akbarzadeh, A., Wang, L., Mordohai, P., Frahm, J.M., Yang, R., Nist\u00e9r, D., Pollefeys, M.: Real-time visibility-based fusion of depth maps. In: CVPR. (2007)\n\nLet there be color! large-scale texturing of 3d reconstructions. M Waechter, N Moehrle, M Goesele, ECCV.Waechter, M., Moehrle, N., Goesele, M.: Let there be color! large-scale texturing of 3d reconstructions. In: ECCV. (2014)\n\nScreened poisson surface reconstruction. M Kazhdan, H Hoppe, ACM Trans. Graphics. Kazhdan, M., Hoppe, H.: Screened poisson surface reconstruction. ACM Trans. Graphics (2013)\n\nLeast commitment, viewpoint-based, multi-view stereo. X Hu, P Mordohai, 3Hu, X., Mordohai, P.: Least commitment, viewpoint-based, multi-view stereo. In: 3DIMPVT. (2012)\n\nRefinement of surface mesh for accurate multi-view reconstruction. R Tylecek, R Sara, IJVRTylecek, R., Sara, R.: Refinement of surface mesh for accurate multi-view recon- struction. IJVR (2010)\n\nTopology-adaptive mesh deformation for surface evolution, morphing, and multiview reconstruction. A Zaharescu, E Boyer, R Horaud, PAMIZaharescu, A., Boyer, E., Horaud, R.: Topology-adaptive mesh deformation for surface evolution, morphing, and multiview reconstruction. PAMI (2011)\n", "annotations": {"author": "[{\"end\":99,\"start\":63},{\"end\":143,\"start\":100},{\"end\":172,\"start\":144},{\"end\":220,\"start\":173}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":74},{\"end\":113,\"start\":108},{\"end\":158,\"start\":149},{\"end\":190,\"start\":185}]", "author_first_name": "[{\"end\":71,\"start\":63},{\"end\":73,\"start\":72},{\"end\":107,\"start\":100},{\"end\":148,\"start\":144},{\"end\":184,\"start\":173}]", "author_affiliation": "[{\"end\":98,\"start\":87},{\"end\":142,\"start\":115},{\"end\":171,\"start\":160},{\"end\":219,\"start\":192}]", "title": "[{\"end\":60,\"start\":1},{\"end\":280,\"start\":221}]", "venue": null, "abstract": "[{\"end\":993,\"start\":364}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1148,\"start\":1145},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1151,\"start\":1148},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1154,\"start\":1151},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1157,\"start\":1154},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1160,\"start\":1157},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1163,\"start\":1160},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1166,\"start\":1163},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1169,\"start\":1166},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1225,\"start\":1222},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1229,\"start\":1225},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1233,\"start\":1229},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1237,\"start\":1233},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1241,\"start\":1237},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1245,\"start\":1241},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1249,\"start\":1245},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1337,\"start\":1333},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1365,\"start\":1361},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1384,\"start\":1380},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2210,\"start\":2206},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2527,\"start\":2524},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2987,\"start\":2983},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3001,\"start\":2997},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3138,\"start\":3135},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3233,\"start\":3230},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3432,\"start\":3428},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3436,\"start\":3432},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3440,\"start\":3436},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3444,\"start\":3440},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3470,\"start\":3467},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3473,\"start\":3470},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3476,\"start\":3473},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3479,\"start\":3476},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3483,\"start\":3479},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3487,\"start\":3483},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3491,\"start\":3487},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3495,\"start\":3491},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3548,\"start\":3544},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3583,\"start\":3579},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3587,\"start\":3583},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3591,\"start\":3587},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3595,\"start\":3591},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3599,\"start\":3595},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3622,\"start\":3618},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3625,\"start\":3622},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3628,\"start\":3625},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3631,\"start\":3628},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3947,\"start\":3943},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3951,\"start\":3947},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3955,\"start\":3951},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4051,\"start\":4047},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4178,\"start\":4174},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4181,\"start\":4178},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4277,\"start\":4273},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4524,\"start\":4520},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4830,\"start\":4826},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5020,\"start\":5016},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5232,\"start\":5228},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5236,\"start\":5232},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":5240,\"start\":5236},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":5330,\"start\":5326},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5992,\"start\":5988},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6358,\"start\":6354},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6682,\"start\":6678},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6930,\"start\":6926},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7315,\"start\":7311},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7338,\"start\":7334},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7341,\"start\":7338},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7579,\"start\":7575},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8858,\"start\":8854},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8995,\"start\":8991},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9948,\"start\":9944},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10075,\"start\":10071},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11203,\"start\":11199},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11796,\"start\":11792},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12066,\"start\":12062},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":12069,\"start\":12066},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13756,\"start\":13752},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":13759,\"start\":13756},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":13762,\"start\":13759},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14665,\"start\":14661},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":16255,\"start\":16251},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":18868,\"start\":18864},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20827,\"start\":20823},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":21003,\"start\":20999},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":21021,\"start\":21017},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":21024,\"start\":21021},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22466,\"start\":22462},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":22469,\"start\":22466},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":22661,\"start\":22657},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24392,\"start\":24388},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25581,\"start\":25577},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":25682,\"start\":25678},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26036,\"start\":26032},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":26039,\"start\":26036},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26750,\"start\":26747},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28017,\"start\":28013},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28035,\"start\":28031},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28087,\"start\":28083},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":30207,\"start\":30203},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":31144,\"start\":31140},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":31291,\"start\":31287},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32167,\"start\":32163},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32283,\"start\":32280},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32822,\"start\":32818},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33299,\"start\":33295},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33983,\"start\":33979},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":34804,\"start\":34800},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":35422,\"start\":35418},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":35546,\"start\":35542},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":35662,\"start\":35658},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":35707,\"start\":35703},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":35738,\"start\":35734},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":35929,\"start\":35925},{\"end\":35936,\"start\":35935},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":36076,\"start\":36072},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":36303,\"start\":36299},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":36306,\"start\":36303},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":36309,\"start\":36306},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":36499,\"start\":36496},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":36501,\"start\":36499},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":37149,\"start\":37146},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":37152,\"start\":37149},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":37155,\"start\":37152},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":39011,\"start\":39008},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39014,\"start\":39011},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":39017,\"start\":39014},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":39573,\"start\":39569},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":39640,\"start\":39637},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39643,\"start\":39640},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":39646,\"start\":39643},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":39649,\"start\":39646},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":39652,\"start\":39649}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38037,\"start\":37538},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38583,\"start\":38038},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38912,\"start\":38584},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39263,\"start\":38913},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39498,\"start\":39264},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39653,\"start\":39499},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39706,\"start\":39654},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39792,\"start\":39707},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39824,\"start\":39793}]", "paragraph": "[{\"end\":2040,\"start\":1009},{\"end\":2341,\"start\":2042},{\"end\":3270,\"start\":2343},{\"end\":3845,\"start\":3287},{\"end\":4929,\"start\":3847},{\"end\":5806,\"start\":4931},{\"end\":7468,\"start\":5808},{\"end\":7844,\"start\":7524},{\"end\":8600,\"start\":7980},{\"end\":9213,\"start\":8675},{\"end\":9406,\"start\":9274},{\"end\":9610,\"start\":9464},{\"end\":9909,\"start\":9624},{\"end\":10524,\"start\":9931},{\"end\":11022,\"start\":10623},{\"end\":12747,\"start\":11108},{\"end\":13306,\"start\":12749},{\"end\":14625,\"start\":13346},{\"end\":15244,\"start\":14627},{\"end\":15321,\"start\":15246},{\"end\":15901,\"start\":15431},{\"end\":17258,\"start\":15903},{\"end\":18010,\"start\":17260},{\"end\":18358,\"start\":18045},{\"end\":18599,\"start\":18360},{\"end\":19506,\"start\":18701},{\"end\":20271,\"start\":19536},{\"end\":20618,\"start\":20280},{\"end\":20782,\"start\":20620},{\"end\":21208,\"start\":20810},{\"end\":21444,\"start\":21283},{\"end\":21964,\"start\":21483},{\"end\":22470,\"start\":21990},{\"end\":23826,\"start\":22472},{\"end\":24225,\"start\":23907},{\"end\":24479,\"start\":24241},{\"end\":25552,\"start\":24588},{\"end\":26142,\"start\":25554},{\"end\":26333,\"start\":26197},{\"end\":28218,\"start\":26515},{\"end\":28397,\"start\":28243},{\"end\":28704,\"start\":28399},{\"end\":29247,\"start\":28836},{\"end\":31313,\"start\":29249},{\"end\":32037,\"start\":31329},{\"end\":34761,\"start\":32039},{\"end\":37156,\"start\":34763},{\"end\":37537,\"start\":37171}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7944,\"start\":7845},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7979,\"start\":7944},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8674,\"start\":8601},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9273,\"start\":9214},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9463,\"start\":9407},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10602,\"start\":10525},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10622,\"start\":10602},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11107,\"start\":11023},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15430,\"start\":15322},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18044,\"start\":18011},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18700,\"start\":18600},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20279,\"start\":20272},{\"attributes\":{\"id\":\"formula_14\"},\"end\":21282,\"start\":21209},{\"attributes\":{\"id\":\"formula_15\"},\"end\":21482,\"start\":21445},{\"attributes\":{\"id\":\"formula_16\"},\"end\":23906,\"start\":23827},{\"attributes\":{\"id\":\"formula_17\"},\"end\":24587,\"start\":24480},{\"attributes\":{\"id\":\"formula_18\"},\"end\":26196,\"start\":26143},{\"attributes\":{\"id\":\"formula_19\"},\"end\":26514,\"start\":26334},{\"attributes\":{\"id\":\"formula_20\"},\"end\":28835,\"start\":28705}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34512,\"start\":34503},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35958,\"start\":35951}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1007,\"start\":995},{\"attributes\":{\"n\":\"2\"},\"end\":3285,\"start\":3273},{\"attributes\":{\"n\":\"3\"},\"end\":7522,\"start\":7471},{\"attributes\":{\"n\":\"4\"},\"end\":9622,\"start\":9613},{\"attributes\":{\"n\":\"4.1\"},\"end\":9929,\"start\":9912},{\"attributes\":{\"n\":\"4.2\"},\"end\":13344,\"start\":13309},{\"attributes\":{\"n\":\"4.3\"},\"end\":19534,\"start\":19509},{\"attributes\":{\"n\":\"4.4\"},\"end\":20808,\"start\":20785},{\"attributes\":{\"n\":\"4.5\"},\"end\":21988,\"start\":21967},{\"attributes\":{\"n\":\"4.6\"},\"end\":24239,\"start\":24228},{\"attributes\":{\"n\":\"4.7\"},\"end\":28241,\"start\":28221},{\"attributes\":{\"n\":\"5\"},\"end\":31327,\"start\":31316},{\"attributes\":{\"n\":\"6\"},\"end\":37169,\"start\":37159},{\"end\":38586,\"start\":38585},{\"end\":38922,\"start\":38914},{\"end\":39508,\"start\":39500},{\"end\":39663,\"start\":39655},{\"end\":39716,\"start\":39708},{\"end\":39803,\"start\":39794}]", "table": null, "figure_caption": "[{\"end\":38037,\"start\":37540},{\"end\":38583,\"start\":38040},{\"end\":38912,\"start\":38587},{\"end\":39263,\"start\":38924},{\"end\":39498,\"start\":39266},{\"end\":39653,\"start\":39510},{\"end\":39706,\"start\":39665},{\"end\":39792,\"start\":39718},{\"end\":39824,\"start\":39805}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14361,\"start\":14353},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15899,\"start\":15893},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18379,\"start\":18373},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":18423,\"start\":18417},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":19893,\"start\":19885},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":20627,\"start\":20624},{\"end\":27969,\"start\":27963},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":29198,\"start\":29185},{\"end\":32415,\"start\":32409},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":32739,\"start\":32729},{\"end\":33105,\"start\":33089},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33361,\"start\":33355},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":33795,\"start\":33789},{\"end\":34155,\"start\":34149},{\"end\":35670,\"start\":35664},{\"end\":37001,\"start\":36995}]", "bib_author_first_name": "[{\"end\":39987,\"start\":39986},{\"end\":40004,\"start\":40003},{\"end\":40204,\"start\":40203},{\"end\":40215,\"start\":40214},{\"end\":40224,\"start\":40223},{\"end\":40397,\"start\":40396},{\"end\":40408,\"start\":40407},{\"end\":40420,\"start\":40419},{\"end\":40431,\"start\":40430},{\"end\":40440,\"start\":40439},{\"end\":40451,\"start\":40450},{\"end\":40460,\"start\":40459},{\"end\":40637,\"start\":40636},{\"end\":40639,\"start\":40638},{\"end\":40648,\"start\":40647},{\"end\":40664,\"start\":40663},{\"end\":40674,\"start\":40673},{\"end\":40685,\"start\":40684},{\"end\":40696,\"start\":40695},{\"end\":40702,\"start\":40701},{\"end\":40704,\"start\":40703},{\"end\":40711,\"start\":40710},{\"end\":40719,\"start\":40718},{\"end\":40728,\"start\":40727},{\"end\":40740,\"start\":40739},{\"end\":41042,\"start\":41041},{\"end\":41052,\"start\":41051},{\"end\":41054,\"start\":41053},{\"end\":41069,\"start\":41068},{\"end\":41077,\"start\":41076},{\"end\":41079,\"start\":41078},{\"end\":41317,\"start\":41316},{\"end\":41326,\"start\":41325},{\"end\":41484,\"start\":41483},{\"end\":41486,\"start\":41485},{\"end\":41501,\"start\":41500},{\"end\":41514,\"start\":41513},{\"end\":41522,\"start\":41521},{\"end\":41524,\"start\":41523},{\"end\":41701,\"start\":41700},{\"end\":41703,\"start\":41702},{\"end\":41718,\"start\":41717},{\"end\":41720,\"start\":41719},{\"end\":41868,\"start\":41867},{\"end\":41880,\"start\":41879},{\"end\":42029,\"start\":42028},{\"end\":42041,\"start\":42040},{\"end\":42052,\"start\":42051},{\"end\":42054,\"start\":42053},{\"end\":42063,\"start\":42062},{\"end\":42227,\"start\":42226},{\"end\":42237,\"start\":42236},{\"end\":42247,\"start\":42246},{\"end\":42249,\"start\":42248},{\"end\":42402,\"start\":42401},{\"end\":42410,\"start\":42409},{\"end\":42419,\"start\":42418},{\"end\":42430,\"start\":42429},{\"end\":42442,\"start\":42441},{\"end\":42444,\"start\":42443},{\"end\":42623,\"start\":42622},{\"end\":42631,\"start\":42630},{\"end\":42642,\"start\":42641},{\"end\":42654,\"start\":42653},{\"end\":42667,\"start\":42666},{\"end\":42669,\"start\":42668},{\"end\":42873,\"start\":42872},{\"end\":42882,\"start\":42881},{\"end\":42890,\"start\":42889},{\"end\":42899,\"start\":42898},{\"end\":42901,\"start\":42900},{\"end\":43110,\"start\":43109},{\"end\":43122,\"start\":43121},{\"end\":43134,\"start\":43133},{\"end\":43350,\"start\":43349},{\"end\":43361,\"start\":43360},{\"end\":43370,\"start\":43369},{\"end\":43380,\"start\":43379},{\"end\":43394,\"start\":43393},{\"end\":43407,\"start\":43406},{\"end\":43416,\"start\":43415},{\"end\":43424,\"start\":43423},{\"end\":43674,\"start\":43673},{\"end\":43676,\"start\":43675},{\"end\":43684,\"start\":43683},{\"end\":43977,\"start\":43976},{\"end\":43988,\"start\":43987},{\"end\":43999,\"start\":43998},{\"end\":44244,\"start\":44243},{\"end\":44246,\"start\":44245},{\"end\":44255,\"start\":44254},{\"end\":44266,\"start\":44265},{\"end\":44276,\"start\":44275},{\"end\":44290,\"start\":44289},{\"end\":44555,\"start\":44554},{\"end\":44566,\"start\":44565},{\"end\":44580,\"start\":44579},{\"end\":44582,\"start\":44581},{\"end\":44590,\"start\":44589},{\"end\":44597,\"start\":44596},{\"end\":44841,\"start\":44840},{\"end\":44843,\"start\":44842},{\"end\":44854,\"start\":44853},{\"end\":44856,\"start\":44855},{\"end\":45030,\"start\":45029},{\"end\":45040,\"start\":45039},{\"end\":45248,\"start\":45247},{\"end\":45262,\"start\":45261},{\"end\":45471,\"start\":45470},{\"end\":45482,\"start\":45481},{\"end\":45491,\"start\":45490},{\"end\":45501,\"start\":45500},{\"end\":45511,\"start\":45510},{\"end\":45742,\"start\":45741},{\"end\":45754,\"start\":45753},{\"end\":45767,\"start\":45766},{\"end\":45780,\"start\":45779},{\"end\":45966,\"start\":45965},{\"end\":45978,\"start\":45977},{\"end\":45989,\"start\":45988},{\"end\":45991,\"start\":45990},{\"end\":46000,\"start\":45999},{\"end\":46155,\"start\":46154},{\"end\":46167,\"start\":46166},{\"end\":46178,\"start\":46177},{\"end\":46180,\"start\":46179},{\"end\":46189,\"start\":46188},{\"end\":46386,\"start\":46385},{\"end\":46398,\"start\":46397},{\"end\":46576,\"start\":46575},{\"end\":46584,\"start\":46583},{\"end\":46592,\"start\":46591},{\"end\":46601,\"start\":46600},{\"end\":46610,\"start\":46609},{\"end\":46874,\"start\":46873},{\"end\":46882,\"start\":46881},{\"end\":46894,\"start\":46893},{\"end\":47140,\"start\":47139},{\"end\":47146,\"start\":47145},{\"end\":47154,\"start\":47153},{\"end\":47156,\"start\":47155},{\"end\":47353,\"start\":47352},{\"end\":47363,\"start\":47362},{\"end\":47516,\"start\":47515},{\"end\":47534,\"start\":47533},{\"end\":47544,\"start\":47543},{\"end\":47546,\"start\":47545},{\"end\":47773,\"start\":47772},{\"end\":47786,\"start\":47785},{\"end\":47788,\"start\":47787},{\"end\":47803,\"start\":47802},{\"end\":47809,\"start\":47808},{\"end\":47811,\"start\":47810},{\"end\":47820,\"start\":47819},{\"end\":47828,\"start\":47827},{\"end\":48085,\"start\":48084},{\"end\":48093,\"start\":48092},{\"end\":48101,\"start\":48100},{\"end\":48109,\"start\":48108},{\"end\":48122,\"start\":48121},{\"end\":48362,\"start\":48361},{\"end\":48369,\"start\":48368},{\"end\":48375,\"start\":48374},{\"end\":48377,\"start\":48376},{\"end\":48385,\"start\":48384},{\"end\":48387,\"start\":48386},{\"end\":48578,\"start\":48577},{\"end\":48580,\"start\":48579},{\"end\":48591,\"start\":48590},{\"end\":48764,\"start\":48763},{\"end\":48766,\"start\":48765},{\"end\":48774,\"start\":48773},{\"end\":48786,\"start\":48785},{\"end\":48968,\"start\":48967},{\"end\":48979,\"start\":48978},{\"end\":48991,\"start\":48990},{\"end\":49194,\"start\":49193},{\"end\":49205,\"start\":49204},{\"end\":49217,\"start\":49216},{\"end\":49390,\"start\":49389},{\"end\":49400,\"start\":49399},{\"end\":49402,\"start\":49401},{\"end\":49411,\"start\":49410},{\"end\":49423,\"start\":49422},{\"end\":49616,\"start\":49615},{\"end\":49626,\"start\":49625},{\"end\":49628,\"start\":49627},{\"end\":49637,\"start\":49636},{\"end\":49649,\"start\":49648},{\"end\":49657,\"start\":49656},{\"end\":49863,\"start\":49862},{\"end\":49871,\"start\":49870},{\"end\":49881,\"start\":49880},{\"end\":50047,\"start\":50046},{\"end\":50061,\"start\":50060},{\"end\":50274,\"start\":50273},{\"end\":50285,\"start\":50284},{\"end\":50505,\"start\":50504},{\"end\":50515,\"start\":50514},{\"end\":50526,\"start\":50525},{\"end\":50711,\"start\":50710},{\"end\":50722,\"start\":50721},{\"end\":50733,\"start\":50732},{\"end\":50744,\"start\":50743},{\"end\":50753,\"start\":50752},{\"end\":50755,\"start\":50754},{\"end\":50945,\"start\":50944},{\"end\":51073,\"start\":51072},{\"end\":51083,\"start\":51082},{\"end\":51096,\"start\":51095},{\"end\":51098,\"start\":51097},{\"end\":51287,\"start\":51286},{\"end\":51296,\"start\":51295},{\"end\":51304,\"start\":51303},{\"end\":51315,\"start\":51314},{\"end\":51317,\"start\":51316},{\"end\":51553,\"start\":51552},{\"end\":51555,\"start\":51554},{\"end\":51563,\"start\":51562},{\"end\":51565,\"start\":51564},{\"end\":51790,\"start\":51789},{\"end\":51799,\"start\":51798},{\"end\":51809,\"start\":51808},{\"end\":51818,\"start\":51817},{\"end\":51961,\"start\":51960},{\"end\":51963,\"start\":51962},{\"end\":52186,\"start\":52185},{\"end\":52202,\"start\":52201},{\"end\":52418,\"start\":52417},{\"end\":52420,\"start\":52419},{\"end\":52428,\"start\":52427},{\"end\":52430,\"start\":52429},{\"end\":52627,\"start\":52626},{\"end\":52636,\"start\":52635},{\"end\":52643,\"start\":52642},{\"end\":52645,\"start\":52644},{\"end\":52653,\"start\":52652},{\"end\":52837,\"start\":52836},{\"end\":52848,\"start\":52847},{\"end\":52862,\"start\":52861},{\"end\":52870,\"start\":52869},{\"end\":52882,\"start\":52881},{\"end\":52884,\"start\":52883},{\"end\":52893,\"start\":52892},{\"end\":52901,\"start\":52900},{\"end\":52911,\"start\":52910},{\"end\":53164,\"start\":53163},{\"end\":53176,\"start\":53175},{\"end\":53187,\"start\":53186},{\"end\":53367,\"start\":53366},{\"end\":53378,\"start\":53377},{\"end\":53555,\"start\":53554},{\"end\":53561,\"start\":53560},{\"end\":53738,\"start\":53737},{\"end\":53749,\"start\":53748},{\"end\":53964,\"start\":53963},{\"end\":53977,\"start\":53976},{\"end\":53986,\"start\":53985}]", "bib_author_last_name": "[{\"end\":40001,\"start\":39988},{\"end\":40014,\"start\":40005},{\"end\":40212,\"start\":40205},{\"end\":40221,\"start\":40216},{\"end\":40233,\"start\":40225},{\"end\":40405,\"start\":40398},{\"end\":40417,\"start\":40409},{\"end\":40428,\"start\":40421},{\"end\":40437,\"start\":40432},{\"end\":40448,\"start\":40441},{\"end\":40457,\"start\":40452},{\"end\":40469,\"start\":40461},{\"end\":40645,\"start\":40640},{\"end\":40661,\"start\":40649},{\"end\":40671,\"start\":40665},{\"end\":40682,\"start\":40675},{\"end\":40693,\"start\":40686},{\"end\":40699,\"start\":40697},{\"end\":40708,\"start\":40705},{\"end\":40716,\"start\":40712},{\"end\":40725,\"start\":40720},{\"end\":40737,\"start\":40729},{\"end\":40750,\"start\":40741},{\"end\":41049,\"start\":41043},{\"end\":41066,\"start\":41055},{\"end\":41074,\"start\":41070},{\"end\":41085,\"start\":41080},{\"end\":41323,\"start\":41318},{\"end\":41329,\"start\":41327},{\"end\":41498,\"start\":41487},{\"end\":41511,\"start\":41502},{\"end\":41519,\"start\":41515},{\"end\":41530,\"start\":41525},{\"end\":41715,\"start\":41704},{\"end\":41726,\"start\":41721},{\"end\":41877,\"start\":41869},{\"end\":41886,\"start\":41881},{\"end\":42038,\"start\":42030},{\"end\":42049,\"start\":42042},{\"end\":42060,\"start\":42055},{\"end\":42072,\"start\":42064},{\"end\":42234,\"start\":42228},{\"end\":42244,\"start\":42238},{\"end\":42256,\"start\":42250},{\"end\":42407,\"start\":42403},{\"end\":42416,\"start\":42411},{\"end\":42427,\"start\":42420},{\"end\":42439,\"start\":42431},{\"end\":42450,\"start\":42445},{\"end\":42628,\"start\":42624},{\"end\":42639,\"start\":42632},{\"end\":42651,\"start\":42643},{\"end\":42664,\"start\":42655},{\"end\":42675,\"start\":42670},{\"end\":42879,\"start\":42874},{\"end\":42887,\"start\":42883},{\"end\":42896,\"start\":42891},{\"end\":42907,\"start\":42902},{\"end\":43119,\"start\":43111},{\"end\":43131,\"start\":43123},{\"end\":43144,\"start\":43135},{\"end\":43358,\"start\":43351},{\"end\":43367,\"start\":43362},{\"end\":43377,\"start\":43371},{\"end\":43391,\"start\":43381},{\"end\":43404,\"start\":43395},{\"end\":43413,\"start\":43408},{\"end\":43421,\"start\":43417},{\"end\":43430,\"start\":43425},{\"end\":43681,\"start\":43677},{\"end\":43693,\"start\":43685},{\"end\":43985,\"start\":43978},{\"end\":43996,\"start\":43989},{\"end\":44010,\"start\":44000},{\"end\":44252,\"start\":44247},{\"end\":44263,\"start\":44256},{\"end\":44273,\"start\":44267},{\"end\":44287,\"start\":44277},{\"end\":44299,\"start\":44291},{\"end\":44563,\"start\":44556},{\"end\":44577,\"start\":44567},{\"end\":44587,\"start\":44583},{\"end\":44594,\"start\":44591},{\"end\":44609,\"start\":44598},{\"end\":44851,\"start\":44844},{\"end\":44863,\"start\":44857},{\"end\":45037,\"start\":45031},{\"end\":45048,\"start\":45041},{\"end\":45259,\"start\":45249},{\"end\":45271,\"start\":45263},{\"end\":45479,\"start\":45472},{\"end\":45488,\"start\":45483},{\"end\":45498,\"start\":45492},{\"end\":45508,\"start\":45502},{\"end\":45519,\"start\":45512},{\"end\":45751,\"start\":45743},{\"end\":45764,\"start\":45755},{\"end\":45777,\"start\":45768},{\"end\":45788,\"start\":45781},{\"end\":45975,\"start\":45967},{\"end\":45986,\"start\":45979},{\"end\":45997,\"start\":45992},{\"end\":46009,\"start\":46001},{\"end\":46164,\"start\":46156},{\"end\":46175,\"start\":46168},{\"end\":46186,\"start\":46181},{\"end\":46198,\"start\":46190},{\"end\":46395,\"start\":46387},{\"end\":46405,\"start\":46399},{\"end\":46581,\"start\":46577},{\"end\":46589,\"start\":46585},{\"end\":46598,\"start\":46593},{\"end\":46607,\"start\":46602},{\"end\":46620,\"start\":46611},{\"end\":46879,\"start\":46875},{\"end\":46891,\"start\":46883},{\"end\":46904,\"start\":46895},{\"end\":47143,\"start\":47141},{\"end\":47151,\"start\":47147},{\"end\":47162,\"start\":47157},{\"end\":47360,\"start\":47354},{\"end\":47371,\"start\":47364},{\"end\":47531,\"start\":47517},{\"end\":47541,\"start\":47535},{\"end\":47552,\"start\":47547},{\"end\":47783,\"start\":47774},{\"end\":47800,\"start\":47789},{\"end\":47806,\"start\":47804},{\"end\":47817,\"start\":47812},{\"end\":47825,\"start\":47821},{\"end\":47834,\"start\":47829},{\"end\":48090,\"start\":48086},{\"end\":48098,\"start\":48094},{\"end\":48106,\"start\":48102},{\"end\":48119,\"start\":48110},{\"end\":48129,\"start\":48123},{\"end\":48366,\"start\":48363},{\"end\":48372,\"start\":48370},{\"end\":48382,\"start\":48378},{\"end\":48392,\"start\":48388},{\"end\":48588,\"start\":48581},{\"end\":48598,\"start\":48592},{\"end\":48771,\"start\":48767},{\"end\":48783,\"start\":48775},{\"end\":48791,\"start\":48787},{\"end\":48976,\"start\":48969},{\"end\":48988,\"start\":48980},{\"end\":49000,\"start\":48992},{\"end\":49202,\"start\":49195},{\"end\":49214,\"start\":49206},{\"end\":49226,\"start\":49218},{\"end\":49397,\"start\":49391},{\"end\":49408,\"start\":49403},{\"end\":49420,\"start\":49412},{\"end\":49433,\"start\":49424},{\"end\":49623,\"start\":49617},{\"end\":49634,\"start\":49629},{\"end\":49646,\"start\":49638},{\"end\":49654,\"start\":49650},{\"end\":49667,\"start\":49658},{\"end\":49868,\"start\":49864},{\"end\":49878,\"start\":49872},{\"end\":49889,\"start\":49882},{\"end\":50058,\"start\":50048},{\"end\":50068,\"start\":50062},{\"end\":50282,\"start\":50275},{\"end\":50296,\"start\":50286},{\"end\":50512,\"start\":50506},{\"end\":50523,\"start\":50516},{\"end\":50533,\"start\":50527},{\"end\":50719,\"start\":50712},{\"end\":50730,\"start\":50723},{\"end\":50741,\"start\":50734},{\"end\":50750,\"start\":50745},{\"end\":50761,\"start\":50756},{\"end\":50950,\"start\":50946},{\"end\":51080,\"start\":51074},{\"end\":51093,\"start\":51084},{\"end\":51104,\"start\":51099},{\"end\":51293,\"start\":51288},{\"end\":51301,\"start\":51297},{\"end\":51312,\"start\":51305},{\"end\":51323,\"start\":51318},{\"end\":51560,\"start\":51556},{\"end\":51572,\"start\":51566},{\"end\":51796,\"start\":51791},{\"end\":51806,\"start\":51800},{\"end\":51815,\"start\":51810},{\"end\":51824,\"start\":51819},{\"end\":51970,\"start\":51964},{\"end\":52199,\"start\":52187},{\"end\":52213,\"start\":52203},{\"end\":52425,\"start\":52421},{\"end\":52436,\"start\":52431},{\"end\":52633,\"start\":52628},{\"end\":52640,\"start\":52637},{\"end\":52650,\"start\":52646},{\"end\":52657,\"start\":52654},{\"end\":52845,\"start\":52838},{\"end\":52859,\"start\":52849},{\"end\":52867,\"start\":52863},{\"end\":52879,\"start\":52871},{\"end\":52890,\"start\":52885},{\"end\":52898,\"start\":52894},{\"end\":52908,\"start\":52902},{\"end\":52921,\"start\":52912},{\"end\":53173,\"start\":53165},{\"end\":53184,\"start\":53177},{\"end\":53195,\"start\":53188},{\"end\":53375,\"start\":53368},{\"end\":53384,\"start\":53379},{\"end\":53558,\"start\":53556},{\"end\":53570,\"start\":53562},{\"end\":53746,\"start\":53739},{\"end\":53754,\"start\":53750},{\"end\":53974,\"start\":53965},{\"end\":53983,\"start\":53978},{\"end\":53993,\"start\":53987}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":40151,\"start\":39891},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":13385757},\"end\":40370,\"start\":40153},{\"attributes\":{\"id\":\"b2\"},\"end\":40600,\"start\":40372},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2333407},\"end\":40946,\"start\":40602},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":60368537},\"end\":41260,\"start\":40948},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3641088},\"end\":41426,\"start\":41262},{\"attributes\":{\"id\":\"b6\"},\"end\":41665,\"start\":41428},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1728538},\"end\":41815,\"start\":41667},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2845053},\"end\":41985,\"start\":41817},{\"attributes\":{\"id\":\"b9\"},\"end\":42192,\"start\":41987},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":16530287},\"end\":42350,\"start\":42194},{\"attributes\":{\"id\":\"b11\"},\"end\":42578,\"start\":42352},{\"attributes\":{\"id\":\"b12\"},\"end\":42807,\"start\":42580},{\"attributes\":{\"id\":\"b13\"},\"end\":43038,\"start\":42809},{\"attributes\":{\"id\":\"b14\"},\"end\":43279,\"start\":43040},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7731948},\"end\":43631,\"start\":43281},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":7680709},\"end\":43891,\"start\":43633},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":15402474},\"end\":44165,\"start\":43893},{\"attributes\":{\"id\":\"b18\"},\"end\":44466,\"start\":44167},{\"attributes\":{\"id\":\"b19\"},\"end\":44787,\"start\":44468},{\"attributes\":{\"id\":\"b20\"},\"end\":44951,\"start\":44789},{\"attributes\":{\"id\":\"b21\"},\"end\":45166,\"start\":44953},{\"attributes\":{\"id\":\"b22\"},\"end\":45403,\"start\":45168},{\"attributes\":{\"id\":\"b23\"},\"end\":45668,\"start\":45405},{\"attributes\":{\"id\":\"b24\"},\"end\":45939,\"start\":45670},{\"attributes\":{\"id\":\"b25\"},\"end\":46105,\"start\":45941},{\"attributes\":{\"id\":\"b26\"},\"end\":46319,\"start\":46107},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2142078},\"end\":46519,\"start\":46321},{\"attributes\":{\"id\":\"b28\"},\"end\":46756,\"start\":46521},{\"attributes\":{\"id\":\"b29\"},\"end\":47076,\"start\":46758},{\"attributes\":{\"id\":\"b30\"},\"end\":47277,\"start\":47078},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1894245},\"end\":47513,\"start\":47279},{\"attributes\":{\"id\":\"b32\"},\"end\":47727,\"start\":47515},{\"attributes\":{\"id\":\"b33\"},\"end\":47976,\"start\":47729},{\"attributes\":{\"id\":\"b34\"},\"end\":48309,\"start\":47978},{\"attributes\":{\"id\":\"b35\"},\"end\":48506,\"start\":48311},{\"attributes\":{\"id\":\"b36\"},\"end\":48713,\"start\":48508},{\"attributes\":{\"id\":\"b37\"},\"end\":48898,\"start\":48715},{\"attributes\":{\"id\":\"b38\"},\"end\":49131,\"start\":48900},{\"attributes\":{\"id\":\"b39\"},\"end\":49350,\"start\":49133},{\"attributes\":{\"id\":\"b40\"},\"end\":49547,\"start\":49352},{\"attributes\":{\"id\":\"b41\"},\"end\":49822,\"start\":49549},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":6413578},\"end\":49986,\"start\":49824},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":7574259},\"end\":50178,\"start\":49988},{\"attributes\":{\"id\":\"b44\"},\"end\":50438,\"start\":50180},{\"attributes\":{\"id\":\"b45\"},\"end\":50657,\"start\":50440},{\"attributes\":{\"id\":\"b46\"},\"end\":50898,\"start\":50659},{\"attributes\":{\"id\":\"b47\"},\"end\":51024,\"start\":50900},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":18494600},\"end\":51244,\"start\":51026},{\"attributes\":{\"id\":\"b49\"},\"end\":51433,\"start\":51246},{\"attributes\":{\"id\":\"b50\"},\"end\":51723,\"start\":51435},{\"attributes\":{\"id\":\"b51\"},\"end\":51958,\"start\":51725},{\"attributes\":{\"id\":\"b52\"},\"end\":52107,\"start\":51960},{\"attributes\":{\"id\":\"b53\"},\"end\":52340,\"start\":52109},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":14988316},\"end\":52560,\"start\":52342},{\"attributes\":{\"id\":\"b55\"},\"end\":52785,\"start\":52562},{\"attributes\":{\"id\":\"b56\"},\"end\":53096,\"start\":52787},{\"attributes\":{\"id\":\"b57\"},\"end\":53323,\"start\":53098},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":1371704},\"end\":53498,\"start\":53325},{\"attributes\":{\"id\":\"b59\"},\"end\":53668,\"start\":53500},{\"attributes\":{\"id\":\"b60\"},\"end\":53863,\"start\":53670},{\"attributes\":{\"id\":\"b61\"},\"end\":54146,\"start\":53865}]", "bib_title": "[{\"end\":40201,\"start\":40153},{\"end\":40634,\"start\":40602},{\"end\":41039,\"start\":40948},{\"end\":41314,\"start\":41262},{\"end\":41698,\"start\":41667},{\"end\":41865,\"start\":41817},{\"end\":42224,\"start\":42194},{\"end\":43347,\"start\":43281},{\"end\":43671,\"start\":43633},{\"end\":43974,\"start\":43893},{\"end\":46383,\"start\":46321},{\"end\":47350,\"start\":47279},{\"end\":49860,\"start\":49824},{\"end\":50044,\"start\":49988},{\"end\":51070,\"start\":51026},{\"end\":52415,\"start\":52342},{\"end\":53364,\"start\":53325}]", "bib_author": "[{\"end\":40003,\"start\":39986},{\"end\":40016,\"start\":40003},{\"end\":40214,\"start\":40203},{\"end\":40223,\"start\":40214},{\"end\":40235,\"start\":40223},{\"end\":40407,\"start\":40396},{\"end\":40419,\"start\":40407},{\"end\":40430,\"start\":40419},{\"end\":40439,\"start\":40430},{\"end\":40450,\"start\":40439},{\"end\":40459,\"start\":40450},{\"end\":40471,\"start\":40459},{\"end\":40647,\"start\":40636},{\"end\":40663,\"start\":40647},{\"end\":40673,\"start\":40663},{\"end\":40684,\"start\":40673},{\"end\":40695,\"start\":40684},{\"end\":40701,\"start\":40695},{\"end\":40710,\"start\":40701},{\"end\":40718,\"start\":40710},{\"end\":40727,\"start\":40718},{\"end\":40739,\"start\":40727},{\"end\":40752,\"start\":40739},{\"end\":41051,\"start\":41041},{\"end\":41068,\"start\":41051},{\"end\":41076,\"start\":41068},{\"end\":41087,\"start\":41076},{\"end\":41325,\"start\":41316},{\"end\":41331,\"start\":41325},{\"end\":41500,\"start\":41483},{\"end\":41513,\"start\":41500},{\"end\":41521,\"start\":41513},{\"end\":41532,\"start\":41521},{\"end\":41717,\"start\":41700},{\"end\":41728,\"start\":41717},{\"end\":41879,\"start\":41867},{\"end\":41888,\"start\":41879},{\"end\":42040,\"start\":42028},{\"end\":42051,\"start\":42040},{\"end\":42062,\"start\":42051},{\"end\":42074,\"start\":42062},{\"end\":42236,\"start\":42226},{\"end\":42246,\"start\":42236},{\"end\":42258,\"start\":42246},{\"end\":42409,\"start\":42401},{\"end\":42418,\"start\":42409},{\"end\":42429,\"start\":42418},{\"end\":42441,\"start\":42429},{\"end\":42452,\"start\":42441},{\"end\":42630,\"start\":42622},{\"end\":42641,\"start\":42630},{\"end\":42653,\"start\":42641},{\"end\":42666,\"start\":42653},{\"end\":42677,\"start\":42666},{\"end\":42881,\"start\":42872},{\"end\":42889,\"start\":42881},{\"end\":42898,\"start\":42889},{\"end\":42909,\"start\":42898},{\"end\":43121,\"start\":43109},{\"end\":43133,\"start\":43121},{\"end\":43146,\"start\":43133},{\"end\":43360,\"start\":43349},{\"end\":43369,\"start\":43360},{\"end\":43379,\"start\":43369},{\"end\":43393,\"start\":43379},{\"end\":43406,\"start\":43393},{\"end\":43415,\"start\":43406},{\"end\":43423,\"start\":43415},{\"end\":43432,\"start\":43423},{\"end\":43683,\"start\":43673},{\"end\":43695,\"start\":43683},{\"end\":43987,\"start\":43976},{\"end\":43998,\"start\":43987},{\"end\":44012,\"start\":43998},{\"end\":44254,\"start\":44243},{\"end\":44265,\"start\":44254},{\"end\":44275,\"start\":44265},{\"end\":44289,\"start\":44275},{\"end\":44301,\"start\":44289},{\"end\":44565,\"start\":44554},{\"end\":44579,\"start\":44565},{\"end\":44589,\"start\":44579},{\"end\":44596,\"start\":44589},{\"end\":44611,\"start\":44596},{\"end\":44853,\"start\":44840},{\"end\":44865,\"start\":44853},{\"end\":45039,\"start\":45029},{\"end\":45050,\"start\":45039},{\"end\":45261,\"start\":45247},{\"end\":45273,\"start\":45261},{\"end\":45481,\"start\":45470},{\"end\":45490,\"start\":45481},{\"end\":45500,\"start\":45490},{\"end\":45510,\"start\":45500},{\"end\":45521,\"start\":45510},{\"end\":45753,\"start\":45741},{\"end\":45766,\"start\":45753},{\"end\":45779,\"start\":45766},{\"end\":45790,\"start\":45779},{\"end\":45977,\"start\":45965},{\"end\":45988,\"start\":45977},{\"end\":45999,\"start\":45988},{\"end\":46011,\"start\":45999},{\"end\":46166,\"start\":46154},{\"end\":46177,\"start\":46166},{\"end\":46188,\"start\":46177},{\"end\":46200,\"start\":46188},{\"end\":46397,\"start\":46385},{\"end\":46407,\"start\":46397},{\"end\":46583,\"start\":46575},{\"end\":46591,\"start\":46583},{\"end\":46600,\"start\":46591},{\"end\":46609,\"start\":46600},{\"end\":46622,\"start\":46609},{\"end\":46881,\"start\":46873},{\"end\":46893,\"start\":46881},{\"end\":46906,\"start\":46893},{\"end\":47145,\"start\":47139},{\"end\":47153,\"start\":47145},{\"end\":47164,\"start\":47153},{\"end\":47362,\"start\":47352},{\"end\":47373,\"start\":47362},{\"end\":47533,\"start\":47515},{\"end\":47543,\"start\":47533},{\"end\":47554,\"start\":47543},{\"end\":47785,\"start\":47772},{\"end\":47802,\"start\":47785},{\"end\":47808,\"start\":47802},{\"end\":47819,\"start\":47808},{\"end\":47827,\"start\":47819},{\"end\":47836,\"start\":47827},{\"end\":48092,\"start\":48084},{\"end\":48100,\"start\":48092},{\"end\":48108,\"start\":48100},{\"end\":48121,\"start\":48108},{\"end\":48131,\"start\":48121},{\"end\":48368,\"start\":48361},{\"end\":48374,\"start\":48368},{\"end\":48384,\"start\":48374},{\"end\":48394,\"start\":48384},{\"end\":48590,\"start\":48577},{\"end\":48600,\"start\":48590},{\"end\":48773,\"start\":48763},{\"end\":48785,\"start\":48773},{\"end\":48793,\"start\":48785},{\"end\":48978,\"start\":48967},{\"end\":48990,\"start\":48978},{\"end\":49002,\"start\":48990},{\"end\":49204,\"start\":49193},{\"end\":49216,\"start\":49204},{\"end\":49228,\"start\":49216},{\"end\":49399,\"start\":49389},{\"end\":49410,\"start\":49399},{\"end\":49422,\"start\":49410},{\"end\":49435,\"start\":49422},{\"end\":49625,\"start\":49615},{\"end\":49636,\"start\":49625},{\"end\":49648,\"start\":49636},{\"end\":49656,\"start\":49648},{\"end\":49669,\"start\":49656},{\"end\":49870,\"start\":49862},{\"end\":49880,\"start\":49870},{\"end\":49891,\"start\":49880},{\"end\":50060,\"start\":50046},{\"end\":50070,\"start\":50060},{\"end\":50284,\"start\":50273},{\"end\":50298,\"start\":50284},{\"end\":50514,\"start\":50504},{\"end\":50525,\"start\":50514},{\"end\":50535,\"start\":50525},{\"end\":50721,\"start\":50710},{\"end\":50732,\"start\":50721},{\"end\":50743,\"start\":50732},{\"end\":50752,\"start\":50743},{\"end\":50763,\"start\":50752},{\"end\":50952,\"start\":50944},{\"end\":51082,\"start\":51072},{\"end\":51095,\"start\":51082},{\"end\":51106,\"start\":51095},{\"end\":51295,\"start\":51286},{\"end\":51303,\"start\":51295},{\"end\":51314,\"start\":51303},{\"end\":51325,\"start\":51314},{\"end\":51562,\"start\":51552},{\"end\":51574,\"start\":51562},{\"end\":51798,\"start\":51789},{\"end\":51808,\"start\":51798},{\"end\":51817,\"start\":51808},{\"end\":51826,\"start\":51817},{\"end\":51972,\"start\":51960},{\"end\":52201,\"start\":52185},{\"end\":52215,\"start\":52201},{\"end\":52427,\"start\":52417},{\"end\":52438,\"start\":52427},{\"end\":52635,\"start\":52626},{\"end\":52642,\"start\":52635},{\"end\":52652,\"start\":52642},{\"end\":52659,\"start\":52652},{\"end\":52847,\"start\":52836},{\"end\":52861,\"start\":52847},{\"end\":52869,\"start\":52861},{\"end\":52881,\"start\":52869},{\"end\":52892,\"start\":52881},{\"end\":52900,\"start\":52892},{\"end\":52910,\"start\":52900},{\"end\":52923,\"start\":52910},{\"end\":53175,\"start\":53163},{\"end\":53186,\"start\":53175},{\"end\":53197,\"start\":53186},{\"end\":53377,\"start\":53366},{\"end\":53386,\"start\":53377},{\"end\":53560,\"start\":53554},{\"end\":53572,\"start\":53560},{\"end\":53748,\"start\":53737},{\"end\":53756,\"start\":53748},{\"end\":53976,\"start\":53963},{\"end\":53985,\"start\":53976},{\"end\":53995,\"start\":53985}]", "bib_venue": "[{\"end\":39984,\"start\":39891},{\"end\":40254,\"start\":40235},{\"end\":40394,\"start\":40372},{\"end\":40756,\"start\":40752},{\"end\":41095,\"start\":41087},{\"end\":41335,\"start\":41331},{\"end\":41481,\"start\":41428},{\"end\":41732,\"start\":41728},{\"end\":41892,\"start\":41888},{\"end\":42026,\"start\":41987},{\"end\":42262,\"start\":42258},{\"end\":42399,\"start\":42352},{\"end\":42620,\"start\":42580},{\"end\":42870,\"start\":42809},{\"end\":43107,\"start\":43040},{\"end\":43441,\"start\":43432},{\"end\":43753,\"start\":43695},{\"end\":44020,\"start\":44012},{\"end\":44241,\"start\":44167},{\"end\":44552,\"start\":44468},{\"end\":44838,\"start\":44789},{\"end\":45027,\"start\":44953},{\"end\":45245,\"start\":45168},{\"end\":45468,\"start\":45405},{\"end\":45739,\"start\":45670},{\"end\":45963,\"start\":45941},{\"end\":46152,\"start\":46107},{\"end\":46411,\"start\":46407},{\"end\":46573,\"start\":46521},{\"end\":46871,\"start\":46758},{\"end\":47137,\"start\":47078},{\"end\":47387,\"start\":47373},{\"end\":47613,\"start\":47554},{\"end\":47770,\"start\":47729},{\"end\":48082,\"start\":47978},{\"end\":48359,\"start\":48311},{\"end\":48575,\"start\":48508},{\"end\":48761,\"start\":48715},{\"end\":48965,\"start\":48900},{\"end\":49191,\"start\":49133},{\"end\":49387,\"start\":49352},{\"end\":49613,\"start\":49549},{\"end\":49895,\"start\":49891},{\"end\":50074,\"start\":50070},{\"end\":50271,\"start\":50180},{\"end\":50502,\"start\":50440},{\"end\":50708,\"start\":50659},{\"end\":50942,\"start\":50900},{\"end\":51125,\"start\":51106},{\"end\":51284,\"start\":51246},{\"end\":51550,\"start\":51435},{\"end\":51787,\"start\":51725},{\"end\":52012,\"start\":51972},{\"end\":52183,\"start\":52109},{\"end\":52442,\"start\":52438},{\"end\":52624,\"start\":52562},{\"end\":52834,\"start\":52787},{\"end\":53161,\"start\":53098},{\"end\":53405,\"start\":53386},{\"end\":53552,\"start\":53500},{\"end\":53735,\"start\":53670},{\"end\":53961,\"start\":53865}]"}}}, "year": 2023, "month": 12, "day": 17}