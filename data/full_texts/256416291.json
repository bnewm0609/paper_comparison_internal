{"id": 256416291, "updated": "2023-10-05 04:33:58.452", "metadata": {"title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt", "authors": "[{\"first\":\"Dongchao\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Songxiang\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Rongjie\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Chao\",\"last\":\"Weng\",\"middle\":[]},{\"first\":\"Helen\",\"last\":\"Meng\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g.,\"Sigh tone in full of sad mood with some helpless feeling\". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2301.13662", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2301-13662", "doi": "10.48550/arxiv.2301.13662"}}, "content": {"source": {"pdf_hash": "f0aa870639e0766ce675e4bf45f65742321dcbe2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2301.13662v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d7958f6ab05fdab27653583b72f1f8d3c5d9b239", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f0aa870639e0766ce675e4bf45f65742321dcbe2.txt", "contents": "\nInstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt\nAUGUST 2021 1\n\nJournal Of L A T E X Class \nFiles \nInstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt\n148AUGUST 2021 1Index Terms-Text to speechprompt-based learningdiffusion modelmetric learning\nExpressive text-to-speech (TTS) aims to synthesize speech with varying speaking styles to better reflect human speech patterns. In this study, we attempt to use natural language as a style prompt to control the styles in the synthetic speech, e.g., \"Sigh tone in full of sad mood with some helpless feeling\". Considering that there is no existing TTS corpus that is suitable to benchmark this novel task, we first construct a speech corpus whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named InstructTTS, which is novel in the sense of the following aspects:(1) We fully take advantage of self-supervised learning and crossmodal metric learning and propose a novel three-stage training procedure to obtain a robust sentence embedding model that can effectively capture semantic information from the style prompts and control the speaking style in the generated speech.(2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonlyused mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt. Extensive objective and subjective evaluation has been conducted to verify the effectiveness and expressiveness of InstructTTS. Experimental results show that InstructTTS can synthesize highfidelity and natural speech with style prompts controlling the speaking style. Synthesized samples are available online 1 .\n\nI. INTRODUCTION\n\nT EXT-to-speech (TTS) aims to generate human-like speech from input text, which attracts broad interest in the audio and speech processing community. Nowadays, the state-of-the-art TTS systems [1]- [3] are able to produce natural and high-quality speech. However, there still exists a big gap between TTS-synthetic speech and human speech in terms of expressiveness, which limits the broad applications of current speech synthesis systems. Many researchers now focus on a more challenging task, i.e., expressive TTS, which aims to model and control the speaking style (e.g., emotion, speaking-rate and so on) in the generated speech according to human's demands. We note that there are generally two types of methods in the literature to learn speaking style information: Dongchao Yang and Helen Meng are with the Chinese University of Hong Kong. This work was done when Dongchao Yang was an intern at Tencent AI Lab. * denotes equal contribution with order determined by alphabetic order. Songxiang Liu and Chao Weng are with Tencent AI Lab. Rongjie Huang is with the Zhejiang University, China. Songxiang Liu is the corresponding author. 1 http://dongchaoyang.top/InstructTTS/ one type uses auxiliary categorical style labels as the condition of the framework [4], [5], the other imitates the speaking style of a reference speech [6]- [9]. However, there are limitations in the diversity of expressiveness when categorical style labels are used, as these models can only generate a few pre-defined styles from the training set. While TTS models that use a reference utterance to generate a particular speaking style can be trained in an unsupervised manner and are generalizable to out-of-domain speaking styles, the style information extracted from the reference speech may not be easily understandable or interpretable. Additionally, it can be challenging to select a reference speech sample that precisely matches a user's requirements.\n\nFor the first time, we study the modelling of expressive TTS with style prompt in natural language, where we meet with the following research problems: (1) how to train a language model that can capture semantic information from the natural language prompt and control the speaking style in the generated speech; (2) how to design an acoustic model to effectively model the challenging one-to-many learning problem of expressive TTS. In this paper, we will address these two challenges.\n\nThe main contributions of this study are summarized as follows:\n\n(1) For the first time, we study the modelling of expressive TTS with natural language prompts, which brings us a step closer to achieving user-controllable expressive TTS.\n\n(2) We introduce a novel three stage training strategy to obtain a robust sentence embedding model that can effectively capture semantic information from the style prompts.\n\n(3) We propose to model acoustic features in discrete latent space and cast speech synthesis as a sequence-to-sequence language modeling task. Specifically, we train a novel discrete diffusion model to generate vector-quantized (VQ) acoustic features rather than to predict the commonly-used hand-crafted intermediate acoustic features, such as the mel-spectrogram. (4) We explore to model two types of VQ acoustic features: mel-spectrogram based VQ features and waveform-based VQ features. We demonstrate that the two types of VQ features can be effectively modeled by our proposed novel discrete diffusion model. Our waveform-based modelling method only needs one-stage training, and it is a non-autoregressive model, which is far different from the concurrent work VALL-E [10] and MusicLM [11]. (5) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.  Where SALN denotes the style-adaptive layer normalization adaptor [12]. (b) shows the details of our proposed style encoder, which aims to extract style features from GT mel-spectrogram (training stage) or style prompt (inference stage). In Figure 1 (c), we give an example of discrete diffusion decoder to generate VQ mel-spectrogram acoustic features (we name it as Mel-VQ-Diffusion).\n\nThe rest of this paper is organized as follows: In Section II, we motivate our study by introducing the background and related work. In Section III, we present the details of the dataset. In Section IV, we introduce the details of our proposed methods. The experimental setting, evaluation metrics and results are presented from Section V to Section VII. The study is concluded in Section VIII.\n\n\nII. RELATED WORK AND BACKGROUND\n\nThis study is built on several previous works on crossmodal representation learning, vector quantization, diffusion probabilistic models and expressive TTS. We briefly introduce the related studies to set the stage for our research and rationalize the novelty of our contributions.\n\n\nA. Cross-modal Representation Learning\n\nCross-modal representation learning aims to learn a common latent space for different modal data (e.g. text and image, text and speech). In general, two different modal encoders are used to extract deep feature representation, and then a variety of supervised or unsupervised strategies are devised to align the two modal representation spaces [13], [14]. In our study, we expect to control the acoustic features (such as pitch, emotion and speed) by a natural language sentence. To realize this target, we turn to cross-modal representation learning. The details will be discussed later.\n\n\nB. Vector Quantization\n\nVector quantization technique has been widely used in different fields, such as image [15]- [17] and speech processing [18]- [21]. VQ-VAE [15] is one of the most successful vector quantization models. VQ-VAE uses an encoder and a quantizer to compress an image into a low-dimensional discrete space, then a decoder is used to reconstruct the image from a sequence of discrete tokens. Inspired by VQ-VAE, a series of works adopt the idea to reconstruct mel-spectrogram or linear-spectrogram [22], [23]. Recently, many works have focused on reconstruct waveform by VQ-VAE. To supplement the information loss during the VQ process, Residual-VQ (RVQ) [21] and Group-residual-VQ (GRVQ) [24] technique are proposed, which uses multiple different codebooks to encode the audio information. Nowadays, the majority of TTS systems focus on using an acoustic model (AM) to directly predict mel-spectrogram, then uses a pre-trained vocoder to recover waveform from the predicted mel-spectrogram [2], [25], [26]. However, the mel-spectrogram is highly correlated along both time and frequency axes in a complicated way, leading to a great difficulty for the AM to predict. Furthermore, the gap between the ground-truth (GT) mel-spectrogram and the predict one from AM degrades the performance due to the vocoder is trained on GT mel-spectrogram. In this study, instead of using AM to predict mel-spectrogram, we turn to predict learnable and vector-quantized acoustic representation, which is transformed to a discrete latent space.\n\n\nC. Expressive Text-to-speech\n\nIn the field of expressive speech synthesis, several works [6], [9], [12], [27]- [30] have been done. Wang et al. [6] propose to use global style tokens to control and transfer global speaking styles. Inspired by [6], many similar works propose to learn speaking style from a reference audio, such as, Li et al. [27] use a multi-scale style encoder capturing style information from reference audio to facilitate expressive speech synthesis; Huang et al. [30] propose a multi-level style adaptor to transfer speaking style. The most related to our work are Style-Tagging-TTS (ST-TTS) [31] and PromptTTS [32]. ST-TTS proposes to use style tags to guide the speaking style of synthsized speech, where style tags denote a short phrase or word representing the style of an utterance, such as emotion, intention, and tone of voice. In this study, we attempt to use longer natural language as style descriptions to control the styles in the synthetic speech, which is more complicated due to the fact that longer natural language prompts carry out more abundant semantic information and result in more complicated acoustic characteristics. Our concurrent work PromptTTS [32] proposed a similar idea with us, using a sentence as a style prompt to control the style  I  EXAMPLE STYLE PROMPTS FROM DIFFERENT CORPORA. SINCE NLSPEECH CORPUS IS IN MANDARIN CHINESE, WE PROVIDE THE TRANSLATED  VERSION. THE FSNR0 IS IN KOREAN, WE PROVIDE THE TRANSLATED ONES IN THE TABLE. FSNR0 [31] PromptSpeech [32] NLSpeech (translated) Seem sad A distressful male sound appeared in low volume The tone of the shock question revealed the sad feelings.\n\n\nBitter\n\nHe sadly turns down his volume, pitch and speed It was a fiery expression of disapproval and condemnation, with a palpable sense of irony, a tinge of disgust and disdain.\n\n\nPleased\n\nThe ladylike person made an increment of the volume and pitch There was a sense of joy in the words, an expression of joy in the heart, mixed with pride.\n\nIn a hurry Men, low tone, said loudly and quickly His voice grew more agitated, and his tone revealed an urge and urgency. information in TTS systems. They define five different style factors (gender, pitch, speaking speed, volume, and emotion), and assume the prompts have obvious style factor words, such as low-pitch, high-speaking speech and so on, which means that the model can obtain style information from local-level description. Different from PromptTTS, our study does not apply constraint on the form of the style prompts and allows the user to use any free-form natural language to describe a speaking style, resulting in a much more challenging machine learning problem. Furthermore, we focus on Mandarin Chinese TTS and construct the first Mandarin Chinese speech corpus applicable for style-prompt-controllable expressive TTS.\n\n\nD. Diffusion Probabilistic Models\n\nDiffusion models have been demonstrated as a strong generative model for image generation [33]- [36], text generation [37], [38] and audio generation [23], [39]- [41]. Roughly speaking, Diffusion models can be divided into two types according to whether the latent spaces are continuous or discrete. Nowadays, most diffusion models focus on continuous latent space, and the Latent Diffusion Model [42] is one of the representative works. Diffusion models with discrete state spaces are first investigated by [43]- [45]. These works try to define a structured categorical corruption process to corrupt the forward process, then train a model to learn the reverse process. Many works have successfully applied discrete diffusion models in image or sound generation, e.g., D3PMs [45] VQ-Diffusion [34], DiffSound [23]. However, no one attempts to apply the discrete diffusion model to synthesize expressive human speech. In the following, we briefly review background knowledge of diffusion models.\n\n1) Vanilla Diffusion Model: Diffusion models define a Markov chain q(x 1:T |x 0 ) = T t=1 q(x t |x t\u22121 ) that gradually destroys data x 0 by adding noise over a fixed number of steps T, so that x T belongs to special noise distribution (e.g. Gaussian distribution). The reverse process using a generative model that gradually denoises towards the original data distribution p(x 0 ).\n\n2) Discrete Diffusion Model: Discrete diffusion models constrain variable x t in the state space so that x t is a discrete random variable falling into one of K categories. For discrete diffusion, a transition probability matrix is defined to indicate how x 0 transits to x t . The forward process can be repre-\nsented as categorical distributions q(x t |x t\u22121 ) = Cat(x t ; p = x t\u22121 Q t ) where Q t denotes the probabilities transition matrix.\nThe more details about discrete diffusion models, please refer to [23], [34], [45].\n\n\nIII. DATASET\n\nWe use an internally collected Mandarin Chinese speech corpus named NLSpeech to conduct experimental evaluation since there is no openly available Mandarin Chinese speech corpus with rich style prompts. The corpus contains 44 hours of speech data (in total 32k utterances) from 7 speakers (5 female and 2 male). Audio waveform has a sampling rate of 24kHz. We randomly spare 0.1 hours of data as the validation set, another 0.1 hours of data as the test set and the remaining data as the training set. Each utterance has 5 style prompts labeled by different annotators. To obtain highquality annotations, we ask annotators to follow three steps of annotation strategy:\n\n\u2022 Step-1: The annotators first use one word to describe the overall perceived emotion of an utterance; \u2022 Step-2: The annotators then listen to the utterance carefully and describe the emotion level of the utterance with one word; \u2022 Step-3: The annotators write a complete sentence in natural language to describe the style of the utterance. Note that we ask annotators to not care about the speech content, which may influence the perception of emotion and style. Table I shows example style prompts in our dataset, and we also compare NLSpeech with other existing related corpora, including the FSNR0 corpus [31] and the PromptSpeech corpus [32]. We note that the style prompts in NLSpeech are in free-form natural language sentences which are more consistent with those used in our daily life, while those in the FSNR0 and the PromptSpeech corpora are in controlled format. Meanwhile, this also brings us a challenging TTS problem since natural language sentences allow for expressing virtually any concepts. Compact and informative representation of style prompt is therefore paramount to achieve effective style controlling during speech synthesis.\n\n\nIV. PROPOSED METHOD\n\nThe overall architecture of the proposed InstructTTS framework is demonstrated in Figure 1, which consists of five parts, including a content encoder, a style encoder, a speaker embedding module, a style-adaptive layer normalization (SALN) adaptor and a discrete diffusion decoder. The detailed design of each part will be introduced in this section. \n\n\nA. Content Encoder\n\nThe content encoder aims to extract content representation from the content prompts. We follow the architecture of FastSpeech2 [2], which consists of 4 feed-forward transformer. The hidden size, number of attention heads, kernel size and filter size of the one-dimensional convolution in the FFT block are set as 256, 2, 9 and 1024, respectively. After that, a variance adaptor is used to predict information such as duration and pitch that is closely related to the style of synthetic speech.\n\n\nB. Style Prompt Embedding Model\n\nTo extract style representation from the style prompts, we adopt the RoBERTa model [46] as prompt embedding model. Assuming we have a style prompt sequence S = [S 1 , S 2 , ..., S M ], where M denotes the sequence length. We add a [CLS] token to the start of the prompt sequence and then feed it into the prompt embedding model. After that, we take the representation of the [CLS] token as the style representation of this sentence. In order to stably control the style of the output of TTS through natural language description, the quality of prompt embedding is of great importance, which should satisfy two conditions: (1) the learned style prompt space must be able to contain important semantic information; (2) the distribution of prompt embedding space should be relatively uniform and smooth, and the model can be generalized to the style description not seen in the training. To realize this target, we propose a novel three-stage training-fine-tuning strategy. The details are shown as follows.\n\n1) Training a base language model for Chinese: Given that most open-source pre-trained language models are trained on English data, we first train a RoBERTa model on Chinese data.\n\n2) Fine-tuning the pre-trained language model on labeled data: We use a small amount of Chinese natural language inference (NLI) to fine-tune the model parameters in a supervised way to achieve a better semantic representation of the model. Specifically, we follow the training strategy proposed in SimCSE [47], which using an InfoNCE loss [48] objective to fine-tune our pre-trained RoBERTa model.\n\n3) Cross-modal representation learning between style prompts and speech: We hope that the prompt embedding vector from the style prompt sentence and the style representation vector from the speech can be mapped to the shared semantic space so that we can control the style in the TTS output through the style description in the inference stage. Thus, we propose a cross-modal representation learning process based on metric learning, as Figure III shows. Specifically, we build an audio-text retrieval task based on the style-prompt and audio pair in our NLSpeech dataset. For any style prompt, we randomly choose N \u22121 negative audio samples, combined with one positive audio sample to build a training batch. Similarly, for one audio sample, we can also build a training batch that includes one positive style prompt and N \u2212 1 negative style prompts. Inspired by previous audio-text retrieval works [14], [49], we try to use contrastive ranking loss [50] and InfoNCE loss [48] as the training objective. Experiments results show that InfoNCE loss brings better retrieval performance. The details will be introduced in Experiments part.\n\n\nC. Style encoder\n\nThe style encoder module, as shown in Fig. 1 (b), includes three parts: A pre-trained robust style prompt embedding model (i.e., the prompt encoder), an adaptor layer to map the style embedding extracted from the prompt encoder into a new latent space, an audio encoder that extracts style information from the reference mel-spectrogram. Note that our pre-trained robust prompt embedding model is fixed when we train the remaining parts in the TTS model. In the training stage, one of the training targets is to minimize the distance between style prompt embedding and audio embedding. We note that the audio encoder may encode speaker and content information. To make sure the audio encoder only encodes style-related information, during training, we jointly minimize the stylespeaker mutual information (i.e., I(z e ; z sid )) and style-content mutual information (i.e., I(z e ; c)). Mutual information (MI) is a key measurement of correlation between random variables. However, the MI of high-dimensional random variables with an unknown distribution is intractable to compute. Previous works focused on either estimating the MI lower bound or the MI upper bound. MINE [51] and InfoNCE [15] compute a lower bound as the MI estimators while CLUB [52] computes an upper bound as the MI estimator. In this work, we use the CLUB method to minimize style-speaker and style-content MI to avoid content and speaker information leakage from the mel-spectrogram during training.\n\n\nD. Modelling Mel-spectrograms in Discrete Latent Space\n\nIn this part, we introduce our hypothesis: modelling melspectrograms in discrete latent space is a suitable way for expressive TTS. Then we introduce how to utilize VQ-VAE as an intermediate representation to help model the mel-spectrogram. Lastly, we introduce our proposed non-autoregressive mel-spectrogram token generation model, which is based on discrete diffusion models.\n\nMost text-to-speech (TTS) methods [2], [40], [53] directly learn the mapping from text to mel-spectrogram in continuous space. Then they use a pre-trained vocoder to decode the predicted mel-spectrogram into waveform. However, frequency bins in a mel-spectrogram are highly correlated along both time and frequency axes in a complicated way, especially  when the speech sample conveys highly expressive emotions and speaking styles, leading to a challenging modeling problem. Furthermore, the gap between the ground-truth melspectrogram and the predicted one also influences the synthesis performance [54]. In this study, we propose to model the mel-spectrogram in discrete latent space, but still use a HiFi-GAN vocoder [54] to recover the waveform from the melspectrogram. Specifically, we first pre-train a VQ-VAE with a large-scale speech dataset so that the pre-trained Mel-VQ-VAE encodes all of the linguistic, pitch, energy, and emotion information into the latent codes. Then we regard the vector quantized latent codes as the predicting targets and hence model the mel-spectrogram in the discrete latent space. A similar idea modeling mel-spectrogram in discrete latent space is applied in VQ-TTS [55], which utilizes self-supervised VQ acoustic feature (vq-wav2vec [18]) rather than traditional mel-spectrogram as intermediate prediction target. VQ-TTS builds an autoregressive classification model for prosody labels and VQ acoustic features. Different from VQ-TTS, we still use the mel-spectrogram as intermediate acoustic feature and use a Mel-VQ-VAE model to transform the mel-spectrogram into a latent discrete space for reducing the time-frequency correlations. As Figure 3 shows, a mel-spectrogram can be represented by a sequence of mel-spectrogram tokens. Thus, the mel-spectrogram synthesis problem transfers to predicting a sequence of discrete tokens, which can be seen as a language modeling problem. In the following, we will introduce the details of Mel-VQ-VAE, and then we will introduce our proposed Mel-VQ-Diffusion decoder.\n\n1) Mel-VQ-VAE: As Figure 3 shows, Mel-VQ-VAE has three parts: an Mel-encoder E mel , Mel-decoder G mel and a codebook Z = {z k } K k=1 \u2208 R K\u00d7nz . we assume that the size of the codebook is K and the dimension of codes is n z . Assuming input a mel-spectrogram s \u2208 R F bin \u00d7T bin , the mel-spectrogram is firstly encoded as a latent representation\nz = E mel (s) \u2208 R F \u2032 bin \u00d7T \u2032 bin \u00d7nz where F \u2032 bin \u00d7 T \u2032\nbin represents the reduced frequency and time dimension . Then we use the quantizer Q(.) to map each feature\u1e91 ij into its closest codebook entry z k to obtain a sequence of discrete spectrogram tokens z q\nz q = Q(\u1e91) := arg min z k \u2208Z ||\u1e91 ij \u2212 z k || 2 2(1)\nLastly, we use the decoder to reconstruct the mel-spectrogram, i.e.,\u015d = G mel (z q ). To improve the reconstruction performance, we follow VQGAN [17] adds an adversarial loss [56] in the training stage.\n\n2) Mel-VQ-Diffusion decoder: With the help of the pretrained Mel-VQ-VAE, we transfer the problem of melspectrogram prediction into that of predicting a sequence of quantization tokens. To generate high-quality mel-spectrogram tokens while maintaining fast inference speed, we propose a Mel-VQ-Diffusion decoder. In the following, we first introduce the basic idea of Mel-VQ-Diffusion, then summarize the training target. Lastly, we introduce classifier-free guidance to enhance the connection between conditional information and training targets.\n\nGiven the paired training data (x 0 , y), where y denotes the combination of phone features, style features and speaker features. x 0 denotes the ground truth mel-spectrogram tokens. We first build a diffusion process, which corrupts the distribution of p(x 0 ) into a controllable stationary distribution p(x T ). Then we build a Transformer-based neural network [57] to learn to recover the p(x 0 ) conditioned on the y. Inspired by previous works [23], [58], we utilize a mask and uniform transition matrix to guide the diffusion process (we use the index K + 1 denotes the mask token). The transition matrices Q t \u2208 R (K+1)\u00d7(K+1) is defined as:\nQ t = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u03b1 t + \u03b2 t \u03b2 t \u03b2 t \u03b2 t \u00b7 \u00b7 \u00b7 0 \u03b2 t \u03b1 t + \u03b2 t \u03b2 t \u03b2 t \u00b7 \u00b7 \u00b7 0 \u03b2 t \u03b2 t \u03b1 t + \u03b2 t \u03b2 t \u00b7 \u00b7 \u00b7 0 . . . . . . . . . . . . . . . . . . \u03b3 t \u03b3 t \u03b3 t \u03b3 t \u00b7 \u00b7 \u00b7 1 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb .(2)\nThe transition matrix denotes that each token has a probability of \u03b3 t transfers to the mask token, a probability of K\u03b2 t be resampled uniformly over all K categories and a probability of \u03b1 t = 1 \u2212 K\u03b2 t \u2212 \u03b3 t to stay the original token. Based [23], we can calculate q(x t |x 0 ) according to following formula:\nQ t c(x 0 ) = \u03b1 t c(x 0 ) + (\u03b3 t \u2212 \u03b2 t )c(K + 1) + \u03b2 t .(3)\nwhere \u03b1 T = T t=1 \u03b1 t , \u03b3 T = 1 \u2212 T t=1 (1 \u2212 \u03b3 t ) and \u03b2 T = (1 \u2212 \u03b1 T \u2212 \u03b3 T )/K. c(\u00b7) denotes transfer a scalar element into a one-hot column vector. The stationary distribution p(x T ) can be:\np(x T ) = [\u03b2 T , \u03b2 T , \u00b7 \u00b7 \u00b7 , \u03b3 T ],(4)\nDecoder Training Target We train a network p \u03b8 (x t\u22121 |x t , y) to estimate the posterior transition distribution q(x t\u22121 |x t , x 0 ). The network is trained to minimize the variational lower bound (VLB).\nL diff = T \u22121 t=1 D KL [q(x t\u22121 |x t , x 0 )||p \u03b8 (x t\u22121 |x t , y)] + D KL (q(x T |x 0 )||p(x T )),(5)\nEnhancing the connection between x 0 and y Based on previous discussion, we can see that the conditional information y inject into the network, to help optimize p(x t\u22121 |x t , y). However, in the last few steps, when x t includes enough information, the network may ignore the conditional information y in the training stage. To solve this problem, we introduce the classifier free guidance [59], [60] to enhance the connection between x 0 and y. Specifically, instead of only optimizing p(x|y), we expect to optimize the following target function:\n\nlog(p(x|y)) + \u03bb log(p(y|x)),\n\nwhere \u03bb is a hyper-parameter to control the degree of posterior constraint. Using Bayes's theorem, Formula (6) can be derived as:\n\narg max \n\nTo predict the unconditional mel-spectrogram token, we follow [60] to use a learnable null vector n to represent unconditional information y. In the training stage, we set 10% probability to use null vector n. In the inference stage, we first generate the conditional mel-spectrogram token's logits p \u03b8 (x t\u22121 |x t , y), then predict the unconditional mel-spectrogram token's logits p \u03b8 (x t\u22121 |x t , n). Based on formula (7), the next step sample probability p \u03b8 (x t\u22121 |x t , y) can be re-write as:\np \u03b8 (x t\u22121 |x t , n) + (\u03bb + 1)(p \u03b8 (x t\u22121 |x t , y) \u2212 p \u03b8 (x t\u22121 |x t , n))(8)\n\nE. Modelling Waveform in Discrete Latent Space Via Multiple Vector Quantizers\n\nInspired by the success of neural audio codec models, such as SoundStream [21], Encodec [20] and HiFi-Codec [24]. In this study, we additionally investigate directly predicting waveform in the discrete latent space with the help of largescale pre-trained neural audio codec models.\n\nRecently, many methods have been proposed to generate speech using neural codec models, e.g. AudioLM [61] trains speech-to-speech language models on both k-means tokens from a self-supervised model and acoustic tokens from a neural codec model, leading to high-quality speech-to-speech generation. The concurrent work VALL-E [10] proposes to train a two-stage model to synthesize speech based on text input and reference audio. However, VALL-E needs a twostage training strategy, and the first stage is an autoregressive language model, which has a significant influence on the synthesis speed. In this study, we propose a non-autoregressive model based on the discrete diffusion model that significantly improves the synthesis speed while maintaining high-quality synthesis performance.\n\nAs Figures 3 and 4 show, compared to Mel-VQ-VAE, the neural audio codec model includes more codebooks. Although using more codebooks can improve reconstruction performance, it also raises a new research problem: How do you model such a long sequence by transformer? As we know, the computational complexity of a transformer is related to the sequence length. For a 10-second speech with 24k sampling rate, if we use 8 codebooks and set 240 times downsampling in the encoder, we will get 8000 tokens. Using a transformer based model to handle such a long sequence is challenging due to GPU memory limitations, thus it is necessary to seek a novel strategy for long sequence modelling. In this study, we propose a U-Transformer architecture to simultaneously model multiple codebooks. As Figure 5 shows, we first use several convolution layers to down-sample the input codebook matrix along the codebook number dimension. Then we use a denoising transformer to model the relationship of tokens in latent space. After that, we use several convolution layers and upsampling layers to recover the codebook number dimension. Lastly, we use different output layers to output prediction results for each codebook simultaneously.\n\n1) Wave-VQ-Diffusion: There are three differences in Wave-VQ-Diffusion compared to Mel-VQ-Diffusion: (1) We adopt a U-transformer architecture to model multiple codebooks simultaneously. Note that we use the same transformer architecture as that in Mel-VQ-Diffusion. (2) We use different embedding tables for different codebooks due to the fact that tokens from different codebooks follow different data distributions. (3) We design an improved mask and uniform strategy for the diffusion process, which is based on the principle that the information included in the codebooks gradually decreases from the first residual vector quantization layer to the last layer. The codebooks in the first layer include most of the text, style, and speaker identity information. The following layers mainly include the fine-grained acoustic details, which are crucial for the speech's quality. We conjecture that the codebook's tokens in the first layer are easy to recover conditioned on y, instead the following layer's tokens are hard to recover due to the fact that they have no obvious connection with y. Following the easy-first-generation principle, we should mask the last layer's codebook (e.g. codebook N q ) at the start of the forward process and mask the foremost layer's codebook (e.g. codebook 1) at the end of the forward process such that the learnable reverse process follows an easy-first generative behavior. However, previous commonly-used mask and uniform strategy assumed all of the tokens in the sequence were of the same importance, which violates the easy-first-generation principle. To solve this problem, we propose an improved mask and uniform strategy, whose details are presented in the following. Improved Mask and uniform strategy We dynamically allocate different weights for different codebooks when we predefine the transition matrix. Considering these aforementioned properties, we construct \u03b1 i t , \u03b3 i t and \u03b2 i t as follows\n\u03b1 i t = 1 \u2212 t T \u2212 exp( i%Nq 2 * Nq ) 2 * T , \u03b3 i t = t T + exp( i%Nq 2 * Nq ) 2 * T , \u03b2 i t = (1 \u2212 \u03b1 i t \u2212 \u03b3 i t )/K,(9)\nwhere N q denotes the index of residual layer in neural audio codec model, i denotes the token position in the sequence. In our study, we concatenate all of the tokens from the first codebook to the last codebook.\n\n\nF. The Training and Inference Details\n\nIn this section, we summarize the overall training objective and the inference process.\n\n1) Training objective: Our proposed InstructTTS can be trained in an end-to-end manner. The overall training objective is as follows:\nL = L diff + L var + \u03bb 1 I(z e ; c) + \u03bb 2 I(z e ; z sid )+ \u03bb 3 D Euc (z p , z e ) \u2212 \u03b2 1 F 1 (\u03b8 1 ) \u2212 \u03b2 2 F 2 (\u03b8 2 ).(10)\nwhere L diff denotes the diffusion loss, L var denotes the duration, pitch and energy reconstruction loss. I(.) denotes mutual information, D Euc denotes the L2 loss. F 1 (\u03b8 1 ) and F 2 (\u03b8 2 ) denote the likelihood approximation model of q \u03b81 (z sid |z e ) and q \u03b82 (z e |c) respectively. Details about the MI estimation and minimization can be found in [52]. The whole training process is summarized on Algorithm 1. Note that we assume a Mel-VQ-Diffusion decoder is used in Algorithm 1. When we use a Wave-VQ-diffusion decoder, a similar process is used.\n\n2) Inference: In the inference process, we directly use the feature extracted by style prompt embedding model as the style features. In our experiments, we set the T = 100 and \u2206 t = 1. The whole inference process is summarized on Algorithm 2. for (conetent prompt, style prompt, audio) in D do 3: mel = get mel spectrogram(audio); 4: x 0 = E vq (mel); 5: c =ContentEncoder(content prompt); 6: z e =AudioEncoder(mel); 7: z p =PromptEmb(style prompt); 8: z s =SpeakerEmb(speaker id); 9: y = c + z e + z s ; 10: sample t from Uniform(1, 2, 3, ..., T ); 11: sample x t from q(x t |x 0 ) based on formula (??); 12: estimate p \u03b8 (x t\u22121 |x t , y); 13: calculate loss according to formula (10); 14: update network \u03b8; 15: end for 16: end for 17: return network \u03b8.\n\n\nV. EXPERIMENTAL SETUP\n\nA. Dataset and Data Pre-processing 1) Dataset for Vector Quantization Pre-training: To obtain a robust and acoustic-informative Vector Quantization model, we combine one internal dataset with the following three commonly-used public-available TTS datasets: (1) Our internal dataset, which is a Mandarin Chinese speech corpus, contains 300 hours speech data. (2) The VCTK dataset 2 . (3) The AISHELL3 dataset [62]. (4) Clean splits of the LibriTTS dataset [63]. In total, the training set has 669 hours speech data.\n\nAlgorithm 2 Inference of the InstructTTS.\n\n\nRequire:\n\nTime stride \u2206 t , timestep T , Content Prompt, Style Prompt, the decoder of VQ-VAE G, network \u03b8, stationary distribution p(x T ); 1: t = T , c =ContentEncoder(content prompt); 2: z s =SpeakerEmb(speaker id); 3: z p =PromptEmb(style prompt); 4: y = c + z p + z s ; 5: sample x t from p(x T ); 6: while t>0 do 7: sample x t based on formula (8) 8: t \u2190 (t \u2212 \u2206 t ) 9: end while 10: return G(x t ).\n\n2) Dataset for InstructTTS: We use our internal dataset NLSpeech as our training and testing dataset. The details are presented in Section III.\n\n3) Data pre-processing: All audio clips have a sampling rate of 24kHz. For Mel-VQ-VAE pre-training, the log melspectrograms extracted using a 1024-points Hanning window with 240-points hop size and 80 mel bins. The PyWorld toolkit 3 is used to compute F0 values from speech signals. Energy features are computed by taking the l 2 -norm of frequency bins in STFT magnitudes.\n\n\nB. Implementation Details\n\nWe first pre-train the Mel-VQ-VAE and neural audio codec models. Then we fix the pre-trained model, and train the InstructTTS model in an end-to-end manner. In the following, we will introduce the details of the network structure and training strategy.\n\n1) VQ-VAE: In this study, the network structure of the Mel-VQ-VAE model is similar to the VQ-GAN model [17], [22]. To preserve more time-dimension information, we set a downsampling factor of 2 along the time axis, and a downsampling factor of 20 along the frequency axis. For the codebook Z, the dimension of each code word vector n z is set as 256, and the codebook dictionary size K is set as 512.\n\nIn our experiments, we set the learning rate as 1 \u00d7 10 \u22124 . The Adam optimizer [64] (the betas are 0.5 and 0.9) is adopted to optimize weights.\n\n2) Neural Audio Codec Model: Inspired by the success of Encodec [20], SoundStream [21] and HiFi-Codec [24]. We explore three types of audio codec models based on different quantization techniques: Residual Vector Quantization (RVQ) [20], [21], Group Vector Quantization (GVQ) [65] and Group Residual Vector Quantization (GRVQ) [24]. The RVQ-based audio codec model is much similar to the Encodec model except that the frame-shift is 10ms, resulting in 100 frames for 1-second audio. The GVQ-based audio codec model follows the one in the MQTTS system [66], which also uses a frameshift of 10 ms. The GRVQ-based audio codec model is the same as the one introduced in HiFi-Codec [24] and we use 3 https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder the officially open-source implementation 4 . For RVQ model, we set the maximum codebook size as 12 in the training process. Similar to SoundStream [21], quantizer dropout is used. For the GVQ and GRVQ models, we set 4 codebooks in the training process. Each codebook includes 1024 code words for all of the models. We train the three types of audio codec models with the same training set as introduced in SectionV-A.\n\n3) InstructTTS: Our proposed InstructTTS consists of three main parts: a style encoder, a content encoder and a discrete diffusion decoder. For the content encoder, we follow Fast-Speech2 [2] and use the same architecture for the phoneme encoder and the variance adaptor. The style encoder contains a pre-trained prompt encoder model (the details presented in Section IV-B) and an audio encoder. The audio encoder consists of two convolution layers and one multi-head attention module. For the discrete diffusion model, we follow a similar architecture as [23], we build a 12-layer 8-head transformer with a dimension of 256 for the decoder. Each transformer block contains a full-context attention layer, a linear fusion layer to combine conditional features, and a feed-forward network block. For the default setting, we set timesteps T = 100. For the diffusion process, we adopt the linear schedule strategy, which linearly increases \u03b3 t and \u03b2 t from 0 to 0.9 and 0.1, and decreases \u03b1 t from 1 to 0. We optimize our network using the AdamW optimizer [67] with \u03b2 1 = 0.9 and \u03b2 2 = 0.94. The basic learning rate is 3 \u00d7 10 \u22126 , and the batch size is 16 for each GPU.\n\n\nC. Baseline Approach\n\nIn the literature, there is no existing expressive TTS model using natural language style prompt to control stylish generation. Following the traditional neural TTS paradigm which predicts intermediate acoustic features (e.g., mel-spectrograms) from text input, we adapt the StyleSpeech model proposed in [12] as the baseline approach. We replace the Mel-Style-Encoder in the StyleSpeech model with the same style encoder module used in InstructTTS, making the comparison as fair as possible. The baseline model uses the same HiFi-GAN vocoder to generate waveform as the proposed model using Mel-VQ-VAE.\n\n\nVI. EVALUATION METRIC\n\n\nA. Objective Evaluation\n\nWe evaluate the synthesized speech from two aspects: speech quality and prosody similarity. For speech quality, we adopt Mel-cepstral distorion (MCD) [68], structural similarity index measure (SSIM) [69] and Short-Time Objective Intelligibility (STOI) [70] to evaluate the speech quality. For prosody similarity, we use three pitch-related metrics: Gross Pitch Error (GPE), Voicing Decision Error (VDE) [71] and F0 Frame Error (FFE) [72]. GPE, VDE and FFE are widely applied to evaluate the performance of expressive TTS. The details of these metrics will be introduced as follows.  II  OBJECTIVE AND SUBJECTIVE EVALUATION AS WELL AS MODEL SIZE RESULTS. MCD, SSIM, STOI, GPE, VDE AND FFE ARE ADOPTED AS OBJECTIVE  METRICS. GT DENOTES THE GROUND TRUTH SPEECH, GT (VOC) DENOTES THAT WE USE PRE-TRAINED VOCODER (HIFI-GAN) RECOVER  1) Mel-cepstral distorion: Spectral features, based on the short-term power spectrum of sound, such as Mel-cepstral coefficients (MCEPs), contain rich information about expressivity and emotion [73]. Mel-cepstral Distortion (MCD) [68] is a widely adopted metric to measure the spectrum similarity, which is computed as\nM CD = 1 T T \u22121 t=0 M m=1 (c m,t \u2212\u0109 m,t ) 2 ,(11)\nwhere c m,t and\u0109 m,t denote the m-th mel-frequency cepstral coefficient (MFCC) of the t-th frame from the reference and synthesized speech. We sum the squared differences over the first M MFCCs. In this study, we set M = 24.\n\n2) SSIM and STOI: Structural similarity index measure (SSIM) [69] and Short-Time Objective Intelligibility (STOI) [70] are effective metrics to evaluate the speech clarity and intelligibility. Following previous work [74], we also adopt them as one of the metrics for speech quality.\n\n3) Prosody-related metrics: Given that pitch is considered as a major prosodic factor contributing to speech emotion and closely correlated to the activity level [75], [76], in this study, we adopt GPE, VDE and FFE as pitch similarity metrics to evaluate the synthesis results. The metrics of GPE, VDE and FFE have been used as common objective evaluation metric for expressive TTS [7].\n\n\nB. Subjective Evaluation\n\nTo further validate the effectiveness of our proposed method, we conduct subjective evaluation from two aspects: speech quality and style relevance.\n\n1) Speech quality: We first conduct the Mean Opinion Score (MOS) test to evaluate speech quality, which aims to evaluate the speech's naturalness, fidelity and intelligibility. All participants are asked to listen to the reference speech (\"Ground truth\") and the synthesized speech and score the \"quality\" of each speech sample on a 5-point scale ('5' for excellent, '4' for good, '3' for fair, '2' for poor, and '1' for bad). Each audio sample is rated by at least 20 testers.\n\n2) The style's relevance between synthesized speech and the natural language prompt: We conduct RMOS (relevance mean opinion score) for speaking style relevance on the testing set to evaluate the relevance between synthesized speech and the prompt. All participants are asked to read the natural language prompt and then listen to the synthesized speech. After that, the participants are asked to score the \"relevance\" of each  [7] to assess the style relevance between the generated speech with its corresponding natural language style prompt. Raters are asked to rate a 7-point score (from -3 to 3) and choose the speech samples which sound closer to the natural language style prompt in terms of style expression. For each natural language style prompt (A), the listeners are asked to choose a preferred one among the samples synthesized by the baseline model (X) and proposed Method (Y), from which AXY preference rates are calculated. The score less than 0 represents that \"X is much closer\", and the score more than 0 represents that \"Y is much closer\". Note that we do not use the ground truth speech as reference, instead we ask raters to read the natural language style prompt, and then evaluate which synthesized speech is closer to the prompt in terms of semantic meaning in emotion and style.\n\n\nC. Emotion Perception Test\n\nGiven that speaking style is related to emotion. We choose three types of test samples (happy, sad and angry) from our test set based on the natural language prompt. Then we expect our proposed methods can generate similar emotional speech with the guidance of natural language prompt. We propose to use emotion classification probability to validate the emotion perception performance. Intuitively, the classification probabilities summarize the useful emotion information from the previous layers for final output layer. Thus, we believe that the classification probabilities can be an effective tool to justify the synthesized speech's performance. To realize this, we first pre-train an emotion classification model in our internal emotion classification dataset. We adopt a pre-trained wav2vec2 [77] model as feature extractor, and then we add two linear layers and one softmax layer.\n\n\nVII. RESULTS AND ANALYSIS\n\nIn this section, we conduct experiments to verify the effectiveness of our proposed InstructTTS. We first compare the performance between our proposed InstructTTS and the baseline. Then we conduct ablation studies to validate the effectiveness of each part of our proposed methods.\n\nA. The comparison between InstructTTS and the Baseline 1) The analysis of objective metrics: Table II shows the objective metrics (MCD, SSIM, STOI, GPE, VDE, FFE) comparison between our proposed InstructTTS and the baseline system. We have the following observations: (1) Our proposed InstructTTS achieves better performance than the baseline system in terms of speech quality and prosody. (2) Using Mel-VQ-Diffusion as decoder can realize better speech quality than Wave-VQ-Diffusion, but Wave-VQ-Diffusion is superior in maintaining prosody details. One of the reasons is that the pre-trained Mel-VQ-VAE downsamples 20 times along the frequency dimension, which may harm the pitch information. Instead, Wave-VQ-Diffusion directly models all of the information in time domain, prosody-related information can be well reserved, but some acoustic details may sacrifice. The concurrent work VALL-E [10] also faces the same problem that speech quality is suboptimal. We find that the audio codec significantly influences the speech quality. As the last three lines in Table II shows, different audio codec modelling can bring different synthesis performance. We can see that using GRVQ model brings the best performance in Wave-VQ-Diffusion. In the following, without specifically stated, we use GRVQ by default for Wave-VQ-Diffusion model.\n\n2) Subjective Evaluation: We conduct crowd-sourced mean opinion score (MOS) tests to evaluate the quality of the synthesized speech perceptually. Furthermore, we also conduct crowd-sourced relevance mean opinion score (RMOS) tests to evaluate the relevance between the synthesized speech and the prompt. The results are shown on Table II. We can see that InstructTTS (mel) gets the best MOS performance, and InstructTTS (wave) gets the best RMOS performance. We can see that both the two types of our proposed InstructTTS models obtain better RMOS performance than the baseline. The subjective evaluation results are consistency with the objective evaluation results. We can also observe that the speech quality of InstructTTS (wave) still has room for improvement in quality, on which we will further study in our future work. We additionally conduct AXY preference test to compare InstructTTS and the baseline in terms of the naturalness of prosody in their generated speech. From Table III, we can see that the raters show much higher preference to the proposed InstructTTS (Mel) and InstructTTS (Wave) than to the baseline model.\n\n3) Emotion Perception Evaluation: To further evaluate the expressiveness in modeling speaking emotion and styles with InstructTTS, we conduct perception evaluation with a speech emotion classification model. The details are introduced in Section VI-C. The results are reported in Table IV. We can see that the pre-trained speech emotion classification (SEC) model obtains good classification performance in the ground truth set, which proves that our SEC model is effective. Furthermore, we can observe that InstructTTS gets better classification performance than the baseline, with the InstructTTS (Wave) model getting the best performance. We note that the evaluation results are consistent with the FFE results.\n\n\nB. Ablation studies for InstructTTS\n\n1) The impact of cross-modal representation learning for robust style embedding: In this section, we explore the effectiveness of our proposed cross-modal representation learning in Section IV-B. Table V presents the results. We can see that, after finetuning with our proposed cross-modal representation learning, the performance of the RoBERTa even achieves better performance in STS task than only finetuning with SimCSE. Furthermore, we also evaluate the text-to-audio retrieval performance in the test set. As Table VI shows, we can see that using InfoNCE loss as training objective can bring better retrieval performance than the contrastive ranking loss.\n\n2) The Impact of Mutual information minimization (MIM) training: In this study, we propose to using mutual information  Table VII. We can see that using MIM training strategy can improvement in both speech quality and pitch similarity (especially in pitch similarity), which proved that effectiveness of feature disentangled strategy.\n\n3) The effectiveness of classifier-free guidance: In this study, we propose to use classifier-free guidance (CFG) strategy to enhance the connection between conditional information and the predicted results. To validate the effectiveness of classifier-free guidance, we conduct ablation study, the experiments are shown on Table VII. We can see that using CFG strategy can bring better performance due to it enhancing the connection between conditional information and the predicted results, which forces the model to better utilize the conditional information.\n\n4) The effectiveness of improved diffusion strategy: Table  VIII shows the experiments results when we use different diffusion strategies. We can see that our proposed improved mask and uniform strategy can bring better performance. The experimental results validate our proposed easy-firstgeneration principle. 5) Exploring the influence of audio codec for speech synthesis.: As we discuss in Section V-B, we train three types of audio codec models by using different vector quantization techniques: Residual Vector Quantization (RVQ), Group Vector Quantization (GVQ) and Group Residual Vector Quantization (GRVQ). For GVQ and GRVQ, we use 4 codebooks in training and inference process. For RVQ, we use 12 codebooks in the training stage. To fair compare with the GVQ and GRVQ, we also choose 4 codebooks in the inference stage. We use an out-of-domain test set (including 1024 high-quality 24kHz audio samples) to evaluate the reconstruction performance of three types of neural audio codec model and the pre-trained Encodec models [20]. Table IX shows the reconstruction Frame F0 (Hz) Fig. 6. Pitch tracks. We present the F0 contours of 10 different runs with the same text input, speaker id and style prompt conditioning. performance (we adopt the popular metrics PESQ and STOI from speech enhancement fields to measure the reconstruction performance). We can see that the GRVQ models obtain the best reconstruction performance. Table V also shows that using the GRVQ model can synthesis better speech, which means that the audio codec models significantly influence the performance of generation model. We can see that using more codebooks (e.g. using 8 codebooks in RVQ models) can improve reconstruction performance. However, we find that the performance of the generated models is not proportional to the number of codebooks. We think one of the reason is that using more codebooks will bring burden for generation models. Furthermore, we conjecture that the amount of data in the NLSpeech dataset is still in-sufficient and using a largerscale dataset can bring extra improvement.\n\n\nC. Synthesis Variation\n\nUnlike the baseline systhem, which output is uniquely determined by the input text and other conditional information (such as speaker identity, natural language prompt) at inference, InstructTTS takes sampling processes at denoising steps and can inject some variations into the generated speech. To demonstrate this, we run a InstructTTS (mel) model 10 times for a particular input text, speaker and natural language prompt, and then compute the F0 contours of the generated speech samples. We visualize in Figure 6 and observe that InstructTTS can synthesize speech with diverse pitches.\n\n\nVIII. CONCLUSION\n\nIn this work, we present InstructTTS, which can synthesize expressive speech with the natural language prompt. To our best of knowledge, this is the first work to use long and complex natural language prompt to control the speaking style. In terms of acoustic model, we propose a novel perspective to model expressive TTS: we propose to model expressive TTS in the discrete latent space and cast speech synthesis as a language modeling task. We explore two kinds of modelling methods: (1) modelling mel-spectrogram with the help of a pre-trained Mel-VQ-VAE model; (2) modeling waveform with the help of a pre-trained neural audio codec model. In terms of model structure, we propose a novel U-transformer, which can effectively model long-sequence. Our experiments demonstrate the advantages of our proposed method.\n\nThis work still has some limitations that need to be addressed in our future work: (1) The inference speed is limited due to the diffusion step is large (we use 100 diffusion steps).\n\n(2) We will build large-scale dataset to train the InstructTTS models, similar to VALL-E and AudioLM. We believe that InstructTTS is expected to be more robust when the amount of training data increases.\n\n\narXiv:2301.13662v2 [cs.SD] 25 Jun 2023\n\nFig. 1 .\n1(a) shows the model architecture of our proposed InstructTTS.\n\nFig. 2 .\n2The model architecture of cross-modal representation learning.\n\nFig. 3 .\n3The overall architecture of the Mel-VQ-VAE and Neural Audio Codec Models.\n\nFig. 4 .\n4The three types of vector quantization. In this figure, we assume only using 4 vector quantization layer (4 codebooks).\n\n\nx [log p(x|y) + \u03bb log p(y|x)] = arg max x [(\u03bb + 1) log p(x|y) \u2212 \u03bb log p(x)] = arg max x [log p(x) + (\u03bb + 1)(log p(x|y) \u2212 log p(x))].\n\nFig. 5 .\n5The framework of our proposed Wave-VQ-Diffusion. The U-transformer consists of several U-net down-sample and up-sample blocks and a denoising transformer.\n\nAlgorithm 1\n1Training of the InstructTTS. Require: Pre-trained prompt encoder, A transition matrix Q t , timestep T , network parameters \u03b8, training epoch N , NLSpeech dataset D, the encoder of VQ-VAE E vq . 1: for i = 1 to N do 2:\n\nTABLE\n\n\n\nMel-VQ-VAEDecoder \n\nMel-spectrogram Codebook \n\nZ 2 Z 3 \nZ 1 \nZ 4 \nZ K \n\nDiscriminator \n\nmel-spectrogram \ntokens \n\nVQ(.) \n\nEncoder \n\n5 \n1 \n7 39 \n5 \n1 \n7 39 \n\nNeural Audio Codec \n\nEncoder \nDecoder \nVector \nQuantization \n\n\n\nTABLE\n\n\n\nSPEECH FROM MEL-SPECTROGRAM. MOS AND RMOS ARE PRESENTED WITH 95% CONFIDENCE INTERVALS.Model \nDecoder \nMCD(\u2193) \nSSIM(\u2191) \nSTOI (\u2191) \nGPE(\u2193) VDE(\u2193) \nFFE(\u2193) \nMOS(\u2191) \nRMOS(\u2191) \nGT \n-\n4.62 \u00b1 0.05 \n4.65 \u00b1 0.05 \nGT (voc) \n-\n5.02 \n0.695 \n0.893 \n0.006 \n0.076 \n0.08 \n4.41 \u00b1 0.07 \n4.61 \u00b1 0.07 \nBaseline \nMel-decoder \n5.75 \n0.422 \n0.663 \n0.433 \n0.286 \n0.33 \n4.04 \u00b1 0.08 \n3.85 \u00b1 0.1 \n\nInstructTTS \n\nMel-VQ-Diff \n5.59 \n0.487 \n0.732 \n0.392 \n0.246 \n0.30 \n4.35 \u00b1 0.07 \n4.22 \u00b1 0.09 \nWave-VQ-Diff (GVQ) \n5.85 \n0.356 \n0.564 \n0.384 \n0.193 \n0.27 \n3.44 \u00b1 0.07 \n4.08 \u00b1 0.06 \nWave-VQ-Diff (RVQ) \n5.77 \n0.365 \n0.587 \n0.370 \n0.166 \n0.25 \n3.59 \u00b1 0.08 \n4.27 \u00b1 0.07 \nWave-VQ-Diff (GRVQ) \n5.68 \n0.384 \n0.615 \n0.359 \n0.151 \n0.23 \n3.95 \u00b1 0.05 \n4.32 \u00b1 0.07 \n\n\n\nTABLE III THE\nIIIAXY PREFERENCE TEST RESULTS FOR SPEAKING STYLERELEVANCE. \n\nX \nY \n7-point score \n\nBaseline \nInstructTTS (Mel) \n0.72 \nInstructTTS (Wave) \n0.84 \n\nTABLE IV \nTHE EMOTION CLASSIFICATION PROBABILITY (%) COMPARISON \nBETWEEN OUR PROPOSED METHODS AND THE BASELINE. FOR EACH TYPE \nOF EMOTION, WE CHOOSE 15 SAMPLES. THE TABLE REPORTS THE \nAVERAGED PROBABILITY VALUES OF 15 UTTERANCES. \n\nModel \nSad \nHappy Angry Overall \nGT \n100 \n88.80 \n94.70 \n95.20 \nBaseline \n64.28 \n66.60 \n68.15 \n66.70 \nInstructTTS (Mel) \n71.42 \n66.60 \n68.40 \n69.10 \nInstructTTS (Wave) \n71.42 \n55.50 \n84.21 \n71.42 \n\nspeech sample on a 5-point scale ('5' for excellent, '4' for \ngood, '3' for fair, '2' for poor, and '1' for bad). Each audio \nsample is rated by at least 20 testers. \n3) AXY test: We propose to use AXY test \n\nTABLE V THE\nVABLATION STUDY FOR CROSS-MODAL REPRESENTATION LEARNING. WE EVALUATED WITH THE TEST SET OF CHINESE STS-B CORPUS. SCC DENOTES SPEARMAN CORRELATION COEFFICIENT.Model \nSCC (%) \nw/o cross-modal learning \n80.4 \nw cross-modal learning \n80.94 \n\n\n\nTABLE VI THE\nVITEXT-TO-AUDIO RETRIEVAL PERFORMANCE IN THE TEST SET. WE USE RECALL AT RANK K (R@K) AS THE METRICS.TABLE VII THE ABLATION STUDY FOR THE EFFECTIVENESS OF CLASSIFIER-FREE GUIDANCE (CFG) AND MUTUAL INFORMATION MINIMIZATION (MIM) TRAINING STRATEGY.Loss Type \nR@1 \nR@5 \nR@10 \nContrastive Loss \n11.62 \n42.97 \n61.72 \nInfoNCE \n15.62 \n42.97 \n63.67 \n\nModel \nMIM CFG MCD(\u2193) SSIM(\u2191) FFE(\u2193) \n\nInstructTTS (Mel) \n\n5.75 \n0.421 \n0.42 \n\u2713 \n5.65 \n0.442 \n0.37 \n\u2713 \n5.66 \n0.451 \n0.32 \n\u2713 \n\u2713 \n5.59 \n0.487 \n0.30 \n\nInstructTTS (Wav) \n\n5.85 \n0.350 \n0.39 \n\u2713 \n5.74 \n0.365 \n0.34 \n\u2713 \n5.75 \n0.353 \n0.33 \n\u2713 \n\u2713 \n5.68 \n0.384 \n0.23 \n\n\n\nTABLE VIII THE\nVIIIABLATION STUDY FOR DIFFERENT DIFFUSION STRATEGY. MAR REPRESENTS THE MASK AND REPLACE STRATEGY. I-MAR DENOTES OUR IMPROVED MASK AND REPLACE STRATEGY. NOTE THAT WE CONDUCT EXPERIMENTS ON WAVE-VQ-DIFFUSION-BASED INSTRUCTTTS.TABLE IX THE NEURAL AUDIO CODEC'S RECONSTRUCTION PERFORMANCE COMPARISON. Nq DENOTES WE USE Nq CODEBOOKS TO RECONSTRUCTION THE AUDIO. minimization strategy to constrain the encoded information by the audio encoder, we expect the audio encoder only encodes the style-related information. In this part, we conduct ablation studies to investigate whether our proposed MIM strategy can bring better performance. The experiments results report onModel \nMCD(\u2193) SSIM(\u2191) STOI(\u2191) FFE(\u2193) \nInstructTTS (MAR) \n5.71 \n0.364 \n0.582 \n0.24 \nInstructTTS (I-MAR) \n5.68 \n0.384 \n0.615 \n0.23 \n\nModel \nNq \nDown-sample times PESQ STOI \n\nRVQ (ours) \n\n2 \n240 \n2.63 \n0.87 \n4 \n240 \n3.24 \n0.91 \n8 \n240 \n3.54 \n0.93 \nGVQ (ours) \n4 \n240 \n3.11 \n0.91 \nGRVQ (ours) \n4 \n240 \n3.63 \n0.95 \nEncodec (Facebook) \n12 \n320 \n3.21 \n0.95 \n\n\nhttps://datashare.ed.ac.uk/handle/10283/2651\nhttps://github.com/yangdongchao/AcademiCodec\nACKNOWLEDGMENTSWe thank the help of our colleagues Mingjie Jin and Dan Su for this paper. They help us build the NLSpeech dataset.\nTacotron: Towards endto-end speech synthesis. Y Wang, R Skerry-Ryan, D Stanton, Y Wu, R J Weiss, N Jaitly, Z Yang, Y Xiao, Z Chen, S Bengio, Proc. Interspeech. InterspeechY. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, et al., \"Tacotron: Towards end- to-end speech synthesis,\" Proc. Interspeech, 2017.\n\nFastspeech 2: Fast and high-quality end-to-end text to speech. Y Ren, C Hu, X Tan, T Qin, S Zhao, International Conference on Learning Representations (ICLR). Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, et al., \"Fastspeech 2: Fast and high-quality end-to-end text to speech,\" International Conference on Learning Representations (ICLR), pp. 1-8, 2020.\n\nConditional variational autoencoder with adversarial learning for end-to-end text-to-speech. J Kim, J Kong, J Son, PMLRInternational Conference on Machine Learning. 2021J. Kim, J. Kong, and J. Son, \"Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech,\" in International Conference on Machine Learning, pp. 5530-5540, PMLR, 2021.\n\nVisualization and interpretation of latent spaces for controlling expressive speech synthesis through audio analysis. N Tits, F Wang, K El Haddad, V Pagel, T Dutoit, Proc. Interspeech. InterspeechN. Tits, F. Wang, K. El Haddad, V. Pagel, and T. Dutoit, \"Visualization and interpretation of latent spaces for controlling expressive speech synthesis through audio analysis,\" Proc. Interspeech 2019, pp. 4475- 4479, 2019.\n\nExploring transfer learning for low resource emotional tts. N Tits, K El Haddad, T Dutoit, Proceedings of SAI Intelligent Systems Conference. SAI Intelligent Systems ConferenceSpringerN. Tits, K. El Haddad, and T. Dutoit, \"Exploring transfer learning for low resource emotional tts,\" in Proceedings of SAI Intelligent Systems Conference, pp. 52-60, Springer, 2019.\n\nStyle tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis. Y Wang, D Stanton, Y Zhang, R.-S Ryan, E Battenberg, J Shor, Y Xiao, Y Jia, F Ren, R A Saurous, International Conference on Machine Learning. PMLRY. Wang, D. Stanton, Y. Zhang, R.-S. Ryan, E. Battenberg, J. Shor, Y. Xiao, Y. Jia, F. Ren, and R. A. Saurous, \"Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis,\" in International Conference on Machine Learning, pp. 5180-5189, PMLR, 2018.\n\nTowards end-to-end prosody transfer for expressive speech synthesis with tacotron. R Skerry-Ryan, E Battenberg, Y Xiao, Y Wang, D Stanton, J Shor, R Weiss, R Clark, R A Saurous, International Conference on Machine Learning. PMLRR. Skerry-Ryan, E. Battenberg, Y. Xiao, Y. Wang, D. Stanton, J. Shor, R. Weiss, R. Clark, and R. A. Saurous, \"Towards end-to-end prosody transfer for expressive speech synthesis with tacotron,\" in International Conference on Machine Learning, pp. 4693-4702, PMLR, 2018.\n\nTransfer learning from speaker verification to multispeaker text-to-speech synthesis. Y Jia, Y Zhang, R Weiss, Q Wang, J Shen, F Ren, P Nguyen, R Pang, I Moreno, Y Wu, Advances in Neural Information Processing Systems. 31Y. Jia, Y. Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, P. Nguyen, R. Pang, I. Lopez Moreno, Y. Wu, et al., \"Transfer learning from speaker verification to multispeaker text-to-speech synthesis,\" Advances in Neural Information Processing Systems, vol. 31, 2018.\n\nNorespeech: Knowledge distillation based conditional diffusion model for noiserobust expressive tts. D Yang, S Liu, J Yu, H Wang, C Weng, Y Zou, Proc. Interspeech 2023. Interspeech 2023D. Yang, S. Liu, J. Yu, H. Wang, C. Weng, and Y. Zou, \"Norespeech: Knowledge distillation based conditional diffusion model for noise- robust expressive tts,\" Proc. Interspeech 2023, pp. 1-5, 2023.\n\nNeural codec language models are zero-shot text to speech synthesizers. C Wang, S Chen, Y Wu, Z Zhang, L Zhou, S Liu, Z Chen, Y Liu, H Wang, J Li, arXiv:2301.02111arXiv preprintC. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, et al., \"Neural codec language models are zero-shot text to speech synthesizers,\" arXiv preprint arXiv:2301.02111, 2023.\n\nMusicLM: Generating music from text. Andrea , So On, arXiv:2301.11325arXiv preprintAndrea and so on, \"MusicLM: Generating music from text,\" arXiv preprint arXiv: 2301.11325, 2023.\n\nMeta-stylespeech: Multispeaker adaptive text-to-speech generation. D Min, D Lee, E Yang, S Hwang, PMLRInternational Conference on Machine Learning. 2021D. Min, D. Lee, E. Yang, and S. Hwang, \"Meta-stylespeech: Multi- speaker adaptive text-to-speech generation,\" in International Conference on Machine Learning, pp. 7748-7759, PMLR, 2021.\n\nLearning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International Conference on Machine Learning. PMLR2021A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., \"Learning transferable visual models from natural language supervision,\" in International Conference on Machine Learning, pp. 8748-8763, PMLR, 2021.\n\nAudio retrieval with natural language queries: A benchmark study. A S Koepke, A.-M Oncescu, J Henriques, Z Akata, S Albanie, IEEE Transactions on Multimedia. A. S. Koepke, A.-M. Oncescu, J. Henriques, Z. Akata, and S. Albanie, \"Audio retrieval with natural language queries: A benchmark study,\" IEEE Transactions on Multimedia, 2022.\n\nNeural discrete representation learning. A Van Den, O Oord, Vinyals, Advances in Neural Information Processing Systems. 30A. Van Den Oord, O. Vinyals, et al., \"Neural discrete representation learning,\" Advances in Neural Information Processing Systems, vol. 30, 2017.\n\nGenerating diverse high-fidelity images with VQ-VAE-2. A Razavi, A Van Den Oord, O Vinyals, Advances in Neural Information Processing Systems. 32A. Razavi, A. Van den Oord, and O. Vinyals, \"Generating diverse high-fidelity images with VQ-VAE-2,\" Advances in Neural Information Processing Systems, vol. 32, 2019.\n\nTaming transformers for highresolution image synthesis. P Esser, R Rombach, B Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionP. Esser, R. Rombach, and B. Ommer, \"Taming transformers for high- resolution image synthesis,\" in Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, pp. 12873-12883, 2021.\n\nVQ-Wav2Vec: Self-supervised learning of discrete speech representations. A Baevski, S Schneider, M Auli, International Conference on Learning Representations (ICLR). A. Baevski, S. Schneider, and M. Auli, \"VQ-Wav2Vec: Self-supervised learning of discrete speech representations,\" International Conference on Learning Representations (ICLR), pp. 1-9, 2019.\n\nHubert: Self-supervised speech representation learning by masked prediction of hidden units. W.-N Hsu, B Bolte, Y.-H H Tsai, K Lakhotia, R Salakhutdinov, A Mohamed, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 29W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \"Hubert: Self-supervised speech representation learning by masked prediction of hidden units,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451-3460, 2021.\n\nHigh fidelity neural audio compression. A D\u00e9fossez, J Copet, G Synnaeve, Y Adi, arXiv:2210.13438arXiv preprintA. D\u00e9fossez, J. Copet, G. Synnaeve, and Y. Adi, \"High fidelity neural audio compression,\" arXiv preprint arXiv:2210.13438, 2022.\n\nSoundstream: An end-to-end neural audio codec. N Zeghidour, A Luebs, A Omran, J Skoglund, M Tagliasacchi, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 30N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi, \"Soundstream: An end-to-end neural audio codec,\" IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 30, pp. 495-507, 2021.\n\nTaming visually guided sound generation. V Iashin, E Rahtu, British Machine Vision Conference (BMVC). 2021V. Iashin and E. Rahtu, \"Taming visually guided sound generation,\" in British Machine Vision Conference (BMVC), 2021.\n\nDiffsound: Discrete diffusion model for text-to-sound generation. D Yang, J Yu, H Wang, W Wang, C Weng, Y Zou, D Yu, Speech, and Language Processing. D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, and D. Yu, \"Diffsound: Discrete diffusion model for text-to-sound generation,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1720-1733, 2023.\n\nHiFi-Codec: Group-residual vector quantization for high fidelity audio codec. D Yang, S Liu, R Huang, J Tian, C Weng, Y Zou, arXiv:2305.02765arXiv preprintD. Yang, S. Liu, R. Huang, J. Tian, C. Weng, and Y. Zou, \"HiFi-Codec: Group-residual vector quantization for high fidelity audio codec,\" arXiv preprint arXiv:2305.02765, 2023.\n\nParallel tacotron 2: A non-autoregressive neural tts model with differentiable duration modeling. I Elias, H Zen, J Shen, Y Zhang, Y Jia, R Skerry-Ryan, Y Wu, Proc. Interspeech 2021. Interspeech 2021I. Elias, H. Zen, J. Shen, Y. Zhang, Y. Jia, R. Skerry-Ryan, and Y. Wu, \"Parallel tacotron 2: A non-autoregressive neural tts model with differentiable duration modeling,\" Proc. Interspeech 2021, pp. 141-145, 2021.\n\nGlow-TTS: A generative flow for text-to-speech via monotonic alignment search. J Kim, S Kim, J Kong, S Yoon, Advances in Neural Information Processing Systems. 33J. Kim, S. Kim, J. Kong, and S. Yoon, \"Glow-TTS: A generative flow for text-to-speech via monotonic alignment search,\" Advances in Neural Information Processing Systems, vol. 33, pp. 8067-8077, 2020.\n\nTowards multiscale style control for expressive speech synthesis. X Li, C Song, J Li, Z Wu, J Jia, H Meng, arXiv:2104.03521arXiv preprintX. Li, C. Song, J. Li, Z. Wu, J. Jia, and H. Meng, \"Towards multi- scale style control for expressive speech synthesis,\" arXiv preprint arXiv:2104.03521, 2021.\n\nSc-glowtts: an efficient zero-shot multi-speaker text-to-speech model. E Casanova, C Shulby, E G\u00f6lge, N M M\u00fcller, F S De Oliveira, A C Junior, A D S Soares, S M Aluisio, M A Ponti, arXiv:2104.05557arXiv preprintE. Casanova, C. Shulby, E. G\u00f6lge, N. M. M\u00fcller, F. S. de Oliveira, A. C. Junior, A. d. S. Soares, S. M. Aluisio, and M. A. Ponti, \"Sc-glowtts: an efficient zero-shot multi-speaker text-to-speech model,\" arXiv preprint arXiv:2104.05557, 2021.\n\nSpeech synthesis with mixed emotions. K Zhou, B Sisman, R Rana, B W Schuller, H Li, IEEE Transactions on Affective Computing. K. Zhou, B. Sisman, R. Rana, B. W. Schuller, and H. Li, \"Speech synthe- sis with mixed emotions,\" IEEE Transactions on Affective Computing, 2022.\n\nGenerspeech: Towards style transfer for generalizable out-of-domain text-to-speech. R Huang, Y Ren, J Liu, C Cui, Z Zhao, Advances in Neural Information Processing Systems. 35R. Huang, Y. Ren, J. Liu, C. Cui, and Z. Zhao, \"Generspeech: Towards style transfer for generalizable out-of-domain text-to-speech,\" Advances in Neural Information Processing Systems, vol. 35, pp. 10970-10983, 2022.\n\nExpressive Text-to-Speech Using Style Tag. M Kim, S J Cheon, B J Choi, J J Kim, N S Kim, Proc. Interspeech 2021. Interspeech 2021M. Kim, S. J. Cheon, B. J. Choi, J. J. Kim, and N. S. Kim, \"Expressive Text-to-Speech Using Style Tag,\" in Proc. Interspeech 2021, pp. 4663- 4667, 2021.\n\nPromptTTS: Controllable text-to-speech with text descriptions. Z Guo, Y Leng, Y Wu, S Zhao, X Tan, arXiv:2211.12171arXiv preprintZ. Guo, Y. Leng, Y. Wu, S. Zhao, and X. Tan, \"PromptTTS: Controllable text-to-speech with text descriptions,\" arXiv preprint arXiv:2211.12171, 2022.\n\nDiffusion models beat gans on image synthesis. P Dhariwal, A Nichol, Advances in Neural Information Processing Systems. 34P. Dhariwal and A. Nichol, \"Diffusion models beat gans on image synthesis,\" Advances in Neural Information Processing Systems, vol. 34, 2021.\n\nVector quantized diffusion model for text-to-image synthesis. S Gu, D Chen, J Bao, F Wen, B Zhang, D Chen, L Yuan, B Guo, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionS. Gu, D. Chen, J. Bao, F. Wen, B. Zhang, D. Chen, L. Yuan, and B. Guo, \"Vector quantized diffusion model for text-to-image synthesis,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10696-10706, 2022.\n\nGLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. A Q Nichol, P Dhariwal, A Ramesh, P Shyam, P Mishkin, B Mcgrew, I Sutskever, M Chen, International Conference on Machine Learning. PMLR2022A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever, and M. Chen, \"GLIDE: Towards photorealistic image gen- eration and editing with text-guided diffusion models,\" in International Conference on Machine Learning, pp. 16784-16804, PMLR, 2022.\n\nImagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis. P Esser, R Rombach, A Blattmann, B Ommer, Advances in Neural Information Processing Systems. 34P. Esser, R. Rombach, A. Blattmann, and B. Ommer, \"Imagebart: Bidi- rectional context with multinomial diffusion for autoregressive image synthesis,\" Advances in Neural Information Processing Systems, vol. 34, 2021.\n\nDiffusion-lm improves controllable text generation. X Li, J Thickstun, I Gulrajani, P S Liang, T B Hashimoto, Advances in Neural Information Processing Systems. 35X. Li, J. Thickstun, I. Gulrajani, P. S. Liang, and T. B. Hashimoto, \"Diffusion-lm improves controllable text generation,\" Advances in Neu- ral Information Processing Systems, vol. 35, pp. 4328-4343, 2022.\n\nDiffuseq: Sequence to sequence text generation with diffusion models. S Gong, M Li, J Feng, Z Wu, L Kong, International Conference on Learning Representations (ICLR. 2023S. Gong, M. Li, J. Feng, Z. Wu, and L. Kong, \"Diffuseq: Sequence to se- quence text generation with diffusion models,\" International Conference on Learning Representations (ICLR), 2023.\n\nDiffWave: A versatile diffusion model for audio synthesis. Z Kong, W Ping, J Huang, K Zhao, B Catanzaro, International Conference on Learning Representations (ICLR). 2020Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, \"DiffWave: A versatile diffusion model for audio synthesis,\" International Conference on Learning Representations (ICLR), 2020.\n\nDiff-TTS: A Denoising Diffusion Model for Text-to-Speech. M Jeong, H Kim, S J Cheon, B J Choi, N S Kim, Proc. Interspeech 2021. Interspeech 2021M. Jeong, H. Kim, S. J. Cheon, B. J. Choi, and N. S. Kim, \"Diff-TTS: A Denoising Diffusion Model for Text-to-Speech,\" in Proc. Interspeech 2021, pp. 3605-3609, 2021.\n\nMake-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. R Huang, J Huang, D Yang, Y Ren, L Liu, M Li, Z Ye, J Liu, X Yin, Z Zhao, International Conference on Machine Learning (ICML). 2023R. Huang, J. Huang, D. Yang, Y. Ren, L. Liu, M. Li, Z. Ye, J. Liu, X. Yin, and Z. Zhao, \"Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models,\" International Conference on Ma- chine Learning (ICML), 2023.\n\nHighresolution image synthesis with latent diffusion models. R Rombach, A Blattmann, D Lorenz, P Esser, B Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionR. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \"High- resolution image synthesis with latent diffusion models,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni- tion, pp. 10684-10695, 2022.\n\nDeep unsupervised learning using nonequilibrium thermodynamics. J Sohl-Dickstein, E Weiss, N Maheswaranathan, S Ganguli, PMLRInternational Conference on Machine Learning. J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, \"Deep unsupervised learning using nonequilibrium thermodynamics,\" in International Conference on Machine Learning, pp. 2256-2265, PMLR, 2015.\n\nArgmax flows and multinomial diffusion: Learning categorical distributions. E Hoogeboom, D Nielsen, P Jaini, P Forr\u00e9, M Welling, Advances in Neural Information Processing Systems. 34E. Hoogeboom, D. Nielsen, P. Jaini, P. Forr\u00e9, and M. Welling, \"Argmax flows and multinomial diffusion: Learning categorical distributions,\" Advances in Neural Information Processing Systems, vol. 34, pp. 12454- 12465, 2021.\n\nStructured denoising diffusion models in discrete state-spaces. J Austin, D Johnson, J Ho, D Tarlow, R Van Den, Berg, Advances in Neural Information Processing Systems. 34J. Austin, D. Johnson, J. Ho, D. Tarlow, and R. van den Berg, \"Structured denoising diffusion models in discrete state-spaces,\" Advances in Neural Information Processing Systems, vol. 34, 2021.\n\nRoberta: A robustly optimized bert pretraining approach. Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, International Conference on Machine Learning. Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, \"Roberta: A robustly optimized bert pretraining approach,\" International Conference on Machine Learning, 2019.\n\nSimcse: Simple contrastive learning of sentence embeddings. T Gao, X Yao, D Chen, 2021 Conference on Empirical Methods in Natural Language Processing. 20212021Association for Computational Linguistics (ACL)T. Gao, X. Yao, and D. Chen, \"Simcse: Simple contrastive learning of sentence embeddings,\" in 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, pp. 6894-6910, Associa- tion for Computational Linguistics (ACL), 2021.\n\nRepresentation learning with contrastive predictive coding. A V Oord, Y Li, O Vinyals, arXiv:1807.03748arXiv preprintA. v. d. Oord, Y. Li, and O. Vinyals, \"Representation learning with contrastive predictive coding,\" arXiv preprint arXiv:1807.03748, 2018.\n\n3cmlf: Three-stage curriculum-based mutual learning framework for audio-text retrieval. Y.-W Chao, D Yang, R Gu, Y Zou, 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). IEEE2022Y.-W. Chao, D. Yang, R. Gu, and Y. Zou, \"3cmlf: Three-stage curriculum-based mutual learning framework for audio-text retrieval,\" in 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), pp. 1602-1607, IEEE, 2022.\n\nLearning a similarity metric discriminatively, with application to face verification. S Chopra, R Hadsell, Y Lecun, 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). IEEE1S. Chopra, R. Hadsell, and Y. LeCun, \"Learning a similarity metric discriminatively, with application to face verification,\" in 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recog- nition (CVPR'05), vol. 1, pp. 539-546, IEEE, 2005.\n\nMutual information neural estimation. M I Belghazi, A Baratin, S Rajeshwar, S Ozair, Y Bengio, A Courville, D Hjelm, PMLRProceedings of the 35th International Conference on Machine Learning. J. Dy and A. Krausethe 35th International Conference on Machine Learning80of Proceedings of Machine Learning ResearchM. I. Belghazi, A. Baratin, S. Rajeshwar, S. Ozair, Y. Bengio, A. Courville, and D. Hjelm, \"Mutual information neural estimation,\" in Proceedings of the 35th International Conference on Machine Learning (J. Dy and A. Krause, eds.), vol. 80 of Proceedings of Machine Learning Research, pp. 531-540, PMLR, 10-15 Jul 2018.\n\nClub: A contrastive log-ratio upper bound of mutual information. P Cheng, W Hao, S Dai, J Liu, Z Gan, L Carin, International conference on machine learning. PMLRP. Cheng, W. Hao, S. Dai, J. Liu, Z. Gan, and L. Carin, \"Club: A con- trastive log-ratio upper bound of mutual information,\" in International conference on machine learning, pp. 1779-1788, PMLR, 2020.\n\nA survey on neural speech synthesis. X Tan, T Qin, F Soong, T.-Y Liu, arXiv:2106.15561arXiv preprintX. Tan, T. Qin, F. Soong, and T.-Y. Liu, \"A survey on neural speech synthesis,\" arXiv preprint arXiv:2106.15561, 2021.\n\nHifi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis. J Kong, J Kim, J Bae, Advances in Neural Information Processing Systems. 33J. Kong, J. Kim, and J. Bae, \"Hifi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis,\" Advances in Neural Information Processing Systems, vol. 33, pp. 17022-17033, 2020.\n\nVQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature. C Du, Y Guo, X Chen, K Yu, Proc. Interspeech 2022. Interspeech 2022C. Du, Y. Guo, X. Chen, and K. Yu, \"VQTTS: High-Fidelity Text-to- Speech Synthesis with Self-Supervised VQ Acoustic Feature,\" in Proc. Interspeech 2022, pp. 1596-1600, 2022.\n\nImage-to-image translation with conditional adversarial networks. P Isola, J.-Y Zhu, T Zhou, A A Efros, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionP. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, \"Image-to-image translation with conditional adversarial networks,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1125- 1134, 2017.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, Advances in Neural Information Processing Systems. 30A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \"Attention is all you need,\" Advances in Neural Information Processing Systems, vol. 30, 2017.\n\nVector quantized diffusion model for text-to-image synthesis. S Gu, D Chen, J Bao, F Wen, B Zhang, D Chen, L Yuan, B Guo, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionS. Gu, D. Chen, J. Bao, F. Wen, B. Zhang, D. Chen, L. Yuan, and B. Guo, \"Vector quantized diffusion model for text-to-image synthesis,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10696-10706, 2022.\n\nClassifier-free diffusion guidance. J Ho, T Salimans, NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications. J. Ho and T. Salimans, \"Classifier-free diffusion guidance,\" NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applica- tions, 2022.\n\nImproved vector quantized diffusion models. Z Tang, S Gu, J Bao, D Chen, F Wen, arXiv:2205.16007arXiv preprintZ. Tang, S. Gu, J. Bao, D. Chen, and F. Wen, \"Improved vector quantized diffusion models,\" arXiv preprint arXiv:2205.16007, 2022.\n\nAudiolm: a language modeling approach to audio generation. Z Borsos, R Marinier, D Vincent, E Kharitonov, O Pietquin, M Sharifi, O Teboul, D Grangier, M Tagliasacchi, N Zeghidour, arXiv:2209.03143arXiv preprintZ. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Shar- ifi, O. Teboul, D. Grangier, M. Tagliasacchi, and N. Zeghidour, \"Audi- olm: a language modeling approach to audio generation,\" arXiv preprint arXiv:2209.03143, 2022.\n\nAISHELL-3: A multi-speaker mandarin tts corpus and the baselines. Y Shi, H Bu, X Xu, S Zhang, M Li, arXiv:2010.11567arXiv preprintY. Shi, H. Bu, X. Xu, S. Zhang, and M. Li, \"AISHELL-3: A multi-speaker mandarin tts corpus and the baselines,\" arXiv preprint arXiv:2010.11567, 2020.\n\nLibrispeech: an asr corpus based on public domain audio books. V Panayotov, G Chen, D Povey, S Khudanpur, 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEEV. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \"Librispeech: an asr corpus based on public domain audio books,\" in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 5206-5210, IEEE, 2015.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" arXiv preprint arXiv:1412.6980, 2014.\n\nA vector quantized approach for text to speech synthesis on real-world spontaneous speech. L.-W Chen, S Watanabe, A Rudnicky, Association for the Advancement of Artificial Intelligence (AAAI). L.-W. Chen, S. Watanabe, and A. Rudnicky, \"A vector quantized approach for text to speech synthesis on real-world spontaneous speech,\" Association for the Advancement of Artificial Intelligence (AAAI), pp. 1- 9, 2023.\n\nA vector quantized approach for text to speech synthesis on real-world spontaneous speech. L.-W Chen, S Watanabe, A Rudnicky, arXiv:2302.04215arXiv preprintL.-W. Chen, S. Watanabe, and A. Rudnicky, \"A vector quantized approach for text to speech synthesis on real-world spontaneous speech,\" arXiv preprint arXiv:2302.04215, 2023.\n\nDecoupled weight decay regularization. I Loshchilov, F Hutter, arXiv:1711.05101arXiv preprintI. Loshchilov and F. Hutter, \"Decoupled weight decay regularization,\" arXiv preprint arXiv:1711.05101, 2017.\n\nMel-cepstral distance measure for objective speech quality assessment. R Kubichek, Proceedings of IEEE pacific rim conference on communications computers and signal processing. IEEE pacific rim conference on communications computers and signal processingIEEE1R. Kubichek, \"Mel-cepstral distance measure for objective speech qual- ity assessment,\" in Proceedings of IEEE pacific rim conference on communications computers and signal processing, vol. 1, pp. 125-128, IEEE, 1993.\n\nImage quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE transactions on image processing. 134Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, \"Image quality assessment: from error visibility to structural similarity,\" IEEE transactions on image processing, vol. 13, no. 4, pp. 600-612, 2004.\n\nA shorttime objective intelligibility measure for time-frequency weighted noisy speech. C H Taal, R C Hendriks, R Heusdens, J Jensen, 2010 IEEE international conference on acoustics, speech and signal processing. IEEEC. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen, \"A short- time objective intelligibility measure for time-frequency weighted noisy speech,\" in 2010 IEEE international conference on acoustics, speech and signal processing, pp. 4214-4217, IEEE, 2010.\n\nA method for fundamental frequency estimation and voicing decision: Application to infant utterances recorded in real acoustical environments. T Nakatani, S Amano, T Irino, K Ishizuka, T Kondo, Speech Communication. 503T. Nakatani, S. Amano, T. Irino, K. Ishizuka, and T. Kondo, \"A method for fundamental frequency estimation and voicing decision: Application to infant utterances recorded in real acoustical environments,\" Speech Communication, vol. 50, no. 3, pp. 203-214, 2008.\n\nReducing f0 frame error of f0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend. W Chu, A Alwan, 2009 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEW. Chu and A. Alwan, \"Reducing f0 frame error of f0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend,\" in 2009 IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 3969-3972, IEEE, 2009.\n\nClass-level spectral features for emotion recognition. D Bitouk, R Verma, A Nenkova, Speech communication. 527-8D. Bitouk, R. Verma, and A. Nenkova, \"Class-level spectral features for emotion recognition,\" Speech communication, vol. 52, no. 7-8, pp. 613- 625, 2010.\n\nDiffGAN-TTS: High-Fidelity and Efficient Text-to-Speech with Denoising Diffusion GANs. S Liu, D Su, D Yu, ICML Workshop. S. Liu, D. Su, and D. Yu, \"DiffGAN-TTS: High-Fidelity and Efficient Text-to-Speech with Denoising Diffusion GANs,\" ICML Workshop, 2022.\n\nRecognition of emotion from vocal cues. W F Johnson, R N Emde, K R Scherer, M D Klinnert, Archives of General Psychiatry. 433W. F. Johnson, R. N. Emde, K. R. Scherer, and M. D. Klinnert, \"Recog- nition of emotion from vocal cues,\" Archives of General Psychiatry, vol. 43, no. 3, pp. 280-283, 1986.\n\nHandbook of emotion elicitation and assessment. M J Owren, J.-A Bachorowski, Measuring emotion-related vocal acousticsM. J. Owren and J.-A. Bachorowski, \"Measuring emotion-related vocal acoustics,\" Handbook of emotion elicitation and assessment, pp. 239- 266, 2007.\n\nwav2vec 2.0: A framework for self-supervised learning of speech representations. A Baevski, Y Zhou, A Mohamed, M Auli, Advances in Neural Information Processing Systems. 33A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \"wav2vec 2.0: A framework for self-supervised learning of speech representations,\" Advances in Neural Information Processing Systems, vol. 33, pp. 12449- 12460, 2020.\n", "annotations": {"author": "[{\"end\":141,\"start\":114},{\"end\":148,\"start\":142}]", "publisher": null, "author_last_name": "[{\"end\":140,\"start\":114},{\"end\":147,\"start\":142}]", "author_first_name": null, "author_affiliation": null, "title": "[{\"end\":98,\"start\":1},{\"end\":246,\"start\":149}]", "venue": null, "abstract": "[{\"end\":2091,\"start\":341}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2306,\"start\":2303},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2311,\"start\":2308},{\"end\":2890,\"start\":2882},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3375,\"start\":3372},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3380,\"start\":3377},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3445,\"start\":3442},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3450,\"start\":3447},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4369,\"start\":4366},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5733,\"start\":5729},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5750,\"start\":5746},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5755,\"start\":5752},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6056,\"start\":6052},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7476,\"start\":7472},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7482,\"start\":7478},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7833,\"start\":7829},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7839,\"start\":7835},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7866,\"start\":7862},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7872,\"start\":7868},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7885,\"start\":7881},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8237,\"start\":8233},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8243,\"start\":8239},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8394,\"start\":8390},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8428,\"start\":8424},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8729,\"start\":8726},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8735,\"start\":8731},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8741,\"start\":8737},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9357,\"start\":9354},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9362,\"start\":9359},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9368,\"start\":9364},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9374,\"start\":9370},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9380,\"start\":9376},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9412,\"start\":9409},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9511,\"start\":9508},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9611,\"start\":9607},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9753,\"start\":9749},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9882,\"start\":9878},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9901,\"start\":9897},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10462,\"start\":10458},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10763,\"start\":10759},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10781,\"start\":10777},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12240,\"start\":12236},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12246,\"start\":12242},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12268,\"start\":12264},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12274,\"start\":12270},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12300,\"start\":12296},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12306,\"start\":12302},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12312,\"start\":12308},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12547,\"start\":12543},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12658,\"start\":12654},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12664,\"start\":12660},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12926,\"start\":12922},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12944,\"start\":12940},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12960,\"start\":12956},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14043,\"start\":14039},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14049,\"start\":14045},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14055,\"start\":14051},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15356,\"start\":15352},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15389,\"start\":15385},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16423,\"start\":16420},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":16909,\"start\":16905},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":18319,\"start\":18315},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":18353,\"start\":18349},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19313,\"start\":19309},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19319,\"start\":19315},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":19364,\"start\":19360},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":19386,\"start\":19382},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":20742,\"start\":20738},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20759,\"start\":20755},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":20818,\"start\":20814},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21514,\"start\":21511},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21520,\"start\":21516},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":21526,\"start\":21522},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":22082,\"start\":22078},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":22202,\"start\":22198},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":22687,\"start\":22683},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22756,\"start\":22752},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24343,\"start\":24339},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":24373,\"start\":24369},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":25314,\"start\":25310},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25400,\"start\":25396},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":25406,\"start\":25402},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26031,\"start\":26027},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":27094,\"start\":27090},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":27100,\"start\":27096},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":27486,\"start\":27482},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28158,\"start\":28154},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28172,\"start\":28168},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28192,\"start\":28188},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":28468,\"start\":28464},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28692,\"start\":28688},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":33403,\"start\":33399},{\"end\":33898,\"start\":33896},{\"end\":33935,\"start\":33933},{\"end\":33956,\"start\":33954},{\"end\":33994,\"start\":33992},{\"end\":34021,\"start\":34019},{\"end\":34054,\"start\":34052},{\"end\":34086,\"start\":34084},{\"end\":34110,\"start\":34107},{\"end\":34155,\"start\":34152},{\"end\":34211,\"start\":34208},{\"end\":34246,\"start\":34243},{\"end\":34292,\"start\":34289},{\"end\":34314,\"start\":34311},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34743,\"start\":34740},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":34794,\"start\":34790},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":34841,\"start\":34837},{\"end\":35262,\"start\":35260},{\"end\":35297,\"start\":35295},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":36256,\"start\":36252},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":36262,\"start\":36258},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":36634,\"start\":36630},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36764,\"start\":36760},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":36782,\"start\":36778},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":36802,\"start\":36798},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36932,\"start\":36928},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":36938,\"start\":36934},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":36976,\"start\":36972},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":37027,\"start\":37023},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":37251,\"start\":37247},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":37377,\"start\":37373},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":37607,\"start\":37603},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":38066,\"start\":38063},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":38435,\"start\":38431},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":38932,\"start\":38928},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":39375,\"start\":39371},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":39875,\"start\":39871},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":39924,\"start\":39920},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":39977,\"start\":39973},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":40128,\"start\":40124},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":40158,\"start\":40154},{\"end\":40547,\"start\":40540},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":40747,\"start\":40743},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":40783,\"start\":40779},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":41209,\"start\":41205},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":41262,\"start\":41258},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":41365,\"start\":41361},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":41595,\"start\":41591},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":41601,\"start\":41597},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":41814,\"start\":41811},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":42904,\"start\":42901},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":44612,\"start\":44608},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":50838,\"start\":50834}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":53771,\"start\":53731},{\"attributes\":{\"id\":\"fig_1\"},\"end\":53844,\"start\":53772},{\"attributes\":{\"id\":\"fig_2\"},\"end\":53918,\"start\":53845},{\"attributes\":{\"id\":\"fig_3\"},\"end\":54003,\"start\":53919},{\"attributes\":{\"id\":\"fig_4\"},\"end\":54134,\"start\":54004},{\"attributes\":{\"id\":\"fig_5\"},\"end\":54269,\"start\":54135},{\"attributes\":{\"id\":\"fig_6\"},\"end\":54435,\"start\":54270},{\"attributes\":{\"id\":\"fig_7\"},\"end\":54668,\"start\":54436},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":54676,\"start\":54669},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":54898,\"start\":54677},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":54906,\"start\":54899},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":55630,\"start\":54907},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":56428,\"start\":55631},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":56680,\"start\":56429},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":57294,\"start\":56681},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":58328,\"start\":57295}]", "paragraph": "[{\"end\":4051,\"start\":2110},{\"end\":4539,\"start\":4053},{\"end\":4604,\"start\":4541},{\"end\":4778,\"start\":4606},{\"end\":4952,\"start\":4780},{\"end\":6372,\"start\":4954},{\"end\":6768,\"start\":6374},{\"end\":7085,\"start\":6804},{\"end\":7716,\"start\":7128},{\"end\":9262,\"start\":7743},{\"end\":10918,\"start\":9295},{\"end\":11099,\"start\":10929},{\"end\":11264,\"start\":11111},{\"end\":12108,\"start\":11266},{\"end\":13141,\"start\":12146},{\"end\":13525,\"start\":13143},{\"end\":13838,\"start\":13527},{\"end\":14056,\"start\":13973},{\"end\":14741,\"start\":14073},{\"end\":15895,\"start\":14743},{\"end\":16270,\"start\":15919},{\"end\":16786,\"start\":16293},{\"end\":17826,\"start\":16822},{\"end\":18007,\"start\":17828},{\"end\":18407,\"start\":18009},{\"end\":19545,\"start\":18409},{\"end\":21038,\"start\":19566},{\"end\":21475,\"start\":21097},{\"end\":23529,\"start\":21477},{\"end\":23877,\"start\":23531},{\"end\":24141,\"start\":23937},{\"end\":24396,\"start\":24194},{\"end\":24944,\"start\":24398},{\"end\":25594,\"start\":24946},{\"end\":26094,\"start\":25784},{\"end\":26348,\"start\":26155},{\"end\":26595,\"start\":26390},{\"end\":27247,\"start\":26699},{\"end\":27277,\"start\":27249},{\"end\":27408,\"start\":27279},{\"end\":27418,\"start\":27410},{\"end\":27920,\"start\":27420},{\"end\":28361,\"start\":28080},{\"end\":29150,\"start\":28363},{\"end\":30372,\"start\":29152},{\"end\":32324,\"start\":30374},{\"end\":32659,\"start\":32446},{\"end\":32788,\"start\":32701},{\"end\":32923,\"start\":32790},{\"end\":33600,\"start\":33045},{\"end\":34356,\"start\":33602},{\"end\":34896,\"start\":34382},{\"end\":34939,\"start\":34898},{\"end\":35345,\"start\":34952},{\"end\":35490,\"start\":35347},{\"end\":35865,\"start\":35492},{\"end\":36147,\"start\":35895},{\"end\":36549,\"start\":36149},{\"end\":36694,\"start\":36551},{\"end\":37873,\"start\":36696},{\"end\":39041,\"start\":37875},{\"end\":39669,\"start\":39066},{\"end\":40867,\"start\":39721},{\"end\":41142,\"start\":40918},{\"end\":41427,\"start\":41144},{\"end\":41815,\"start\":41429},{\"end\":41992,\"start\":41844},{\"end\":42471,\"start\":41994},{\"end\":43777,\"start\":42473},{\"end\":44697,\"start\":43808},{\"end\":45008,\"start\":44727},{\"end\":46347,\"start\":45010},{\"end\":47482,\"start\":46349},{\"end\":48198,\"start\":47484},{\"end\":48899,\"start\":48238},{\"end\":49235,\"start\":48901},{\"end\":49798,\"start\":49237},{\"end\":51889,\"start\":49800},{\"end\":52505,\"start\":51916},{\"end\":53341,\"start\":52526},{\"end\":53525,\"start\":53343},{\"end\":53730,\"start\":53527}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13972,\"start\":13839},{\"attributes\":{\"id\":\"formula_1\"},\"end\":23936,\"start\":23878},{\"attributes\":{\"id\":\"formula_2\"},\"end\":24193,\"start\":24142},{\"attributes\":{\"id\":\"formula_3\"},\"end\":25783,\"start\":25595},{\"attributes\":{\"id\":\"formula_4\"},\"end\":26154,\"start\":26095},{\"attributes\":{\"id\":\"formula_5\"},\"end\":26389,\"start\":26349},{\"attributes\":{\"id\":\"formula_6\"},\"end\":26698,\"start\":26596},{\"attributes\":{\"id\":\"formula_9\"},\"end\":27999,\"start\":27921},{\"attributes\":{\"id\":\"formula_10\"},\"end\":32445,\"start\":32325},{\"attributes\":{\"id\":\"formula_11\"},\"end\":33044,\"start\":32924},{\"attributes\":{\"id\":\"formula_12\"},\"end\":40917,\"start\":40868}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":10752,\"start\":10553},{\"end\":15214,\"start\":15207},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":40539,\"start\":40304},{\"end\":45111,\"start\":45103},{\"end\":46083,\"start\":46075},{\"end\":46686,\"start\":46678},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":47341,\"start\":47332},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":47772,\"start\":47764},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":48441,\"start\":48434},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":48761,\"start\":48753},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":49030,\"start\":49021},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":49569,\"start\":49560},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":49864,\"start\":49853},{\"end\":50848,\"start\":50840}]", "section_header": "[{\"end\":2108,\"start\":2093},{\"end\":6802,\"start\":6771},{\"end\":7126,\"start\":7088},{\"end\":7741,\"start\":7719},{\"end\":9293,\"start\":9265},{\"end\":10927,\"start\":10921},{\"end\":11109,\"start\":11102},{\"end\":12144,\"start\":12111},{\"end\":14071,\"start\":14059},{\"end\":15917,\"start\":15898},{\"end\":16291,\"start\":16273},{\"end\":16820,\"start\":16789},{\"end\":19564,\"start\":19548},{\"end\":21095,\"start\":21041},{\"end\":28078,\"start\":28001},{\"end\":32699,\"start\":32662},{\"end\":34380,\"start\":34359},{\"end\":34950,\"start\":34942},{\"end\":35893,\"start\":35868},{\"end\":39064,\"start\":39044},{\"end\":39693,\"start\":39672},{\"end\":39719,\"start\":39696},{\"end\":41842,\"start\":41818},{\"end\":43806,\"start\":43780},{\"end\":44725,\"start\":44700},{\"end\":48236,\"start\":48201},{\"end\":51914,\"start\":51892},{\"end\":52524,\"start\":52508},{\"end\":53781,\"start\":53773},{\"end\":53854,\"start\":53846},{\"end\":53928,\"start\":53920},{\"end\":54013,\"start\":54005},{\"end\":54279,\"start\":54271},{\"end\":54448,\"start\":54437},{\"end\":54675,\"start\":54670},{\"end\":54905,\"start\":54900},{\"end\":55645,\"start\":55632},{\"end\":56441,\"start\":56430},{\"end\":56694,\"start\":56682},{\"end\":57310,\"start\":57296}]", "table": "[{\"end\":54898,\"start\":54689},{\"end\":55630,\"start\":54995},{\"end\":56428,\"start\":55695},{\"end\":56680,\"start\":56600},{\"end\":57294,\"start\":56940},{\"end\":58328,\"start\":57976}]", "figure_caption": "[{\"end\":53771,\"start\":53733},{\"end\":53844,\"start\":53783},{\"end\":53918,\"start\":53856},{\"end\":54003,\"start\":53930},{\"end\":54134,\"start\":54015},{\"end\":54269,\"start\":54137},{\"end\":54435,\"start\":54281},{\"end\":54668,\"start\":54450},{\"end\":54689,\"start\":54679},{\"end\":54995,\"start\":54909},{\"end\":55695,\"start\":55649},{\"end\":56600,\"start\":56443},{\"end\":56940,\"start\":56697},{\"end\":57976,\"start\":57315}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6235,\"start\":6227},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16009,\"start\":16001},{\"end\":18856,\"start\":18846},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19610,\"start\":19604},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23166,\"start\":23158},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23557,\"start\":23549},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29170,\"start\":29155},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29946,\"start\":29938},{\"end\":50894,\"start\":50888},{\"end\":52432,\"start\":52424}]", "bib_author_first_name": "[{\"end\":58597,\"start\":58596},{\"end\":58605,\"start\":58604},{\"end\":58620,\"start\":58619},{\"end\":58631,\"start\":58630},{\"end\":58637,\"start\":58636},{\"end\":58639,\"start\":58638},{\"end\":58648,\"start\":58647},{\"end\":58658,\"start\":58657},{\"end\":58666,\"start\":58665},{\"end\":58674,\"start\":58673},{\"end\":58682,\"start\":58681},{\"end\":58975,\"start\":58974},{\"end\":58982,\"start\":58981},{\"end\":58988,\"start\":58987},{\"end\":58995,\"start\":58994},{\"end\":59002,\"start\":59001},{\"end\":59354,\"start\":59353},{\"end\":59361,\"start\":59360},{\"end\":59369,\"start\":59368},{\"end\":59749,\"start\":59748},{\"end\":59757,\"start\":59756},{\"end\":59765,\"start\":59764},{\"end\":59768,\"start\":59766},{\"end\":59778,\"start\":59777},{\"end\":59787,\"start\":59786},{\"end\":60111,\"start\":60110},{\"end\":60119,\"start\":60118},{\"end\":60122,\"start\":60120},{\"end\":60132,\"start\":60131},{\"end\":60513,\"start\":60512},{\"end\":60521,\"start\":60520},{\"end\":60532,\"start\":60531},{\"end\":60544,\"start\":60540},{\"end\":60552,\"start\":60551},{\"end\":60566,\"start\":60565},{\"end\":60574,\"start\":60573},{\"end\":60582,\"start\":60581},{\"end\":60589,\"start\":60588},{\"end\":60596,\"start\":60595},{\"end\":60598,\"start\":60597},{\"end\":61028,\"start\":61027},{\"end\":61043,\"start\":61042},{\"end\":61057,\"start\":61056},{\"end\":61065,\"start\":61064},{\"end\":61073,\"start\":61072},{\"end\":61084,\"start\":61083},{\"end\":61092,\"start\":61091},{\"end\":61101,\"start\":61100},{\"end\":61110,\"start\":61109},{\"end\":61112,\"start\":61111},{\"end\":61530,\"start\":61529},{\"end\":61537,\"start\":61536},{\"end\":61546,\"start\":61545},{\"end\":61555,\"start\":61554},{\"end\":61563,\"start\":61562},{\"end\":61571,\"start\":61570},{\"end\":61578,\"start\":61577},{\"end\":61588,\"start\":61587},{\"end\":61596,\"start\":61595},{\"end\":61606,\"start\":61605},{\"end\":62027,\"start\":62026},{\"end\":62035,\"start\":62034},{\"end\":62042,\"start\":62041},{\"end\":62048,\"start\":62047},{\"end\":62056,\"start\":62055},{\"end\":62064,\"start\":62063},{\"end\":62382,\"start\":62381},{\"end\":62390,\"start\":62389},{\"end\":62398,\"start\":62397},{\"end\":62404,\"start\":62403},{\"end\":62413,\"start\":62412},{\"end\":62421,\"start\":62420},{\"end\":62428,\"start\":62427},{\"end\":62436,\"start\":62435},{\"end\":62443,\"start\":62442},{\"end\":62451,\"start\":62450},{\"end\":62736,\"start\":62730},{\"end\":62741,\"start\":62739},{\"end\":62942,\"start\":62941},{\"end\":62949,\"start\":62948},{\"end\":62956,\"start\":62955},{\"end\":62964,\"start\":62963},{\"end\":63285,\"start\":63284},{\"end\":63296,\"start\":63295},{\"end\":63298,\"start\":63297},{\"end\":63305,\"start\":63304},{\"end\":63316,\"start\":63315},{\"end\":63326,\"start\":63325},{\"end\":63333,\"start\":63332},{\"end\":63344,\"start\":63343},{\"end\":63354,\"start\":63353},{\"end\":63364,\"start\":63363},{\"end\":63375,\"start\":63374},{\"end\":63772,\"start\":63771},{\"end\":63774,\"start\":63773},{\"end\":63787,\"start\":63783},{\"end\":63798,\"start\":63797},{\"end\":63811,\"start\":63810},{\"end\":63820,\"start\":63819},{\"end\":64082,\"start\":64081},{\"end\":64093,\"start\":64092},{\"end\":64365,\"start\":64364},{\"end\":64375,\"start\":64374},{\"end\":64391,\"start\":64390},{\"end\":64679,\"start\":64678},{\"end\":64688,\"start\":64687},{\"end\":64699,\"start\":64698},{\"end\":65138,\"start\":65137},{\"end\":65149,\"start\":65148},{\"end\":65162,\"start\":65161},{\"end\":65518,\"start\":65514},{\"end\":65525,\"start\":65524},{\"end\":65537,\"start\":65533},{\"end\":65539,\"start\":65538},{\"end\":65547,\"start\":65546},{\"end\":65559,\"start\":65558},{\"end\":65576,\"start\":65575},{\"end\":65968,\"start\":65967},{\"end\":65980,\"start\":65979},{\"end\":65989,\"start\":65988},{\"end\":66001,\"start\":66000},{\"end\":66215,\"start\":66214},{\"end\":66228,\"start\":66227},{\"end\":66237,\"start\":66236},{\"end\":66246,\"start\":66245},{\"end\":66258,\"start\":66257},{\"end\":66595,\"start\":66594},{\"end\":66605,\"start\":66604},{\"end\":66845,\"start\":66844},{\"end\":66853,\"start\":66852},{\"end\":66859,\"start\":66858},{\"end\":66867,\"start\":66866},{\"end\":66875,\"start\":66874},{\"end\":66883,\"start\":66882},{\"end\":66890,\"start\":66889},{\"end\":67224,\"start\":67223},{\"end\":67232,\"start\":67231},{\"end\":67239,\"start\":67238},{\"end\":67248,\"start\":67247},{\"end\":67256,\"start\":67255},{\"end\":67264,\"start\":67263},{\"end\":67576,\"start\":67575},{\"end\":67585,\"start\":67584},{\"end\":67592,\"start\":67591},{\"end\":67600,\"start\":67599},{\"end\":67609,\"start\":67608},{\"end\":67616,\"start\":67615},{\"end\":67631,\"start\":67630},{\"end\":67972,\"start\":67971},{\"end\":67979,\"start\":67978},{\"end\":67986,\"start\":67985},{\"end\":67994,\"start\":67993},{\"end\":68322,\"start\":68321},{\"end\":68328,\"start\":68327},{\"end\":68336,\"start\":68335},{\"end\":68342,\"start\":68341},{\"end\":68348,\"start\":68347},{\"end\":68355,\"start\":68354},{\"end\":68625,\"start\":68624},{\"end\":68637,\"start\":68636},{\"end\":68647,\"start\":68646},{\"end\":68656,\"start\":68655},{\"end\":68658,\"start\":68657},{\"end\":68668,\"start\":68667},{\"end\":68670,\"start\":68669},{\"end\":68685,\"start\":68684},{\"end\":68687,\"start\":68686},{\"end\":68697,\"start\":68696},{\"end\":68701,\"start\":68698},{\"end\":68711,\"start\":68710},{\"end\":68713,\"start\":68712},{\"end\":68724,\"start\":68723},{\"end\":68726,\"start\":68725},{\"end\":69046,\"start\":69045},{\"end\":69054,\"start\":69053},{\"end\":69064,\"start\":69063},{\"end\":69072,\"start\":69071},{\"end\":69074,\"start\":69073},{\"end\":69086,\"start\":69085},{\"end\":69365,\"start\":69364},{\"end\":69374,\"start\":69373},{\"end\":69381,\"start\":69380},{\"end\":69388,\"start\":69387},{\"end\":69395,\"start\":69394},{\"end\":69716,\"start\":69715},{\"end\":69723,\"start\":69722},{\"end\":69725,\"start\":69724},{\"end\":69734,\"start\":69733},{\"end\":69736,\"start\":69735},{\"end\":69744,\"start\":69743},{\"end\":69746,\"start\":69745},{\"end\":69753,\"start\":69752},{\"end\":69755,\"start\":69754},{\"end\":70019,\"start\":70018},{\"end\":70026,\"start\":70025},{\"end\":70034,\"start\":70033},{\"end\":70040,\"start\":70039},{\"end\":70048,\"start\":70047},{\"end\":70282,\"start\":70281},{\"end\":70294,\"start\":70293},{\"end\":70562,\"start\":70561},{\"end\":70568,\"start\":70567},{\"end\":70576,\"start\":70575},{\"end\":70583,\"start\":70582},{\"end\":70590,\"start\":70589},{\"end\":70599,\"start\":70598},{\"end\":70607,\"start\":70606},{\"end\":70615,\"start\":70614},{\"end\":71111,\"start\":71110},{\"end\":71113,\"start\":71112},{\"end\":71123,\"start\":71122},{\"end\":71135,\"start\":71134},{\"end\":71145,\"start\":71144},{\"end\":71154,\"start\":71153},{\"end\":71165,\"start\":71164},{\"end\":71175,\"start\":71174},{\"end\":71188,\"start\":71187},{\"end\":71621,\"start\":71620},{\"end\":71630,\"start\":71629},{\"end\":71641,\"start\":71640},{\"end\":71654,\"start\":71653},{\"end\":71985,\"start\":71984},{\"end\":71991,\"start\":71990},{\"end\":72004,\"start\":72003},{\"end\":72017,\"start\":72016},{\"end\":72019,\"start\":72018},{\"end\":72028,\"start\":72027},{\"end\":72030,\"start\":72029},{\"end\":72373,\"start\":72372},{\"end\":72381,\"start\":72380},{\"end\":72387,\"start\":72386},{\"end\":72395,\"start\":72394},{\"end\":72401,\"start\":72400},{\"end\":72719,\"start\":72718},{\"end\":72727,\"start\":72726},{\"end\":72735,\"start\":72734},{\"end\":72744,\"start\":72743},{\"end\":72752,\"start\":72751},{\"end\":73072,\"start\":73071},{\"end\":73081,\"start\":73080},{\"end\":73088,\"start\":73087},{\"end\":73090,\"start\":73089},{\"end\":73099,\"start\":73098},{\"end\":73101,\"start\":73100},{\"end\":73109,\"start\":73108},{\"end\":73111,\"start\":73110},{\"end\":73404,\"start\":73403},{\"end\":73413,\"start\":73412},{\"end\":73422,\"start\":73421},{\"end\":73430,\"start\":73429},{\"end\":73437,\"start\":73436},{\"end\":73444,\"start\":73443},{\"end\":73450,\"start\":73449},{\"end\":73456,\"start\":73455},{\"end\":73463,\"start\":73462},{\"end\":73470,\"start\":73469},{\"end\":73827,\"start\":73826},{\"end\":73838,\"start\":73837},{\"end\":73851,\"start\":73850},{\"end\":73861,\"start\":73860},{\"end\":73870,\"start\":73869},{\"end\":74330,\"start\":74329},{\"end\":74348,\"start\":74347},{\"end\":74357,\"start\":74356},{\"end\":74376,\"start\":74375},{\"end\":74721,\"start\":74720},{\"end\":74734,\"start\":74733},{\"end\":74745,\"start\":74744},{\"end\":74754,\"start\":74753},{\"end\":74763,\"start\":74762},{\"end\":75116,\"start\":75115},{\"end\":75126,\"start\":75125},{\"end\":75137,\"start\":75136},{\"end\":75143,\"start\":75142},{\"end\":75153,\"start\":75152},{\"end\":75475,\"start\":75474},{\"end\":75482,\"start\":75481},{\"end\":75489,\"start\":75488},{\"end\":75498,\"start\":75497},{\"end\":75504,\"start\":75503},{\"end\":75513,\"start\":75512},{\"end\":75521,\"start\":75520},{\"end\":75529,\"start\":75528},{\"end\":75538,\"start\":75537},{\"end\":75553,\"start\":75552},{\"end\":75887,\"start\":75886},{\"end\":75894,\"start\":75893},{\"end\":75901,\"start\":75900},{\"end\":76341,\"start\":76340},{\"end\":76343,\"start\":76342},{\"end\":76351,\"start\":76350},{\"end\":76357,\"start\":76356},{\"end\":76629,\"start\":76625},{\"end\":76637,\"start\":76636},{\"end\":76645,\"start\":76644},{\"end\":76651,\"start\":76650},{\"end\":77127,\"start\":77126},{\"end\":77137,\"start\":77136},{\"end\":77148,\"start\":77147},{\"end\":77548,\"start\":77547},{\"end\":77550,\"start\":77549},{\"end\":77562,\"start\":77561},{\"end\":77573,\"start\":77572},{\"end\":77586,\"start\":77585},{\"end\":77595,\"start\":77594},{\"end\":77605,\"start\":77604},{\"end\":77618,\"start\":77617},{\"end\":78204,\"start\":78203},{\"end\":78213,\"start\":78212},{\"end\":78220,\"start\":78219},{\"end\":78227,\"start\":78226},{\"end\":78234,\"start\":78233},{\"end\":78241,\"start\":78240},{\"end\":78539,\"start\":78538},{\"end\":78546,\"start\":78545},{\"end\":78553,\"start\":78552},{\"end\":78565,\"start\":78561},{\"end\":78814,\"start\":78813},{\"end\":78822,\"start\":78821},{\"end\":78829,\"start\":78828},{\"end\":79184,\"start\":79183},{\"end\":79190,\"start\":79189},{\"end\":79197,\"start\":79196},{\"end\":79205,\"start\":79204},{\"end\":79492,\"start\":79491},{\"end\":79504,\"start\":79500},{\"end\":79511,\"start\":79510},{\"end\":79519,\"start\":79518},{\"end\":79521,\"start\":79520},{\"end\":79918,\"start\":79917},{\"end\":79929,\"start\":79928},{\"end\":79940,\"start\":79939},{\"end\":79950,\"start\":79949},{\"end\":79963,\"start\":79962},{\"end\":79972,\"start\":79971},{\"end\":79974,\"start\":79973},{\"end\":79983,\"start\":79982},{\"end\":79993,\"start\":79992},{\"end\":80320,\"start\":80319},{\"end\":80326,\"start\":80325},{\"end\":80334,\"start\":80333},{\"end\":80341,\"start\":80340},{\"end\":80348,\"start\":80347},{\"end\":80357,\"start\":80356},{\"end\":80365,\"start\":80364},{\"end\":80373,\"start\":80372},{\"end\":80811,\"start\":80810},{\"end\":80817,\"start\":80816},{\"end\":81097,\"start\":81096},{\"end\":81105,\"start\":81104},{\"end\":81111,\"start\":81110},{\"end\":81118,\"start\":81117},{\"end\":81126,\"start\":81125},{\"end\":81353,\"start\":81352},{\"end\":81363,\"start\":81362},{\"end\":81375,\"start\":81374},{\"end\":81386,\"start\":81385},{\"end\":81400,\"start\":81399},{\"end\":81412,\"start\":81411},{\"end\":81423,\"start\":81422},{\"end\":81433,\"start\":81432},{\"end\":81445,\"start\":81444},{\"end\":81461,\"start\":81460},{\"end\":81810,\"start\":81809},{\"end\":81817,\"start\":81816},{\"end\":81823,\"start\":81822},{\"end\":81829,\"start\":81828},{\"end\":81838,\"start\":81837},{\"end\":82088,\"start\":82087},{\"end\":82101,\"start\":82100},{\"end\":82109,\"start\":82108},{\"end\":82118,\"start\":82117},{\"end\":82502,\"start\":82501},{\"end\":82504,\"start\":82503},{\"end\":82514,\"start\":82513},{\"end\":82752,\"start\":82748},{\"end\":82760,\"start\":82759},{\"end\":82772,\"start\":82771},{\"end\":83164,\"start\":83160},{\"end\":83172,\"start\":83171},{\"end\":83184,\"start\":83183},{\"end\":83440,\"start\":83439},{\"end\":83454,\"start\":83453},{\"end\":83675,\"start\":83674},{\"end\":84156,\"start\":84155},{\"end\":84164,\"start\":84163},{\"end\":84166,\"start\":84165},{\"end\":84175,\"start\":84174},{\"end\":84177,\"start\":84176},{\"end\":84187,\"start\":84186},{\"end\":84189,\"start\":84188},{\"end\":84542,\"start\":84541},{\"end\":84544,\"start\":84543},{\"end\":84552,\"start\":84551},{\"end\":84554,\"start\":84553},{\"end\":84566,\"start\":84565},{\"end\":84578,\"start\":84577},{\"end\":85072,\"start\":85071},{\"end\":85084,\"start\":85083},{\"end\":85093,\"start\":85092},{\"end\":85102,\"start\":85101},{\"end\":85114,\"start\":85113},{\"end\":85533,\"start\":85532},{\"end\":85540,\"start\":85539},{\"end\":85942,\"start\":85941},{\"end\":85952,\"start\":85951},{\"end\":85961,\"start\":85960},{\"end\":86241,\"start\":86240},{\"end\":86248,\"start\":86247},{\"end\":86254,\"start\":86253},{\"end\":86452,\"start\":86451},{\"end\":86454,\"start\":86453},{\"end\":86465,\"start\":86464},{\"end\":86467,\"start\":86466},{\"end\":86475,\"start\":86474},{\"end\":86477,\"start\":86476},{\"end\":86488,\"start\":86487},{\"end\":86490,\"start\":86489},{\"end\":86759,\"start\":86758},{\"end\":86761,\"start\":86760},{\"end\":86773,\"start\":86769},{\"end\":87059,\"start\":87058},{\"end\":87070,\"start\":87069},{\"end\":87078,\"start\":87077},{\"end\":87089,\"start\":87088}]", "bib_author_last_name": "[{\"end\":58602,\"start\":58598},{\"end\":58617,\"start\":58606},{\"end\":58628,\"start\":58621},{\"end\":58634,\"start\":58632},{\"end\":58645,\"start\":58640},{\"end\":58655,\"start\":58649},{\"end\":58663,\"start\":58659},{\"end\":58671,\"start\":58667},{\"end\":58679,\"start\":58675},{\"end\":58689,\"start\":58683},{\"end\":58979,\"start\":58976},{\"end\":58985,\"start\":58983},{\"end\":58992,\"start\":58989},{\"end\":58999,\"start\":58996},{\"end\":59007,\"start\":59003},{\"end\":59358,\"start\":59355},{\"end\":59366,\"start\":59362},{\"end\":59373,\"start\":59370},{\"end\":59754,\"start\":59750},{\"end\":59762,\"start\":59758},{\"end\":59775,\"start\":59769},{\"end\":59784,\"start\":59779},{\"end\":59794,\"start\":59788},{\"end\":60116,\"start\":60112},{\"end\":60129,\"start\":60123},{\"end\":60139,\"start\":60133},{\"end\":60518,\"start\":60514},{\"end\":60529,\"start\":60522},{\"end\":60538,\"start\":60533},{\"end\":60549,\"start\":60545},{\"end\":60563,\"start\":60553},{\"end\":60571,\"start\":60567},{\"end\":60579,\"start\":60575},{\"end\":60586,\"start\":60583},{\"end\":60593,\"start\":60590},{\"end\":60606,\"start\":60599},{\"end\":61040,\"start\":61029},{\"end\":61054,\"start\":61044},{\"end\":61062,\"start\":61058},{\"end\":61070,\"start\":61066},{\"end\":61081,\"start\":61074},{\"end\":61089,\"start\":61085},{\"end\":61098,\"start\":61093},{\"end\":61107,\"start\":61102},{\"end\":61120,\"start\":61113},{\"end\":61534,\"start\":61531},{\"end\":61543,\"start\":61538},{\"end\":61552,\"start\":61547},{\"end\":61560,\"start\":61556},{\"end\":61568,\"start\":61564},{\"end\":61575,\"start\":61572},{\"end\":61585,\"start\":61579},{\"end\":61593,\"start\":61589},{\"end\":61603,\"start\":61597},{\"end\":61609,\"start\":61607},{\"end\":62032,\"start\":62028},{\"end\":62039,\"start\":62036},{\"end\":62045,\"start\":62043},{\"end\":62053,\"start\":62049},{\"end\":62061,\"start\":62057},{\"end\":62068,\"start\":62065},{\"end\":62387,\"start\":62383},{\"end\":62395,\"start\":62391},{\"end\":62401,\"start\":62399},{\"end\":62410,\"start\":62405},{\"end\":62418,\"start\":62414},{\"end\":62425,\"start\":62422},{\"end\":62433,\"start\":62429},{\"end\":62440,\"start\":62437},{\"end\":62448,\"start\":62444},{\"end\":62454,\"start\":62452},{\"end\":62744,\"start\":62742},{\"end\":62946,\"start\":62943},{\"end\":62953,\"start\":62950},{\"end\":62961,\"start\":62957},{\"end\":62970,\"start\":62965},{\"end\":63293,\"start\":63286},{\"end\":63302,\"start\":63299},{\"end\":63313,\"start\":63306},{\"end\":63323,\"start\":63317},{\"end\":63330,\"start\":63327},{\"end\":63341,\"start\":63334},{\"end\":63351,\"start\":63345},{\"end\":63361,\"start\":63355},{\"end\":63372,\"start\":63365},{\"end\":63381,\"start\":63376},{\"end\":63781,\"start\":63775},{\"end\":63795,\"start\":63788},{\"end\":63808,\"start\":63799},{\"end\":63817,\"start\":63812},{\"end\":63828,\"start\":63821},{\"end\":64090,\"start\":64083},{\"end\":64098,\"start\":64094},{\"end\":64107,\"start\":64100},{\"end\":64372,\"start\":64366},{\"end\":64388,\"start\":64376},{\"end\":64399,\"start\":64392},{\"end\":64685,\"start\":64680},{\"end\":64696,\"start\":64689},{\"end\":64705,\"start\":64700},{\"end\":65146,\"start\":65139},{\"end\":65159,\"start\":65150},{\"end\":65167,\"start\":65163},{\"end\":65522,\"start\":65519},{\"end\":65531,\"start\":65526},{\"end\":65544,\"start\":65540},{\"end\":65556,\"start\":65548},{\"end\":65573,\"start\":65560},{\"end\":65584,\"start\":65577},{\"end\":65977,\"start\":65969},{\"end\":65986,\"start\":65981},{\"end\":65998,\"start\":65990},{\"end\":66005,\"start\":66002},{\"end\":66225,\"start\":66216},{\"end\":66234,\"start\":66229},{\"end\":66243,\"start\":66238},{\"end\":66255,\"start\":66247},{\"end\":66271,\"start\":66259},{\"end\":66602,\"start\":66596},{\"end\":66611,\"start\":66606},{\"end\":66850,\"start\":66846},{\"end\":66856,\"start\":66854},{\"end\":66864,\"start\":66860},{\"end\":66872,\"start\":66868},{\"end\":66880,\"start\":66876},{\"end\":66887,\"start\":66884},{\"end\":66893,\"start\":66891},{\"end\":67229,\"start\":67225},{\"end\":67236,\"start\":67233},{\"end\":67245,\"start\":67240},{\"end\":67253,\"start\":67249},{\"end\":67261,\"start\":67257},{\"end\":67268,\"start\":67265},{\"end\":67582,\"start\":67577},{\"end\":67589,\"start\":67586},{\"end\":67597,\"start\":67593},{\"end\":67606,\"start\":67601},{\"end\":67613,\"start\":67610},{\"end\":67628,\"start\":67617},{\"end\":67634,\"start\":67632},{\"end\":67976,\"start\":67973},{\"end\":67983,\"start\":67980},{\"end\":67991,\"start\":67987},{\"end\":67999,\"start\":67995},{\"end\":68325,\"start\":68323},{\"end\":68333,\"start\":68329},{\"end\":68339,\"start\":68337},{\"end\":68345,\"start\":68343},{\"end\":68352,\"start\":68349},{\"end\":68360,\"start\":68356},{\"end\":68634,\"start\":68626},{\"end\":68644,\"start\":68638},{\"end\":68653,\"start\":68648},{\"end\":68665,\"start\":68659},{\"end\":68682,\"start\":68671},{\"end\":68694,\"start\":68688},{\"end\":68708,\"start\":68702},{\"end\":68721,\"start\":68714},{\"end\":68732,\"start\":68727},{\"end\":69051,\"start\":69047},{\"end\":69061,\"start\":69055},{\"end\":69069,\"start\":69065},{\"end\":69083,\"start\":69075},{\"end\":69089,\"start\":69087},{\"end\":69371,\"start\":69366},{\"end\":69378,\"start\":69375},{\"end\":69385,\"start\":69382},{\"end\":69392,\"start\":69389},{\"end\":69400,\"start\":69396},{\"end\":69720,\"start\":69717},{\"end\":69731,\"start\":69726},{\"end\":69741,\"start\":69737},{\"end\":69750,\"start\":69747},{\"end\":69759,\"start\":69756},{\"end\":70023,\"start\":70020},{\"end\":70031,\"start\":70027},{\"end\":70037,\"start\":70035},{\"end\":70045,\"start\":70041},{\"end\":70052,\"start\":70049},{\"end\":70291,\"start\":70283},{\"end\":70301,\"start\":70295},{\"end\":70565,\"start\":70563},{\"end\":70573,\"start\":70569},{\"end\":70580,\"start\":70577},{\"end\":70587,\"start\":70584},{\"end\":70596,\"start\":70591},{\"end\":70604,\"start\":70600},{\"end\":70612,\"start\":70608},{\"end\":70619,\"start\":70616},{\"end\":71120,\"start\":71114},{\"end\":71132,\"start\":71124},{\"end\":71142,\"start\":71136},{\"end\":71151,\"start\":71146},{\"end\":71162,\"start\":71155},{\"end\":71172,\"start\":71166},{\"end\":71185,\"start\":71176},{\"end\":71193,\"start\":71189},{\"end\":71627,\"start\":71622},{\"end\":71638,\"start\":71631},{\"end\":71651,\"start\":71642},{\"end\":71660,\"start\":71655},{\"end\":71988,\"start\":71986},{\"end\":72001,\"start\":71992},{\"end\":72014,\"start\":72005},{\"end\":72025,\"start\":72020},{\"end\":72040,\"start\":72031},{\"end\":72378,\"start\":72374},{\"end\":72384,\"start\":72382},{\"end\":72392,\"start\":72388},{\"end\":72398,\"start\":72396},{\"end\":72406,\"start\":72402},{\"end\":72724,\"start\":72720},{\"end\":72732,\"start\":72728},{\"end\":72741,\"start\":72736},{\"end\":72749,\"start\":72745},{\"end\":72762,\"start\":72753},{\"end\":73078,\"start\":73073},{\"end\":73085,\"start\":73082},{\"end\":73096,\"start\":73091},{\"end\":73106,\"start\":73102},{\"end\":73115,\"start\":73112},{\"end\":73410,\"start\":73405},{\"end\":73419,\"start\":73414},{\"end\":73427,\"start\":73423},{\"end\":73434,\"start\":73431},{\"end\":73441,\"start\":73438},{\"end\":73447,\"start\":73445},{\"end\":73453,\"start\":73451},{\"end\":73460,\"start\":73457},{\"end\":73467,\"start\":73464},{\"end\":73475,\"start\":73471},{\"end\":73835,\"start\":73828},{\"end\":73848,\"start\":73839},{\"end\":73858,\"start\":73852},{\"end\":73867,\"start\":73862},{\"end\":73876,\"start\":73871},{\"end\":74345,\"start\":74331},{\"end\":74354,\"start\":74349},{\"end\":74373,\"start\":74358},{\"end\":74384,\"start\":74377},{\"end\":74731,\"start\":74722},{\"end\":74742,\"start\":74735},{\"end\":74751,\"start\":74746},{\"end\":74760,\"start\":74755},{\"end\":74771,\"start\":74764},{\"end\":75123,\"start\":75117},{\"end\":75134,\"start\":75127},{\"end\":75140,\"start\":75138},{\"end\":75150,\"start\":75144},{\"end\":75161,\"start\":75154},{\"end\":75167,\"start\":75163},{\"end\":75479,\"start\":75476},{\"end\":75486,\"start\":75483},{\"end\":75495,\"start\":75490},{\"end\":75501,\"start\":75499},{\"end\":75510,\"start\":75505},{\"end\":75518,\"start\":75514},{\"end\":75526,\"start\":75522},{\"end\":75535,\"start\":75530},{\"end\":75550,\"start\":75539},{\"end\":75562,\"start\":75554},{\"end\":75891,\"start\":75888},{\"end\":75898,\"start\":75895},{\"end\":75906,\"start\":75902},{\"end\":76348,\"start\":76344},{\"end\":76354,\"start\":76352},{\"end\":76365,\"start\":76358},{\"end\":76634,\"start\":76630},{\"end\":76642,\"start\":76638},{\"end\":76648,\"start\":76646},{\"end\":76655,\"start\":76652},{\"end\":77134,\"start\":77128},{\"end\":77145,\"start\":77138},{\"end\":77154,\"start\":77149},{\"end\":77559,\"start\":77551},{\"end\":77570,\"start\":77563},{\"end\":77583,\"start\":77574},{\"end\":77592,\"start\":77587},{\"end\":77602,\"start\":77596},{\"end\":77615,\"start\":77606},{\"end\":77624,\"start\":77619},{\"end\":78210,\"start\":78205},{\"end\":78217,\"start\":78214},{\"end\":78224,\"start\":78221},{\"end\":78231,\"start\":78228},{\"end\":78238,\"start\":78235},{\"end\":78247,\"start\":78242},{\"end\":78543,\"start\":78540},{\"end\":78550,\"start\":78547},{\"end\":78559,\"start\":78554},{\"end\":78569,\"start\":78566},{\"end\":78819,\"start\":78815},{\"end\":78826,\"start\":78823},{\"end\":78833,\"start\":78830},{\"end\":79187,\"start\":79185},{\"end\":79194,\"start\":79191},{\"end\":79202,\"start\":79198},{\"end\":79208,\"start\":79206},{\"end\":79498,\"start\":79493},{\"end\":79508,\"start\":79505},{\"end\":79516,\"start\":79512},{\"end\":79527,\"start\":79522},{\"end\":79926,\"start\":79919},{\"end\":79937,\"start\":79930},{\"end\":79947,\"start\":79941},{\"end\":79960,\"start\":79951},{\"end\":79969,\"start\":79964},{\"end\":79980,\"start\":79975},{\"end\":79990,\"start\":79984},{\"end\":80004,\"start\":79994},{\"end\":80323,\"start\":80321},{\"end\":80331,\"start\":80327},{\"end\":80338,\"start\":80335},{\"end\":80345,\"start\":80342},{\"end\":80354,\"start\":80349},{\"end\":80362,\"start\":80358},{\"end\":80370,\"start\":80366},{\"end\":80377,\"start\":80374},{\"end\":80814,\"start\":80812},{\"end\":80826,\"start\":80818},{\"end\":81102,\"start\":81098},{\"end\":81108,\"start\":81106},{\"end\":81115,\"start\":81112},{\"end\":81123,\"start\":81119},{\"end\":81130,\"start\":81127},{\"end\":81360,\"start\":81354},{\"end\":81372,\"start\":81364},{\"end\":81383,\"start\":81376},{\"end\":81397,\"start\":81387},{\"end\":81409,\"start\":81401},{\"end\":81420,\"start\":81413},{\"end\":81430,\"start\":81424},{\"end\":81442,\"start\":81434},{\"end\":81458,\"start\":81446},{\"end\":81471,\"start\":81462},{\"end\":81814,\"start\":81811},{\"end\":81820,\"start\":81818},{\"end\":81826,\"start\":81824},{\"end\":81835,\"start\":81830},{\"end\":81841,\"start\":81839},{\"end\":82098,\"start\":82089},{\"end\":82106,\"start\":82102},{\"end\":82115,\"start\":82110},{\"end\":82128,\"start\":82119},{\"end\":82511,\"start\":82505},{\"end\":82517,\"start\":82515},{\"end\":82757,\"start\":82753},{\"end\":82769,\"start\":82761},{\"end\":82781,\"start\":82773},{\"end\":83169,\"start\":83165},{\"end\":83181,\"start\":83173},{\"end\":83193,\"start\":83185},{\"end\":83451,\"start\":83441},{\"end\":83461,\"start\":83455},{\"end\":83684,\"start\":83676},{\"end\":84161,\"start\":84157},{\"end\":84172,\"start\":84167},{\"end\":84184,\"start\":84178},{\"end\":84200,\"start\":84190},{\"end\":84549,\"start\":84545},{\"end\":84563,\"start\":84555},{\"end\":84575,\"start\":84567},{\"end\":84585,\"start\":84579},{\"end\":85081,\"start\":85073},{\"end\":85090,\"start\":85085},{\"end\":85099,\"start\":85094},{\"end\":85111,\"start\":85103},{\"end\":85120,\"start\":85115},{\"end\":85537,\"start\":85534},{\"end\":85546,\"start\":85541},{\"end\":85949,\"start\":85943},{\"end\":85958,\"start\":85953},{\"end\":85969,\"start\":85962},{\"end\":86245,\"start\":86242},{\"end\":86251,\"start\":86249},{\"end\":86257,\"start\":86255},{\"end\":86462,\"start\":86455},{\"end\":86472,\"start\":86468},{\"end\":86485,\"start\":86478},{\"end\":86499,\"start\":86491},{\"end\":86767,\"start\":86762},{\"end\":86785,\"start\":86774},{\"end\":87067,\"start\":87060},{\"end\":87075,\"start\":87071},{\"end\":87086,\"start\":87079},{\"end\":87094,\"start\":87090}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4689304},\"end\":58909,\"start\":58550},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":219531522},\"end\":59258,\"start\":58911},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b2\",\"matched_paper_id\":235417304},\"end\":59628,\"start\":59260},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":85531374},\"end\":60048,\"start\":59630},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":58004671},\"end\":60414,\"start\":60050},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4349820},\"end\":60942,\"start\":60416},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4425995},\"end\":61441,\"start\":60944},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":48363067},\"end\":61923,\"start\":61443},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":253370556},\"end\":62307,\"start\":61925},{\"attributes\":{\"doi\":\"arXiv:2301.02111\",\"id\":\"b9\"},\"end\":62691,\"start\":62309},{\"attributes\":{\"doi\":\"arXiv:2301.11325\",\"id\":\"b10\"},\"end\":62872,\"start\":62693},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b11\",\"matched_paper_id\":235359041},\"end\":63211,\"start\":62874},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":231591445},\"end\":63703,\"start\":63213},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":245329572},\"end\":64038,\"start\":63705},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":20282961},\"end\":64307,\"start\":64040},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":173990382},\"end\":64620,\"start\":64309},{\"attributes\":{\"id\":\"b16\"},\"end\":65062,\"start\":64622},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":204512445},\"end\":65419,\"start\":65064},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":235421619},\"end\":65925,\"start\":65421},{\"attributes\":{\"doi\":\"arXiv:2210.13438\",\"id\":\"b19\"},\"end\":66165,\"start\":65927},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":236149944},\"end\":66551,\"start\":66167},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":239015904},\"end\":66776,\"start\":66553},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":250698823},\"end\":67143,\"start\":66778},{\"attributes\":{\"doi\":\"arXiv:2305.02765\",\"id\":\"b23\"},\"end\":67475,\"start\":67145},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":232380080},\"end\":67890,\"start\":67477},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":218862956},\"end\":68253,\"start\":67892},{\"attributes\":{\"doi\":\"arXiv:2104.03521\",\"id\":\"b26\"},\"end\":68551,\"start\":68255},{\"attributes\":{\"doi\":\"arXiv:2104.05557\",\"id\":\"b27\"},\"end\":69005,\"start\":68553},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":251492999},\"end\":69278,\"start\":69007},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":248811428},\"end\":69670,\"start\":69280},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":232478871},\"end\":69953,\"start\":69672},{\"attributes\":{\"doi\":\"arXiv:2211.12171\",\"id\":\"b31\"},\"end\":70232,\"start\":69955},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":234357997},\"end\":70497,\"start\":70234},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":244714856},\"end\":71014,\"start\":70499},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":245335086},\"end\":71522,\"start\":71016},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":237213665},\"end\":71930,\"start\":71524},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":249192356},\"end\":72300,\"start\":71932},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":252917661},\"end\":72657,\"start\":72302},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":221818900},\"end\":73011,\"start\":72659},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":233025015},\"end\":73322,\"start\":73013},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":256390046},\"end\":73763,\"start\":73324},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":245335280},\"end\":74263,\"start\":73765},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b42\",\"matched_paper_id\":14888175},\"end\":74642,\"start\":74265},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":235262511},\"end\":75049,\"start\":74644},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":235755106},\"end\":75415,\"start\":75051},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":198953378},\"end\":75824,\"start\":75417},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":233296292},\"end\":76278,\"start\":75826},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b47\"},\"end\":76535,\"start\":76280},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":254930101},\"end\":77038,\"start\":76537},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":5555257},\"end\":77507,\"start\":77040},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b50\",\"matched_paper_id\":44220142},\"end\":78136,\"start\":77509},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":219966650},\"end\":78499,\"start\":78138},{\"attributes\":{\"doi\":\"arXiv:2106.15561\",\"id\":\"b52\"},\"end\":78719,\"start\":78501},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":222291664},\"end\":79093,\"start\":78721},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":247939783},\"end\":79423,\"start\":79095},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":6200260},\"end\":79888,\"start\":79425},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":13756489},\"end\":80255,\"start\":79890},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":244714856},\"end\":80772,\"start\":80257},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":249145348},\"end\":81050,\"start\":80774},{\"attributes\":{\"doi\":\"arXiv:2205.16007\",\"id\":\"b59\"},\"end\":81291,\"start\":81052},{\"attributes\":{\"doi\":\"arXiv:2209.03143\",\"id\":\"b60\"},\"end\":81741,\"start\":81293},{\"attributes\":{\"doi\":\"arXiv:2010.11567\",\"id\":\"b61\"},\"end\":82022,\"start\":81743},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":2191379},\"end\":82455,\"start\":82024},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b63\"},\"end\":82655,\"start\":82457},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":256662411},\"end\":83067,\"start\":82657},{\"attributes\":{\"doi\":\"arXiv:2302.04215\",\"id\":\"b65\"},\"end\":83398,\"start\":83069},{\"attributes\":{\"doi\":\"arXiv:1711.05101\",\"id\":\"b66\"},\"end\":83601,\"start\":83400},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":62230377},\"end\":84079,\"start\":83603},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":207761262},\"end\":84451,\"start\":84081},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":6467803},\"end\":84926,\"start\":84453},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":24315469},\"end\":85408,\"start\":84928},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":5637941},\"end\":85884,\"start\":85410},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":669868},\"end\":86151,\"start\":85886},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":246411236},\"end\":86409,\"start\":86153},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":29984919},\"end\":86708,\"start\":86411},{\"attributes\":{\"id\":\"b75\"},\"end\":86975,\"start\":86710},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":219966759},\"end\":87361,\"start\":86977}]", "bib_title": "[{\"end\":58594,\"start\":58550},{\"end\":58972,\"start\":58911},{\"end\":59351,\"start\":59260},{\"end\":59746,\"start\":59630},{\"end\":60108,\"start\":60050},{\"end\":60510,\"start\":60416},{\"end\":61025,\"start\":60944},{\"end\":61527,\"start\":61443},{\"end\":62024,\"start\":61925},{\"end\":62939,\"start\":62874},{\"end\":63282,\"start\":63213},{\"end\":63769,\"start\":63705},{\"end\":64079,\"start\":64040},{\"end\":64362,\"start\":64309},{\"end\":64676,\"start\":64622},{\"end\":65135,\"start\":65064},{\"end\":65512,\"start\":65421},{\"end\":66212,\"start\":66167},{\"end\":66592,\"start\":66553},{\"end\":66842,\"start\":66778},{\"end\":67573,\"start\":67477},{\"end\":67969,\"start\":67892},{\"end\":69043,\"start\":69007},{\"end\":69362,\"start\":69280},{\"end\":69713,\"start\":69672},{\"end\":70279,\"start\":70234},{\"end\":70559,\"start\":70499},{\"end\":71108,\"start\":71016},{\"end\":71618,\"start\":71524},{\"end\":71982,\"start\":71932},{\"end\":72370,\"start\":72302},{\"end\":72716,\"start\":72659},{\"end\":73069,\"start\":73013},{\"end\":73401,\"start\":73324},{\"end\":73824,\"start\":73765},{\"end\":74327,\"start\":74265},{\"end\":74718,\"start\":74644},{\"end\":75113,\"start\":75051},{\"end\":75472,\"start\":75417},{\"end\":75884,\"start\":75826},{\"end\":76623,\"start\":76537},{\"end\":77124,\"start\":77040},{\"end\":77545,\"start\":77509},{\"end\":78201,\"start\":78138},{\"end\":78811,\"start\":78721},{\"end\":79181,\"start\":79095},{\"end\":79489,\"start\":79425},{\"end\":79915,\"start\":79890},{\"end\":80317,\"start\":80257},{\"end\":80808,\"start\":80774},{\"end\":82085,\"start\":82024},{\"end\":82746,\"start\":82657},{\"end\":83672,\"start\":83603},{\"end\":84153,\"start\":84081},{\"end\":84539,\"start\":84453},{\"end\":85069,\"start\":84928},{\"end\":85530,\"start\":85410},{\"end\":85939,\"start\":85886},{\"end\":86238,\"start\":86153},{\"end\":86449,\"start\":86411},{\"end\":87056,\"start\":86977}]", "bib_author": "[{\"end\":58604,\"start\":58596},{\"end\":58619,\"start\":58604},{\"end\":58630,\"start\":58619},{\"end\":58636,\"start\":58630},{\"end\":58647,\"start\":58636},{\"end\":58657,\"start\":58647},{\"end\":58665,\"start\":58657},{\"end\":58673,\"start\":58665},{\"end\":58681,\"start\":58673},{\"end\":58691,\"start\":58681},{\"end\":58981,\"start\":58974},{\"end\":58987,\"start\":58981},{\"end\":58994,\"start\":58987},{\"end\":59001,\"start\":58994},{\"end\":59009,\"start\":59001},{\"end\":59360,\"start\":59353},{\"end\":59368,\"start\":59360},{\"end\":59375,\"start\":59368},{\"end\":59756,\"start\":59748},{\"end\":59764,\"start\":59756},{\"end\":59777,\"start\":59764},{\"end\":59786,\"start\":59777},{\"end\":59796,\"start\":59786},{\"end\":60118,\"start\":60110},{\"end\":60131,\"start\":60118},{\"end\":60141,\"start\":60131},{\"end\":60520,\"start\":60512},{\"end\":60531,\"start\":60520},{\"end\":60540,\"start\":60531},{\"end\":60551,\"start\":60540},{\"end\":60565,\"start\":60551},{\"end\":60573,\"start\":60565},{\"end\":60581,\"start\":60573},{\"end\":60588,\"start\":60581},{\"end\":60595,\"start\":60588},{\"end\":60608,\"start\":60595},{\"end\":61042,\"start\":61027},{\"end\":61056,\"start\":61042},{\"end\":61064,\"start\":61056},{\"end\":61072,\"start\":61064},{\"end\":61083,\"start\":61072},{\"end\":61091,\"start\":61083},{\"end\":61100,\"start\":61091},{\"end\":61109,\"start\":61100},{\"end\":61122,\"start\":61109},{\"end\":61536,\"start\":61529},{\"end\":61545,\"start\":61536},{\"end\":61554,\"start\":61545},{\"end\":61562,\"start\":61554},{\"end\":61570,\"start\":61562},{\"end\":61577,\"start\":61570},{\"end\":61587,\"start\":61577},{\"end\":61595,\"start\":61587},{\"end\":61605,\"start\":61595},{\"end\":61611,\"start\":61605},{\"end\":62034,\"start\":62026},{\"end\":62041,\"start\":62034},{\"end\":62047,\"start\":62041},{\"end\":62055,\"start\":62047},{\"end\":62063,\"start\":62055},{\"end\":62070,\"start\":62063},{\"end\":62389,\"start\":62381},{\"end\":62397,\"start\":62389},{\"end\":62403,\"start\":62397},{\"end\":62412,\"start\":62403},{\"end\":62420,\"start\":62412},{\"end\":62427,\"start\":62420},{\"end\":62435,\"start\":62427},{\"end\":62442,\"start\":62435},{\"end\":62450,\"start\":62442},{\"end\":62456,\"start\":62450},{\"end\":62739,\"start\":62730},{\"end\":62746,\"start\":62739},{\"end\":62948,\"start\":62941},{\"end\":62955,\"start\":62948},{\"end\":62963,\"start\":62955},{\"end\":62972,\"start\":62963},{\"end\":63295,\"start\":63284},{\"end\":63304,\"start\":63295},{\"end\":63315,\"start\":63304},{\"end\":63325,\"start\":63315},{\"end\":63332,\"start\":63325},{\"end\":63343,\"start\":63332},{\"end\":63353,\"start\":63343},{\"end\":63363,\"start\":63353},{\"end\":63374,\"start\":63363},{\"end\":63383,\"start\":63374},{\"end\":63783,\"start\":63771},{\"end\":63797,\"start\":63783},{\"end\":63810,\"start\":63797},{\"end\":63819,\"start\":63810},{\"end\":63830,\"start\":63819},{\"end\":64092,\"start\":64081},{\"end\":64100,\"start\":64092},{\"end\":64109,\"start\":64100},{\"end\":64374,\"start\":64364},{\"end\":64390,\"start\":64374},{\"end\":64401,\"start\":64390},{\"end\":64687,\"start\":64678},{\"end\":64698,\"start\":64687},{\"end\":64707,\"start\":64698},{\"end\":65148,\"start\":65137},{\"end\":65161,\"start\":65148},{\"end\":65169,\"start\":65161},{\"end\":65524,\"start\":65514},{\"end\":65533,\"start\":65524},{\"end\":65546,\"start\":65533},{\"end\":65558,\"start\":65546},{\"end\":65575,\"start\":65558},{\"end\":65586,\"start\":65575},{\"end\":65979,\"start\":65967},{\"end\":65988,\"start\":65979},{\"end\":66000,\"start\":65988},{\"end\":66007,\"start\":66000},{\"end\":66227,\"start\":66214},{\"end\":66236,\"start\":66227},{\"end\":66245,\"start\":66236},{\"end\":66257,\"start\":66245},{\"end\":66273,\"start\":66257},{\"end\":66604,\"start\":66594},{\"end\":66613,\"start\":66604},{\"end\":66852,\"start\":66844},{\"end\":66858,\"start\":66852},{\"end\":66866,\"start\":66858},{\"end\":66874,\"start\":66866},{\"end\":66882,\"start\":66874},{\"end\":66889,\"start\":66882},{\"end\":66895,\"start\":66889},{\"end\":67231,\"start\":67223},{\"end\":67238,\"start\":67231},{\"end\":67247,\"start\":67238},{\"end\":67255,\"start\":67247},{\"end\":67263,\"start\":67255},{\"end\":67270,\"start\":67263},{\"end\":67584,\"start\":67575},{\"end\":67591,\"start\":67584},{\"end\":67599,\"start\":67591},{\"end\":67608,\"start\":67599},{\"end\":67615,\"start\":67608},{\"end\":67630,\"start\":67615},{\"end\":67636,\"start\":67630},{\"end\":67978,\"start\":67971},{\"end\":67985,\"start\":67978},{\"end\":67993,\"start\":67985},{\"end\":68001,\"start\":67993},{\"end\":68327,\"start\":68321},{\"end\":68335,\"start\":68327},{\"end\":68341,\"start\":68335},{\"end\":68347,\"start\":68341},{\"end\":68354,\"start\":68347},{\"end\":68362,\"start\":68354},{\"end\":68636,\"start\":68624},{\"end\":68646,\"start\":68636},{\"end\":68655,\"start\":68646},{\"end\":68667,\"start\":68655},{\"end\":68684,\"start\":68667},{\"end\":68696,\"start\":68684},{\"end\":68710,\"start\":68696},{\"end\":68723,\"start\":68710},{\"end\":68734,\"start\":68723},{\"end\":69053,\"start\":69045},{\"end\":69063,\"start\":69053},{\"end\":69071,\"start\":69063},{\"end\":69085,\"start\":69071},{\"end\":69091,\"start\":69085},{\"end\":69373,\"start\":69364},{\"end\":69380,\"start\":69373},{\"end\":69387,\"start\":69380},{\"end\":69394,\"start\":69387},{\"end\":69402,\"start\":69394},{\"end\":69722,\"start\":69715},{\"end\":69733,\"start\":69722},{\"end\":69743,\"start\":69733},{\"end\":69752,\"start\":69743},{\"end\":69761,\"start\":69752},{\"end\":70025,\"start\":70018},{\"end\":70033,\"start\":70025},{\"end\":70039,\"start\":70033},{\"end\":70047,\"start\":70039},{\"end\":70054,\"start\":70047},{\"end\":70293,\"start\":70281},{\"end\":70303,\"start\":70293},{\"end\":70567,\"start\":70561},{\"end\":70575,\"start\":70567},{\"end\":70582,\"start\":70575},{\"end\":70589,\"start\":70582},{\"end\":70598,\"start\":70589},{\"end\":70606,\"start\":70598},{\"end\":70614,\"start\":70606},{\"end\":70621,\"start\":70614},{\"end\":71122,\"start\":71110},{\"end\":71134,\"start\":71122},{\"end\":71144,\"start\":71134},{\"end\":71153,\"start\":71144},{\"end\":71164,\"start\":71153},{\"end\":71174,\"start\":71164},{\"end\":71187,\"start\":71174},{\"end\":71195,\"start\":71187},{\"end\":71629,\"start\":71620},{\"end\":71640,\"start\":71629},{\"end\":71653,\"start\":71640},{\"end\":71662,\"start\":71653},{\"end\":71990,\"start\":71984},{\"end\":72003,\"start\":71990},{\"end\":72016,\"start\":72003},{\"end\":72027,\"start\":72016},{\"end\":72042,\"start\":72027},{\"end\":72380,\"start\":72372},{\"end\":72386,\"start\":72380},{\"end\":72394,\"start\":72386},{\"end\":72400,\"start\":72394},{\"end\":72408,\"start\":72400},{\"end\":72726,\"start\":72718},{\"end\":72734,\"start\":72726},{\"end\":72743,\"start\":72734},{\"end\":72751,\"start\":72743},{\"end\":72764,\"start\":72751},{\"end\":73080,\"start\":73071},{\"end\":73087,\"start\":73080},{\"end\":73098,\"start\":73087},{\"end\":73108,\"start\":73098},{\"end\":73117,\"start\":73108},{\"end\":73412,\"start\":73403},{\"end\":73421,\"start\":73412},{\"end\":73429,\"start\":73421},{\"end\":73436,\"start\":73429},{\"end\":73443,\"start\":73436},{\"end\":73449,\"start\":73443},{\"end\":73455,\"start\":73449},{\"end\":73462,\"start\":73455},{\"end\":73469,\"start\":73462},{\"end\":73477,\"start\":73469},{\"end\":73837,\"start\":73826},{\"end\":73850,\"start\":73837},{\"end\":73860,\"start\":73850},{\"end\":73869,\"start\":73860},{\"end\":73878,\"start\":73869},{\"end\":74347,\"start\":74329},{\"end\":74356,\"start\":74347},{\"end\":74375,\"start\":74356},{\"end\":74386,\"start\":74375},{\"end\":74733,\"start\":74720},{\"end\":74744,\"start\":74733},{\"end\":74753,\"start\":74744},{\"end\":74762,\"start\":74753},{\"end\":74773,\"start\":74762},{\"end\":75125,\"start\":75115},{\"end\":75136,\"start\":75125},{\"end\":75142,\"start\":75136},{\"end\":75152,\"start\":75142},{\"end\":75163,\"start\":75152},{\"end\":75169,\"start\":75163},{\"end\":75481,\"start\":75474},{\"end\":75488,\"start\":75481},{\"end\":75497,\"start\":75488},{\"end\":75503,\"start\":75497},{\"end\":75512,\"start\":75503},{\"end\":75520,\"start\":75512},{\"end\":75528,\"start\":75520},{\"end\":75537,\"start\":75528},{\"end\":75552,\"start\":75537},{\"end\":75564,\"start\":75552},{\"end\":75893,\"start\":75886},{\"end\":75900,\"start\":75893},{\"end\":75908,\"start\":75900},{\"end\":76350,\"start\":76340},{\"end\":76356,\"start\":76350},{\"end\":76367,\"start\":76356},{\"end\":76636,\"start\":76625},{\"end\":76644,\"start\":76636},{\"end\":76650,\"start\":76644},{\"end\":76657,\"start\":76650},{\"end\":77136,\"start\":77126},{\"end\":77147,\"start\":77136},{\"end\":77156,\"start\":77147},{\"end\":77561,\"start\":77547},{\"end\":77572,\"start\":77561},{\"end\":77585,\"start\":77572},{\"end\":77594,\"start\":77585},{\"end\":77604,\"start\":77594},{\"end\":77617,\"start\":77604},{\"end\":77626,\"start\":77617},{\"end\":78212,\"start\":78203},{\"end\":78219,\"start\":78212},{\"end\":78226,\"start\":78219},{\"end\":78233,\"start\":78226},{\"end\":78240,\"start\":78233},{\"end\":78249,\"start\":78240},{\"end\":78545,\"start\":78538},{\"end\":78552,\"start\":78545},{\"end\":78561,\"start\":78552},{\"end\":78571,\"start\":78561},{\"end\":78821,\"start\":78813},{\"end\":78828,\"start\":78821},{\"end\":78835,\"start\":78828},{\"end\":79189,\"start\":79183},{\"end\":79196,\"start\":79189},{\"end\":79204,\"start\":79196},{\"end\":79210,\"start\":79204},{\"end\":79500,\"start\":79491},{\"end\":79510,\"start\":79500},{\"end\":79518,\"start\":79510},{\"end\":79529,\"start\":79518},{\"end\":79928,\"start\":79917},{\"end\":79939,\"start\":79928},{\"end\":79949,\"start\":79939},{\"end\":79962,\"start\":79949},{\"end\":79971,\"start\":79962},{\"end\":79982,\"start\":79971},{\"end\":79992,\"start\":79982},{\"end\":80006,\"start\":79992},{\"end\":80325,\"start\":80319},{\"end\":80333,\"start\":80325},{\"end\":80340,\"start\":80333},{\"end\":80347,\"start\":80340},{\"end\":80356,\"start\":80347},{\"end\":80364,\"start\":80356},{\"end\":80372,\"start\":80364},{\"end\":80379,\"start\":80372},{\"end\":80816,\"start\":80810},{\"end\":80828,\"start\":80816},{\"end\":81104,\"start\":81096},{\"end\":81110,\"start\":81104},{\"end\":81117,\"start\":81110},{\"end\":81125,\"start\":81117},{\"end\":81132,\"start\":81125},{\"end\":81362,\"start\":81352},{\"end\":81374,\"start\":81362},{\"end\":81385,\"start\":81374},{\"end\":81399,\"start\":81385},{\"end\":81411,\"start\":81399},{\"end\":81422,\"start\":81411},{\"end\":81432,\"start\":81422},{\"end\":81444,\"start\":81432},{\"end\":81460,\"start\":81444},{\"end\":81473,\"start\":81460},{\"end\":81816,\"start\":81809},{\"end\":81822,\"start\":81816},{\"end\":81828,\"start\":81822},{\"end\":81837,\"start\":81828},{\"end\":81843,\"start\":81837},{\"end\":82100,\"start\":82087},{\"end\":82108,\"start\":82100},{\"end\":82117,\"start\":82108},{\"end\":82130,\"start\":82117},{\"end\":82513,\"start\":82501},{\"end\":82519,\"start\":82513},{\"end\":82759,\"start\":82748},{\"end\":82771,\"start\":82759},{\"end\":82783,\"start\":82771},{\"end\":83171,\"start\":83160},{\"end\":83183,\"start\":83171},{\"end\":83195,\"start\":83183},{\"end\":83453,\"start\":83439},{\"end\":83463,\"start\":83453},{\"end\":83686,\"start\":83674},{\"end\":84163,\"start\":84155},{\"end\":84174,\"start\":84163},{\"end\":84186,\"start\":84174},{\"end\":84202,\"start\":84186},{\"end\":84551,\"start\":84541},{\"end\":84565,\"start\":84551},{\"end\":84577,\"start\":84565},{\"end\":84587,\"start\":84577},{\"end\":85083,\"start\":85071},{\"end\":85092,\"start\":85083},{\"end\":85101,\"start\":85092},{\"end\":85113,\"start\":85101},{\"end\":85122,\"start\":85113},{\"end\":85539,\"start\":85532},{\"end\":85548,\"start\":85539},{\"end\":85951,\"start\":85941},{\"end\":85960,\"start\":85951},{\"end\":85971,\"start\":85960},{\"end\":86247,\"start\":86240},{\"end\":86253,\"start\":86247},{\"end\":86259,\"start\":86253},{\"end\":86464,\"start\":86451},{\"end\":86474,\"start\":86464},{\"end\":86487,\"start\":86474},{\"end\":86501,\"start\":86487},{\"end\":86769,\"start\":86758},{\"end\":86787,\"start\":86769},{\"end\":87069,\"start\":87058},{\"end\":87077,\"start\":87069},{\"end\":87088,\"start\":87077},{\"end\":87096,\"start\":87088}]", "bib_venue": "[{\"end\":58708,\"start\":58691},{\"end\":59068,\"start\":59009},{\"end\":59423,\"start\":59379},{\"end\":59813,\"start\":59796},{\"end\":60190,\"start\":60141},{\"end\":60652,\"start\":60608},{\"end\":61166,\"start\":61122},{\"end\":61660,\"start\":61611},{\"end\":62092,\"start\":62070},{\"end\":62379,\"start\":62309},{\"end\":62728,\"start\":62693},{\"end\":63020,\"start\":62976},{\"end\":63427,\"start\":63383},{\"end\":63861,\"start\":63830},{\"end\":64158,\"start\":64109},{\"end\":64450,\"start\":64401},{\"end\":64788,\"start\":64707},{\"end\":65228,\"start\":65169},{\"end\":65649,\"start\":65586},{\"end\":65965,\"start\":65927},{\"end\":66336,\"start\":66273},{\"end\":66653,\"start\":66613},{\"end\":66926,\"start\":66895},{\"end\":67221,\"start\":67145},{\"end\":67658,\"start\":67636},{\"end\":68050,\"start\":68001},{\"end\":68319,\"start\":68255},{\"end\":68622,\"start\":68553},{\"end\":69131,\"start\":69091},{\"end\":69451,\"start\":69402},{\"end\":69783,\"start\":69761},{\"end\":70016,\"start\":69955},{\"end\":70352,\"start\":70303},{\"end\":70702,\"start\":70621},{\"end\":71239,\"start\":71195},{\"end\":71711,\"start\":71662},{\"end\":72091,\"start\":72042},{\"end\":72466,\"start\":72408},{\"end\":72823,\"start\":72764},{\"end\":73139,\"start\":73117},{\"end\":73528,\"start\":73477},{\"end\":73959,\"start\":73878},{\"end\":74434,\"start\":74390},{\"end\":74822,\"start\":74773},{\"end\":75218,\"start\":75169},{\"end\":75608,\"start\":75564},{\"end\":75975,\"start\":75908},{\"end\":76338,\"start\":76280},{\"end\":76762,\"start\":76657},{\"end\":77246,\"start\":77156},{\"end\":77698,\"start\":77630},{\"end\":78293,\"start\":78249},{\"end\":78536,\"start\":78501},{\"end\":78884,\"start\":78835},{\"end\":79232,\"start\":79210},{\"end\":79606,\"start\":79529},{\"end\":80055,\"start\":80006},{\"end\":80460,\"start\":80379},{\"end\":80903,\"start\":80828},{\"end\":81094,\"start\":81052},{\"end\":81350,\"start\":81293},{\"end\":81807,\"start\":81743},{\"end\":82216,\"start\":82130},{\"end\":82499,\"start\":82457},{\"end\":82848,\"start\":82783},{\"end\":83158,\"start\":83069},{\"end\":83437,\"start\":83400},{\"end\":83778,\"start\":83686},{\"end\":84239,\"start\":84202},{\"end\":84664,\"start\":84587},{\"end\":85142,\"start\":85122},{\"end\":85625,\"start\":85548},{\"end\":85991,\"start\":85971},{\"end\":86272,\"start\":86259},{\"end\":86531,\"start\":86501},{\"end\":86756,\"start\":86710},{\"end\":87145,\"start\":87096},{\"end\":58721,\"start\":58710},{\"end\":59826,\"start\":59815},{\"end\":60226,\"start\":60192},{\"end\":62110,\"start\":62094},{\"end\":64856,\"start\":64790},{\"end\":67676,\"start\":67660},{\"end\":69801,\"start\":69785},{\"end\":70770,\"start\":70704},{\"end\":73157,\"start\":73141},{\"end\":74027,\"start\":73961},{\"end\":77772,\"start\":77719},{\"end\":79250,\"start\":79234},{\"end\":79670,\"start\":79608},{\"end\":80528,\"start\":80462},{\"end\":83857,\"start\":83780}]"}}}, "year": 2023, "month": 12, "day": 17}