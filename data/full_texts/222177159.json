{"id": 222177159, "updated": "2023-11-07 19:37:56.345", "metadata": {"title": "Adversarial Patch Attacks on Monocular Depth Estimation Networks", "authors": "[{\"first\":\"Koichiro\",\"last\":\"Yamanaka\",\"middle\":[]},{\"first\":\"Ryutaroh\",\"last\":\"Matsumoto\",\"middle\":[]},{\"first\":\"Keita\",\"last\":\"Takahashi\",\"middle\":[]},{\"first\":\"Toshiaki\",\"last\":\"Fujii\",\"middle\":[]}]", "venue": "IEEE Access, vol.8, pp.179094-179104, October 2020", "journal": null, "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Thanks to the excellent learning capability of deep convolutional neural networks (CNN), monocular depth estimation using CNNs has achieved great success in recent years. However, depth estimation from a monocular image alone is essentially an ill-posed problem, and thus, it seems that this approach would have inherent vulnerabilities. To reveal this limitation, we propose a method of adversarial patch attack on monocular depth estimation. More specifically, we generate artificial patterns (adversarial patches) that can fool the target methods into estimating an incorrect depth for the regions where the patterns are placed. Our method can be implemented in the real world by physically placing the printed patterns in real scenes. We also analyze the behavior of monocular depth estimation under attacks by visualizing the activation levels of the intermediate layers and the regions potentially affected by the adversarial attack.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2010.03072", "mag": "3090358134", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/access/YamanakaM0F20", "doi": "10.1109/access.2020.3027372"}}, "content": {"source": {"pdf_hash": "54065d21ad9f7b8597f14a39debcee383114b092", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.03072v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09207958.pdf", "status": "GOLD"}}, "grobid": {"id": "ce47d9572ec58945aa317687971a252b501272e2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/54065d21ad9f7b8597f14a39debcee383114b092.txt", "contents": "\nAdversarial Patch Attacks on Monocular Depth Estimation Networks\n\n\nKoichiro Yamanaka koichiro.ymnk@fujii.nuee.nagoya-u.ac.jp \nGraduate School of Engineering\nNagoya University\n464-8603NagoyaJapan\n\nRyutaroh Matsumoto \nDepartment of Information and Communications Engineering\nTokyo Institute of Technology\n152-8550TokyoJapan\n\nDepartment of Mathematical Sciences\nAalborg University\n9220AalborgDenmark\n\nMember, IEEEKeita Takahashi \nGraduate School of Engineering\nNagoya University\n464-8603NagoyaJapan\n\nANDToshiaki Fujii \nGraduate School of Engineering\nNagoya University\n464-8603NagoyaJapan\n\nKoichiro Yamanaka \nAdversarial Patch Attacks on Monocular Depth Estimation Networks\n10.1109/ACCESS.2020.3027372Received September 10, 2020, accepted September 23, 2020, date of publication September 28, 2020, date of current version October 9, 2020.Corresponding author:\nThanks to the excellent learning capability of deep convolutional neural networks (CNN), monocular depth estimation using CNNs has achieved great success in recent years. However, depth estimation from a monocular image alone is essentially an ill-posed problem, and thus, it seems that this approach would have inherent vulnerabilities. To reveal this limitation, we propose a method of adversarial patch attack on monocular depth estimation. More specifically, we generate artificial patterns (adversarial patches) that can fool the target methods into estimating an incorrect depth for the regions where the patterns are placed. Our method can be implemented in the real world by physically placing the printed patterns in real scenes. We also analyze the behavior of monocular depth estimation under attacks by visualizing the activation levels of the intermediate layers and the regions potentially affected by the adversarial attack.INDEX TERMS Adversarial attack, monocular depth estimation, CNN.\n\nI. INTRODUCTION\n\nEstimating pixel-wise depth from 2-D images has become increasingly important with the recent development of autonomous driving, augmented realities (AR), and robotics. A large body of previous work has been devoted to depth estimation from stereo or more than two images [1]- [4]. At the same time, monocular depth estimation [5]- [8], in which depth is estimated from a single image, 1 has attracted attention due to its less demanding hardware requirements. Monocular depth estimation has been greatly enhanced by the excellent learning capability of deep convolutional neural networks (CNN). As a result, current state-of-the-art results with monocular depth estimation are quite impressive, and seemingly comparable to those with stereo methods (see Fig. 1, where (a) is the input image and (b) is the depth estimated by Guo et al. [7]). However, monocular depth estimation is essentially an ill-posed problem because a monocular image alone does not contain sufficient physical cues\n\nThe associate editor coordinating the review of this manuscript and approving it for publication was Syed Islam . 1 Generally, monocular depth estimation includes techniques that use a temporal sequence of images captured from a single camera [9]- [11]. However, in this article, we focus on methods that use only a single image from a single viewpoint for depth estimation.\n\nfor scene depth. Instead of using the physical cues, these methods seem to rely on implicit knowledge (e.g., the color, vertical position, or shadows) that are learned from the training dataset [12]. We argue that monocular depth estimation depends too much on non-depth features in the given image, which makes it quite vulnerable to attacks.\n\nTo reveal the limitation mentioned above, we propose a method of adversarial patch attack for CNN-based monocular depth estimation. Specifically, we generate artificial patterns (adversarial patches) that can fool the target methods into estimating an incorrect depth for the regions where the patterns are placed. Figure 1(c) shows an example of our adversarial patches superimposed on the input image. As shown in (d), Guo et al.'s method [7] failed to estimate correct depth in the region where the patch was located; closer depth values were obtained than the original result in (b), as was intended with our design for this pattern. In this case, the attack was conducted in a digital manner; we digitally manipulated the pixel values of the input image to superimpose the patch. Our method can also be implemented in the real world, and we have achieved similar effects by physically placing the printed patterns in a real scene.\n\nMoreover, to further analyze the behavior of monocular depth estimation under attacks, we visualize the activation levels of the intermediate layers ( Fig. 1(e)) and the regions that are potentially affected by adversarial attacks (Fig. 1(f)). These visualizations lead to a deeper understanding of the mechanism by which adversarial patches affect the target CNN. Our source code, learned patches and demo video are available at https://www.fujii.nuee.nagoyau.ac.jp/Research/MonoDepth.\n\n\nII. BACKGROUND\n\nA. ADVERSARIAL ATTACKS 1) DIGITAL ADVERSARIAL ATTACKS Biggio et al. [13] were the first to demonstrate that deep neural networks (DNN) could be deceived by vicious attacks on the input. After that, a number of different adversarial attacks were proposed for image classification tasks [14]- [19]. Their purpose was to find small perturbations to be added to the original image to cause mis-classification. Compared to classification tasks, fewer works have been conducted for regression tasks. Hendrik Metzen et al. [20] demonstrated that nearly imperceptible perturbations could also fool an image segmentation method into producing incorrect results. Following their work, Xie et al. [21] proposed a method that can deceive both segmentation and object detection models simultaneously, and Wei et al. [22] extended the target of attack from an image to a video. More recently, Zhang et al. [23] attacked monocular depth estimation.\n\nIt should be noted that these methods implicitly assume ''digital adversarial attack'', in which the attacks are implemented by digitally manipulating the pixel values on the input image. The perturbation pattern is usually designed to have small amplitude, and thus, the difference between the original image and the attacked image is imperceptible to the human eye. However, the perturbation patterns usually cover the entire image, which makes it unsuitable to implement them in the real world.\n\n\n2) PHYSICAL ADVERSARIAL ATTACKS\n\nA number of studies have also been conducted on ''physical adversarial attacks'' that can be implemented in the real world, e.g., by placing printed patterns in a target scene. These patterns are not necessarily designed to be imperceptible to the human eye depending on the applications [24]- [27].\n\nKurakin et al. [28] demonstrated that images with adversarial patterns for a classification task created in [15] would remain adversarial when they are printed and captured by cameras. Athalye et al. [29] extended this idea to 3D physical adversarial objects. Eykholt et al. [30] showed that stop signs can be misclassified if various stickers are placed on top of them. Their adversarial objects were designed to be indistinguishable to the human eye, similarly to the case with ''digital adversarial attacks''.\n\nIn a similar vein, Brown et al. [24] took a small designed patch (adversarial patch), which was clearly visible to the eye, and placed it in target scenes to induce errors in a classification task. Their patches were designed to be placed anywhere in an input image. Moreover, as their patches are independent of target scenes, they can be used for ''physical adversarial attacks'' without prior knowledge of lighting conditions, camera angles, or other objects in the target scene. This is not trivial, as a pattern located in the real world usually receives a series of transformations (e.g., geometric transform, digitization, and color gamut transform) before it is recorded in a digital image, which would invalidate the effect as an adversarial pattern [31]. Following [24], adversarial patches have been used in several tasks such as face recognition [25], object detection [26] and optical flow estimation [27]. Komkov and Petiushko [25] attacked face recognition by sticking an adversarial patch on a hat. Ranjan et al. [27] conducted an adversarial patch attack on optical flow that is, to our knowledge, the first work to apply adversarial patch attacks to a regression problem. Note that our method can also be located in the context of adversarial patch attack for a regression problem.\n\n\nB. MONOCULAR DEPTH ESTIMATION\n\nMonocular depth estimation refers to the process of predicting pixel-wise depth from a single image. As a seminal work, Eigen et al. [32] proposed a multi-scale CNN architecture that can produce pixel-wise depth estimation from a single image. Unlike other previous works in single image depth estimation [33]- [38], their network did not rely on hand crafted features. Since then, significant improvements have been made by using techniques such as incorporation of strong scene priors for surface normal estimation [36], conditional random fields [39], conversion from a regression problem to a classification problem [40], and a quantized ordinal regression problem [6]. Lee et al. [8] achieved stateof-the-art result by introducing novel local planar guidance layers located at multiple stages in the decoding phase. All of the methods mentioned above are trained in a supervised manner, where ground-truth depth taken by RGB-D cameras or 3D laser scanners are required to train the networks.  Recently, semi-supervised and unsupervised methods have also been proposed. As examples of the semi-supervised approach, Chen et al. [41] used relative depth information and Kuznietsov et al. [42] used sparse depth data obtained from LiDAR. As for the unsupervised approaches, they require only rectified stereo image pairs to train networks. Xie et al. [43] proposed Deep3D to create a new right view from an input left image using depth image-based rendering [44], where a disparity map was estimated as an intermediate product. Garg et al. [45] extended this using a network similar to FlowNet [46]. However, since their network was not fully differentiable, they performed a Taylor series approximation to linearize their loss, which made the optimization difficult. To address this problem, Godard et al. [5] introduced differentiable bilinear sampling [47] into the framework of Xie et al. [43]. Additionally, they considered left-right consistency of the predicted disparities estimated from the given pair of stereo images. Their network consisted of an encoder and a decoder, where the encoder's structure was taken from VGG-16 [48] and the decoder was composed of stacked deconvolution layers and shortcut connections from the encoder. More recently, Guo et al. [7] adopted the concept of knowledge distillation [49], and trained their monocular depth estimation network to produce the same disparity maps as the ones that were predicted by a pre-trained stereo depth estimation network.\n\nvan Dijk and de Croon [12] analyzed the behavior of monocular depth estimation and argued that depth prediction would actually depend on non-depth features such as the vertical position and the texture.\n\n\nC. OUR CONTRIBUTION\n\nWe propose a method of adversarial patch attack for CNN-based monocular depth estimation methods. Similarly to some previous works [24]- [27], the patches used for our attack are recognizable to human eye. Zhang et al. [23] also attacked monocular depth estimation using imperceptibly small perturbation patterns covering the entire image, but their method was not applicable to the real world. In contrast, our patch-based approach enables physical attacks using printed patterns located in the target scene. Our method is similar to Ranjan et al.'s [27], which attacks optical flow CNNs using printed patches that are physically located in real scenes. We would like to stress several differences between our work and Ranjan et al.'s [27]. First, our method is designed to cause depth errors only around the patches rather than the entire image frame. Moreover, our method considers perspective transform [30] in the image formation model to increase the robustness of the attack in the real world. Finally, our target is monocular depth estimation, which we feel has inherent vulnerabilities due to its over-dependence on non-depth cues. To the best of our knowledge, we are the first to attack monocular depth estimation in real scenes. We also visualize the behavior of monocular depth CNNs under attack, which will contribute to a deeper understanding of monocular depth estimation in concert with other approaches (e.g., van Dijk and de Croon's [12]).\n\n\nIII. PROPOSED METHOD\n\nA. OVERVIEW Figure 2 illustrates the overview of our method. Given a target monocular depth estimation method implemented as a CNN, our goal is to derive an adversarial patch (denoted as P) that induces the target method to produce incorrect depth estimates for the region on the input image where the patch is located.\n\nOur method is designed to be implemented in the real world; namely, we want to deceive the target method by physically placing printed patches in the target scene. To achieve this goal, we need to consider various shooting conditions in physical settings. When a patch is printed and placed in the target scene, it is subject to a series of transformations (luminance change, geometric transformations, noise, etc.) depending on the shooting condition before it is finally recorded in a digital image. Therefore, the imaging process of the patch is modeled so as to cover various shooting conditions. Moreover, we implement the imaging process as fully differentiable so that the gradient with respect to the estimated depth can be propagated back to the patch through the network. During the training stage, we keep the target method's network unchanged and update only the adversarial patch through the framework of back-propagation. We utilize the Adam optimizer [50] for this purpose. We could also utilize custom-made iterative updating rules such as FGSM [15], but we found that using the Adam optimizer led to better results.\n\n\nB. MODEL OF IMAGING PROCESS\n\nLet F be the target monocular depth estimation CNN and I the original input image. The estimated depth map is represented as D = F(I ). The adversarial patch we aim to derive is denoted as P. We assume that the patch P receives various transformations T \u03b8 through the imaging process and finally falls into the region R \u03b8 in the input image. The attacked image is represented as\u00ce = I + R \u03b8 T \u03b8 (P), where + R \u03b8 refers to the pixel overwriting on the region R \u03b8 .\n\nWe aim to train the patch P so that the depth values in the region R \u03b8 become a certain depth value d t , regardless of the actual depth. This is formalized as\nargmin P (i,j)\u2208R \u03b8 d t \u2212 F i,j (I + R \u03b8 T \u03b8 (P)) ,(1)\nwhere (i, j) denotes a pixel coordinate in the estimated depth map. By minimizing Eq. (1), we force the estimated depth in the region R \u03b8 to be a specific depth value d t . Depending on the value of d t , the estimated depth is guided to be different from the actual depth. The patch P should be robust to various transformations in the imaging process. Therefore, we randomly change the transformation T \u03b8 for each mini-batch during training. Specifically, T \u03b8 includes random brightness shifts \n(v x , v y ) \u2192 (v x + u, v y + v) (u, v \u2208 [\u22120.1, 0.1]),(2)\nwhere (v x , v y ) represents each vertex of the unit square and u, v represents horizontal and vertical shifts, respectively. Only a single patch is used to cover different spatial resolutions, since we include a larger range of scaling than [27] in the transform (in [27], several patches were learned for different resolutions). We used a sufficiently large resolution for the patch (256 \u00d7 256 pixels) to reduce the unexpected effects of pixel interpolation when printing it.\n\n\nC. LOSS FUNCTION\n\nOur loss function is defined for the target adversarial patch P and is composed of three terms, L = L depth (P) + \u03b1L NPS (P) + \u03b2L TV (P),\n\nwhere \u03b1 and \u03b2 are weighting coefficients that are determined experimentally.\n\n\n1) DEPTH LOSS\n\nThe most important term in our loss function, the depth loss, L depth , is given in accordance with Eq. (1).\nL depth (P) = (i,j)\u2208R \u03b8 d t \u2212 F i,j (I + R \u03b8 T \u03b8 (P)) .(4)\nWhen we attack more than one monocular depth estimation methods simultaneously, we replace Eq. (4) with the ensemble depth loss L ens depth , as\nL ens depth = k (i,j)\u2208R \u03b8 d t \u2212 F k i,j (I + R \u03b8 T \u03b8 (P)) ,(5)\nwhere F k denotes the k-th network.\n\n\n2) NON-PRINTABILITY SCORE (NPS)\n\nWe included NPS [51] in our loss function to limit the color space of the patch within the printable color gamut, as\nL NPS (P) = i,j min c\u2208C p i,j \u2212 c 1 ,(6)\nwhere . 1 denotes the L 1 norm and p i,j denotes the color vector of a pixel (i, j) in the patch P. We seek the closest color vector c from the set of printable colors C. A smaller L NPS means better printability.\n\n\n3) TOTAL VARIATION (TV)\n\nNon-smooth patches are more likely to be affected by aliasing artifacts when they are printed and captured by a camera. Therefore, we encourage the smoothness of the patch P by using total variation (TV) loss, similarly to [52].\nL TV (P) = i,j (p i,j \u2212 p i+1,j ) 2 + (p i,j \u2212 p i,j+1 ) 2 . (7)\nD. IMPLEMENTATION\n\n\n1) TARGET METHODS\n\nAs the target of our proposed attack, we used two state-ofthe-art monocular depth estimation methods [7], [8].\n\nGuo et al.'s method [7] is trained in an unsupervised manner using knowledge distillation. Their network consists of an encoder and a decoder. The encoder part is implemented using VGG-16 [48] and the decoder part is composed of stacked deconvolution layers and skip connections. The output from the network is a disparity map, which is converted into the depth map using the relation: where the baseline and the focal length were provided in the KITTI dataset. Meanwhile, Lee et al.'s [8] takes a supervised training framework, where the estimated depth is directly supervised by the corresponding ground-truth. Their network consists of an encoder and a decoder as well, but the structures are more complex than Guo et al.'s [7]; DenseNet-161 [53] was adopted as the encoder, and atrous spatial pyramid pooling [54] and local planar guidance layers were used in the decoder. This method is one of the top performing monocular depth estimation methods in the KITTI benchmark [55].\ndisparity = (baseline * focal length)/depth,(8)\nThese two networks were pre-trained on the KITTI dataset [56] using a data-splitting rule proposed by Eigen et al. [32]. The Eigen split consisted of 22,600 stereo image pairs for training, 888 for validation, and 697 for testing.\n\n\n2) TRAINING DETAILS\n\nWe trained adversarial patches under several conditions. We tested two target depths (d t = 3 m and 150 m) and three target configurations (individual and simultaneous attacks to either and both of [7] and [8]).\n\nFor training of our adversarial patch, we used only the left view images from the Eigen's 22,600 training split [32]. The width and height of the input images were set to 512 and 256 pixels, respectively to fit the input size of the target networks. The input images were randomly augmented by horizontal flipping, zooming with a factor of [0. The resolution for the patch was set to 256\u00d7256 pixels, but its apparent size in the input image was changed by the patch transformer T \u03b8 .\n\nWe used a Linux-based PC equipped with a NVIDIA Geforce GTX 1080 Ti. The networks were implemented using Python version 3.6.9 and PyTorch [57] version 1.1.0. We used the Adam optimizer [50] with learning rate 10 \u22123 , and batch size was set to 8. The number of epochs was 40.\n\nThe resulting adversarial patches are shown in Fig. 4. \n\n\nIV. EXPERIMENTAL RESULTS\n\n\nA. DIGITAL ADVERSARIAL ATTACK\n\nWe implemented the digital attack by superimposing the patches shown in Fig. 4 onto input images. Figure 3 shows the input images (w/ and w/o the attack), estimated depth maps, and the difference between the original and attacked depth maps.\n\nAs shown in (a), our attack was quite effective for Guo et al.'s method [7]; in both cases, namely, where the patch was trained exclusively for Guo et al.'s method [7] and where it was trained for both the methods [7], [8], the network produced incorrect depths corresponding to d t in the regions where the patches were located.\n\nIn contrast, as shown in Fig. 3(b), the attack had a limited effect for Lee et al.'s method [8]; in particular, the patches trained for both methods were less effective. One possible VOLUME 8, 2020   [7], which would bring more robustness to attacks. However, we conclude that our attack was effective to some extent because significant depth errors were induced by the adversarial patches.\n\nIt should be noted that simultaneous attack was possible for both methods, which had different network structures and training strategies. See the results with P * n and P * f in Fig. 3. This would indicate the inherent vulnerability of monocular depth estimation, where few geometric cues are available from the input image itself.\n\nTo further demonstrate the effectiveness of our attack, we present additional results on Guo et al.'s method [7] in Fig. 5, which shows that our adversarial patches were effective in various conditions (location, scale, orientation, and background).\n\n\nB. PHYSICAL ADVERSARIAL ATTACK\n\nWe also conducted a physical adversarial attack by using the printed adversarial patches. Figure 6 shows several input images and estimated depth maps. We can see a similar tendency as the one with the digital adversarial attack: namely, the attack was less effective against Lee et al.'s method [8] than Guo et al.'s [7]. In particular, the effect of P 2 f on Lee et al.'s method [8] was almost non-existent. This is seemingly related to the property of Lee et al.'s method that the estimated depth is closely related to the vertical position in the image; P 2 f (d t = 150 m) was placed near the bottom of the image, where Lee et al.'s method is more likely to produce small depth values. In contrast, when P 2 f was placed near the top of the image, we obtained a result closer to our intention (see Fig.8). To conclude, both of the monocular depth estimation methods could be attacked in the real world, although the results were not always sufficient against Lee et al.'s method. In Fig. 7, we present more results of the physical adversarial attack against Guo et al.'s method. In (b), the patch P 1 f was located upright, but in (c) and (d), it was rotated f was similar to that of P * f , and in (f), P * f was effective on the same method. This indicates that similarity to the human eye does not always correspond to the similarities to the depth estimation methods.\n\nPlease refer to the supplementary video for more results.\n\n\nV. ANALYSIS THROUGH VISUALIZATION\n\nAs discussed in the previous section, the effect of our adversarial attack was spatially localized in the resulting depth map; incorrect depth estimates were induced only in the regions around the adversarial patches. To analyze this effect, we present two methods for visualizing the effects of adversarial patches. First, we visualize the potential regions on a depth map where the depth values are likely to be affected by an adversarial patch. Second, we visualize the network's activation incurred by the adversarial patch, which helps to analyze the mechanism where the adversarial patch induces incorrect depth estimates. We use digital attack (adversarial patches superimposed by manipulating the image digitally) for this analysis, similarly to the work of van Dijk and de Croon [12]. We adopted Guo et al.'s method [7] as the target of visualization.\n\n\nA. POTENTIALLY AFFECTED REGIONS\n\nGiven a region R \u03b8 for an adversarial patch to be placed in the input image I , we can predict the potential effect on the resulting depth value D(u, v) for a pixel (u, v) as\nH (u, v) = c (i,j)\u2208R \u03b8 \u2202D(u, v) \u2202I (i, j, c) ,(9)\nwhere D = F(I ). In the above equation, the partial derivative can be obtained by standard back-propagation. The map H (u, v) shows how much the estimated depth of each pixel (u, v) is potentially affected by the adversarial patch located at R \u03b8 . Note here that we only consider the location R \u03b8 , and do not specify the pattern for the adversarial patch. Therefore, with this visualization, we actually analyze the intrinsic sensitivity of the pre-trained target network F. Figure 9 shows an example of this visualization. The original input image and an attacked image are shown in (a), (b), and (c) from which Guo et al.'s method [7] predicted the depth maps shown in (d), (e), and (f), respectively. Shown in (g) is H (u, v), a prediction of the potentially affected region, where R \u03b8 was set to the region of the adversarial patch in (b) and (c). To verify this prediction, the difference between (d) and (e) is presented in (h) and the difference between (d) and (f) is presented in (i). As we can see, the predicted region in (g) was well aligned to the resulting depth differences in (h) and (i). \n\n\nB. NETWORK ACTIVATION\n\nFor classification tasks, a well known method for visualizing network activation is Grad-CAM [58]. This method can be applied only for classification tasks, where the output from the network is typically a global label and no spatial information is involved. However, in our case, the output from the network is a pixel-wise depth map that depends on the location of a pixel. Therefore, we extend the idea of Grad-CAM to our problem as follows.\n\nFirst, we focus on the encoder part of the target network F. We obtain a layer-wise activation map for a spatial position (u, v) in the output depth map D(u, v) as\nG u,v m (i, j) = k \u2202D(u, v) \u2202A k m (i, j) \u00b7 A k m (i, j) ,(10)\nwhere A k m (i, j) is the k-th feature map of the m-th convolution layer and (i, j) denotes the spatial location on the feature map. We then resize activation maps G u,v m (i, j) to the same size as the input image by bilinear interpolation and aggregate them over all the encoder's layers. We finally take the summation over the regions (u, v) \u2208 R \u03b8 to obtain the final activation map, as\nG(i, j) = (u,v)\u2208R \u03b8 m Resize(G u,v m (i, j)) .(11)\nThis activation map shows the extent to which spatial neighbors are involved for the erroneous estimates on (u, v) \u2208 R \u03b8 , when an adversarial patch covers the region R \u03b8 . We present an example in Fig. 10, where input images, depth maps, and activation maps (G(i, j)) are shown for three cases: (a) without attacks, (b) attacked by P 1 n , and (c) attacked\n\nby P 1 f . In all cases, R \u03b8 was set to the same region (the region of the adversarial patches). It should be noted that the activation heavily depended on the presence of the adversarial patches: in (a), the activation covered a wide area around R \u03b8 , but in (b) and (c), the activation seemingly concentrated on R \u03b8 . These results suggest that the adversarial patches attracted the attention of the target method to itself, which in turn led to incorrect depth estimation, as intended.\n\n\nVI. CONCLUSION\n\nWe have proposed a method of adversarial patch attack for CNN-based monocular depth estimation methods. The adversarial patches were trained to induce incorrect depth estimation around the region where they were located, in the presence of various transformations such as perspective transform, scaling, and translation. We demonstrated that our method can be implemented in the real world by placing a printed pattern in the target scene. We also analyzed the behavior of monocular depth methods under attack by visualizing the potentially affected regions and activation maps. To the best of our knowledge, we are the first to achieve physical adversarial attacks on depth estimation methods. Our future work will include extending our method to similar tasks such as optical flow estimation and stereo depth estimation. We hope our work will lead to a wider recognition of the vulnerability of monocular depth estimation methods and thus to the development of safer depth estimation techniques.\n\nFIGURE 1 .\n1Monocular depth estimation[7] and our adversarial patch attack.\n\nFIGURE 2 .\n2Overview of the proposed method.\n\n\n(in the [\u22120.05, 0.05] range), contrast shifts (in the [0.9, 1.1] range), addition of noise (in the [\u22120,1, 0.1] range), scaling (with factors of [0.1225, 0.2025]), and rotation (in [\u221220, 20] degrees). Finally, the patch P receives perspective transformation, in which four vertices of the unit square ((0, 0), (0, 1), (1, 0), (1, 1)) are randomly perturbed as\n\nFIGURE 3 .\n3Results of digital adversarial attacks.\n\n\nthe range of [0.8, 1.2], illumination change with a factor of [0.8, 1.2], and color jittering in the range of [0.95, 1.05].\n\nFIGURE 4 .\n4Adversarial patches trained with various conditions.\n\nFIGURE 5 .\n5Results of digital adversarial attacks by P 1 n and P 1 f on Guo et al.'s method[7].\n\nFIGURE 6 .\n6Results of physical adversarial attacks.\n\nFIGURE 7 .\n7Results of physical adversarial attacks on Guo et al.'s method [7] under various conditions. reason for this is the higher complexity of Lee et al.'s method [8] compared to Guo et al.'s\n\nFIGURE 8 .\n8Successful case of our attack on Lee et al's method[8].\n\nFIGURE 9 .\n9Results on visualizing potentially affected regions. Red lines indicate the region of R \u03b8 . by \u00b1 15 degrees. The attack was effective for all three cases, indicating the robustness of the adversarial patch derived by our method. In (e), we can see that the patch P 2 f (trained for Lee et al.'s method) had little effect on Guo et al.'s method, although the appearance of P 2\n\nFIGURE 10 .\n10Results on visualizing activation map of Guo's network[7]. Top row: input image. Middle row: estimated depth maps. Bottom row: visualization results. Red lines indicate the region of R \u03b8 .\nVOLUME 8, 2020   \n\nStructure-from-Motion revisited. J L Schonberger, J.-M Frahm, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)J. L. Schonberger and J.-M. Frahm, ''Structure-from-Motion revisited,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 4104-4113.\n\nPixelwise view selection for unstructured multi-view stereo. J L Sch\u00f6nberger, E Zheng, J.-M Frahm, M Pollefeys, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)J. L. Sch\u00f6nberger, E. Zheng, J.-M. Frahm, and M. Pollefeys, ''Pixelwise view selection for unstructured multi-view stereo,'' in Proc. Eur. Conf. Comput. Vis. (ECCV), 2016, pp. 501-518.\n\nDepth estimation with occlusion modeling using light-field cameras. T.-C Wang, A A Efros, R Ramamoorthi, IEEE Trans. Pattern Anal. Mach. Intell. 3811T.-C. Wang, A. A. Efros, and R. Ramamoorthi, ''Depth estimation with occlusion modeling using light-field cameras,'' IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, no. 11, pp. 2170-2181, Nov. 2016.\n\nPyramid stereo matching network. J.-R Chang, Y.-S Chen, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitJ.-R. Chang and Y.-S. Chen, ''Pyramid stereo matching network,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 5410-5418.\n\nUnsupervised monocular depth estimation with left-right consistency. C Godard, O M Aodha, G J Brostow, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)C. Godard, O. M. Aodha, and G. J. Brostow, ''Unsupervised monocular depth estimation with left-right consistency,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 270-279.\n\nDeep ordinal regression network for monocular depth estimation. H Fu, M Gong, C Wang, K Batmanghelich, D Tao, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitH. Fu, M. Gong, C. Wang, K. Batmanghelich, and D. Tao, ''Deep ordinal regression network for monocular depth estimation,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 2002-2011.\n\nLearning monocular depth by distilling cross-domain stereo networks. X Guo, H Li, S Yi, J Ren, X Wang, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)X. Guo, H. Li, S. Yi, J. Ren, and X. Wang, ''Learning monocular depth by distilling cross-domain stereo networks,'' in Proc. Eur. Conf. Comput. Vis. (ECCV), 2018, pp. 484-500.\n\nFrom big to small: Multi-scale local planar guidance for monocular depth estimation. J Han Lee, M.-K Han, D Wook Ko, I Hong Suh, arXiv:1907.10326J. Han Lee, M.-K. Han, D. Wook Ko, and I. Hong Suh, ''From big to small: Multi-scale local planar guidance for monocular depth estimation,'' 2019, arXiv:1907.10326. [Online]. Available: http://arxiv.org/abs/1907.10326\n\nDense monocular depth estimation in complex dynamic scenes. R Ranftl, V Vineet, Q Chen, V Koltun, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)R. Ranftl, V. Vineet, Q. Chen, and V. Koltun, ''Dense monocular depth estimation in complex dynamic scenes,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 4058-4066.\n\nDepth from videos in the wild: Unsupervised monocular depth learning from unknown cameras. A Gordon, H Li, R Jonschkowski, A Angelova, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)A. Gordon, H. Li, R. Jonschkowski, and A. Angelova, ''Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras,'' in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 8977-8986.\n\nLearning the depths of moving people by watching frozen people. Z Li, T Dekel, F Cole, R Tucker, N Snavely, C Liu, W T Freeman, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)Z. Li, T. Dekel, F. Cole, R. Tucker, N. Snavely, C. Liu, and W. T. Freeman, ''Learning the depths of moving people by watching frozen people,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 4521-4530.\n\nHow do neural networks see depth in single images?. T Van Dijk, G De Croon, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)T. Van Dijk and G. De Croon, ''How do neural networks see depth in single images?'' in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 2183-2191.\n\nEvasion attacks against machine learning at test time. B Biggio, I Corona, D Maiorca, B Nelson, N \u0160rndi\u0107, P Laskov, G Giacinto, F Roli, Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases (ECML PKDD. Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases (ECML PKDDB. Biggio, I. Corona, D. Maiorca, B. Nelson, N. \u0160rndi\u0107, P. Laskov, G. Giacinto, and F. Roli, ''Evasion attacks against machine learning at test time,'' in Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases (ECML PKDD), 2013, pp. 387-402.\n\nIntriguing properties of neural networks. C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, R Fergus, Proc. Int. Conf. Learn. Represent. (ICLR). Int. Conf. Learn. Represent. (ICLR)C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, ''Intriguing properties of neural networks,'' in Proc. Int. Conf. Learn. Represent. (ICLR), 2014. [Online]. Available: https://arxiv.org/abs/1312.6199\n\nExplaining and harnessing adversarial examples. I J Goodfellow, J Shlens, C Szegedy, Int. Conf. Learn. Represent. (ICLR). I. J. Goodfellow, J. Shlens, and C. Szegedy, ''Explaining and harness- ing adversarial examples,'' in Int. Conf. Learn. Represent. (ICLR), 2015. [Online]. Available: https://arxiv.org/abs/1412.6572\n\nTowards evaluating the robustness of neural networks. N Carlini, D Wagner, Proc. IEEE Symp. Secur. Privacy (SP). IEEE Symp. Secur. Privacy (SP)N. Carlini and D. Wagner, ''Towards evaluating the robustness of neural networks,'' in Proc. IEEE Symp. Secur. Privacy (SP), May 2017, pp. 39-57.\n\nDeepFool: A simple and accurate method to fool deep neural networks. S.-M Moosavi-Dezfooli, A Fawzi, P Frossard, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, ''DeepFool: A simple and accurate method to fool deep neural networks,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 2574-2582.\n\nThe limitations of deep learning in adversarial settings. N Papernot, P Mcdaniel, S Jha, M Fredrikson, Z B Celik, A Swami, Proc. IEEE Eur. Symp. Secur. Privacy (EuroS&P). IEEE Eur. Symp. Secur. Privacy (EuroS&P)N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami, ''The limitations of deep learning in adversarial settings,'' in Proc. IEEE Eur. Symp. Secur. Privacy (EuroS&P), Mar. 2016, pp. 372-387.\n\nRobust synthesis of adversarial visual examples using a deep image prior. T Gittings, S Schneider, J Collomosse, Proc. Brit. Mach. Vis. Conf. (BMVC). Brit. Mach. Vis. Conf. (BMVC)T. Gittings, S. Schneider, and J. Collomosse, ''Robust synthesis of adver- sarial visual examples using a deep image prior,'' in Proc. Brit. Mach. Vis. Conf. (BMVC), 2019. [Online]. Available: https://bmvc2019.org/wp- content/uploads/papers/0165-paper.pdf\n\nUniversal adversarial perturbations against semantic image segmentation. J H Metzen, M C Kumar, T Brox, V Fischer, Proc. IEEE Int. Conf. Comput. Vis. (ICCV). IEEE Int. Conf. Comput. Vis. (ICCV)J. H. Metzen, M. C. Kumar, T. Brox, and V. Fischer, ''Universal adversarial perturbations against semantic image segmentation,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 2755-2764.\n\nAdversarial examples for semantic segmentation and object detection. C Xie, J Wang, Z Zhang, Y Zhou, L Xie, A Yuille, Proc. IEEE Int. Conf. Comput. Vis. (ICCV). IEEE Int. Conf. Comput. Vis. (ICCV)C. Xie, J. Wang, Z. Zhang, Y. Zhou, L. Xie, and A. Yuille, ''Adversarial examples for semantic segmentation and object detection,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 1369-1378.\n\nTransferable adversarial attacks for image and video object detection. X Wei, S Liang, N Chen, X Cao, Proc. 28th Int. Joint Conf. 28th Int. Joint ConfX. Wei, S. Liang, N. Chen, and X. Cao, ''Transferable adversarial attacks for image and video object detection,'' in Proc. 28th Int. Joint Conf. Artif. Intell., Aug. 2019, pp. 954-960.\n\nAdversarial attacks on monocular depth estimation. Z Zhang, X Zhu, Y Li, X Chen, Y Guo, arXiv:2003.103152020Z. Zhang, X. Zhu, Y. Li, X. Chen, and Y. Guo, ''Adversarial attacks on monocular depth estimation,'' 2020, arXiv:2003.10315. [Online]. Avail- able: http://arxiv.org/abs/2003.10315\n\nAdversarial patch. T B Brown, D Man\u00e9, A Roy, M Abadi, J Gilmer, arXiv:1712.09665T. B. Brown, D. Man\u00e9, A. Roy, M. Abadi, and J. Gilmer, ''Adversarial patch,'' 2017, arXiv:1712.09665. [Online]. Available: http://arxiv.org/abs/1712.09665\n\nAdvHat: Real-world adversarial attack on ArcFace face ID system. S Komkov, A Petiushko, arXiv:1908.08705[25] S. Komkov and A. Petiushko, ''AdvHat: Real-world adversarial attack on ArcFace face ID system,'' 2019, arXiv:1908.08705. [Online]. Available: http://arxiv.org/abs/1908.08705\n\nFooling automated surveillance cameras: Adversarial patches to attack person detection. S Thys, W V Ranst, T Goedeme, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)S. Thys, W. V. Ranst, and T. Goedeme, ''Fooling automated surveil- lance cameras: Adversarial patches to attack person detection,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Jun. 2019, pp. 1-7.\n\nAttacking optical flow. A Ranjan, J Janai, A Geiger, M Black, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)A. Ranjan, J. Janai, A. Geiger, and M. Black, ''Attacking optical flow,'' in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 2404-2413.\n\nAdversarial examples in the physical world. A Kurakin, I Goodfellow, S Bengio, Int. Conf. Learn. Represent. A. Kurakin, I. Goodfellow, and S. Bengio, ''Adversarial examples in the physical world,'' in Int. Conf. Learn. Represent. (ICLR), 2017. [Online]. Available: https://arxiv.org/abs/1607.02533\n\nSynthesizing robust adversarial examples. A Athalye, L Engstrom, A Ilyas, K Kwok, Proc. Int. Conf. Mach. Learn. (ICML). Int. Conf. Mach. Learn. (ICML)A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok, ''Synthesizing robust adversarial examples,'' in Proc. Int. Conf. Mach. Learn. (ICML), 2018, pp. 284-293.\n\nRobust physical-world attacks on deep learning visual classification. K Eykholt, I Evtimov, E Fernandes, B Li, A Rahmati, C Xiao, A Prakash, T Kohno, D Song, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitK. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, and D. Song, ''Robust physical-world attacks on deep learning visual classification,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 1625-1634.\n\nNO need to worry about adversarial examples in object detection in autonomous vehicles. J Lu, H Sibai, E Fabry, D Forsyth, arXiv:1707.03501J. Lu, H. Sibai, E. Fabry, and D. Forsyth, ''NO need to worry about adversarial examples in object detection in autonomous vehicles,'' 2017, arXiv:1707.03501. [Online]. Available: http://arxiv.org/abs/1707.03501\n\nDepth map prediction from a single image using a multi-scale deep network,'' in Proc. D Eigen, C Puhrsch, R Fergus, Adv. Neural Inf. Process. Syst. D. Eigen, C. Puhrsch, and R. Fergus, ''Depth map prediction from a single image using a multi-scale deep network,'' in Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2014, pp. 2366-2374.\n\nLearning depth from single monocular images,'' in Proc. A Saxena, S H Chung, A Y Ng, Adv. Neural Inf. Process. Syst. A. Saxena, S. H. Chung, and A. Y. Ng, ''Learning depth from single monocular images,'' in Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2006, pp. 1161-1168.\n\nMake3D: Learning 3D scene structure from a single still image. A Saxena, M Sun, A Y Ng, IEEE Trans. Pattern Anal. Mach. Intell. 315A. Saxena, M. Sun, and A. Y. Ng, ''Make3D: Learning 3D scene structure from a single still image,'' IEEE Trans. Pattern Anal. Mach. Intell., vol. 31, no. 5, pp. 824-840, May 2009.\n\nLearning depth from single monocular images using deep convolutional neural fields. F Liu, C Shen, G Lin, I Reid, IEEE Trans. Pattern Anal. Mach. Intell. 3810F. Liu, C. Shen, G. Lin, and I. Reid, ''Learning depth from single monoc- ular images using deep convolutional neural fields,'' IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, no. 10, pp. 2024-2039, Oct. 2016.\n\nDesigning deep networks for surface normal estimation. X Wang, D F Fouhey, A Gupta, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)X. Wang, D. F. Fouhey, and A. Gupta, ''Designing deep networks for surface normal estimation,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 539-547.\n\nPulling things out of perspective. L Ladicky, J Shi, M Pollefeys, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitL. Ladicky, J. Shi, and M. Pollefeys, ''Pulling things out of perspective,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2014, pp. 89-96.\n\nDepth transfer: Depth extraction from video using non-parametric sampling. K Karsch, C Liu, S B Kang, IEEE Trans. Pattern Anal. Mach. Intell. 3611K. Karsch, C. Liu, and S. B. Kang, ''Depth transfer: Depth extraction from video using non-parametric sampling,'' IEEE Trans. Pattern Anal. Mach. Intell., vol. 36, no. 11, pp. 2144-2158, Nov. 2014.\n\nDepth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs. B Li, C Shen, Y Dai, A Van Den Hengel, M He, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)B. Li, C. Shen, Y. Dai, A. van den Hengel, and M. He, ''Depth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 1119-1127.\n\nEstimating depth from monocular images as classification using deep fully convolutional residual networks. Y Cao, Z Wu, C Shen, IEEE Trans. Circuits Syst. Video Technol. 2811Y. Cao, Z. Wu, and C. Shen, ''Estimating depth from monocular images as classification using deep fully convolutional residual networks,'' IEEE Trans. Circuits Syst. Video Technol., vol. 28, no. 11, pp. 3174-3182, Nov. 2018.\n\nSingle-image depth perception in the wild,'' in Proc. W Chen, Z Fu, D Yang, J Deng, Adv. Neural Inf. Process. Syst. W. Chen, Z. Fu, D. Yang, and J. Deng, ''Single-image depth percep- tion in the wild,'' in Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2016, pp. 730-738.\n\nSemi-supervised deep learning for monocular depth map prediction. Y Kuznietsov, J Stuckler, B Leibe, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)Y. Kuznietsov, J. Stuckler, and B. Leibe, ''Semi-supervised deep learning for monocular depth map prediction,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 6647-6655.\n\nDeep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks. J Xie, R Girshick, A Farhadi, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)J. Xie, R. Girshick, and A. Farhadi, ''Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks,'' in Proc. Eur. Conf. Comput. Vis. (ECCV), 2016, pp. 842-857.\n\nDepth-image-based rendering (DIBR), compression, and transmission for a new approach on 3D-TV. C Fehn, Proc. SPIE. SPIE5291C. Fehn, ''Depth-image-based rendering (DIBR), compression, and trans- mission for a new approach on 3D-TV,'' Proc. SPIE, vol. 5291, pp. 93-104, May 2004.\n\nUnsupervised cnn for single view depth estimation: Geometry to the rescue. R Garg, V K Bg, G Carneiro, I Reid, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)R. Garg, V. K. BG, G. Carneiro, and I. Reid, ''Unsupervised cnn for single view depth estimation: Geometry to the rescue,'' in Proc. Eur. Conf. Comput. Vis. (ECCV), 2016, pp. 740-756.\n\nFlowNet: Learning optical flow with convolutional networks. A Dosovitskiy, P Fischer, E Ilg, P Hausser, C Hazirbas, V Golkov, P V D Smagt, D Cremers, T Brox, Proc. IEEE Int. Conf. Comput. Vis. (ICCV). IEEE Int. Conf. Comput. Vis. (ICCV)A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, P. V. D. Smagt, D. Cremers, and T. Brox, ''FlowNet: Learning optical flow with convolutional networks,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Dec. 2015, pp. 2758-2766.\n\nSpatial transformer networks,'' in Proc. M Jaderberg, K Simonyan, A Zisserman, Adv. Neural Inf. Process. Syst. M. Jaderberg, K. Simonyan, and A. Zisserman, ''Spatial transformer networks,'' in Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2015, pp. 2017-2025.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, Proc. Int. Conf. Learn. Represent. (ICLR). Int. Conf. Learn. Represent. (ICLR)K. Simonyan and A. Zisserman, ''Very deep convolutional networks for large-scale image recognition,'' in Proc. Int. Conf. Learn. Repre- sent. (ICLR), 2015. [Online]. Available: https://arxiv.org/abs/1409.1556\n\nDistilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, Stat. 10509G. Hinton, O. Vinyals, and J. Dean, ''Distilling the knowledge in a neural network,'' Stat, vol. 1050, p. 9, 2015.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, Proc. Int. Conf. Learn. Represent. (ICLR). Int. Conf. Learn. Represent. (ICLR)D. P. Kingma and J. Ba, ''Adam: A method for stochastic optimization,'' in Proc. Int. Conf. Learn. Represent. (ICLR), 2015. [Online]. Available: https://arxiv.org/abs/1412.6980\n\nAccessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition,'' in Proc. M Sharif, S Bhagavatula, L Bauer, M K Reiter, ACM SIGSAC Conf. Comput. Commun. Secur. ACM SIGSACM. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, ''Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition,'' in Proc. ACM SIGSAC Conf. Comput. Commun. Secur. (ACM SIGSAC), 2016, pp. 1528-1540.\n\nUnderstanding deep image representations by inverting them. A Mahendran, A Vedaldi, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)A. Mahendran and A. Vedaldi, ''Understanding deep image representations by inverting them,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 5188-5196.\n\nDensely connected convolutional networks. G Huang, Z Liu, L Van Der Maaten, K Q Weinberger, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, ''Densely connected convolutional networks,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 4700-4708.\n\nDeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. L.-C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, IEEE Trans. Pattern Anal. Mach. Intell. 404L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, ''DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs,'' IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 4, pp. 834-848, Apr. 2018.\n\nSparsity invariant CNNs. J Uhrig, N Schneider, L Schneider, U Franke, T Brox, A Geiger, Proc. Int. Conf. 3D Vis. (3DV). Int. Conf. 3D Vis. (3DV)J. Uhrig, N. Schneider, L. Schneider, U. Franke, T. Brox, and A. Geiger, ''Sparsity invariant CNNs,'' in Proc. Int. Conf. 3D Vis. (3DV), Oct. 2017, pp. 11-20.\n\nVision meets robotics: The KITTI dataset. A Geiger, P Lenz, C Stiller, R Urtasun, Int. J. Robot. Res. 3211A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, ''Vision meets robotics: The KITTI dataset,'' Int. J. Robot. Res., vol. 32, no. 11, pp. 1231-1237, Sep. 2013.\n\nPyTorch: An imperative style, high-performance deep learning library,'' in Proc. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, Adv. Neural Inf. Process. Syst. A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, and L. Antiga, ''PyTorch: An imperative style, high-performance deep learning library,'' in Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2019, pp. 8024-8035.\n\nGrad-CAM: Visual explanations from deep networks via gradient-based localization. R R Selvaraju, M Cogswell, A Das, R Vedantam, D Parikh, D Batra, Proc. IEEE Int. Conf. Comput. Vis. (ICCV). IEEE Int. Conf. Comput. Vis. (ICCV)R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, ''Grad-CAM: Visual explanations from deep networks via gradient-based localization,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 618-626.\n\nwhere he is currently a Graduate Student with the Graduate School of Engineering. His research interests include monocular depth estimation and adversarial examples. JapanKOICHIRO YAMANAKA received the B.E. degree in electrical engineering from Nagoya UniversityKOICHIRO YAMANAKA received the B.E. degree in electrical engineering from Nagoya University, Japan, in 2019, where he is currently a Graduate Student with the Graduate School of Engineering. His research interests include monocular depth estimation and adversarial examples.\n", "annotations": {"author": "[{\"end\":196,\"start\":68},{\"end\":398,\"start\":197},{\"end\":497,\"start\":399},{\"end\":586,\"start\":498},{\"end\":605,\"start\":587}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":77},{\"end\":215,\"start\":206},{\"end\":426,\"start\":417},{\"end\":515,\"start\":510},{\"end\":604,\"start\":596}]", "author_first_name": "[{\"end\":76,\"start\":68},{\"end\":205,\"start\":197},{\"end\":416,\"start\":411},{\"end\":509,\"start\":501},{\"end\":595,\"start\":587}]", "author_affiliation": "[{\"end\":195,\"start\":127},{\"end\":322,\"start\":217},{\"end\":397,\"start\":324},{\"end\":496,\"start\":428},{\"end\":585,\"start\":517}]", "title": "[{\"end\":65,\"start\":1},{\"end\":670,\"start\":606}]", "venue": null, "abstract": "[{\"end\":1861,\"start\":858}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2155,\"start\":2152},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2160,\"start\":2157},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2210,\"start\":2207},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2215,\"start\":2212},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2720,\"start\":2717},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2985,\"start\":2984},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3116,\"start\":3113},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3122,\"start\":3118},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3444,\"start\":3440},{\"end\":4023,\"start\":4012},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4035,\"start\":4032},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5105,\"start\":5101},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5322,\"start\":5318},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5328,\"start\":5324},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5553,\"start\":5549},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5723,\"start\":5719},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5840,\"start\":5836},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5929,\"start\":5925},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6793,\"start\":6789},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6799,\"start\":6795},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6821,\"start\":6817},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6914,\"start\":6910},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7006,\"start\":7002},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7081,\"start\":7077},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7352,\"start\":7348},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8079,\"start\":8075},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8095,\"start\":8091},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8201,\"start\":8197},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8234,\"start\":8230},{\"end\":8261,\"start\":8236},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8349,\"start\":8345},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8786,\"start\":8782},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8958,\"start\":8954},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8964,\"start\":8960},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9170,\"start\":9166},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9202,\"start\":9198},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9273,\"start\":9269},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9321,\"start\":9318},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9337,\"start\":9334},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9784,\"start\":9780},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9843,\"start\":9839},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10005,\"start\":10001},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10112,\"start\":10108},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10194,\"start\":10190},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10248,\"start\":10244},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10460,\"start\":10457},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10509,\"start\":10505},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10547,\"start\":10543},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10788,\"start\":10784},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10922,\"start\":10919},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10973,\"start\":10969},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11172,\"start\":11168},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11507,\"start\":11503},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11513,\"start\":11509},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11595,\"start\":11591},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11927,\"start\":11923},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12112,\"start\":12108},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12283,\"start\":12279},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12828,\"start\":12824},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":14146,\"start\":14142},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14241,\"start\":14237},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15821,\"start\":15817},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15847,\"start\":15843},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":16773,\"start\":16769},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":17379,\"start\":17375},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17589,\"start\":17586},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17594,\"start\":17591},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17620,\"start\":17617},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":17789,\"start\":17785},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18086,\"start\":18083},{\"end\":18327,\"start\":18311},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":18346,\"start\":18342},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":18414,\"start\":18410},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":18577,\"start\":18573},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":18688,\"start\":18684},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18746,\"start\":18742},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19082,\"start\":19079},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19090,\"start\":19087},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19210,\"start\":19206},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":19768,\"start\":19764},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20289,\"start\":20286},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20381,\"start\":20378},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20431,\"start\":20428},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20436,\"start\":20433},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20640,\"start\":20637},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20748,\"start\":20745},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21383,\"start\":21380},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21854,\"start\":21851},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21876,\"start\":21873},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21939,\"start\":21936},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23820,\"start\":23816},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23856,\"start\":23853},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24786,\"start\":24783},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":25378,\"start\":25374},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28301,\"start\":28298},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29084,\"start\":29081},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29406,\"start\":29403},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29869,\"start\":29866}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28335,\"start\":28259},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28381,\"start\":28336},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28742,\"start\":28382},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28795,\"start\":28743},{\"attributes\":{\"id\":\"fig_5\"},\"end\":28921,\"start\":28796},{\"attributes\":{\"id\":\"fig_6\"},\"end\":28987,\"start\":28922},{\"attributes\":{\"id\":\"fig_7\"},\"end\":29085,\"start\":28988},{\"attributes\":{\"id\":\"fig_8\"},\"end\":29139,\"start\":29086},{\"attributes\":{\"id\":\"fig_9\"},\"end\":29338,\"start\":29140},{\"attributes\":{\"id\":\"fig_10\"},\"end\":29407,\"start\":29339},{\"attributes\":{\"id\":\"fig_11\"},\"end\":29796,\"start\":29408},{\"attributes\":{\"id\":\"fig_12\"},\"end\":30000,\"start\":29797}]", "paragraph": "[{\"end\":2868,\"start\":1880},{\"end\":3244,\"start\":2870},{\"end\":3589,\"start\":3246},{\"end\":4526,\"start\":3591},{\"end\":5014,\"start\":4528},{\"end\":5966,\"start\":5033},{\"end\":6465,\"start\":5968},{\"end\":6800,\"start\":6501},{\"end\":7314,\"start\":6802},{\"end\":8615,\"start\":7316},{\"end\":11144,\"start\":8649},{\"end\":11348,\"start\":11146},{\"end\":12830,\"start\":11372},{\"end\":13174,\"start\":12855},{\"end\":14308,\"start\":13176},{\"end\":14802,\"start\":14340},{\"end\":14963,\"start\":14804},{\"end\":15514,\"start\":15018},{\"end\":16052,\"start\":15574},{\"end\":16210,\"start\":16073},{\"end\":16288,\"start\":16212},{\"end\":16414,\"start\":16306},{\"end\":16618,\"start\":16474},{\"end\":16717,\"start\":16682},{\"end\":16869,\"start\":16753},{\"end\":17124,\"start\":16911},{\"end\":17380,\"start\":17152},{\"end\":17463,\"start\":17446},{\"end\":17595,\"start\":17485},{\"end\":18578,\"start\":17597},{\"end\":18857,\"start\":18627},{\"end\":19092,\"start\":18881},{\"end\":19577,\"start\":19094},{\"end\":19853,\"start\":19579},{\"end\":19910,\"start\":19855},{\"end\":20212,\"start\":19971},{\"end\":20543,\"start\":20214},{\"end\":20935,\"start\":20545},{\"end\":21269,\"start\":20937},{\"end\":21520,\"start\":21271},{\"end\":22931,\"start\":21555},{\"end\":22990,\"start\":22933},{\"end\":23888,\"start\":23028},{\"end\":24098,\"start\":23924},{\"end\":25255,\"start\":24149},{\"end\":25725,\"start\":25281},{\"end\":25890,\"start\":25727},{\"end\":26343,\"start\":25954},{\"end\":26752,\"start\":26395},{\"end\":27242,\"start\":26754},{\"end\":28258,\"start\":27261}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15017,\"start\":14964},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15573,\"start\":15515},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16473,\"start\":16415},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16681,\"start\":16619},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16910,\"start\":16870},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17445,\"start\":17381},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18626,\"start\":18579},{\"attributes\":{\"id\":\"formula_8\"},\"end\":24148,\"start\":24099},{\"attributes\":{\"id\":\"formula_9\"},\"end\":25953,\"start\":25891},{\"attributes\":{\"id\":\"formula_10\"},\"end\":26394,\"start\":26344}]", "table_ref": null, "section_header": "[{\"end\":1878,\"start\":1863},{\"end\":5031,\"start\":5017},{\"end\":6499,\"start\":6468},{\"end\":8647,\"start\":8618},{\"end\":11370,\"start\":11351},{\"end\":12853,\"start\":12833},{\"end\":14338,\"start\":14311},{\"end\":16071,\"start\":16055},{\"end\":16304,\"start\":16291},{\"end\":16751,\"start\":16720},{\"end\":17150,\"start\":17127},{\"end\":17483,\"start\":17466},{\"end\":18879,\"start\":18860},{\"end\":19937,\"start\":19913},{\"end\":19969,\"start\":19940},{\"end\":21553,\"start\":21523},{\"end\":23026,\"start\":22993},{\"end\":23922,\"start\":23891},{\"end\":25279,\"start\":25258},{\"end\":27259,\"start\":27245},{\"end\":28270,\"start\":28260},{\"end\":28347,\"start\":28337},{\"end\":28754,\"start\":28744},{\"end\":28933,\"start\":28923},{\"end\":28999,\"start\":28989},{\"end\":29097,\"start\":29087},{\"end\":29151,\"start\":29141},{\"end\":29350,\"start\":29340},{\"end\":29419,\"start\":29409},{\"end\":29809,\"start\":29798}]", "table": null, "figure_caption": "[{\"end\":28335,\"start\":28272},{\"end\":28381,\"start\":28349},{\"end\":28742,\"start\":28384},{\"end\":28795,\"start\":28756},{\"end\":28921,\"start\":28798},{\"end\":28987,\"start\":28935},{\"end\":29085,\"start\":29001},{\"end\":29139,\"start\":29099},{\"end\":29338,\"start\":29153},{\"end\":29407,\"start\":29352},{\"end\":29796,\"start\":29421},{\"end\":30000,\"start\":29812}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2641,\"start\":2635},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3914,\"start\":3906},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4688,\"start\":4679},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4769,\"start\":4759},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12875,\"start\":12867},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":19908,\"start\":19902},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":20049,\"start\":20043},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20077,\"start\":20069},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20579,\"start\":20570},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21122,\"start\":21116},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":21393,\"start\":21387},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":21653,\"start\":21645},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":22363,\"start\":22358},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":22549,\"start\":22543},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":24633,\"start\":24625},{\"end\":24878,\"start\":24867},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26600,\"start\":26593}]", "bib_author_first_name": "[{\"end\":30054,\"start\":30053},{\"end\":30056,\"start\":30055},{\"end\":30074,\"start\":30070},{\"end\":30406,\"start\":30405},{\"end\":30408,\"start\":30407},{\"end\":30423,\"start\":30422},{\"end\":30435,\"start\":30431},{\"end\":30444,\"start\":30443},{\"end\":30782,\"start\":30778},{\"end\":30790,\"start\":30789},{\"end\":30792,\"start\":30791},{\"end\":30801,\"start\":30800},{\"end\":31098,\"start\":31094},{\"end\":31110,\"start\":31106},{\"end\":31431,\"start\":31430},{\"end\":31441,\"start\":31440},{\"end\":31443,\"start\":31442},{\"end\":31452,\"start\":31451},{\"end\":31454,\"start\":31453},{\"end\":31833,\"start\":31832},{\"end\":31839,\"start\":31838},{\"end\":31847,\"start\":31846},{\"end\":31855,\"start\":31854},{\"end\":31872,\"start\":31871},{\"end\":32250,\"start\":32249},{\"end\":32257,\"start\":32256},{\"end\":32263,\"start\":32262},{\"end\":32269,\"start\":32268},{\"end\":32276,\"start\":32275},{\"end\":32614,\"start\":32613},{\"end\":32618,\"start\":32615},{\"end\":32628,\"start\":32624},{\"end\":32635,\"start\":32634},{\"end\":32640,\"start\":32636},{\"end\":32646,\"start\":32645},{\"end\":32651,\"start\":32647},{\"end\":32953,\"start\":32952},{\"end\":32963,\"start\":32962},{\"end\":32973,\"start\":32972},{\"end\":32981,\"start\":32980},{\"end\":33382,\"start\":33381},{\"end\":33392,\"start\":33391},{\"end\":33398,\"start\":33397},{\"end\":33414,\"start\":33413},{\"end\":33800,\"start\":33799},{\"end\":33806,\"start\":33805},{\"end\":33815,\"start\":33814},{\"end\":33823,\"start\":33822},{\"end\":33833,\"start\":33832},{\"end\":33844,\"start\":33843},{\"end\":33851,\"start\":33850},{\"end\":33853,\"start\":33852},{\"end\":34262,\"start\":34261},{\"end\":34274,\"start\":34273},{\"end\":34588,\"start\":34587},{\"end\":34598,\"start\":34597},{\"end\":34608,\"start\":34607},{\"end\":34619,\"start\":34618},{\"end\":34629,\"start\":34628},{\"end\":34639,\"start\":34638},{\"end\":34649,\"start\":34648},{\"end\":34661,\"start\":34660},{\"end\":35104,\"start\":35103},{\"end\":35115,\"start\":35114},{\"end\":35126,\"start\":35125},{\"end\":35139,\"start\":35138},{\"end\":35148,\"start\":35147},{\"end\":35157,\"start\":35156},{\"end\":35171,\"start\":35170},{\"end\":35547,\"start\":35546},{\"end\":35549,\"start\":35548},{\"end\":35563,\"start\":35562},{\"end\":35573,\"start\":35572},{\"end\":35874,\"start\":35873},{\"end\":35885,\"start\":35884},{\"end\":36182,\"start\":36178},{\"end\":36202,\"start\":36201},{\"end\":36211,\"start\":36210},{\"end\":36595,\"start\":36594},{\"end\":36607,\"start\":36606},{\"end\":36619,\"start\":36618},{\"end\":36626,\"start\":36625},{\"end\":36640,\"start\":36639},{\"end\":36642,\"start\":36641},{\"end\":36651,\"start\":36650},{\"end\":37036,\"start\":37035},{\"end\":37048,\"start\":37047},{\"end\":37061,\"start\":37060},{\"end\":37471,\"start\":37470},{\"end\":37473,\"start\":37472},{\"end\":37483,\"start\":37482},{\"end\":37485,\"start\":37484},{\"end\":37494,\"start\":37493},{\"end\":37502,\"start\":37501},{\"end\":37862,\"start\":37861},{\"end\":37869,\"start\":37868},{\"end\":37877,\"start\":37876},{\"end\":37886,\"start\":37885},{\"end\":37894,\"start\":37893},{\"end\":37901,\"start\":37900},{\"end\":38265,\"start\":38264},{\"end\":38272,\"start\":38271},{\"end\":38281,\"start\":38280},{\"end\":38289,\"start\":38288},{\"end\":38581,\"start\":38580},{\"end\":38590,\"start\":38589},{\"end\":38597,\"start\":38596},{\"end\":38603,\"start\":38602},{\"end\":38611,\"start\":38610},{\"end\":38838,\"start\":38837},{\"end\":38840,\"start\":38839},{\"end\":38849,\"start\":38848},{\"end\":38857,\"start\":38856},{\"end\":38864,\"start\":38863},{\"end\":38873,\"start\":38872},{\"end\":39120,\"start\":39119},{\"end\":39130,\"start\":39129},{\"end\":39427,\"start\":39426},{\"end\":39435,\"start\":39434},{\"end\":39437,\"start\":39436},{\"end\":39446,\"start\":39445},{\"end\":39842,\"start\":39841},{\"end\":39852,\"start\":39851},{\"end\":39861,\"start\":39860},{\"end\":39871,\"start\":39870},{\"end\":40161,\"start\":40160},{\"end\":40172,\"start\":40171},{\"end\":40186,\"start\":40185},{\"end\":40458,\"start\":40457},{\"end\":40469,\"start\":40468},{\"end\":40481,\"start\":40480},{\"end\":40490,\"start\":40489},{\"end\":40791,\"start\":40790},{\"end\":40802,\"start\":40801},{\"end\":40813,\"start\":40812},{\"end\":40826,\"start\":40825},{\"end\":40832,\"start\":40831},{\"end\":40843,\"start\":40842},{\"end\":40851,\"start\":40850},{\"end\":40862,\"start\":40861},{\"end\":40871,\"start\":40870},{\"end\":41321,\"start\":41320},{\"end\":41327,\"start\":41326},{\"end\":41336,\"start\":41335},{\"end\":41345,\"start\":41344},{\"end\":41671,\"start\":41670},{\"end\":41680,\"start\":41679},{\"end\":41691,\"start\":41690},{\"end\":41976,\"start\":41975},{\"end\":41986,\"start\":41985},{\"end\":41988,\"start\":41987},{\"end\":41997,\"start\":41996},{\"end\":41999,\"start\":41998},{\"end\":42258,\"start\":42257},{\"end\":42268,\"start\":42267},{\"end\":42275,\"start\":42274},{\"end\":42277,\"start\":42276},{\"end\":42591,\"start\":42590},{\"end\":42598,\"start\":42597},{\"end\":42606,\"start\":42605},{\"end\":42613,\"start\":42612},{\"end\":42933,\"start\":42932},{\"end\":42941,\"start\":42940},{\"end\":42943,\"start\":42942},{\"end\":42953,\"start\":42952},{\"end\":43281,\"start\":43280},{\"end\":43292,\"start\":43291},{\"end\":43299,\"start\":43298},{\"end\":43627,\"start\":43626},{\"end\":43637,\"start\":43636},{\"end\":43644,\"start\":43643},{\"end\":43646,\"start\":43645},{\"end\":44012,\"start\":44011},{\"end\":44018,\"start\":44017},{\"end\":44026,\"start\":44025},{\"end\":44033,\"start\":44032},{\"end\":44051,\"start\":44050},{\"end\":44527,\"start\":44526},{\"end\":44534,\"start\":44533},{\"end\":44540,\"start\":44539},{\"end\":44874,\"start\":44873},{\"end\":44882,\"start\":44881},{\"end\":44888,\"start\":44887},{\"end\":44896,\"start\":44895},{\"end\":45158,\"start\":45157},{\"end\":45172,\"start\":45171},{\"end\":45184,\"start\":45183},{\"end\":45586,\"start\":45585},{\"end\":45593,\"start\":45592},{\"end\":45605,\"start\":45604},{\"end\":45972,\"start\":45971},{\"end\":46231,\"start\":46230},{\"end\":46239,\"start\":46238},{\"end\":46241,\"start\":46240},{\"end\":46247,\"start\":46246},{\"end\":46259,\"start\":46258},{\"end\":46580,\"start\":46579},{\"end\":46595,\"start\":46594},{\"end\":46606,\"start\":46605},{\"end\":46613,\"start\":46612},{\"end\":46624,\"start\":46623},{\"end\":46636,\"start\":46635},{\"end\":46646,\"start\":46645},{\"end\":46650,\"start\":46647},{\"end\":46659,\"start\":46658},{\"end\":46670,\"start\":46669},{\"end\":47047,\"start\":47046},{\"end\":47060,\"start\":47059},{\"end\":47072,\"start\":47071},{\"end\":47335,\"start\":47334},{\"end\":47347,\"start\":47346},{\"end\":47694,\"start\":47693},{\"end\":47704,\"start\":47703},{\"end\":47715,\"start\":47714},{\"end\":47894,\"start\":47893},{\"end\":47896,\"start\":47895},{\"end\":47906,\"start\":47905},{\"end\":48267,\"start\":48266},{\"end\":48277,\"start\":48276},{\"end\":48292,\"start\":48291},{\"end\":48301,\"start\":48300},{\"end\":48303,\"start\":48302},{\"end\":48655,\"start\":48654},{\"end\":48668,\"start\":48667},{\"end\":49004,\"start\":49003},{\"end\":49013,\"start\":49012},{\"end\":49020,\"start\":49019},{\"end\":49038,\"start\":49037},{\"end\":49040,\"start\":49039},{\"end\":49465,\"start\":49461},{\"end\":49473,\"start\":49472},{\"end\":49487,\"start\":49486},{\"end\":49499,\"start\":49498},{\"end\":49509,\"start\":49508},{\"end\":49511,\"start\":49510},{\"end\":49857,\"start\":49856},{\"end\":49866,\"start\":49865},{\"end\":49879,\"start\":49878},{\"end\":49892,\"start\":49891},{\"end\":49902,\"start\":49901},{\"end\":49910,\"start\":49909},{\"end\":50178,\"start\":50177},{\"end\":50188,\"start\":50187},{\"end\":50196,\"start\":50195},{\"end\":50207,\"start\":50206},{\"end\":50482,\"start\":50481},{\"end\":50492,\"start\":50491},{\"end\":50501,\"start\":50500},{\"end\":50510,\"start\":50509},{\"end\":50519,\"start\":50518},{\"end\":50531,\"start\":50530},{\"end\":50541,\"start\":50540},{\"end\":50552,\"start\":50551},{\"end\":50559,\"start\":50558},{\"end\":50573,\"start\":50572},{\"end\":50957,\"start\":50956},{\"end\":50959,\"start\":50958},{\"end\":50972,\"start\":50971},{\"end\":50984,\"start\":50983},{\"end\":50991,\"start\":50990},{\"end\":51003,\"start\":51002},{\"end\":51013,\"start\":51012}]", "bib_author_last_name": "[{\"end\":30068,\"start\":30057},{\"end\":30080,\"start\":30075},{\"end\":30420,\"start\":30409},{\"end\":30429,\"start\":30424},{\"end\":30441,\"start\":30436},{\"end\":30454,\"start\":30445},{\"end\":30787,\"start\":30783},{\"end\":30798,\"start\":30793},{\"end\":30813,\"start\":30802},{\"end\":31104,\"start\":31099},{\"end\":31115,\"start\":31111},{\"end\":31438,\"start\":31432},{\"end\":31449,\"start\":31444},{\"end\":31462,\"start\":31455},{\"end\":31836,\"start\":31834},{\"end\":31844,\"start\":31840},{\"end\":31852,\"start\":31848},{\"end\":31869,\"start\":31856},{\"end\":31876,\"start\":31873},{\"end\":32254,\"start\":32251},{\"end\":32260,\"start\":32258},{\"end\":32266,\"start\":32264},{\"end\":32273,\"start\":32270},{\"end\":32281,\"start\":32277},{\"end\":32622,\"start\":32619},{\"end\":32632,\"start\":32629},{\"end\":32643,\"start\":32641},{\"end\":32655,\"start\":32652},{\"end\":32960,\"start\":32954},{\"end\":32970,\"start\":32964},{\"end\":32978,\"start\":32974},{\"end\":32988,\"start\":32982},{\"end\":33389,\"start\":33383},{\"end\":33395,\"start\":33393},{\"end\":33411,\"start\":33399},{\"end\":33423,\"start\":33415},{\"end\":33803,\"start\":33801},{\"end\":33812,\"start\":33807},{\"end\":33820,\"start\":33816},{\"end\":33830,\"start\":33824},{\"end\":33841,\"start\":33834},{\"end\":33848,\"start\":33845},{\"end\":33861,\"start\":33854},{\"end\":34271,\"start\":34263},{\"end\":34283,\"start\":34275},{\"end\":34595,\"start\":34589},{\"end\":34605,\"start\":34599},{\"end\":34616,\"start\":34609},{\"end\":34626,\"start\":34620},{\"end\":34636,\"start\":34630},{\"end\":34646,\"start\":34640},{\"end\":34658,\"start\":34650},{\"end\":34666,\"start\":34662},{\"end\":35112,\"start\":35105},{\"end\":35123,\"start\":35116},{\"end\":35136,\"start\":35127},{\"end\":35145,\"start\":35140},{\"end\":35154,\"start\":35149},{\"end\":35168,\"start\":35158},{\"end\":35178,\"start\":35172},{\"end\":35560,\"start\":35550},{\"end\":35570,\"start\":35564},{\"end\":35581,\"start\":35574},{\"end\":35882,\"start\":35875},{\"end\":35892,\"start\":35886},{\"end\":36199,\"start\":36183},{\"end\":36208,\"start\":36203},{\"end\":36220,\"start\":36212},{\"end\":36604,\"start\":36596},{\"end\":36616,\"start\":36608},{\"end\":36623,\"start\":36620},{\"end\":36637,\"start\":36627},{\"end\":36648,\"start\":36643},{\"end\":36657,\"start\":36652},{\"end\":37045,\"start\":37037},{\"end\":37058,\"start\":37049},{\"end\":37072,\"start\":37062},{\"end\":37480,\"start\":37474},{\"end\":37491,\"start\":37486},{\"end\":37499,\"start\":37495},{\"end\":37510,\"start\":37503},{\"end\":37866,\"start\":37863},{\"end\":37874,\"start\":37870},{\"end\":37883,\"start\":37878},{\"end\":37891,\"start\":37887},{\"end\":37898,\"start\":37895},{\"end\":37908,\"start\":37902},{\"end\":38269,\"start\":38266},{\"end\":38278,\"start\":38273},{\"end\":38286,\"start\":38282},{\"end\":38293,\"start\":38290},{\"end\":38587,\"start\":38582},{\"end\":38594,\"start\":38591},{\"end\":38600,\"start\":38598},{\"end\":38608,\"start\":38604},{\"end\":38615,\"start\":38612},{\"end\":38846,\"start\":38841},{\"end\":38854,\"start\":38850},{\"end\":38861,\"start\":38858},{\"end\":38870,\"start\":38865},{\"end\":38880,\"start\":38874},{\"end\":39127,\"start\":39121},{\"end\":39140,\"start\":39131},{\"end\":39432,\"start\":39428},{\"end\":39443,\"start\":39438},{\"end\":39454,\"start\":39447},{\"end\":39849,\"start\":39843},{\"end\":39858,\"start\":39853},{\"end\":39868,\"start\":39862},{\"end\":39877,\"start\":39872},{\"end\":40169,\"start\":40162},{\"end\":40183,\"start\":40173},{\"end\":40193,\"start\":40187},{\"end\":40466,\"start\":40459},{\"end\":40478,\"start\":40470},{\"end\":40487,\"start\":40482},{\"end\":40495,\"start\":40491},{\"end\":40799,\"start\":40792},{\"end\":40810,\"start\":40803},{\"end\":40823,\"start\":40814},{\"end\":40829,\"start\":40827},{\"end\":40840,\"start\":40833},{\"end\":40848,\"start\":40844},{\"end\":40859,\"start\":40852},{\"end\":40868,\"start\":40863},{\"end\":40876,\"start\":40872},{\"end\":41324,\"start\":41322},{\"end\":41333,\"start\":41328},{\"end\":41342,\"start\":41337},{\"end\":41353,\"start\":41346},{\"end\":41677,\"start\":41672},{\"end\":41688,\"start\":41681},{\"end\":41698,\"start\":41692},{\"end\":41983,\"start\":41977},{\"end\":41994,\"start\":41989},{\"end\":42002,\"start\":42000},{\"end\":42265,\"start\":42259},{\"end\":42272,\"start\":42269},{\"end\":42280,\"start\":42278},{\"end\":42595,\"start\":42592},{\"end\":42603,\"start\":42599},{\"end\":42610,\"start\":42607},{\"end\":42618,\"start\":42614},{\"end\":42938,\"start\":42934},{\"end\":42950,\"start\":42944},{\"end\":42959,\"start\":42954},{\"end\":43289,\"start\":43282},{\"end\":43296,\"start\":43293},{\"end\":43309,\"start\":43300},{\"end\":43634,\"start\":43628},{\"end\":43641,\"start\":43638},{\"end\":43651,\"start\":43647},{\"end\":44015,\"start\":44013},{\"end\":44023,\"start\":44019},{\"end\":44030,\"start\":44027},{\"end\":44048,\"start\":44034},{\"end\":44054,\"start\":44052},{\"end\":44531,\"start\":44528},{\"end\":44537,\"start\":44535},{\"end\":44545,\"start\":44541},{\"end\":44879,\"start\":44875},{\"end\":44885,\"start\":44883},{\"end\":44893,\"start\":44889},{\"end\":44901,\"start\":44897},{\"end\":45169,\"start\":45159},{\"end\":45181,\"start\":45173},{\"end\":45190,\"start\":45185},{\"end\":45590,\"start\":45587},{\"end\":45602,\"start\":45594},{\"end\":45613,\"start\":45606},{\"end\":45977,\"start\":45973},{\"end\":46236,\"start\":46232},{\"end\":46244,\"start\":46242},{\"end\":46256,\"start\":46248},{\"end\":46264,\"start\":46260},{\"end\":46592,\"start\":46581},{\"end\":46603,\"start\":46596},{\"end\":46610,\"start\":46607},{\"end\":46621,\"start\":46614},{\"end\":46633,\"start\":46625},{\"end\":46643,\"start\":46637},{\"end\":46656,\"start\":46651},{\"end\":46667,\"start\":46660},{\"end\":46675,\"start\":46671},{\"end\":47057,\"start\":47048},{\"end\":47069,\"start\":47061},{\"end\":47082,\"start\":47073},{\"end\":47344,\"start\":47336},{\"end\":47357,\"start\":47348},{\"end\":47701,\"start\":47695},{\"end\":47712,\"start\":47705},{\"end\":47720,\"start\":47716},{\"end\":47903,\"start\":47897},{\"end\":47909,\"start\":47907},{\"end\":48274,\"start\":48268},{\"end\":48289,\"start\":48278},{\"end\":48298,\"start\":48293},{\"end\":48310,\"start\":48304},{\"end\":48665,\"start\":48656},{\"end\":48676,\"start\":48669},{\"end\":49010,\"start\":49005},{\"end\":49017,\"start\":49014},{\"end\":49035,\"start\":49021},{\"end\":49051,\"start\":49041},{\"end\":49470,\"start\":49466},{\"end\":49484,\"start\":49474},{\"end\":49496,\"start\":49488},{\"end\":49506,\"start\":49500},{\"end\":49518,\"start\":49512},{\"end\":49863,\"start\":49858},{\"end\":49876,\"start\":49867},{\"end\":49889,\"start\":49880},{\"end\":49899,\"start\":49893},{\"end\":49907,\"start\":49903},{\"end\":49917,\"start\":49911},{\"end\":50185,\"start\":50179},{\"end\":50193,\"start\":50189},{\"end\":50204,\"start\":50197},{\"end\":50215,\"start\":50208},{\"end\":50489,\"start\":50483},{\"end\":50498,\"start\":50493},{\"end\":50507,\"start\":50502},{\"end\":50516,\"start\":50511},{\"end\":50528,\"start\":50520},{\"end\":50538,\"start\":50532},{\"end\":50549,\"start\":50542},{\"end\":50556,\"start\":50553},{\"end\":50570,\"start\":50560},{\"end\":50580,\"start\":50574},{\"end\":50969,\"start\":50960},{\"end\":50981,\"start\":50973},{\"end\":50988,\"start\":50985},{\"end\":51000,\"start\":50992},{\"end\":51010,\"start\":51004},{\"end\":51019,\"start\":51014}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1728538},\"end\":30342,\"start\":30020},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":977535},\"end\":30708,\"start\":30344},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":16897186},\"end\":31059,\"start\":30710},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4252896},\"end\":31359,\"start\":31061},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":206596513},\"end\":31766,\"start\":31361},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":46968214},\"end\":32178,\"start\":31768},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52051348},\"end\":32526,\"start\":32180},{\"attributes\":{\"doi\":\"arXiv:1907.10326\",\"id\":\"b7\"},\"end\":32890,\"start\":32528},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6385934},\"end\":33288,\"start\":32892},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":131774103},\"end\":33733,\"start\":33290},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":131775632},\"end\":34207,\"start\":33735},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":158046946},\"end\":34530,\"start\":34209},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":18716873},\"end\":35059,\"start\":34532},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":604334},\"end\":35496,\"start\":35061},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6706414},\"end\":35817,\"start\":35498},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2893830},\"end\":36107,\"start\":35819},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":12387176},\"end\":36534,\"start\":36109},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":7004303},\"end\":36959,\"start\":36536},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":195791409},\"end\":37395,\"start\":36961},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5737015},\"end\":37790,\"start\":37397},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3350285},\"end\":38191,\"start\":37792},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":54434451},\"end\":38527,\"start\":38193},{\"attributes\":{\"doi\":\"arXiv:2003.10315\",\"id\":\"b22\"},\"end\":38816,\"start\":38529},{\"attributes\":{\"doi\":\"arXiv:1712.09665\",\"id\":\"b23\"},\"end\":39052,\"start\":38818},{\"attributes\":{\"doi\":\"arXiv:1908.08705\",\"id\":\"b24\"},\"end\":39336,\"start\":39054},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":121124946},\"end\":39815,\"start\":39338},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":204823874},\"end\":40114,\"start\":39817},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1257772},\"end\":40413,\"start\":40116},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":2645819},\"end\":40718,\"start\":40415},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":29162614},\"end\":41230,\"start\":40720},{\"attributes\":{\"doi\":\"arXiv:1707.03501\",\"id\":\"b30\"},\"end\":41582,\"start\":41232},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2255738},\"end\":41917,\"start\":41584},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":10748875},\"end\":42192,\"start\":41919},{\"attributes\":{\"id\":\"b33\"},\"end\":42504,\"start\":42194},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":15774646},\"end\":42875,\"start\":42506},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":9363077},\"end\":43243,\"start\":42877},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":11106957},\"end\":43549,\"start\":43245},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":12230426},\"end\":43894,\"start\":43551},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":206592782},\"end\":44417,\"start\":43896},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":14811066},\"end\":44817,\"start\":44419},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":14395783},\"end\":45089,\"start\":44819},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":16790081},\"end\":45492,\"start\":45091},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":5458522},\"end\":45874,\"start\":45494},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":62566129},\"end\":46153,\"start\":45876},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":299085},\"end\":46517,\"start\":46155},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":12552176},\"end\":47003,\"start\":46519},{\"attributes\":{\"id\":\"b46\"},\"end\":47264,\"start\":47005},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":14124313},\"end\":47645,\"start\":47266},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":7200347},\"end\":47847,\"start\":47647},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":6628106},\"end\":48165,\"start\":47849},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":207241700},\"end\":48592,\"start\":48167},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":206593185},\"end\":48959,\"start\":48594},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":9433631},\"end\":49346,\"start\":48961},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":3429309},\"end\":49829,\"start\":49348},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":206429195},\"end\":50133,\"start\":49831},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":9455111},\"end\":50398,\"start\":50135},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":202786778},\"end\":50872,\"start\":50400},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":15019293},\"end\":51330,\"start\":50874},{\"attributes\":{\"id\":\"b58\"},\"end\":51868,\"start\":51332}]", "bib_title": "[{\"end\":30051,\"start\":30020},{\"end\":30403,\"start\":30344},{\"end\":30776,\"start\":30710},{\"end\":31092,\"start\":31061},{\"end\":31428,\"start\":31361},{\"end\":31830,\"start\":31768},{\"end\":32247,\"start\":32180},{\"end\":32950,\"start\":32892},{\"end\":33379,\"start\":33290},{\"end\":33797,\"start\":33735},{\"end\":34259,\"start\":34209},{\"end\":34585,\"start\":34532},{\"end\":35101,\"start\":35061},{\"end\":35544,\"start\":35498},{\"end\":35871,\"start\":35819},{\"end\":36176,\"start\":36109},{\"end\":36592,\"start\":36536},{\"end\":37033,\"start\":36961},{\"end\":37468,\"start\":37397},{\"end\":37859,\"start\":37792},{\"end\":38262,\"start\":38193},{\"end\":39424,\"start\":39338},{\"end\":39839,\"start\":39817},{\"end\":40158,\"start\":40116},{\"end\":40455,\"start\":40415},{\"end\":40788,\"start\":40720},{\"end\":41668,\"start\":41584},{\"end\":41973,\"start\":41919},{\"end\":42255,\"start\":42194},{\"end\":42588,\"start\":42506},{\"end\":42930,\"start\":42877},{\"end\":43278,\"start\":43245},{\"end\":43624,\"start\":43551},{\"end\":44009,\"start\":43896},{\"end\":44524,\"start\":44419},{\"end\":44871,\"start\":44819},{\"end\":45155,\"start\":45091},{\"end\":45583,\"start\":45494},{\"end\":45969,\"start\":45876},{\"end\":46228,\"start\":46155},{\"end\":46577,\"start\":46519},{\"end\":47044,\"start\":47005},{\"end\":47332,\"start\":47266},{\"end\":47691,\"start\":47647},{\"end\":47891,\"start\":47849},{\"end\":48264,\"start\":48167},{\"end\":48652,\"start\":48594},{\"end\":49001,\"start\":48961},{\"end\":49459,\"start\":49348},{\"end\":49854,\"start\":49831},{\"end\":50175,\"start\":50135},{\"end\":50479,\"start\":50400},{\"end\":50954,\"start\":50874}]", "bib_author": "[{\"end\":30070,\"start\":30053},{\"end\":30082,\"start\":30070},{\"end\":30422,\"start\":30405},{\"end\":30431,\"start\":30422},{\"end\":30443,\"start\":30431},{\"end\":30456,\"start\":30443},{\"end\":30789,\"start\":30778},{\"end\":30800,\"start\":30789},{\"end\":30815,\"start\":30800},{\"end\":31106,\"start\":31094},{\"end\":31117,\"start\":31106},{\"end\":31440,\"start\":31430},{\"end\":31451,\"start\":31440},{\"end\":31464,\"start\":31451},{\"end\":31838,\"start\":31832},{\"end\":31846,\"start\":31838},{\"end\":31854,\"start\":31846},{\"end\":31871,\"start\":31854},{\"end\":31878,\"start\":31871},{\"end\":32256,\"start\":32249},{\"end\":32262,\"start\":32256},{\"end\":32268,\"start\":32262},{\"end\":32275,\"start\":32268},{\"end\":32283,\"start\":32275},{\"end\":32624,\"start\":32613},{\"end\":32634,\"start\":32624},{\"end\":32645,\"start\":32634},{\"end\":32657,\"start\":32645},{\"end\":32962,\"start\":32952},{\"end\":32972,\"start\":32962},{\"end\":32980,\"start\":32972},{\"end\":32990,\"start\":32980},{\"end\":33391,\"start\":33381},{\"end\":33397,\"start\":33391},{\"end\":33413,\"start\":33397},{\"end\":33425,\"start\":33413},{\"end\":33805,\"start\":33799},{\"end\":33814,\"start\":33805},{\"end\":33822,\"start\":33814},{\"end\":33832,\"start\":33822},{\"end\":33843,\"start\":33832},{\"end\":33850,\"start\":33843},{\"end\":33863,\"start\":33850},{\"end\":34273,\"start\":34261},{\"end\":34285,\"start\":34273},{\"end\":34597,\"start\":34587},{\"end\":34607,\"start\":34597},{\"end\":34618,\"start\":34607},{\"end\":34628,\"start\":34618},{\"end\":34638,\"start\":34628},{\"end\":34648,\"start\":34638},{\"end\":34660,\"start\":34648},{\"end\":34668,\"start\":34660},{\"end\":35114,\"start\":35103},{\"end\":35125,\"start\":35114},{\"end\":35138,\"start\":35125},{\"end\":35147,\"start\":35138},{\"end\":35156,\"start\":35147},{\"end\":35170,\"start\":35156},{\"end\":35180,\"start\":35170},{\"end\":35562,\"start\":35546},{\"end\":35572,\"start\":35562},{\"end\":35583,\"start\":35572},{\"end\":35884,\"start\":35873},{\"end\":35894,\"start\":35884},{\"end\":36201,\"start\":36178},{\"end\":36210,\"start\":36201},{\"end\":36222,\"start\":36210},{\"end\":36606,\"start\":36594},{\"end\":36618,\"start\":36606},{\"end\":36625,\"start\":36618},{\"end\":36639,\"start\":36625},{\"end\":36650,\"start\":36639},{\"end\":36659,\"start\":36650},{\"end\":37047,\"start\":37035},{\"end\":37060,\"start\":37047},{\"end\":37074,\"start\":37060},{\"end\":37482,\"start\":37470},{\"end\":37493,\"start\":37482},{\"end\":37501,\"start\":37493},{\"end\":37512,\"start\":37501},{\"end\":37868,\"start\":37861},{\"end\":37876,\"start\":37868},{\"end\":37885,\"start\":37876},{\"end\":37893,\"start\":37885},{\"end\":37900,\"start\":37893},{\"end\":37910,\"start\":37900},{\"end\":38271,\"start\":38264},{\"end\":38280,\"start\":38271},{\"end\":38288,\"start\":38280},{\"end\":38295,\"start\":38288},{\"end\":38589,\"start\":38580},{\"end\":38596,\"start\":38589},{\"end\":38602,\"start\":38596},{\"end\":38610,\"start\":38602},{\"end\":38617,\"start\":38610},{\"end\":38848,\"start\":38837},{\"end\":38856,\"start\":38848},{\"end\":38863,\"start\":38856},{\"end\":38872,\"start\":38863},{\"end\":38882,\"start\":38872},{\"end\":39129,\"start\":39119},{\"end\":39142,\"start\":39129},{\"end\":39434,\"start\":39426},{\"end\":39445,\"start\":39434},{\"end\":39456,\"start\":39445},{\"end\":39851,\"start\":39841},{\"end\":39860,\"start\":39851},{\"end\":39870,\"start\":39860},{\"end\":39879,\"start\":39870},{\"end\":40171,\"start\":40160},{\"end\":40185,\"start\":40171},{\"end\":40195,\"start\":40185},{\"end\":40468,\"start\":40457},{\"end\":40480,\"start\":40468},{\"end\":40489,\"start\":40480},{\"end\":40497,\"start\":40489},{\"end\":40801,\"start\":40790},{\"end\":40812,\"start\":40801},{\"end\":40825,\"start\":40812},{\"end\":40831,\"start\":40825},{\"end\":40842,\"start\":40831},{\"end\":40850,\"start\":40842},{\"end\":40861,\"start\":40850},{\"end\":40870,\"start\":40861},{\"end\":40878,\"start\":40870},{\"end\":41326,\"start\":41320},{\"end\":41335,\"start\":41326},{\"end\":41344,\"start\":41335},{\"end\":41355,\"start\":41344},{\"end\":41679,\"start\":41670},{\"end\":41690,\"start\":41679},{\"end\":41700,\"start\":41690},{\"end\":41985,\"start\":41975},{\"end\":41996,\"start\":41985},{\"end\":42004,\"start\":41996},{\"end\":42267,\"start\":42257},{\"end\":42274,\"start\":42267},{\"end\":42282,\"start\":42274},{\"end\":42597,\"start\":42590},{\"end\":42605,\"start\":42597},{\"end\":42612,\"start\":42605},{\"end\":42620,\"start\":42612},{\"end\":42940,\"start\":42932},{\"end\":42952,\"start\":42940},{\"end\":42961,\"start\":42952},{\"end\":43291,\"start\":43280},{\"end\":43298,\"start\":43291},{\"end\":43311,\"start\":43298},{\"end\":43636,\"start\":43626},{\"end\":43643,\"start\":43636},{\"end\":43653,\"start\":43643},{\"end\":44017,\"start\":44011},{\"end\":44025,\"start\":44017},{\"end\":44032,\"start\":44025},{\"end\":44050,\"start\":44032},{\"end\":44056,\"start\":44050},{\"end\":44533,\"start\":44526},{\"end\":44539,\"start\":44533},{\"end\":44547,\"start\":44539},{\"end\":44881,\"start\":44873},{\"end\":44887,\"start\":44881},{\"end\":44895,\"start\":44887},{\"end\":44903,\"start\":44895},{\"end\":45171,\"start\":45157},{\"end\":45183,\"start\":45171},{\"end\":45192,\"start\":45183},{\"end\":45592,\"start\":45585},{\"end\":45604,\"start\":45592},{\"end\":45615,\"start\":45604},{\"end\":45979,\"start\":45971},{\"end\":46238,\"start\":46230},{\"end\":46246,\"start\":46238},{\"end\":46258,\"start\":46246},{\"end\":46266,\"start\":46258},{\"end\":46594,\"start\":46579},{\"end\":46605,\"start\":46594},{\"end\":46612,\"start\":46605},{\"end\":46623,\"start\":46612},{\"end\":46635,\"start\":46623},{\"end\":46645,\"start\":46635},{\"end\":46658,\"start\":46645},{\"end\":46669,\"start\":46658},{\"end\":46677,\"start\":46669},{\"end\":47059,\"start\":47046},{\"end\":47071,\"start\":47059},{\"end\":47084,\"start\":47071},{\"end\":47346,\"start\":47334},{\"end\":47359,\"start\":47346},{\"end\":47703,\"start\":47693},{\"end\":47714,\"start\":47703},{\"end\":47722,\"start\":47714},{\"end\":47905,\"start\":47893},{\"end\":47911,\"start\":47905},{\"end\":48276,\"start\":48266},{\"end\":48291,\"start\":48276},{\"end\":48300,\"start\":48291},{\"end\":48312,\"start\":48300},{\"end\":48667,\"start\":48654},{\"end\":48678,\"start\":48667},{\"end\":49012,\"start\":49003},{\"end\":49019,\"start\":49012},{\"end\":49037,\"start\":49019},{\"end\":49053,\"start\":49037},{\"end\":49472,\"start\":49461},{\"end\":49486,\"start\":49472},{\"end\":49498,\"start\":49486},{\"end\":49508,\"start\":49498},{\"end\":49520,\"start\":49508},{\"end\":49865,\"start\":49856},{\"end\":49878,\"start\":49865},{\"end\":49891,\"start\":49878},{\"end\":49901,\"start\":49891},{\"end\":49909,\"start\":49901},{\"end\":49919,\"start\":49909},{\"end\":50187,\"start\":50177},{\"end\":50195,\"start\":50187},{\"end\":50206,\"start\":50195},{\"end\":50217,\"start\":50206},{\"end\":50491,\"start\":50481},{\"end\":50500,\"start\":50491},{\"end\":50509,\"start\":50500},{\"end\":50518,\"start\":50509},{\"end\":50530,\"start\":50518},{\"end\":50540,\"start\":50530},{\"end\":50551,\"start\":50540},{\"end\":50558,\"start\":50551},{\"end\":50572,\"start\":50558},{\"end\":50582,\"start\":50572},{\"end\":50971,\"start\":50956},{\"end\":50983,\"start\":50971},{\"end\":50990,\"start\":50983},{\"end\":51002,\"start\":50990},{\"end\":51012,\"start\":51002},{\"end\":51021,\"start\":51012}]", "bib_venue": "[{\"end\":30186,\"start\":30138},{\"end\":30524,\"start\":30494},{\"end\":31213,\"start\":31169},{\"end\":31568,\"start\":31520},{\"end\":31974,\"start\":31930},{\"end\":32351,\"start\":32321},{\"end\":33094,\"start\":33046},{\"end\":33511,\"start\":33472},{\"end\":33975,\"start\":33923},{\"end\":34371,\"start\":34332},{\"end\":34810,\"start\":34743},{\"end\":35258,\"start\":35223},{\"end\":35962,\"start\":35932},{\"end\":36326,\"start\":36278},{\"end\":36747,\"start\":36707},{\"end\":37140,\"start\":37111},{\"end\":37590,\"start\":37555},{\"end\":37988,\"start\":37953},{\"end\":38343,\"start\":38323},{\"end\":39590,\"start\":39527},{\"end\":39965,\"start\":39926},{\"end\":40565,\"start\":40535},{\"end\":40974,\"start\":40930},{\"end\":43065,\"start\":43017},{\"end\":43399,\"start\":43359},{\"end\":44160,\"start\":44112},{\"end\":45296,\"start\":45248},{\"end\":45683,\"start\":45653},{\"end\":45995,\"start\":45991},{\"end\":46334,\"start\":46304},{\"end\":46755,\"start\":46720},{\"end\":47437,\"start\":47402},{\"end\":47989,\"start\":47954},{\"end\":48782,\"start\":48734},{\"end\":49157,\"start\":49109},{\"end\":49975,\"start\":49951},{\"end\":51099,\"start\":51064},{\"end\":30136,\"start\":30082},{\"end\":30492,\"start\":30456},{\"end\":30853,\"start\":30815},{\"end\":31167,\"start\":31117},{\"end\":31518,\"start\":31464},{\"end\":31928,\"start\":31878},{\"end\":32319,\"start\":32283},{\"end\":32611,\"start\":32528},{\"end\":33044,\"start\":32990},{\"end\":33470,\"start\":33425},{\"end\":33921,\"start\":33863},{\"end\":34330,\"start\":34285},{\"end\":34741,\"start\":34668},{\"end\":35221,\"start\":35180},{\"end\":35618,\"start\":35583},{\"end\":35930,\"start\":35894},{\"end\":36276,\"start\":36222},{\"end\":36705,\"start\":36659},{\"end\":37109,\"start\":37074},{\"end\":37553,\"start\":37512},{\"end\":37951,\"start\":37910},{\"end\":38321,\"start\":38295},{\"end\":38578,\"start\":38529},{\"end\":38835,\"start\":38818},{\"end\":39117,\"start\":39054},{\"end\":39525,\"start\":39456},{\"end\":39924,\"start\":39879},{\"end\":40222,\"start\":40195},{\"end\":40533,\"start\":40497},{\"end\":40928,\"start\":40878},{\"end\":41318,\"start\":41232},{\"end\":41730,\"start\":41700},{\"end\":42034,\"start\":42004},{\"end\":42320,\"start\":42282},{\"end\":42658,\"start\":42620},{\"end\":43015,\"start\":42961},{\"end\":43357,\"start\":43311},{\"end\":43691,\"start\":43653},{\"end\":44110,\"start\":44056},{\"end\":44587,\"start\":44547},{\"end\":44933,\"start\":44903},{\"end\":45246,\"start\":45192},{\"end\":45651,\"start\":45615},{\"end\":45989,\"start\":45979},{\"end\":46302,\"start\":46266},{\"end\":46718,\"start\":46677},{\"end\":47114,\"start\":47084},{\"end\":47400,\"start\":47359},{\"end\":47726,\"start\":47722},{\"end\":47952,\"start\":47911},{\"end\":48350,\"start\":48312},{\"end\":48732,\"start\":48678},{\"end\":49107,\"start\":49053},{\"end\":49558,\"start\":49520},{\"end\":49949,\"start\":49919},{\"end\":50235,\"start\":50217},{\"end\":50612,\"start\":50582},{\"end\":51062,\"start\":51021},{\"end\":51496,\"start\":51332}]"}}}, "year": 2023, "month": 12, "day": 17}