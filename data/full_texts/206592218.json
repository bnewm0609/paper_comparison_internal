{"id": 206592218, "updated": "2023-04-05 00:04:28.206", "metadata": {"title": "Large-Scale Video Classification with Convolutional Neural Networks", "authors": "[{\"first\":\"Andrej\",\"last\":\"Karpathy\",\"middle\":[]},{\"first\":\"George\",\"last\":\"Toderici\",\"middle\":[]},{\"first\":\"Sanketh\",\"last\":\"Shetty\",\"middle\":[]},{\"first\":\"Thomas\",\"last\":\"Leung\",\"middle\":[]},{\"first\":\"Rahul\",\"last\":\"Sukthankar\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Fei-Fei\",\"middle\":[]}]", "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition", "journal": "2014 IEEE Conference on Computer Vision and Pattern Recognition", "publication_date": {"year": 2014, "month": null, "day": null}, "abstract": "Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2016053056", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/KarpathyTSLSF14", "doi": "10.1109/cvpr.2014.223"}}, "content": {"source": {"pdf_hash": "193b3363994b4b3e632506ee82d1c40b120d5f30", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://www.cs.cmu.edu/~rahuls/pub/cvpr2014-deepvideo-rahuls.pdf", "status": "GREEN"}}, "grobid": {"id": "b43b24e0bd2f9a60befa6cbf72905b51f24dd103", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/193b3363994b4b3e632506ee82d1c40b120d5f30.txt", "contents": "\nLarge-scale Video Classification with Convolutional Neural Networks\n\n\nAndrej Karpathy karpathy@cs.stanford.edu \nGoogle Research\n\n\nComputer Science Department\nStanford University\n\n\nGeorge Toderici gtoderici@google.com \nGoogle Research\n\n\nSanketh Shetty sanketh@google.com \nGoogle Research\n\n\nThomas Leung leungt@google.com \nGoogle Research\n\n\nRahul Sukthankar sukthankar@google.com \nGoogle Research\n\n\nLi Fei-Fei feifeili@cs.stanford.edu \nComputer Science Department\nStanford University\n\n\nLarge-scale Video Classification with Convolutional Neural Networks\n\nConvolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on largescale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of the a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).\n\nIntroduction\n\nImages and videos have become ubiquitous on the internet, which has encouraged the development of algorithms that can analyze their semantic content for various applications, including search and summarization. Recently, Convolutional Neural Networks (CNNs) [15] have been demonstrated as an effective class of models for understanding image content, giving state-of-the-art results on image recognition, segmentation, detection and retrieval [11,3,2,20,9,18]. The key enabling factors behind these results were techniques for scaling up the networks to tens of millions of parameters and massive labeled datasets that can support the learning process. Under these conditions, CNNs have been shown to learn powerful and interpretable image features [28]. Encouraged by positive results in domain of images, we study the performance of CNNs in large-scale video classification, where the networks have access to not only the appearance information present in single, static images, but also their complex temporal evolution. There are several challenges to extending and applying CNNs in this setting.\n\nFrom a practical standpoint, there are currently no video classification benchmarks that match the scale and variety of existing image datasets because videos are significantly more difficult to collect, annotate and store. To obtain sufficient amount of data needed to train our CNN architectures, we collected a new Sports-1M dataset, which consists of 1 million YouTube videos belonging to a taxonomy of 487 classes of sports. We make Sports-1M available to the research community to support future work in this area.\n\nFrom a modeling perspective, we are interested in answering the following questions: what temporal connectivity pattern in a CNN architecture is best at taking advantage of local motion information present in the video? How does the additional motion information influence the predictions of a CNN and how much does it improve performance overall? We examine these questions empirically by evaluating multiple CNN architectures that each take a different approach to combining information across the time domain.\n\nFrom a computational perspective, CNNs require extensively long periods of training time to effectively optimize the millions of parameters that parametrize the model. This difficulty is further compounded when extending the connectivity of the architecture in time because the network must process not just one image but several frames of video at a time. To mitigate this issue, we show that an effective approach to speeding up the runtime performance of CNNs is to modify the architecture to contain two separate streams of processing: a context stream that learns features on low-resolution frames and a high-resolution fovea stream that only operates on the middle portion of the frame. We observe a 2-4x increase in runtime performance of the network due to the reduced dimensionality of the input, while retaining the classification accuracy.\n\nFinally, a natural question that arises is whether features learned on the Sports-1M dataset are generic enough to generalize to a different, smaller dataset. We investigate the transfer learning problem empirically, achieving significantly better performance (65.4%, up from 41.3%) on UCF-101 by re-purposing low-level features learned on the Sports-1M dataset than by training the entire network on UCF-101 alone. Furthermore, since only some classes in UCF-101 are related to sports, we can quantify the relative improvements of the transfer learning in both settings.\n\nOur contributions can be summarized as follows:\n\n\u2022 We provide extensive experimental evaluation of multiple approaches for extending CNNs into video classification on a large-scale dataset of 1 million videos with 487 categories (which we release as Sports-1M dataset) and report significant gains in performance over strong feature-based baselines.\n\n\u2022 We highlight an architecture that processes input at two spatial resolutions -a low-resolution context stream and a high-resolution fovea stream -as a promising way of improving the runtime performance of CNNs at no cost in accuracy.\n\n\u2022 We apply our networks to the UCF-101 dataset and report significant improvement over feature-based stateof-the-art results and baselines established by training networks on UCF-101 alone.\n\n\nRelated Work\n\nThe standard approach to video classification [26,16,21,17] involves three major stages: First, local visual features that describe a region of the video are extracted either densely [25] or at a sparse set of interest points [12,8]. Next, the features get combined into a fixed-sized videolevel description. One popular approach is to quantize all features using a learned k-means dictionary and accumulate the visual words over the duration of the video into histograms of varying spatio-temporal positions and extents [13]. Lastly, a classifier (such as an SVM) is trained on the resulting \"bag of words\" representation to distinguish among the visual classes of interest.\n\nConvolutional Neural Networks [15] are a biologicallyinspired class of deep learning models that replace all three stages with a single neural network that is trained end to end from raw pixel values to classifier outputs. The spatial structure of images is explicitly taken advantage of for regularization through restricted connectivity between layers (local filters), parameter sharing (convolutions) and special local invariance-building neurons (max pooling). Thus, these architectures effectively shift the required engineer-ing from feature design and accumulation strategies to design of the network connectivity structure and hyperparameter choices. Due to computational constraints, CNNs have until recently been applied to relatively small scale image recognition problems (on datasets such as MNIST, CIFAR-10/100, NORB, and Caltech-101/256), but improvements on GPU hardware have enabled CNNs to scale to networks of millions of parameters, which has in turn led to significant improvements in image classification [11], object detection [20,9], scene labeling [3], indoor segmentation [4] and house number digit classification [19]. Additionally, features learned by large networks trained on ImageNet [7] have been shown to yield state-of-the-art performance across many standard image recognition datasets when classified with an SVM, even with no fine-tuning [18].\n\nCompared to image data domains, there is relatively little work on applying CNNs to video classification. Since all successful applications of CNNs in image domains share the availability of a large training set, we speculate that this is partly attributable to lack of large-scale video classification benchmarks. In particular, commonly used datasets (KTH, Weizmann, UCF Sports, IXMAS, Hollywood 2, UCF-50) only contain up to few thousand clips and up to few dozen classes. Even the largest available datasets such as CCV (9,317 videos and 20 classes) and the recently introduced UCF-101 [22] (13,320 videos and 101 classes) are still dwarfed by available image datasets in the number of instances and their variety [7]. Despite these limitations, some extensions of CNNs into the video domain have been explored. [1] and [10] extend an image CNN to video domains by treating space and time as equivalent dimensions of the input and perform convolutions in both time and space. We consider these extensions as only one of the possible generalizations in this work. Unsupervised learning schemes for training spatio-temporal features have also been developed, based on Convolutional Gated Restricted Boltzmann Machines [23] and Independent Subspace Analysis [14]. In contrast, our models are trained end to end fully supervised.\n\n\nModels\n\nUnlike images which can be cropped and rescaled to a fixed size, videos vary widely in temporal extent and cannot be easily processed with a fixed-sized architecture. In this work we treat every video as a bag of short, fixed-sized clips. Since each clip contains several contiguous frames in time, we can extend the connectivity of the network in time dimension to learn spatio-temporal features. There are multiple options for the precise details of the extended connectivity and we describe three broad connectivity pattern categories (Early Fusion, Late Fusion and Slow Fusion) below. Afterwards, we describe a multiresolution architecture for addressing the computational efficiency. Figure 1: Explored approaches for fusing information over temporal dimension through the network. Red, green and blue boxes indicate convolutional, normalization and pooling layers respectively. In the Slow Fusion model, the depicted columns share parameters.\n\n\nTime Information Fusion in CNNs\n\nWe investigate several approaches to fusing information across temporal domain ( Figure 1): the fusion can be done early in the network by modifying the first layer convolutional filters to extend in time, or it can be done late by placing two separate single-frame networks some distance in time apart and fusing their outputs later in the processing. We first describe a baseline single-frame CNN and then discuss its extensions in time according to different types of fusion.\n\nSingle-frame. We use a single-frame baseline architecture to understand the contribution of static appearance to the classification accuracy. This network is similar to the ImageNet challenge winning model [11], but accepts inputs of size 170 \u00d7 170 \u00d7 3 pixels instead of the original 224 \u00d7 224 \u00d7 3. Using shorthand notation, the full architecture is C(96, 11, 3)-N -P -C(256, 5, 1)-N -P -C(384, 3, 1)-C(384, 3, 1)-C(256, 3, 1)-P -F C(4096)-F C(4096), where C(d, f, s) indicates a convolutional layer with d filters of spatial size f \u00d7 f , applied to the input with stride s. F C(n) is a fully connected layer with n nodes. All pooling layers P pool spatially in non-overlapping 2 \u00d7 2 regions and all normalization layers N are defined as described in Krizhevsky et al. [11] and use the same parameters: k = 2, n = 5, \u03b1 = 10 \u22124 , \u03b2 = 0.5. The final layer is connected to a softmax classifier with dense connections.\n\nEarly Fusion. The Early Fusion extension combines information across an entire time window immediately on the pixel level. This is implemented by modifying the filters on the first convolutional layer in the single-frame model by extending them to be of size 11 \u00d7 11 \u00d7 3 \u00d7 T pixels, where T is some temporal extent (we use T = 10, or approximately a third of a second). The early and direct connectivity to pixel data allows the network to precisely detect local motion direction and speed.\n\nLate Fusion. The Late Fusion model places two separate single-frame networks (as described above, up to last convolutional layer C(256, 3, 1) with shared parameters a distance of 15 frames apart and then merges the two streams in the first fully connected layer. Therefore, neither singleframe tower alone can detect any motion, but the first fully connected layer can compute global motion characteristics by comparing outputs of both towers.\n\nSlow Fusion. The Slow Fusion model is a balanced mix between the two approaches that slowly fuses temporal information throughout the network such that higher layers get access to progressively more global information in both spatial and temporal dimensions. This is implemented by extending the connectivity of all convolutional layers in time and carrying out temporal convolutions in addition to spatial convolutions to compute activations, as seen in [1,10]. In the model we use, the first convolutional layer is extended to apply every filter of temporal extent T = 4 on an input clip of 10 frames through valid convolution with stride 2 and produces 4 responses in time. The second and third layers above iterate this process with filters of temporal extent T = 2 and stride 2. Thus, the third convolutional layer has access to information across all 10 input frames.\n\n\nMultiresolution CNNs\n\nSince CNNs normally take on orders of weeks to train on large-scale datasets even on the fastest available GPUs, the runtime performance is a critical component to our ability to experiment with different architecture and hyperparameter settings. This motivates approaches for speeding up the models while still retaining their performance. There are multiple fronts to these endeavors, including improvements in hardware, weight quantization schemes, better optimization algorithms and initialization strategies, but in this work we focus on changes in the architecture that enable faster running times without sacrificing performance.\n\nOne approach to speeding up the networks is to reduce the number of layers and neurons in each layer, but similar to [28] we found that this consistently lowers the performance. Instead of reducing the size of the network, we conducted further experiments on training with images of lower resolution. However, while this improved the running time of the network, the high-frequency detail in the images proved critical to achieving good accuracy.\n\nFovea and context streams. The proposed multiresolution architecture aims to strike a compromise by having two separate streams of processing over two spatial resolutions ( Figure 2). A 178 \u00d7 178 frame video clip forms an input to the network. The context stream receives the downsampled frames at half the original spatial resolution (89 \u00d7 89 pixels), while the fovea stream receives the center 89 \u00d7 89 region at the original resolution. In this way, the the total input dimensionality is halved. Notably, this design takes advantage of the camera bias present in many online videos, since the object of interest often occupies the center region.\n\nArchitecture changes. Both streams are processed by identical network as the full frame models, but starting at Figure 2: Multiresolution CNN architecture. Input frames are fed into two separate streams of processing: a context stream that models low-resolution image and a fovea stream that processes high-resolution center crop. Both streams consist of alternating convolution (red), normalization (green) and pooling (blue) layers. Both streams converge to two fully connected layers (yellow).\n\n89 \u00d7 89 clips of video. Since the input is only of half the spatial size as the full-frame models, we take out the last pooling layer to ensure that both streams still terminate in a layer of size 7 \u00d7 7 \u00d7 256. The activations from both streams are concatenated and fed into the first fully connected layer with dense connections.\n\n\nLearning\n\nOptimization. We use Downpour Stochastic Gradient Descent [6] to optimize our models across a computing cluster. The number of replicas for each model varies between 10 and 50 and every model is further split across 4 to 32 partitions. We use mini-batches of 32 examples, momentum of 0.9 and weight decay of 0.0005. All models are initialized with learning rates of 1e \u22123 and this value is further reduced by hand whenever the validation error stops improving.\n\nData augmentation and preprocessing. Following [11], we take advantage of data augmentation to reduce the effects of overfitting. Before presenting an example to a network, we preprocess all images by first cropping to center region, resizing them to 200 \u00d7 200 pixels, randomly sampling a 170 \u00d7 170 region, and finally randomly flipping the images horizontally with 50% probability. These preprocessing steps are applied consistently to all frames that are part of the same clip. As a last step of preprocessing we subtract a constant value of 117 from raw pixel values, which is the approximate value of the mean of all pixels in our images.\n\n\nResults\n\nWe first present results on our Sports-1M dataset and qualitatively analyze the learned features and network pre-dictions. We then describe our transfer learning experiments on UCF-101.\n\n\nExperiments on Sports-1M\n\nDataset. The Sports-1M dataset consists of 1 million YouTube videos annotated with 487 classes. The classes are arranged in a manually-curated taxonomy that contains internal nodes such as Aquatic Sports, Team Sports, Winter Sports, Ball Sports, Combat Sports, Sports with Animals, and generally becomes fine-grained by the leaf level. For example, our dataset contains 6 different types of bowling, 7 different types of American football and 23 types of billiards.\n\nThere are 1000-3000 videos per class and approximately 5% of the videos are annotated with more than one class. The annotations are produced automatically by analyzing the text metadata surrounding the videos. Thus, our data is weakly annotated on two levels: first, the label of a video may be wrong if the tag prediction algorithm fails or if the provided description does not match the video content, and second, even when a video is correctly annotated it may still exhibit significant variation on the frame level. For example, a video tagged as soccer may contain several shots of the scoreboard, interviews, news anchors, the crowd, etc.\n\nWe split the dataset by assigning 70% of the videos to the training set, 10% to a validation set and 20% to a test set. As YouTube may contain duplicate videos, it is possible that the same video could appear in both the training and test set. To get an idea about the extent of this problem we processed all videos with a near-duplicate finding algorithm on the frame level and determined that only 1755 videos (out of 1 million) contain a significant fraction of near-duplicate frames. Furthermore, since we only use a random collection of up to 100 half-second clips from every video and our videos are 5 minutes and 36 seconds in length on average, it is unlikely that the same frames occur across data splits.\n\nTraining. We trained our models over a period of one month, with models processing approximately 5 clips per second for full-frame networks and up to 20 clips per second for multiresolution networks on a single model replica. The rate of 5 clips per second is roughly 20 times slower than what one could expect from a high-end GPU, but we expect to reach comparable speeds overall given that we use 10-50 model replicas. We further estimate the size of our dataset of sampled frames to be on the order of 50 million examples and that our networks have each seen approximately 500 million examples throughout the training period in total.\n\nVideo-level predictions. To produce predictions for an entire video we randomly sample 20 clips and present each clip individually to the network. Every clip is propagated through the network 4 times (with different crops and flips)  and the network class predictions are averaged to produce a more robust estimate of the class probabilities. To produce video-level predictions we opted for the simplest approach of averaging individual clip predictions over the durations of each video. We expect more elaborate techniques to further improve performance but consider these to be outside of the scope of the paper.\n\nFeature histogram baselines. In addition to comparing CNN architectures among each other, we also report the accuracy of a feature-based approach. Following a standard bag-of-words pipeline we extract several types of features at all frames of our videos, discretize them using k-means vector quantization and accumulate words into histograms with spatial pyramid encoding and soft quantization. Every histogram is normalized to sum to 1 and all histograms are concatenated into a 25,000 dimensional video-level fea-ture vector. Our features are similar to Yang & Toderici [27] and consist of local features (HOG [5], Texton [24], Cuboids [8], etc.) extracted both densely and at sparse interest points, as well as global features (such as Hue-Saturation, Color moments, number of faces detected). As a classifier we use a multilayer neural network with Rectified Linear Units followed by a Softmax classifier. We found that a multilayer network performs consistently and significantly better than linear models on separate validation experiments. Furthermore, we performed extensive crossvalidations across many of the network's hyperparameters by training multiple models and choosing the one with best performance on a validation set. The tuned hyper parameters include the learning rate, weight decay, the number of hidden layers (between 1-2), dropout probabilities and the Figure 5: Examples that illustrate qualitative differences between single-frame network and Slow Fusion (motion-aware) network in the same color scheme as Figure 4. A few classes are easier to disambiguate with motion information (left three). Quantitative results. The results for the Sports-1M dataset test set, which consists of 200,000 videos and 4,000,000 clips, are summarized in Table 1. As can be seen from the table, our networks consistently and significantly outperform the feature-based baseline. We emphasize that the feature-based approach computes visual words densely over the duration of the video and produces predictions based on the entire video-level feature vector, while our networks only see 20 randomly sampled clips individually. Moreover, our networks seem to learn well despite significant label noise: the training videos are subject to incorrect annotations and even the correctly-labeled videos often contain a large amount of artifacts such as text, effects, cuts, and logos, none of which we attempted to filter out explicitly.\n\nCompared to the wide gap relative to the feature-based baseline, the variation among different CNN architectures turns out to be surprisingly insignificant. Notably, the single-frame model already displays strong performance. Furthermore, we observe that the foveated architectures are between 2-4\u00d7 faster in practice due to reduced input dimensionality. The precise speedups are in part a function of the details of model partitioning and our implementation, but in our experiments we observe a speedup during training of 6 to 21 clips per second (3.5x) for the single-frame model and 5 to 10 clips per second (2x) for the Slow Fusion model.\n\nContributions of motion. We conduct further exper-  iments to understand the differences between the singleframe network and networks that have access to motion information. We choose the Slow Fusion network as a representative motion-aware network because it performs best. We compute and compare the per-class average precision for all Sports classes and highlight the ones that exhibit largest differences (Table 2). Manually inspecting some of the associated clips ( Figure 5), we qualitatively observe that the motion-aware network clearly benefits from motion information in some cases, but these seem to be relatively uncommon. On the other hand, balancing the improvements from access to motion information, we observe that motionaware networks are more likely to underperform when there is camera motion present. We hypothesize that the CNNs struggle to learn complete invariance across all possible angles and speeds of camera translation and zoom. Qualitative analysis. Our learned features for the first convolutional layer can be inspected on Figure 3. Interestingly, the context stream learns more color features while the high-resolution fovea stream learns high frequency grayscale filters.\n\nAs can be seen on Figure 4, our networks produce interpretable predictions and generally make reasonable mistakes. Further analysis of the confusion matrix (attached in the supplementary material) reveals that most errors are among the fine-grained classes of our dataset. For example, the top 5 most commonly confused pairs of classes are deer hunting vs. hunting, hiking vs. backpacking, powered paragliding vs. paragliding, sledding vs. toboggan, and bujinkan vs. ninjutsu.\n\n\nModel\n\n\n3-fold Accuracy\n\nSoomro et al [22] 43.9% Feature Histograms + Neural Net 59.0% Train from scratch 41.3% Fine-tune top layer 64.1% Fine-tune top 3 layers 65.4% Fine-tune all layers 62.2% Table 3: Results on UCF-101 for various Transfer Learning approaches using the Slow Fusion network.\n\n\nTransfer Learning Experiments on UCF-101\n\nThe results of our analysis on the Sports-1M dataset indicate that the networks learn powerful motion features. A natural question that arises is whether these features also generalize to other datasets and class categories. We examine this question in detail by performing transfer learning experiments on the UCF-101 [22] Activity Recognition dataset. The dataset consists of 13,320 videos belonging to 101 categories that are separated into 5 broad groups: Human-Object interaction (Applying eye makeup, brushing teeth, hammering, etc.), Body-Motion (Baby crawling, push ups, blowing candles, etc.), Human-Human interaction (Head massage, salsa spin, haircut, etc.), Playing Instruments (flute, guitar, piano, etc.) and Sports. This grouping allows us to separately study the performance improvements on Sports classes relative to classes from unrelated videos that are less numerous in our training data.\n\nTransfer learning. Since we expect that CNNs learn more generic features on the bottom of the network (such as edges, local shapes) and more intricate, dataset-specific features near the top of the network, we consider the following scenarios for our transfer learning experiments:\n\nFine-tune top layer. We treat the CNN as a fixed feature extractor and train a classifier on the last 4096-dimensional layer, with dropout regularization. We found that as little as 10% chance of keeping each unit active to be effective.\n\nFine-tune top 3 layers. Instead of only retraining the final classifier layer, we consider also retraining both fully connected layers. We initialize with a fully trained Sports CNN and then begin training the top 3 layers. We introduce dropout before all trained layers, with as little as 10% chance of keeping units active.\n\nFine-tune all layers. In this scenario we retrain all network parameters, including all convolutional layers on the bottom of the network.\n\nTrain from scratch. As a baseline we train the full network from scratch on UCF-101 alone.\n\nResults. To prepare UCF-101 data for classification we sampled 50 clips from every video and followed the same evaluation protocol as for Sports across the 3 suggested folds. We reached out to the authors of [22] to obtain the YouTube video IDs of UCF-101 videos, but unfortunately  these were not available and hence we cannot guarantee that the Sports-1M dataset has no overlap with UCF-101. However, these concerns are somewhat mitigated as we only use a few sampled clips from every video. We use the Slow Fusion network in our UCF-101 experiments as it provides the best performance on Sports-1M. The results of the experiments can be seen on Table 3. Interestingly, retraining the softmax layer alone does not perform best (possibly because the high-level features are too specific to sports) and the other extreme of fine-tuning all layers is also not adequate (likely due to overfitting). Instead, the best performance is obtained by taking a balanced approach and retraining the top few layers of the network. Lastly, training the entire network from scratch consistently leads to massive overfitting and dismal performance.\n\nPerformance by group. We further break down our performance by 5 broad groups of classes present in the UCF-101 dataset. We compute the average precision of every class and then compute the mean average precision over classes in each group. As can be seen from Table 4, large fractions of our performance can be attributed to the Sports categories in UCF-101, but the other groups still display impressive performance considering that the only way to observe these types of frames in the training data is due to label noise. Moreover, the gain in performance when retraining only the top to retraining the top 3 layers is almost entirely due to improvements on non-Sports categories: Sports performance only decreases from 0.80 to 0.79, while mAP improves on all other categories.\n\n\nConclusions\n\nWe studied the performance of convolutional neural networks in large-scale video classification. We found that CNN architectures are capable of learning powerful features from weakly-labeled data that far surpass featurebased methods in performance and that these benefits are surprisingly robust to details of the connectivity of the architectures in time. Qualitative examination of network outputs and confusion matrices reveals interpretable errors.\n\nOur results indicate that while the performance is not particularly sensitive to the architectural details of the connectivity in time, a Slow Fusion model consistently performs better than the early and late fusion alternatives. Sur-prisingly, we find that a single-frame model already displays very strong performance, suggesting that local motion cues may not be critically important, even for a dynamic dataset such as Sports. An alternative theory is that more careful treatment of camera motion may be necessary (for example by extracting features in the local coordinate system of a tracked point, as seen in [25]), but this requires significant changes to a CNN architecture that we leave for future work. We also identified mixed-resolution architectures that consist of a low-resolution context and a highresolution fovea stream as an effective way of speeding up CNNs without sacrificing accuracy.\n\nOur transfer learning experiments on UCF-101 suggest that the learned features are generic and generalize other video classification tasks. In particular, we achieved the highest transfer learning performance by retraining the top 3 layers of the network.\n\nIn future work we hope to incorporate broader categories in the dataset to obtain more powerful and generic features, investigate approaches that explicitly reason about camera motion, and explore recurrent neural networks as a more powerful technique for combining clip-level predictions into global video-level predictions.\n\nFigure 4 :\n4Predictions on Sports-1M test data. Blue (first row) indicates ground truth label and the bars below show model predictions sorted in decreasing confidence. Green and red distinguish correct and incorrect predictions, respectively.\n\nFigure 3 :\n3Filters learned on first layer of a multiresolution network. Left: context stream, Right: fovea stream. Notably, the fovea stream learns grayscale, high-frequency features while the context stream models lower frequencies and colors. GIFs of moving video features can be found on our website (linked on first page).number of nodes in all layers.\n\n\nResults on the 200,000 videos of the Sports-1M test set. Hit@k values indicate the fraction of test samples that contained at least one of the ground truth labels in the top k predictions.Model \nClip Hit@1 Video Hit@1 Video Hit@5 \n\nFeature Histograms + Neural Net \n-\n55.3 \n-\nSingle-Frame \n41.1 \n59.3 \n77.7 \nSingle-Frame + Multires \n42.4 \n60.0 \n78.5 \nSingle-Frame Fovea Only \n30.0 \n49.9 \n72.8 \nSingle-Frame Context Only \n38.1 \n56.0 \n77.2 \nEarly Fusion \n38.9 \n57.7 \n76.8 \nLate Fusion \n40.7 \n59.3 \n78.7 \nSlow Fusion \n41.9 \n60.9 \n80.2 \nCNN Average (Single+Early+Late+Slow) \n41.4 \n63.9 \n82.4 \nTable 1: \n\nTable 2 :\n2Classes for which a (motion-aware) Slow Fusion \nCNN performs better than the single-frame CNN (left) and \nvice versa (right), as measured by difference in per-class \naverage precision. \n\n\n\nTable 4 :\n4Mean Average Precision of the Slow Fusion network on UCF-101 classes broken down by category groups.\nAcknowledgments:We thank Saurabh Singh, Abhinav Shrivastava, Jay Yagnik, Alex Krizhevsky, Quoc Le, Jeff Dean and Rajat Monga for helpful discussions.\nSequential deep learning for human action recognition. M Baccouche, F Mamalet, C Wolf, C Garcia, A Baskurt, Human Behavior Understanding. Springer23M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and A. Baskurt. Sequential deep learning for human action recognition. In Human Behavior Understanding, pages 29- 39. Springer, 2011. 2, 3\n\nDeep neural networks segment neuronal membranes in electron microscopy images. D Ciresan, A Giusti, J Schmidhuber, NIPS. D. Ciresan, A. Giusti, J. Schmidhuber, et al. Deep neural net- works segment neuronal membranes in electron microscopy images. In NIPS, 2012. 1\n\nLearning hierarchical features for scene labeling. L N Clement Farabet, Camille Couprie, Y Lecun, PAMI35L. N. Clement Farabet, Camille Couprie and Y. LeCun. Learning hierarchical features for scene labeling. PAMI, 35(8), 2013. 1, 2\n\nIndoor semantic segmentation using depth information. C Couprie, C Farabet, L Najman, Y Lecun, Internatinal Conference on Learning Representation. 2C. Couprie, C. Farabet, L. Najman, and Y. LeCun. Indoor semantic segmentation using depth information. Internatinal Conference on Learning Representation, 2013. 2\n\nHistograms of oriented gradients for human detection. N Dalal, B Triggs, CVPR. 1N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, volume 1, 2005. 5\n\nLarge scale distributed deep networks. J Dean, G Corrado, R Monga, K Chen, M Devin, Q V Le, M Z Mao, M Ranzato, A Senior, P Tucker, K Yang, A Y Ng, NIPS. J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Y. Ng. Large scale distributed deep networks. In NIPS, 2012. 4\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, CVPR. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 2\n\nBehavior recognition via sparse spatio-temporal features. P Doll\u00e1r, V Rabaud, G Cottrell, S Belongie, International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance. 25P. Doll\u00e1r, V. Rabaud, G. Cottrell, and S. Belongie. Behav- ior recognition via sparse spatio-temporal features. In Inter- national Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance, 2005. 2, 5\n\nRich feature hierarchies for accurate object detection and semantic segmentation. R Girshick, J Donahue, T Darrell, J Malik, CVPR. 1R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea- ture hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 1, 2\n\n3D convolutional neural networks for human action recognition. S Ji, W Xu, M Yang, K Yu, PAMI353S. Ji, W. Xu, M. Yang, and K. Yu. 3D convolutional neural networks for human action recognition. PAMI, 35(1):221- 231, 2013. 2, 3\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G Hinton, NIPS. A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet clas- sification with deep convolutional neural networks. In NIPS, 2012. 1, 2, 3, 4\n\nOn space-time interest points. I Laptev, IJCV. 642-3I. Laptev. On space-time interest points. IJCV, 64(2-3):107- 123, 2005. 2\n\nLearning realistic human actions from movies. I Laptev, M Marszalek, C Schmid, B Rozenfeld, CVPR. I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In CVPR, 2008. 2\n\nLearning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis. Q V Le, W Y Zou, S Y Yeung, A Y Ng, CVPR. Q. V. Le, W. Y. Zou, S. Y. Yeung, and A. Y. Ng. Learn- ing hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis. In CVPR, 2011. 2\n\nGradientbased learning applied to document recognition. Proceedings of the IEEE. Y Lecun, L Bottou, Y Bengio, P Haffner, 86Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278-2324, 1998. 1, 2\n\nRecognizing realistic actions from videos \"in the wild. J Liu, J Luo, M Shah, CVPR. J. Liu, J. Luo, and M. Shah. Recognizing realistic actions from videos \"in the wild\". In CVPR, 2009. 2\n\nModeling temporal structure of decomposable motion segments for activity classification. J C Niebles, C.-W Chen, L Fei-Fei, ECCV. SpringerJ. C. Niebles, C.-W. Chen, and L. Fei-Fei. Modeling tempo- ral structure of decomposable motion segments for activity classification. In ECCV, pages 392-405. Springer, 2010. 2\n\nA S Razavian, H Azizpour, J Sullivan, S Carlsson, arXiv:1403.6382CNN features off-the-shelf: an astounding baseline for recognition. 1arXiv preprintA. S. Razavian, H. Azizpour, J. Sullivan, and S. Carls- son. CNN features off-the-shelf: an astounding baseline for recognition. arXiv preprint arXiv:1403.6382, 2014. 1, 2\n\nConvolutional neural networks applied to house numbers digit classification. P Sermanet, S Chintala, Y Lecun, ICPR. P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neu- ral networks applied to house numbers digit classification. In ICPR, 2012. 2\n\nOverFeat: Integrated recognition, localization and detection using convolutional networks. P Sermanet, D Eigen, X Zhang, M Mathieu, R Fergus, Y Lecun, arXiv:1312.62291arXiv preprintP. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. OverFeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229, 2013. 1, 2\n\nVideo Google: A text retrieval approach to object matching in videos. J Sivic, A Zisserman, ICCV. J. Sivic and A. Zisserman. Video Google: A text retrieval approach to object matching in videos. In ICCV, 2003. 2\n\nUCF101: A dataset of 101 human actions classes from videos in the wild. K Soomro, A R Zamir, M Shah, arXiv:1212.040227arXiv preprintK. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 2, 7\n\nConvolutional learning of spatio-temporal features. G W Taylor, R Fergus, Y Lecun, C Bregler, ECCV. SpringerG. W. Taylor, R. Fergus, Y. LeCun, and C. Bregler. Con- volutional learning of spatio-temporal features. In ECCV. Springer, 2010. 2\n\nA statistical approach to texture classification from single images. M Varma, A Zisserman, IJCV. 621-2M. Varma and A. Zisserman. A statistical approach to tex- ture classification from single images. IJCV, 62(1-2):61-81, 2005. 5\n\nAction recognition by dense trajectories. H Wang, A Klaser, C Schmid, C.-L Liu, CVPR. IEEE. H. Wang, A. Klaser, C. Schmid, and C.-L. Liu. Action recog- nition by dense trajectories. In CVPR. IEEE, 2011. 2, 8\n\nEvaluation of local spatio-temporal features for action recognition. H Wang, M M Ullah, A Klaser, I Laptev, C Schmid, BMVC. H. Wang, M. M. Ullah, A. Klaser, I. Laptev, C. Schmid, et al. Evaluation of local spatio-temporal features for action recog- nition. In BMVC, 2009. 2\n\nDiscriminative tag learning on youtube videos with latent sub-tags. W Yang, G Toderici, CVPR. W. Yang and G. Toderici. Discriminative tag learning on youtube videos with latent sub-tags. In CVPR, 2011. 5\n\nM D Zeiler, R Fergus, arXiv:1311.2901Visualizing and understanding convolutional neural networks. 13arXiv preprintM. D. Zeiler and R. Fergus. Visualizing and under- standing convolutional neural networks. arXiv preprint arXiv:1311.2901, 2013. 1, 3\n", "annotations": {"author": "[{\"end\":180,\"start\":71},{\"end\":236,\"start\":181},{\"end\":289,\"start\":237},{\"end\":339,\"start\":290},{\"end\":397,\"start\":340},{\"end\":484,\"start\":398}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":78},{\"end\":196,\"start\":188},{\"end\":251,\"start\":245},{\"end\":302,\"start\":297},{\"end\":356,\"start\":346},{\"end\":408,\"start\":401}]", "author_first_name": "[{\"end\":77,\"start\":71},{\"end\":187,\"start\":181},{\"end\":244,\"start\":237},{\"end\":296,\"start\":290},{\"end\":345,\"start\":340},{\"end\":400,\"start\":398}]", "author_affiliation": "[{\"end\":129,\"start\":113},{\"end\":179,\"start\":131},{\"end\":235,\"start\":219},{\"end\":288,\"start\":272},{\"end\":338,\"start\":322},{\"end\":396,\"start\":380},{\"end\":483,\"start\":435}]", "title": "[{\"end\":68,\"start\":1},{\"end\":552,\"start\":485}]", "venue": null, "abstract": "[{\"end\":1590,\"start\":554}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1868,\"start\":1864},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2053,\"start\":2049},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2055,\"start\":2053},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2057,\"start\":2055},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2060,\"start\":2057},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2062,\"start\":2060},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2065,\"start\":2062},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2359,\"start\":2355},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6013,\"start\":6009},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6016,\"start\":6013},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6019,\"start\":6016},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6022,\"start\":6019},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6150,\"start\":6146},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6193,\"start\":6189},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6195,\"start\":6193},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6488,\"start\":6484},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6674,\"start\":6670},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7671,\"start\":7667},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7694,\"start\":7690},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7696,\"start\":7694},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7716,\"start\":7713},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7741,\"start\":7738},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7784,\"start\":7780},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7858,\"start\":7855},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8019,\"start\":8015},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8616,\"start\":8612},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8743,\"start\":8740},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8841,\"start\":8838},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8850,\"start\":8846},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9246,\"start\":9242},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9285,\"start\":9281},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11036,\"start\":11032},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11599,\"start\":11595},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13137,\"start\":13134},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13140,\"start\":13137},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14336,\"start\":14332},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16213,\"start\":16210},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16665,\"start\":16661},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21143,\"start\":21139},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21182,\"start\":21179},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21195,\"start\":21191},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21208,\"start\":21205},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25380,\"start\":25376},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25999,\"start\":25995},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27879,\"start\":27875},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30673,\"start\":30669}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31790,\"start\":31546},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32149,\"start\":31791},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32749,\"start\":32150},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32949,\"start\":32750},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":33062,\"start\":32950}]", "paragraph": "[{\"end\":2706,\"start\":1606},{\"end\":3228,\"start\":2708},{\"end\":3742,\"start\":3230},{\"end\":4594,\"start\":3744},{\"end\":5167,\"start\":4596},{\"end\":5216,\"start\":5169},{\"end\":5518,\"start\":5218},{\"end\":5755,\"start\":5520},{\"end\":5946,\"start\":5757},{\"end\":6638,\"start\":5963},{\"end\":8020,\"start\":6640},{\"end\":9351,\"start\":8022},{\"end\":10310,\"start\":9362},{\"end\":10824,\"start\":10346},{\"end\":11740,\"start\":10826},{\"end\":12232,\"start\":11742},{\"end\":12677,\"start\":12234},{\"end\":13552,\"start\":12679},{\"end\":14213,\"start\":13577},{\"end\":14661,\"start\":14215},{\"end\":15310,\"start\":14663},{\"end\":15808,\"start\":15312},{\"end\":16139,\"start\":15810},{\"end\":16612,\"start\":16152},{\"end\":17256,\"start\":16614},{\"end\":17453,\"start\":17268},{\"end\":17947,\"start\":17482},{\"end\":18593,\"start\":17949},{\"end\":19309,\"start\":18595},{\"end\":19948,\"start\":19311},{\"end\":20564,\"start\":19950},{\"end\":23005,\"start\":20566},{\"end\":23649,\"start\":23007},{\"end\":24857,\"start\":23651},{\"end\":25335,\"start\":24859},{\"end\":25631,\"start\":25363},{\"end\":26584,\"start\":25676},{\"end\":26867,\"start\":26586},{\"end\":27106,\"start\":26869},{\"end\":27433,\"start\":27108},{\"end\":27573,\"start\":27435},{\"end\":27665,\"start\":27575},{\"end\":28800,\"start\":27667},{\"end\":29582,\"start\":28802},{\"end\":30051,\"start\":29598},{\"end\":30961,\"start\":30053},{\"end\":31218,\"start\":30963},{\"end\":31545,\"start\":31220}]", "formula": null, "table_ref": "[{\"end\":22338,\"start\":22331},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24068,\"start\":24060},{\"end\":25539,\"start\":25532},{\"end\":28322,\"start\":28315},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":29070,\"start\":29063}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1604,\"start\":1592},{\"attributes\":{\"n\":\"2.\"},\"end\":5961,\"start\":5949},{\"attributes\":{\"n\":\"3.\"},\"end\":9360,\"start\":9354},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10344,\"start\":10313},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13575,\"start\":13555},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16150,\"start\":16142},{\"attributes\":{\"n\":\"4.\"},\"end\":17266,\"start\":17259},{\"attributes\":{\"n\":\"4.1.\"},\"end\":17480,\"start\":17456},{\"end\":25343,\"start\":25338},{\"end\":25361,\"start\":25346},{\"attributes\":{\"n\":\"4.2.\"},\"end\":25674,\"start\":25634},{\"attributes\":{\"n\":\"5.\"},\"end\":29596,\"start\":29585},{\"end\":31557,\"start\":31547},{\"end\":31802,\"start\":31792},{\"end\":32760,\"start\":32751},{\"end\":32960,\"start\":32951}]", "table": "[{\"end\":32749,\"start\":32340},{\"end\":32949,\"start\":32762}]", "figure_caption": "[{\"end\":31790,\"start\":31559},{\"end\":32149,\"start\":31804},{\"end\":32340,\"start\":32152},{\"end\":33062,\"start\":32962}]", "figure_ref": "[{\"end\":10059,\"start\":10051},{\"end\":10435,\"start\":10427},{\"end\":14845,\"start\":14836},{\"end\":15432,\"start\":15424},{\"end\":21953,\"start\":21945},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22108,\"start\":22100},{\"end\":24130,\"start\":24122},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24715,\"start\":24707},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24885,\"start\":24877}]", "bib_author_first_name": "[{\"end\":33269,\"start\":33268},{\"end\":33282,\"start\":33281},{\"end\":33293,\"start\":33292},{\"end\":33301,\"start\":33300},{\"end\":33311,\"start\":33310},{\"end\":33627,\"start\":33626},{\"end\":33638,\"start\":33637},{\"end\":33648,\"start\":33647},{\"end\":33865,\"start\":33864},{\"end\":33867,\"start\":33866},{\"end\":33892,\"start\":33885},{\"end\":33903,\"start\":33902},{\"end\":34101,\"start\":34100},{\"end\":34112,\"start\":34111},{\"end\":34123,\"start\":34122},{\"end\":34133,\"start\":34132},{\"end\":34413,\"start\":34412},{\"end\":34422,\"start\":34421},{\"end\":34584,\"start\":34583},{\"end\":34592,\"start\":34591},{\"end\":34603,\"start\":34602},{\"end\":34612,\"start\":34611},{\"end\":34620,\"start\":34619},{\"end\":34629,\"start\":34628},{\"end\":34631,\"start\":34630},{\"end\":34637,\"start\":34636},{\"end\":34639,\"start\":34638},{\"end\":34646,\"start\":34645},{\"end\":34657,\"start\":34656},{\"end\":34667,\"start\":34666},{\"end\":34677,\"start\":34676},{\"end\":34685,\"start\":34684},{\"end\":34687,\"start\":34686},{\"end\":34937,\"start\":34936},{\"end\":34945,\"start\":34944},{\"end\":34953,\"start\":34952},{\"end\":34966,\"start\":34962},{\"end\":34972,\"start\":34971},{\"end\":34978,\"start\":34977},{\"end\":35187,\"start\":35186},{\"end\":35197,\"start\":35196},{\"end\":35207,\"start\":35206},{\"end\":35219,\"start\":35218},{\"end\":35650,\"start\":35649},{\"end\":35662,\"start\":35661},{\"end\":35673,\"start\":35672},{\"end\":35684,\"start\":35683},{\"end\":35919,\"start\":35918},{\"end\":35925,\"start\":35924},{\"end\":35931,\"start\":35930},{\"end\":35939,\"start\":35938},{\"end\":36148,\"start\":36147},{\"end\":36162,\"start\":36161},{\"end\":36175,\"start\":36174},{\"end\":36360,\"start\":36359},{\"end\":36502,\"start\":36501},{\"end\":36512,\"start\":36511},{\"end\":36525,\"start\":36524},{\"end\":36535,\"start\":36534},{\"end\":36788,\"start\":36787},{\"end\":36790,\"start\":36789},{\"end\":36796,\"start\":36795},{\"end\":36798,\"start\":36797},{\"end\":36805,\"start\":36804},{\"end\":36807,\"start\":36806},{\"end\":36816,\"start\":36815},{\"end\":36818,\"start\":36817},{\"end\":37095,\"start\":37094},{\"end\":37104,\"start\":37103},{\"end\":37114,\"start\":37113},{\"end\":37124,\"start\":37123},{\"end\":37356,\"start\":37355},{\"end\":37363,\"start\":37362},{\"end\":37370,\"start\":37369},{\"end\":37577,\"start\":37576},{\"end\":37579,\"start\":37578},{\"end\":37593,\"start\":37589},{\"end\":37601,\"start\":37600},{\"end\":37803,\"start\":37802},{\"end\":37805,\"start\":37804},{\"end\":37817,\"start\":37816},{\"end\":37829,\"start\":37828},{\"end\":37841,\"start\":37840},{\"end\":38201,\"start\":38200},{\"end\":38213,\"start\":38212},{\"end\":38225,\"start\":38224},{\"end\":38468,\"start\":38467},{\"end\":38480,\"start\":38479},{\"end\":38489,\"start\":38488},{\"end\":38498,\"start\":38497},{\"end\":38509,\"start\":38508},{\"end\":38519,\"start\":38518},{\"end\":38833,\"start\":38832},{\"end\":38842,\"start\":38841},{\"end\":39048,\"start\":39047},{\"end\":39058,\"start\":39057},{\"end\":39060,\"start\":39059},{\"end\":39069,\"start\":39068},{\"end\":39313,\"start\":39312},{\"end\":39315,\"start\":39314},{\"end\":39325,\"start\":39324},{\"end\":39335,\"start\":39334},{\"end\":39344,\"start\":39343},{\"end\":39571,\"start\":39570},{\"end\":39580,\"start\":39579},{\"end\":39774,\"start\":39773},{\"end\":39782,\"start\":39781},{\"end\":39792,\"start\":39791},{\"end\":39805,\"start\":39801},{\"end\":40010,\"start\":40009},{\"end\":40018,\"start\":40017},{\"end\":40020,\"start\":40019},{\"end\":40029,\"start\":40028},{\"end\":40039,\"start\":40038},{\"end\":40049,\"start\":40048},{\"end\":40284,\"start\":40283},{\"end\":40292,\"start\":40291},{\"end\":40421,\"start\":40420},{\"end\":40423,\"start\":40422},{\"end\":40433,\"start\":40432}]", "bib_author_last_name": "[{\"end\":33279,\"start\":33270},{\"end\":33290,\"start\":33283},{\"end\":33298,\"start\":33294},{\"end\":33308,\"start\":33302},{\"end\":33319,\"start\":33312},{\"end\":33635,\"start\":33628},{\"end\":33645,\"start\":33639},{\"end\":33660,\"start\":33649},{\"end\":33883,\"start\":33868},{\"end\":33900,\"start\":33893},{\"end\":33909,\"start\":33904},{\"end\":34109,\"start\":34102},{\"end\":34120,\"start\":34113},{\"end\":34130,\"start\":34124},{\"end\":34139,\"start\":34134},{\"end\":34419,\"start\":34414},{\"end\":34429,\"start\":34423},{\"end\":34589,\"start\":34585},{\"end\":34600,\"start\":34593},{\"end\":34609,\"start\":34604},{\"end\":34617,\"start\":34613},{\"end\":34626,\"start\":34621},{\"end\":34634,\"start\":34632},{\"end\":34643,\"start\":34640},{\"end\":34654,\"start\":34647},{\"end\":34664,\"start\":34658},{\"end\":34674,\"start\":34668},{\"end\":34682,\"start\":34678},{\"end\":34690,\"start\":34688},{\"end\":34942,\"start\":34938},{\"end\":34950,\"start\":34946},{\"end\":34960,\"start\":34954},{\"end\":34969,\"start\":34967},{\"end\":34975,\"start\":34973},{\"end\":34986,\"start\":34979},{\"end\":35194,\"start\":35188},{\"end\":35204,\"start\":35198},{\"end\":35216,\"start\":35208},{\"end\":35228,\"start\":35220},{\"end\":35659,\"start\":35651},{\"end\":35670,\"start\":35663},{\"end\":35681,\"start\":35674},{\"end\":35690,\"start\":35685},{\"end\":35922,\"start\":35920},{\"end\":35928,\"start\":35926},{\"end\":35936,\"start\":35932},{\"end\":35942,\"start\":35940},{\"end\":36159,\"start\":36149},{\"end\":36172,\"start\":36163},{\"end\":36182,\"start\":36176},{\"end\":36367,\"start\":36361},{\"end\":36509,\"start\":36503},{\"end\":36522,\"start\":36513},{\"end\":36532,\"start\":36526},{\"end\":36545,\"start\":36536},{\"end\":36793,\"start\":36791},{\"end\":36802,\"start\":36799},{\"end\":36813,\"start\":36808},{\"end\":36821,\"start\":36819},{\"end\":37101,\"start\":37096},{\"end\":37111,\"start\":37105},{\"end\":37121,\"start\":37115},{\"end\":37132,\"start\":37125},{\"end\":37360,\"start\":37357},{\"end\":37367,\"start\":37364},{\"end\":37375,\"start\":37371},{\"end\":37587,\"start\":37580},{\"end\":37598,\"start\":37594},{\"end\":37609,\"start\":37602},{\"end\":37814,\"start\":37806},{\"end\":37826,\"start\":37818},{\"end\":37838,\"start\":37830},{\"end\":37850,\"start\":37842},{\"end\":38210,\"start\":38202},{\"end\":38222,\"start\":38214},{\"end\":38231,\"start\":38226},{\"end\":38477,\"start\":38469},{\"end\":38486,\"start\":38481},{\"end\":38495,\"start\":38490},{\"end\":38506,\"start\":38499},{\"end\":38516,\"start\":38510},{\"end\":38525,\"start\":38520},{\"end\":38839,\"start\":38834},{\"end\":38852,\"start\":38843},{\"end\":39055,\"start\":39049},{\"end\":39066,\"start\":39061},{\"end\":39074,\"start\":39070},{\"end\":39322,\"start\":39316},{\"end\":39332,\"start\":39326},{\"end\":39341,\"start\":39336},{\"end\":39352,\"start\":39345},{\"end\":39577,\"start\":39572},{\"end\":39590,\"start\":39581},{\"end\":39779,\"start\":39775},{\"end\":39789,\"start\":39783},{\"end\":39799,\"start\":39793},{\"end\":39809,\"start\":39806},{\"end\":40015,\"start\":40011},{\"end\":40026,\"start\":40021},{\"end\":40036,\"start\":40030},{\"end\":40046,\"start\":40040},{\"end\":40056,\"start\":40050},{\"end\":40289,\"start\":40285},{\"end\":40301,\"start\":40293},{\"end\":40430,\"start\":40424},{\"end\":40440,\"start\":40434}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":12591063},\"end\":33545,\"start\":33213},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":7725346},\"end\":33811,\"start\":33547},{\"attributes\":{\"id\":\"b2\"},\"end\":34044,\"start\":33813},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6681692},\"end\":34356,\"start\":34046},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":206590483},\"end\":34542,\"start\":34358},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":372467},\"end\":34881,\"start\":34544},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":57246310},\"end\":35126,\"start\":34883},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1956774},\"end\":35565,\"start\":35128},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":215827080},\"end\":35853,\"start\":35567},{\"attributes\":{\"id\":\"b9\"},\"end\":36080,\"start\":35855},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":195908774},\"end\":36326,\"start\":36082},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2619278},\"end\":36453,\"start\":36328},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":12365014},\"end\":36669,\"start\":36455},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6006618},\"end\":37011,\"start\":36671},{\"attributes\":{\"id\":\"b14\"},\"end\":37297,\"start\":37013},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206597309},\"end\":37485,\"start\":37299},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14779543},\"end\":37800,\"start\":37487},{\"attributes\":{\"doi\":\"arXiv:1403.6382\",\"id\":\"b17\"},\"end\":38121,\"start\":37802},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6788752},\"end\":38374,\"start\":38123},{\"attributes\":{\"doi\":\"arXiv:1312.6229\",\"id\":\"b19\"},\"end\":38760,\"start\":38376},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14457153},\"end\":38973,\"start\":38762},{\"attributes\":{\"doi\":\"arXiv:1212.0402\",\"id\":\"b21\"},\"end\":39258,\"start\":38975},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16347832},\"end\":39499,\"start\":39260},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2313314},\"end\":39729,\"start\":39501},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":13537104},\"end\":39938,\"start\":39731},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6367640},\"end\":40213,\"start\":39940},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":8094757},\"end\":40418,\"start\":40215},{\"attributes\":{\"doi\":\"arXiv:1311.2901\",\"id\":\"b27\"},\"end\":40667,\"start\":40420}]", "bib_title": "[{\"end\":33266,\"start\":33213},{\"end\":33624,\"start\":33547},{\"end\":34098,\"start\":34046},{\"end\":34410,\"start\":34358},{\"end\":34581,\"start\":34544},{\"end\":34934,\"start\":34883},{\"end\":35184,\"start\":35128},{\"end\":35647,\"start\":35567},{\"end\":36145,\"start\":36082},{\"end\":36357,\"start\":36328},{\"end\":36499,\"start\":36455},{\"end\":36785,\"start\":36671},{\"end\":37353,\"start\":37299},{\"end\":37574,\"start\":37487},{\"end\":38198,\"start\":38123},{\"end\":38830,\"start\":38762},{\"end\":39310,\"start\":39260},{\"end\":39568,\"start\":39501},{\"end\":39771,\"start\":39731},{\"end\":40007,\"start\":39940},{\"end\":40281,\"start\":40215}]", "bib_author": "[{\"end\":33281,\"start\":33268},{\"end\":33292,\"start\":33281},{\"end\":33300,\"start\":33292},{\"end\":33310,\"start\":33300},{\"end\":33321,\"start\":33310},{\"end\":33637,\"start\":33626},{\"end\":33647,\"start\":33637},{\"end\":33662,\"start\":33647},{\"end\":33885,\"start\":33864},{\"end\":33902,\"start\":33885},{\"end\":33911,\"start\":33902},{\"end\":34111,\"start\":34100},{\"end\":34122,\"start\":34111},{\"end\":34132,\"start\":34122},{\"end\":34141,\"start\":34132},{\"end\":34421,\"start\":34412},{\"end\":34431,\"start\":34421},{\"end\":34591,\"start\":34583},{\"end\":34602,\"start\":34591},{\"end\":34611,\"start\":34602},{\"end\":34619,\"start\":34611},{\"end\":34628,\"start\":34619},{\"end\":34636,\"start\":34628},{\"end\":34645,\"start\":34636},{\"end\":34656,\"start\":34645},{\"end\":34666,\"start\":34656},{\"end\":34676,\"start\":34666},{\"end\":34684,\"start\":34676},{\"end\":34692,\"start\":34684},{\"end\":34944,\"start\":34936},{\"end\":34952,\"start\":34944},{\"end\":34962,\"start\":34952},{\"end\":34971,\"start\":34962},{\"end\":34977,\"start\":34971},{\"end\":34988,\"start\":34977},{\"end\":35196,\"start\":35186},{\"end\":35206,\"start\":35196},{\"end\":35218,\"start\":35206},{\"end\":35230,\"start\":35218},{\"end\":35661,\"start\":35649},{\"end\":35672,\"start\":35661},{\"end\":35683,\"start\":35672},{\"end\":35692,\"start\":35683},{\"end\":35924,\"start\":35918},{\"end\":35930,\"start\":35924},{\"end\":35938,\"start\":35930},{\"end\":35944,\"start\":35938},{\"end\":36161,\"start\":36147},{\"end\":36174,\"start\":36161},{\"end\":36184,\"start\":36174},{\"end\":36369,\"start\":36359},{\"end\":36511,\"start\":36501},{\"end\":36524,\"start\":36511},{\"end\":36534,\"start\":36524},{\"end\":36547,\"start\":36534},{\"end\":36795,\"start\":36787},{\"end\":36804,\"start\":36795},{\"end\":36815,\"start\":36804},{\"end\":36823,\"start\":36815},{\"end\":37103,\"start\":37094},{\"end\":37113,\"start\":37103},{\"end\":37123,\"start\":37113},{\"end\":37134,\"start\":37123},{\"end\":37362,\"start\":37355},{\"end\":37369,\"start\":37362},{\"end\":37377,\"start\":37369},{\"end\":37589,\"start\":37576},{\"end\":37600,\"start\":37589},{\"end\":37611,\"start\":37600},{\"end\":37816,\"start\":37802},{\"end\":37828,\"start\":37816},{\"end\":37840,\"start\":37828},{\"end\":37852,\"start\":37840},{\"end\":38212,\"start\":38200},{\"end\":38224,\"start\":38212},{\"end\":38233,\"start\":38224},{\"end\":38479,\"start\":38467},{\"end\":38488,\"start\":38479},{\"end\":38497,\"start\":38488},{\"end\":38508,\"start\":38497},{\"end\":38518,\"start\":38508},{\"end\":38527,\"start\":38518},{\"end\":38841,\"start\":38832},{\"end\":38854,\"start\":38841},{\"end\":39057,\"start\":39047},{\"end\":39068,\"start\":39057},{\"end\":39076,\"start\":39068},{\"end\":39324,\"start\":39312},{\"end\":39334,\"start\":39324},{\"end\":39343,\"start\":39334},{\"end\":39354,\"start\":39343},{\"end\":39579,\"start\":39570},{\"end\":39592,\"start\":39579},{\"end\":39781,\"start\":39773},{\"end\":39791,\"start\":39781},{\"end\":39801,\"start\":39791},{\"end\":39811,\"start\":39801},{\"end\":40017,\"start\":40009},{\"end\":40028,\"start\":40017},{\"end\":40038,\"start\":40028},{\"end\":40048,\"start\":40038},{\"end\":40058,\"start\":40048},{\"end\":40291,\"start\":40283},{\"end\":40303,\"start\":40291},{\"end\":40432,\"start\":40420},{\"end\":40442,\"start\":40432}]", "bib_venue": "[{\"end\":33349,\"start\":33321},{\"end\":33666,\"start\":33662},{\"end\":33862,\"start\":33813},{\"end\":34191,\"start\":34141},{\"end\":34435,\"start\":34431},{\"end\":34696,\"start\":34692},{\"end\":34992,\"start\":34988},{\"end\":35331,\"start\":35230},{\"end\":35696,\"start\":35692},{\"end\":35916,\"start\":35855},{\"end\":36188,\"start\":36184},{\"end\":36373,\"start\":36369},{\"end\":36551,\"start\":36547},{\"end\":36827,\"start\":36823},{\"end\":37092,\"start\":37013},{\"end\":37381,\"start\":37377},{\"end\":37615,\"start\":37611},{\"end\":37933,\"start\":37867},{\"end\":38237,\"start\":38233},{\"end\":38465,\"start\":38376},{\"end\":38858,\"start\":38854},{\"end\":39045,\"start\":38975},{\"end\":39358,\"start\":39354},{\"end\":39596,\"start\":39592},{\"end\":39821,\"start\":39811},{\"end\":40062,\"start\":40058},{\"end\":40307,\"start\":40303},{\"end\":40516,\"start\":40457}]"}}}, "year": 2023, "month": 12, "day": 17}