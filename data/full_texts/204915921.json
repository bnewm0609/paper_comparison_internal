{"id": 204915921, "updated": "2023-09-27 22:57:24.574", "metadata": {"title": "QASC: A Dataset for Question Answering via Sentence Composition", "authors": "[{\"first\":\"Tushar\",\"last\":\"Khot\",\"middle\":[]},{\"first\":\"Peter\",\"last\":\"Clark\",\"middle\":[]},{\"first\":\"Michal\",\"last\":\"Guerquin\",\"middle\":[]},{\"first\":\"Peter\",\"last\":\"Jansen\",\"middle\":[]},{\"first\":\"Ashish\",\"last\":\"Sabharwal\",\"middle\":[]}]", "venue": "ArXiv", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": ".", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1910.11473", "mag": "2996848635", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/KhotCGJS20", "doi": "10.1609/aaai.v34i05.6319"}}, "content": {"source": {"pdf_hash": "d913d5cda1106eb2d2d5d6d87a64ed23b022d876", "pdf_src": "Anansi", "pdf_uri": "[\"https://ojs.aaai.org/index.php/AAAI/article/download/6319/6175\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://ojs.aaai.org/index.php/AAAI/article/download/6319/6175", "status": "GOLD"}}, "grobid": {"id": "69314d3b4ef0429f8a34d9ee8ca28887f191e656", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d913d5cda1106eb2d2d5d6d87a64ed23b022d876.txt", "contents": "\nQASC: A Dataset for Question Answering via Sentence Composition\n\n\nTushar Khot \nPeter Clark \nMichal Guerquin \nPeter Jansen pajansen@email.arizona.edu \nUniversity of Arizona\nTucsonAZU.S.A\n\nAshish Sabharwal ashishs@allenai.org \n\nAllen Institute for AI\nSeattleWAU.S.A\n\nQASC: A Dataset for Question Answering via Sentence Composition\n\nComposing knowledge from multiple pieces of texts is a key challenge in multi-hop question answering. We present a multi-hop reasoning dataset, Question Answering via Sentence Composition (QASC), that requires retrieving facts from a large corpus and composing them to answer a multiple-choice question. QASC is the first dataset to offer two desirable properties: (a) the facts to be composed are annotated in a large corpus, and (b) the decomposition into these facts is not evident from the question itself. The latter makes retrieval challenging as the system must introduce new concepts or relations in order to discover potential decompositions. Further, the reasoning model must then learn to identify valid compositions of these retrieved facts using commonsense reasoning. To help address these challenges, we provide annotation for supporting facts as well as their composition. Guided by these annotations, we present a two-step approach to mitigate the retrieval challenges. We use other multiplechoice datasets as additional training data to strengthen the reasoning model. Our proposed approach improves over current state-of-the-art language models by 11% (absolute). The reasoning and retrieval problems, however, remain unsolved as this model still lags by 20% behind human performance.\n\nIntroduction\n\nSeveral multi-hop question-answering (QA) datasets have been proposed to promote research on multi-sentence machine comprehension. On one hand, many of these datasets (Mihaylov et al. 2018;Clark et al. 2018;Welbl, Stenetorp, and Riedel 2018;Talmor and Berant 2018) do not come annotated with sentences or documents that can be combined to produce an answer. Models must thus learn to reason without direct supervision. On the other hand, datasets that come with such annotations involve either single-document questions (Khashabi et al. 2018a) leading to a strong focus on coreference resolution and entity tracking, or multi-document questions (Yang et al. 2018) whose decomposition into simpler single-hop queries is often evident from the question itself.\n\nWe propose a novel dataset, Question Answering via Sentence Composition (QASC; pronounced kask) of 9,980 Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. fL: Wind is used for producing electricity .\n\nComposed fact fC : Differential heating of air can be harnessed for electricity production . Training data includes the associated facts f S and f L shown above, as well as their composition f C . The term wind connects f S and f L , but appears neither in f C nor in the question. Further, decomposing the question relation \"harnessed for\" into f S and f L requires introducing the new relation \"produces\" in f S . The question can be answered by using broad knowledge to compose these facts together and infer f C .\n\nmulti-hop multiple-choice questions (MCQs) where simple syntactic cues are insufficient to determine how to decompose the question into simpler queries. Fig. 1 gives an example, where the question is answered by decomposing its main relation \"harnessed for\" (in f C ) into a similar relation \"used for\" (in f L ) and a newly introduced relation \"produces\" (in f S ), and then composing these back to infer f C . While the question in Figure 1 can be answered by composing the two facts f S and f L , that this is the case is unclear based solely on the question. This property of relation decomposition not being evident from reading the question pushes reasoning models towards focusing on learning to compose new pieces of knowledge, a key challenge in language understanding. Further, f L has no overlap with the question, making it difficult to retrieve it in the first place.\n\nLet's contrast this with an alternative question formulation: \"What can something produced by differential heating of air be used for?\" Although awkwardly phrased, this variation is easy to syntactically decompose into two simpler  Table 1: QASC has several desirable properties not simultaneously present in any single existing multihop QA dataset. Here \"available\" indicates that the dataset comes with a corpus that is guaranteed to contain supporting facts, while \"annotated\" indicates that these supporting facts are additionally annotated.\nN N Y Y N N Y Decomposition is not evident N - N Y Y Y Y Multi-document inference Y N N N Y N Y Requires knowledge retrieval Y N Y N Y N Y\nqueries, as well as to identify what knowledge to retrieve. In fact, multi-hop questions in many existing datasets (Yang et al. 2018;Talmor and Berant 2018) often follow this syntactically decomposable pattern, with questions such as: \"Which government position was held by the lead actress of X?\" All questions in QASC are human-authored, obtained via a multi-step crowdsourcing process (Section 3). To better enable development of both the reasoning and retrieval models, we also provide the pair of facts that were composed to create the question. 1 We use these annotations to develop a novel two-step retrieval technique that uses question-relevant facts to guide a second retrieval step. To make the dataset difficult for fine-tuned language models using our proposed retrieval (Section 5), we further augment the answer choices in our dataset via a multi-adversary distractor choice selection method (Section 6) that does not rely on computationally expensive multiple iterations of adversarial filtering (Zellers et al. 2018).\n\nEven 2-hop reasoning for questions with implicit decomposition requires new approaches for retrieval and reasoning not captured by current datasets. Similar to other recent multi-hop reasoning tasks (Yang et al. 2018;Talmor and Berant 2018), we also focus on 2-hop reasoning, solving which will go a long way towards more general N-hop solutions.\n\nIn summary, we make the following contributions: (1) a dataset QASC of 9,980 8-way multiple-choice questions from elementary and middle school level science, with a focus on fact composition; (2) a pair of facts f S ,f L from associated corpora annotated for each question, along with a composed fact f C entailed by f S and f L , which can be viewed as a form of multi-sentence entailment dataset; (3) a novel two-step information retrieval approach designed for multi-hop QA that improves the recall of gold facts (by 43 pts) and QA accuracy (by 14 pts); and (4) an efficient multimodel adversarial answer choice selection approach.\n\nQASC is challenging for current large pre-trained language models (Peters et al. 2018;Devlin et al. 2019), which exhibit a gap of 20% (absolute) to a human baseline of 93%, even when massively fine-tuned on 100K external QA examples in addition to QASC and provided with relevant knowledge using our proposed two-step retrieval.\n\n1 Questions, annotated facts, and corpora are available at https://github.com/allenai/qasc. Supplementary details are provided in a longer version of this paper at https://arxiv.org/abs/1910.11473. Table 1 summarizes how QASC compares with several existing datasets along five key dimensions (discussed below), which we believe are necessary for effectively developing retrieval and reasoning models for knowledge composition.\n\n\nComparison With Existing Datasets\n\nExisting datasets for the science domain require different reasoning techniques for each question 2018). The dataset most similar to our work is OpenBookQA (Mihaylov et al. 2018), which comes with multiple-choice questions and a book of core science facts used as the seed for question generation. Each question requires combining the seed core fact with additional knowledge. However, it is unclear how many additional facts are needed, or whether these facts can even be retrieved from any existing knowledge sources. QASC, on the other hand, explicitly identifies two facts deemed (by crowd workers) to be sufficient to answer a question. These facts exist in an associated corpus and are provided for model development.\n\nMultiRC (Khashabi et al. 2018a) uses passages to create multi-hop questions. However, MultiRC and other singlepassage datasets (Mishra et al. 2018;Weston et al. 2015) have a stronger emphasis on passage discourse and entity tracking, rather than relation composition.\n\nMulti-hop datasets from the Web domain use complex questions that bridge multiple sentences. We discuss 4 such datasets. (a) WikiHop (Welbl, Stenetorp, and Riedel 2018) contains questions in the tuple form (e, r, ?) based on edges in a knowledge graph. However, WikiHop lacks questions with natural text or annotations on the passages that could be used to answer these questions. (b) ComplexWebQuestions (Talmor and Berant 2018) was derived by converting multi-hop paths in a knowledge-base into a text query. By construction, the questions can be decomposed into simpler queries corresponding to knowledge graph edges in the path. (c) HotPotQA (Yang et al. 2018) contains a mix of multihop questions authored by crowd workers using a pair of Wikipedia pages. While these questions were authored in a similar way, due to their domain and task setup, they also end up being more amenable to decomposition. (d) A recent dataset, DROP (Dua et al. 2019), requires discrete reasoning over text (such as counting or addition). Its focus is on performing discrete (e.g., mathematical) operations on extracted pieces of information, unlike our proposed sentence composition task.\n\nMany systems answer science questions by composing multiple facts from semi-structured and unstructured knowl-edge sources Khot, Sabharwal, and Clark 2017;Jansen et al. 2017;Khashabi et al. 2018b). However, these often require careful manual tuning due to the large variety of reasoning techniques needed for these questions (Boratko et al. 2018) and the large number of facts that often must be composed together (Jansen 2018;Jansen et al. 2016). By limiting QASC to require exactly 2 hops (thereby avoiding semantic drift issues with longer paths (Fried et al. 2015;Khashabi et al. 2019)) and explicitly annotating these hops, we hope to constrain the problem enough so as to enable the development of supervised models for identifying and composing relevant knowledge.\n\n\nImplicit Relation Decomposition\n\nAs mentioned earlier, a key challenge in QASC is that syntactic cues in the question are insufficient to determine how one should decompose the question relation, r Q , into two sub-relations, r S and r L , corresponding to the associated facts f S and f L . At an abstract level, 2-hop questions in QASC generally exhibit the following form:\nQ r Q (x q , z ? a ) r ? S (x q , y ? ) \u2227 r ? L (y ? , z ? a ) \u21d2 r Q (x q , z ? a )\nwhere terms with a '?' superscript represent unknowns: the decomposed relations r S and r L as well as the bridge concept y. (The answer to the question, z ? a , is an obvious unknown.) To assess whether relation r Q holds between some concept x q in the question and some concept z a in an answer candidate, one must come up with the missing or implicit relations and bridge concept. In our previous example, r Q =\"harnessed for\", x q =\"Differential heating of air\", y =\"wind\", r S =\"produces\", and r L =\"used for\".\n\nIn contrast, syntactically decomposable questions in many existing datasets often spell out both r S and r L : Q r S (x q , y ? ) \u2227 r L (y ? , z ? a ). The example from the introduction, \"Which government position was held by the lead actress of X?\", could be stated in this notation as: lead-actress(X, y ? ) \u2227 held-govt-posn(y ? , z ? a ). This difference in how the question is presented in QASC makes it challenging to both retrieve relevant facts and reason with them via knowledge composition. This difficulty is further compounded by the property that a single relation r Q can often be decomposed in multiple ways into r S and r L . We defer a discussion of this aspect to later, when describing QASC examples in Table 3. Figure 2 gives an overall view of the crowdsourcing process. The process is designed such that each question in QASC is produced by composing two facts from an existing text corpus. Rather than creating compositional questions from scratch or using a specific pair of facts, we provide workers with only one seed fact f S as the starting point. They are then given the creative freedom to find other relevant facts from a large corpus, F L that could be composed with this seed fact. This allows workers to find other facts compose naturally with f S and thereby prevent complex questions that describe the composition explicitly.\n\n\nMultihop Question Collection\n\nOnce crowd-workers identify a relevant fact f L \u2208F L that can be composed with f S , they create a new composed fact f C and use it to create a multiple-choice question. To ensure that the composed facts and questions are consistent with our instructions, we introduce automated checks to catch any inadvertent mistakes. E.g., we require that at least one intermediate entity (marked in black in subsequent sections) must be dropped to create f C . We also ensure that the intermediate entity wasn't re-introduced in the question.\n\nThese questions are next evaluated against baseline systems to ensure hardness, i.e., at least one of the incorrect answer choices had to be be preferred over the correct choice by one of two QA systems (IR or BERT; described next), with a bonus incentive if both systems were distracted.\n\n\nInput Facts\n\nSeed Facts, F S : We noticed that the quality of the seed facts can have a strong correlation with the quality of the question. So we created a small set of 928 good quality seed facts F S from clean knowledge resources. We start with two medium size corpora of grade school level science facts: the WorldTree corpus ) and a collection of facts from the CK-12 Foundation. 2 Since the WorldTree corpus contains only facts covering elementary science, we used their annotation protocol to expand it to middle-school science. We then manually selected facts from these three sources that are amenable to creating 2-hop questions. 3 The resulting corpus F S contains a total of 928 facts: 356 facts from WorldTree, 123 from our middle-school extension, and 449 from CK-12.\n\nLarge Text Corpus, F L : To ensure that the workers are able to find any potentially composable fact, we used a large web corpus of 17M cleaned up facts F L . We processed and filtered a corpus of 73M web documents (281GB) from Clark et al. (2016) to produce this clean corpus of 17M sentences (1GB). The procedure to process this corpus involved using spaCy 4 to segment documents into sentences, a Python implementation of Google's langdetect 5 to identify English-language sentences, ftfy 6 to correct Unicode encoding problems, and custom heuristics to exclude sentences with artifacts of web scraping like HTML, CSS and JavaScript markup, runs of numbers originating from tables, email addresses, URLs, page navigation fragments, etc.\n\n\nBaseline QA Systems\n\nOur first baseline is the IR system ) designed for science QA with its associated corpora of web and science text (henceforth referred as the Aristo corpora). It retrieves sentences for each question and answer choice from the associated corpora, and returns the answer choice with the highest scoring sentence (based on the retrieval score). Our second baseline uses the language model BERT of Devlin et al. (2019). We follow their QA approach for the multiple-choice situation inference task SWAG (Zellers et al. 2018). Given question q and an answer choice\nc i , we create [CLS] q [SEP] c i [SEP]\nas the input to the model, with q being assigned to segment 0 and c i to segment 1. 7 The model learns a linear layer to project the representation of the [CLS] token to a score for each choice c i . We normalize the scores across all answer choices using softmax and train the model using the cross-entropy loss. When context/passage is available, we append the passage to segment 0, i.e., given a retrieved passage p i , we provide\n[CLS] p i q [SEP] c i [SEP]\nas the input. We refer to this model as BERT-MCQ in subsequent sections.\n\nFor the crowdsourcing step, we use the bert-large-uncased model and fine-tuned it sequentially on two datasets: (1) RACE (Lai et al. 2017) with context; (2) SCI questions (ARC-Challenge+Easy ) + OpenBookQA (Mihaylov et al. 2018) + Regents 12th Grade Exams 8 ).\n\n\nQuestion Validation\n\nWe validated these questions by having 5 crowdworkers answer them. Any question answered incorrectly or considered unanswerable by at least 2 workers was dropped, reducing the collection to 7,660 questions. The accuracy of the IR and BERT models used in Step 4 was 32.25% and 38.73%, resp., on this reduced subset. 9 By design, every question has the desirable property of being annotated with two sentences from F QASC that can be composed to answer it. The low score of the IR model also suggests that these questions can not be answered using a single fact from the corpus. We next analyze the retrieval and reasoning challenges associated with these questions. Based on these analyses, we 7 We assume familiarity with BERT's notation such as [CLS], [SEP], uncased models, and masking (Devlin et al. 2019). 8 http://www.nysedregents.org/livingenvironment 9 The scores are not 0% as crowdworkers were not required to distract both systems for every question.\n\nwill propose a new baseline model for multi-hop questions that substantially outperforms existing models on this task. We use this improved model to adversarially select additional distractor choices to produce the final QASC dataset. Table 2 shows sample crowd-sourced questions along with the associated facts. Consider the first question: \"What can trigger immune response?\". One way to answer it is to first retrieve the two annotated facts (or similar facts) from the corpus. But the first fact, like many other facts in the corpus, overlaps only with the words in the answer \"transplanted organs\" and not with the question, making retrieval challenging. Even if the right facts are retrieved, the QA model would have to know how to compose the \"found on\" relation in the first fact with the \"trigger\" relation in the second fact. Unlike previous datasets (Yang et al. 2018;Talmor and Berant 2018), the relations to be composed are not explicitly mentioned in the question, making reasoning also challenging. We next discuss these two issues in detail.\n\n\nChallenges\n\n\nRetrieval Challenges\n\nWe analyze the retrieval challenges associated with finding the two supporting facts associated with each question. Note that, unlike OpenBookQA, we consider the more general setting of retrieving relevant facts from a single large corpus F QASC = F S \u222a F L instead of assuming the availability of a separate small book of facts (i.e., F S ).\n\nStandard IR approaches for QA retrieve facts using question + answer as their IR query Khot, Sabharwal, and Clark 2017;Khashabi et al. 2018b). While this can be effective for lookup questions, it is likely to miss important facts needed for multi-hop questions. In 96% of our crowd-sourced questions, at least one of the two annotated facts had an overlap of fewer than 3 tokens (ignoring stop words) with this question + answer query, making it difficult to retrieve such facts. 10 Note that our annotated facts form one possible pair that could be used to answer the question. While retrieving these specific facts isn't necessary, these crowd-authored questions are generally expected to have a similar overlap level to other relevant facts in our corpus.\n\nNeural retrieval methods that use distributional representations can help mitigate the brittleness of word overlap measures, but also vastly open up the space of possibly relevant sentences. We hope that our annotated facts will be useful for training better neural retrieval approaches for multi-hop reasoning in future work. In this work, we focused on a modified non-neural IR approach that exploits the intermediate concepts not mentioned in the question ( black words in our examples), which is explained in Section 5.1.\n\n\nReasoning Challenges\n\nAs described earlier, we collected these questions to require compositional reasoning where the relations to be composed are not obvious from the question. To verify this, we analyzed 50 questions from our final dataset and identified (A) carbon dioxide in groundwater (B) oxygen in groundwater (C) pure oxygen (D) magma in groundwater f S : a cavern is formed by carbonic acid in groundwater seeping through rock and dissolving limestone. f L : When carbon dioxide is in water, it creates carbonic acid .  the key relations in f S , f L , and the question, referred to as r S , r L , and r Q , respectively (see examples in Table 3). 7 of the 50 questions could be answered using only one fact and 4 of them didn't use either of the two facts. We analyzed the remaining 39 questions to categorize the associated reasoning challenges. In only 2 questions, the two relations needed to answer the question were explicitly mentioned in the question itself. In comparison, the composition questions in HotpotQA had both the relations mentioned in 47 out of 50 dev questions in our analysis.\n\nSince there are a large number of lexical relations, we focus on 16 semantic relations in our analysis such as causes, performs, etc. These relations were defined based on previous analyses on science datasets (Clark et al. 2014;Jansen et al. 2016;Khashabi et al. 2016). We found 25 unique relation composition rules (i.e., r S (X, Y ), r L (Y, Z) \u21d2 r Q (X, Z)). On average, we found every query relation r Q had 1.6 unique relation compositions. Table 3 illustrates two different relation compositions that lead to the same causes query relation. As a result, models for QASC have a strong incentive to learn various possible compositions that lead to the same semantic relation, as well as extract them from text.\n\n\nQuestion Answering Model\n\nWe now discuss our proposed two-step retrieval method and how it substantially boosts the performance of BERT-based QA models on crowd-sourced questions. This will motivate the need for adversarial choice generation.\n\n\nRetrieval: Two-step IR\n\nConsider the first question in Table 2. An IR approach that uses the standard q + a query is unlikely to find the first fact since many irrelevant facts would also have the same overlapping words -\"transplanted organs\". However, it is likely to retrieve facts similar to the second fact, i.e., \"Antigens trigger immune response\". If we could recognize antigen as an important intermediate entity that would lead to the answer, we can then query for sentences connecting this intermediate entity (\"antigens\" ) to the answer (\"transplanted organs\" ) which is then likely to find the first fact (\"antigens are found on transplanted organs\" ). One potential way to identify such an intermediate concept is to consider the new entities introduced in the first retrieved fact that are absent from the question, i.e., f 1 \\ q.\n\nBased on this intuition, we present a simple but effective two-step IR baseline for multi-hop QA: (1) Retrieve K (=20 for efficiency) facts F 1 based on the query Q=q + a; (2) For each f 1 \u2208 F 1 , retrieve L (=4 to promote diversity) facts F 2 each of which contains at least one word from Q \\ f 1 and from f 1 \\ Q; (3) Filter {f 1 , f 2 } pairs that do not contain any word from q or a; (4) Select top M unique facts from {f 1 , f 2 } pairs sorted by the sum of their individual IR score.\n\nEach retrieval query is run against an ElasticSearch 11 index built over F QASC with retrieved sentences filtered to reduce noise . We use the set-difference between the stemmed, non-stopword tokens in q + a and f 1 to identify the intermediate entity. Generally, we are interested in finding facts that connect new concepts introduced in the first fact (i.e., f 1 \\ Q) to concepts not yet covered in question+answer (i.e., Q \\ f 1 ).\n\nTraining a model on our annotations or essential terms (Khashabi et al. 2017) could help better identify these concepts. Recently, Khot, Sabharwal, and Clark (2019) proposed a span-prediction model to identify such intermediate entities for OpenBookQA questions. Their approach, however, assumes that one of the gold facts is provided as input to the model. Our approach, while specifically designed for 2-hop questions, can serve as a stepping stone towards developing retrieval methods for N-hop questions.\n\nThe single step retrieval approach (using only f 1 but still requiring overlap with q and a) has an overall recall of only 2.9% (i.e., both f S and f L were in the top 10 sentences for 2.9% of the questions). The two-step approach, on the other hand, has a recall of 44.4%-a 15X improvement (also limited to top M=10 sentences). Even if we relax the recall metric to finding f S or f L , the single step approach underperforms by 28% compared to the two-step retrieval (42.0 vs 69.9%). We will show in the next section that this improved recall also translates to improved QA scores. This shows the value of our two-step approach as well as the associated annotations: progress on the retrieval sub-task enabled by our fact-level annotations can lead to progress on the QA task.\n\n\nReasoning: BERT Models\n\nWe primarily use BERT-models fine-tuned on other QA datasets and with retrieved sentences as context, similar to prior state-of-the-art models on MCQ datasets Pan et al. 2019). 12 There is a large space of possible configurations to build such a QA model (e.g., fine-tuning datasets, corpora) which we will explore later in our experimental comparisons. For simplicity, the next few sections will focus on one particular model: the bert-large-cased model fine-tuned on the RACE + SCI questions (with retrieved context 13 ) and then fine-tuned on our dataset with single-step/two-step retrieval. For consistency, we use the same hyper-parameter sweep in all finetuning experiments (cf. Appendix D).\n\n\nResults on Crowd-Sourced Questions\n\nTo enable fine-tuning models, we split the questions them into 5962/825/873 questions in train/dev/test folds, resp. To limit memorization, any two questions using the same seed fact, f S , were always put in the same fold. Since multiple facts can cover similar topics, we further ensure that similar facts are also in the same fold. (See Appendix B for details.)\n\nWhile these crowd-sourced questions were challenging for the baseline QA models (by design), models fine-tuned on this dataset perform much better. The BERT baseline that scored 38.7% on the crowd-sourced questions now scores 63.3% on the dev set after fine-tuning. Even the basic singlestep retrieval context can improve over this baseline score by 14.9% (score: 78.2%) and our proposed two-step retrieval improves it even further by 8.2% (score: 86.4%). This shows that the distractor choices selected by the crowdsource workers were not as challenging once the model is provided with the right context. This can be also seen in the incorrect answer choices selected by them in Table 2 where they used words such as \"Pain\" that are associated with words in the question but may not have a plausible reasoning chain. To make this dataset more challenging for these models, we next introduce adversarial distractor choices. \n\n\nAdversarial Choice Generation\n\nTo make the crowdsourced dataset challenging for finetuned language models, we use model-guided adversarial choice generation to expand each crowdsourced question into an 8-way question. Importantly, the humanauthored body of the question is left intact (only the choices are augmented), to avoid a system mechanically reverseengineering how a question was generated.\n\nPrevious approaches to adversarially create a hard dataset have focused on iteratively making a dataset harder by sampling harder choices and training stronger models (Zellers et al. 2018;2019a). While this strategy has been effective, it involves multiple iterations of model training that can be prohibitively expensive with large LMs. In some cases (Zellers et al. 2018;2019b), they need a generative model such as GPT-2 (Radford et al. 2019) to produce the distractor choices. We, on the other hand, have a simpler setup where we train only a few models and do not require a model to generate the distractor choices.\n\n\nDistractor Options\n\nTo create the space of distractors, we follow Zellers et al. (2019a) and use correct answer choices from other questions. This ensures that a model won't be able to predict the correct answer purely based on the answer choices (one of the issues with OpenBookQA). To reduce the chances of a correct answer being added to the set of distractors, we pick them from the most dissimilar questions. We further filter these choices down to \u223c30 distractor choices per question by removing the easy distractors based on the fine-tuned BERT baseline model. Further implementation details are provided in Appendix C.\n\nThis approach of generating distractors has an additional benefit: we can recover the questions that were rejected earlier for having multiple valid answers (in \u00a7 3.3). We add back 2,774 of the 3,361 rejected questions that (a) had at least one worker select the right answer, and (b) were deemed unanswerable by at most two workers. We, however, ignore all crowdsourced distractors for these questions since they were considered potentially correct answers in the validation task.  We use the adversarial distractor selection process (to be described shortly) to add the remaining 7 answer choices.\n\nTo ensure a clean evaluation set, we use another crowdsourcing task where we ask 3 annotators to identify all possible valid answers from the candidate distractors for the dev and test sets. We filter out answer choices in the distractor set that were considered valid by at least one turker. Additionally, we filter out low-quality questions where more than four distractor choices were marked valid or the correct answer was not included in the selection. This dropped 20% of the dev and test set questions and finally resulted in train/dev/test sets of size 8134/926/920 questions with an average of 30/26.9/26.1 answer choices (including the correct one) per question.\n\n\nMulti-Adversary Choice Selection\n\nWe first explain our approach, assuming access to K models for multiple-choice QA. Given the number of datasets and models proposed for this task, this is not an unreasonable assumption. In this work, we use K BERT models, but the approach is applicable to any QA model.\n\nOur approach aims to select a diverse set of answers that are challenging for different models. As described above, we first create \u223c30 distractor options, D for each question. We then sort these distractor options based on their relative difficulty for these models, defined as the number of models fooled by this distractor: k I m k (q, d i ) > m k (q, a) where m k (q, c i ) is the k-th model's score for the question q and choice c i . In case of ties, we then sort these distractors based on the difference between the scores of the distractor choice and the correct answer: k m k (q, d i ) \u2212 m k (q, a) . 14 We used BERT-MCQ models that were fine-tuned on the RACE +SCI dataset as described in the previous section. We additionally fine-tune these models on the training questions with random answer choices added from the the space of distractors to make each question an 8-way multiple-choice question. This ensures that our models have seen answer choices from both the human-authored and algorithmically selected space of distractors. Drawing inspiration from bootstrapping (Breiman 1996), we create two such datasets with randomly selected distractors from D and use the models fine-tuned on these datasets as m k (i.e, K = 2). There is a large space of possible models and scoring functions that may be explored, 15 but we found this simple approach to be effective at identifying good distractors. This process of generating the adversarial dataset is depicted in Figure 3.\n\n\nEvaluating Dataset Difficulty\n\nWe select the top scoring distractors using the two BERT-MCQ models such that each question is converted into an 8-way MCQ (including the correct answer and humanauthored valid distractors). To verify that this results in challenging questions, we again evaluate using the BERT-MCQ models with two different kinds of retrieval. Table 4 compares the difficulty of the adversarial dataset to the original dataset and the dataset with random distractors (used for fine-tuning BERT-MCQ models).\n\nThe original 4-way MCQ dataset was almost solved by the two-step retrieval approach and increasing it to 8-way with random distractors had almost no impact on the scores. But our adversarial choices drop the scores of the BERT model given context from either of the retrieval approaches.\n\n\nQASC Dataset\n\nThe final dataset contains 9,980 questions split into [8134|926|920] questions in the [train|dev|test] folds. Each question is annotated with two facts that can be used to answer the question. These facts are present in a corpus of 17M sentences (also provided). The questions are similar to the examples in Table 2 but expanded to an 8-way MCQ and with shuffled answer choices. E.g., the second example there was changed to \"What forms caverns by seeping through rock and dissolving limestone? (A) pure oxygen (B) Something with a head, thorax, and abdomen (C) basic building blocks of life (D) carbon dioxide in groundwater (E) magma in groundwater (F) oxygen in groundwater (G) At the peak of a mountain (H) underground systems\". Table 6 gives a summary of QASC statistics, and Table 7 in the Appendix provides additional examples.\n\n\nExperiments\n\nWhile we used large pre-trained language models first finetuned on other QA datasets (\u223c100K examples) to ensure that QASC is challenging, we also evaluate BERT models without any additional fine-tuning here. All models are still finetuned on the QASC dataset.\n\nTo verify that our dataset is challenging also for models that do not use BERT (or any other transformer-based architecture), we evaluate Glove (Pennington, Socher, and Manning 2014) Table 5: QASC scores for previous state-of-the-art models on multi-hop Science MCQ(OBQA), and BERT models with different corpora, retrieval approaches and additional fine-tuning. While the simpler models only show a small increase relative to random guessing, BERT can achieve upto 67% accuracy by fine-tuning on QASC and using the two-step retrieval. Using the BERT models pre-trained with whole-word masking and first fine-tuning on four relevant MCQ datasets (RACE and SCI (3) Table 5, OpenBookQA models, that had close to the state-of-the-art results on OpenBookQA, perform close to the random baseline on QASC. Since these mostly rely on statistical correlations between questions and across choices, 16 this shows that this dataset doesn't have any easy shortcuts that can be exploited by these models.\n\nSecond, we evaluate BERT models with different corpora and retrieval. We show that our two-step approach always out-performs the single-step retrieval, even when given a larger corpus. Interestingly, when we compare the two single-step retrieval models, the larger corpus outperforms the smaller corpus, presumably because it increases the chances of having a single fact that answers the question. On the other hand, the smaller corpus is better for the two-step retrieval approach, as larger and noisier corpora are more likely to lead a 2-step search astray.\n\nFinally, to compute the current gap to human performance, we consider a recent state-of-the-art model on multiple leaderboards: AristoBertV7 that uses the BERT model trained with whole-word masking, 17 fine-tuned on the RACE 16 Their knowledge-based models do not scale to our corpus of 17M sentences. 17 https://github.com/google-research/bert +SCI questions and retrieves knowledge from a very large corpus. Our two-step retrieval based model outperforms this model and improves even further with more fine-tuning. Replacing the pre-trained bert-large-cased model with the whole-word masking based model further improves the score by 4.7%, but there is still a gap of \u223c20% to the human score of 93% on this dataset.\n\n\nConclusion\n\nWe present QASC, the first QA dataset for multi-hop reasoning beyond a single paragraph where two facts needed to answer a question are annotated for training, but questions cannot be easily syntactically decomposed into these facts. Instead, models must learn to retrieve and compose candidate pieces of knowledge. QASC is generated via a crowdsourcing process, and further enhanced via multi-adversary distractor choice selection. State-of-the-art BERT models, even with massive fine-tuning on over 100K questions from previous relevant datasets and using our proposed two-step retrieval, leave a large margin to human performance levels, thus making QASC a new challenge for the community.\n\ncussions about, and guidance with, early versions of the MTurk task. We thank the Amazon Mechanical Turk workers for their effort in creating and annotating QASC questions. Computations on beaker.org were supported in part by credits from Google Cloud.\n\n\nQuestion: Differential heating of air can be harnessed for what? (A) electricity production (D) reduce acidity of food (B) erosion prevention . . . (C) transfer of electrons . . .Annotated facts:fS: Differential heating of air produces wind .\n\nFigure 1 :\n1A sample 8-way multiple choice QASC question.\n\nFigure 2 :\n2Crowd-sourcing questions using the seed corpus F S and the full corpus F L .\n\nFigure 3 :\n3Generating QASC questions using adversarial choice selection.\n\n\nPropertyCompWebQ DROP HotPotQA MultiRC OpenBookQA WikiHop QASCSupporting facts are available \nN \nY \nY \nY \nN \nN \nY \nSupporting facts are annotated \n\n\n\nAntigens are found on cancer cells and the cells of transplanted organs. f L : Anything that can trigger an immune response is called an antigen .Question \n\nChoices \nAnnotated Facts \nWhat can trigger immune \nresponse? \n\n(A) Transplanted organs \n(B) Desire \n(C) Pain \n(D) Death \n\nf S : What forms caverns by \nseeping through rock and \ndissolving limestone? \n\n\n\nTable 2 :\n2Examples of questions generated via the crowd-sourcing process along with the facts used to create each question.Fact 1 \nr S \nFact 2 \nr L \nComposed Fact \nr Q \nAntigens are found on cancer cells \nand the cells of transplanted organs. \n\nlocated Anything that can trigger an im-\nmune response is called an antigen. \n\ncauses transplanted organs can \ntrigger an immune response \n\ncauses \n\na cavern is formed by carbonic acid \nin groundwater seeping through \nrock and dissolving limestone \n\ncauses Any time water and carbon dioxide \nmix, carbonic acid is the result. \n\ncauses carbon dioxide in ground-\nwater creates caverns \n\ncauses \n\n\n\nTable 3 :\n3These examples of sentence compositions result in the same composed relation, causes, but via two different composition rules: located + causes \u21d2 causes and causes + causes \u21d2 causes These rules are not evident from the composed fact, requiring a model reasoning about the composed fact to learn the various possible decompositions of causes.\n\n\nSingle-step retr. Two-step retr.Dev Accuracy \n\nOriginal Dataset (4-way) \n78.2 \n86.4 \nRandom Distractors (8-way) \n74.9 \n83.3 \nAdversarial Distractors (8-way) \n61.7 \n72.9 \n\n\n\nTable 4 :\n4Results of the BERT-MCQ model on the adversarial dataset using bert-large-cased model and pre-trained on RACE + SCI questions.\n\n\nbased models developed for multiple-choice science questions in OpenBookQA. Specifically, we consider these non-BERT baseline models: \u2022 Odd-one-out: Answers the question based on just the choices by identifying the most dissimilar answer. \u2022 ESIM Q2Choice (with and without Elmo): Uses the ESIM model(Chen et al. 2017) with Elmo embeddings(Peters et al. 2018) to compute how much does the question entail each answer choice.Retr. Corpus \n\nRetrieval \nAddnl. fine-tuning \nDev \nTest \nModel \nEmbedding \n(#docs) \nApproach \n(#examples) \nAcc. \nAcc. \n\nHuman Score \n93.0 \nRandom \n12.5 \n12.5 \n\nOBQA \nModels \nESIM Q2Choice \nGlove \n21.1 \n17.2 \nESIM Q2Choice \nGlove Elmo \n17.1 \n15.2 \nOdd-one-out \nGlove \n22.4 \n18.0 \n\nBERT \nModels \n\nBERT-MCQ \nBERT-LC \nF QASC (17M) \nSingle-step \n59.8 \n53.2 \nBERT-MCQ \nBERT-LC \nF QASC + ARC (31M) \nSingle-step \n62.3 \n57.0 \nBERT-MCQ \nBERT-LC \nF QASC + ARC(31M) \nTwo-step \n66.6 \n58.3 \nBERT-MCQ \nBERT-LC \nF QASC (17M) \nTwo-step \n71.0 \n67.0 \n\nAddnl. \nFine-\ntuning \nAristoBertV7 \nBERT-LC[WM] \nAristo (1.7B) \nSingle-step \nRACE + SCI (97K) \n69.5 \n62.6 \nBERT-MCQ \nBERT-LC \nF QASC (17M) \nTwo-step \nRACE + SCI (97K) \n72.9 \n68.5 \nBERT-MCQ \nBERT-LC[WM] \nF QASC (17M) \nTwo-step \nRACE + SCI (97K) \n78.0 \n73.2 \n\n\n\n\n) improves the score to 73.2%, leaving a gap of over 19.8% to the human baseline of 93%. ARC refers to the corpus of 14M sentences fromClark et al. (2018), BERT-LC indicates 'bert-large-cased' and BERT-LC[WM] indicates whole-word masking.Train Dev Test \n\nNumber of questions \n8,134 926 920 \nNumber of unique f S \n722 103 103 \nNumber of unique f L \n6,157 753 762 \nAverage question length (chars) \n46.4 45.5 44.0 \n\n\n\nTable 6 :\n6QASC dataset statistics As shown in\nhttps://www.ck12.org 3 While this is a subjective decision, it served our main goal of identifying a reasonable set of seed facts for this task. 4 https://spacy.io/ 5 https://pypi.org/project/spacy-langdetect/ 6 https://github.com/LuminosoInsight/python-ftfy\nSeeTable 9in Appendix E (provided in the longer version at https://arxiv.org/abs/1910.11473) for more details.\nhttps://www.elastic.co\nExperiments section contains numbers for other QA models.13  We use the same single-step retrieval over the large Aristo corpus as used by other BERT-based systems on ARC and Open-BookQA leaderboards.\nSince we use normalized probabilities as model scores, we do not normalize them here.\nFor example, we evaluated the impact of increasing K, but didn't notice any change in the fine-tuned model's score.\nAcknowledgmentsWe thank Oyvind Tafjord for his extension to AllenNLP that was used to train our BERT models, Nicholas Lourie for his \"A Mechanical Turk Interface (amti)\" tool used to launch crowdsourcing tasks, Dirk Groeneveld for his help collecting seed facts, and Sumithra Bhakthavatsalam for helping generate the QASC fact corpus. We thank Sumithra Bhakthavatsalam, Kaj Bostrom, Kyle Richardson, and Madeleine van Zuylen for initial human evaluations. We also thank Jonathan Borchardt and Dustin Schwenk for inspiring dis-\nA systematic classification of knowledge, reasoning, and context within the ARC dataset. M Boratko, H Padigela, D Mikkilineni, P Yuvraj, R Das, A Mccallum, M Chang, A Fokoue-Nkoutche, P Kapanipathi, N Mattei, R Musa, K Talamadupula, M Witbrock, Proceedings of the Workshop on Machine Reading for Question Answering. the Workshop on Machine Reading for Question AnsweringBoratko, M.; Padigela, H.; Mikkilineni, D.; Yuvraj, P.; Das, R.; McCallum, A.; Chang, M.; Fokoue-Nkoutche, A.; Kapanipathi, P.; Mattei, N.; Musa, R.; Talamadupula, K.; and Witbrock, M. 2018. A systematic classification of knowledge, reasoning, and context within the ARC dataset. In Proceedings of the Workshop on Ma- chine Reading for Question Answering.\n\nBagging predictors. L Breiman, Machine learning. 242Breiman, L. 1996. Bagging predictors. Machine learning 24(2):123-140.\n\nEnhanced lstm for natural language inference. Q Chen, X.-D Zhu, Z.-H Ling, S Wei, H Jiang, D Inkpen, ACL. Chen, Q.; Zhu, X.-D.; Ling, Z.-H.; Wei, S.; Jiang, H.; and Inkpen, D. 2017. Enhanced lstm for natural language inference. In ACL.\n\nAutomatic construction of inference-supporting knowledge bases. P Clark, N Balasubramanian, S Bhakthavatsalam, K Humphreys, J Kinkead, A Sabharwal, O Tafjord, 4th Workshop on Automated Knowledge Base Construction (AKBC). Clark, P.; Balasubramanian, N.; Bhakthavatsalam, S.; Humphreys, K.; Kinkead, J.; Sabharwal, A.; and Tafjord, O. 2014. Auto- matic construction of inference-supporting knowledge bases. In 4th Workshop on Automated Knowledge Base Construction (AKBC).\n\nCombining retrieval, statistics, and inference to answer elementary science questions. P Clark, O Etzioni, T Khot, A Sabharwal, O Tafjord, P D Turney, D Khashabi, AAAI. Clark, P.; Etzioni, O.; Khot, T.; Sabharwal, A.; Tafjord, O.; Turney, P. D.; and Khashabi, D. 2016. Combining retrieval, statistics, and inference to answer elementary science questions. In AAAI.\n\nThink you have solved question answering? Try ARC, the AI2 reasoning challenge. P Clark, I Cowhey, O Etzioni, T Khot, A Sabharwal, C Schoenick, O Tafjord, CoRR abs/1803.05457Clark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.; Schoenick, C.; and Tafjord, O. 2018. Think you have solved ques- tion answering? Try ARC, the AI2 reasoning challenge. CoRR abs/1803.05457.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, NAACL. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In NAACL.\n\nDROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. D Dua, Y Wang, P Dasigi, G Stanovsky, S Singh, M Gardner, NAACL. Dua, D.; Wang, Y.; Dasigi, P.; Stanovsky, G.; Singh, S.; and Gard- ner, M. 2019. DROP: A reading comprehension benchmark requir- ing discrete reasoning over paragraphs. In NAACL.\n\n. D Fried, P Jansen, G Hahn-Powell, M Surdeanu, P Clark, Fried, D.; Jansen, P.; Hahn-Powell, G.; Surdeanu, M.; and Clark, P.\n\nHigher-order lexical semantic models for non-factoid answer reranking. TACL. 3Higher-order lexical semantic models for non-factoid answer reranking. TACL 3:197-210.\n\nWhat's in an explanation? characterizing knowledge and inference requirements for elementary science exams. P Jansen, N Balasubramanian, M Surdeanu, P Clark, COLING. Jansen, P.; Balasubramanian, N.; Surdeanu, M.; and Clark, P. 2016. What's in an explanation? characterizing knowledge and inference requirements for elementary science exams. In COLING.\n\nFraming qa as building and ranking intersentence answer justifications. P Jansen, R Sharp, M Surdeanu, P Clark, Computational Linguistics. 43Jansen, P.; Sharp, R.; Surdeanu, M.; and Clark, P. 2017. Fram- ing qa as building and ranking intersentence answer justifications. Computational Linguistics 43:407-449.\n\nWorldTree: A corpus of explanation graphs for elementary science questions supporting multi-hop inference. P A Jansen, E Wainwright, S Marmorstein, C T Morrison, Proceedings of LREC. LRECJansen, P. A.; Wainwright, E.; Marmorstein, S.; and Morrison, C. T. 2018. WorldTree: A corpus of explanation graphs for elementary science questions supporting multi-hop inference. In Proceedings of LREC.\n\nMulti-hop inference for sentence-level textgraphs: How challenging is meaningfully combining information for science question answering?. P Jansen, TextGraphs@NAACL-HLT. Jansen, P. 2018. Multi-hop inference for sentence-level textgraphs: How challenging is meaningfully combining information for sci- ence question answering? In TextGraphs@NAACL-HLT.\n\nQuestion answering via integer programming over semi-structured knowledge. D Khashabi, T Khot, A Sabharwal, P Clark, O Etzioni, D Roth, IJCAI. Khashabi, D.; Khot, T.; Sabharwal, A.; Clark, P.; Etzioni, O.; and Roth, D. 2016. Question answering via integer programming over semi-structured knowledge. In IJCAI.\n\nLearning what is essential in questions. D Khashabi, T Khot, A Sabharwal, D Roth, CoNLL. Khashabi, D.; Khot, T.; Sabharwal, A.; and Roth, D. 2017. Learn- ing what is essential in questions. In CoNLL.\n\nLooking beyond the surface: A challenge set for reading comprehension over multiple sentences. D Khashabi, S Chaturvedi, M Roth, S Upadhyay, D Roth, NAACL. Khashabi, D.; Chaturvedi, S.; Roth, M.; Upadhyay, S.; and Roth, D. 2018a. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In NAACL.\n\nQuestion answering as global reasoning over semantic abstractions. D Khashabi, T Khot, A Sabharwal, D Roth, AAAI. Khashabi, D.; Khot, T.; Sabharwal, A.; and Roth, D. 2018b. Ques- tion answering as global reasoning over semantic abstractions. In AAAI.\n\nOn the capabilities and limitations of reasoning for natural language understanding. D Khashabi, E S Azer, T Khot, A Sabharwal, D Roth, CoRR abs/1901.02522Khashabi, D.; Azer, E. S.; Khot, T.; Sabharwal, A.; and Roth, D. 2019. On the capabilities and limitations of reasoning for natural language understanding. CoRR abs/1901.02522.\n\nAnswering complex questions using open information extraction. T Khot, A Sabharwal, P Clark, ACL. Khot, T.; Sabharwal, A.; and Clark, P. 2017. Answering complex questions using open information extraction. In ACL.\n\nWhat's missing: A knowledge gap guided approach for multi-hop question answering. T Khot, A Sabharwal, P Clark, EMNLP. Khot, T.; Sabharwal, A.; and Clark, P. 2019. What's missing: A knowledge gap guided approach for multi-hop question answering. In EMNLP.\n\nRACE: Large-scale reading comprehension dataset from examinations. G Lai, Q Xie, H Liu, Y Yang, E H Hovy, EMNLP. Lai, G.; Xie, Q.; Liu, H.; Yang, Y.; and Hovy, E. H. 2017. RACE: Large-scale reading comprehension dataset from examinations. In EMNLP.\n\nCan a suit of armor conduct electricity? A new dataset for open book question answering. T Mihaylov, P Clark, T Khot, A Sabharwal, EMNLP. Mihaylov, T.; Clark, P.; Khot, T.; and Sabharwal, A. 2018. Can a suit of armor conduct electricity? A new dataset for open book question answering. In EMNLP.\n\nTracking state changes in procedural text: A challenge dataset and models for process paragraph comprehension. B D Mishra, L Huang, N Tandon, W Tau Yih, P ; X Clark, K Sun, D Yu, H Ji, D Yu, MRQA: Machine Reading for Question Answering Workshop at EMNLP-IJCNLP. NAACL. Pan,Mishra, B. D.; Huang, L.; Tandon, N.; tau Yih, W.; and Clark, P. 2018. Tracking state changes in procedural text: A chal- lenge dataset and models for process paragraph comprehension. In NAACL. Pan, X.; Sun, K.; Yu, D.; Ji, H.; and Yu, D. 2019. Improving question answering with external knowledge. In MRQA: Machine Reading for Question Answering Workshop at EMNLP-IJCNLP 2019.\n\nGloVe: Global vectors for word representation. J Pennington, R Socher, C D Manning, Proceedings of EMNLP. EMNLPPennington, J.; Socher, R.; and Manning, C. D. 2014. GloVe: Global vectors for word representation. In Proceedings of EMNLP.\n\nDeep contextualized word representations. M E Peters, M Neumann, M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer, NAACL. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. In NAACL.\n\nLanguage models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019. Language models are unsupervised multitask learners.\n\nImproving machine reading comprehension with general reading strategies. K Sun, D Yu, D Yu, C Cardie, CoRR abs/1810.13441Sun, K.; Yu, D.; Yu, D.; and Cardie, C. 2018. Improving machine reading comprehension with general reading strategies. CoRR abs/1810.13441.\n\nThe web as a knowledge-base for answering complex questions. A Talmor, J Berant, NAACL-HLTTalmor, A., and Berant, J. 2018. The web as a knowledge-base for answering complex questions. In NAACL-HLT.\n\nConstructing datasets for multi-hop reading comprehension across documents. J Welbl, P Stenetorp, S Riedel, TACL. 6Welbl, J.; Stenetorp, P.; and Riedel, S. 2018. Constructing datasets for multi-hop reading comprehension across documents. TACL 6:287-302.\n\nTowards AI-complete question answering: A set of prerequisite toy tasks. J Weston, A Bordes, S Chopra, T Mikolov, CoRR abs/1502.05698Weston, J.; Bordes, A.; Chopra, S.; and Mikolov, T. 2015. Towards AI-complete question answering: A set of prerequisite toy tasks. CoRR abs/1502.05698.\n\nSWAG: A large-scale adversarial dataset for grounded commonsense inference. Z Yang, P Qi, S Zhang, Y Bengio, W W Cohen, R Salakhutdinov, C D Manning, R Zellers, Y Bisk, R Schwartz, Y Choi, EMNLP. EMNLPYang, Z.; Qi, P.; Zhang, S.; Bengio, Y.; Cohen, W. W.; Salakhut- dinov, R.; and Manning, C. D. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In EMNLP. Zellers, R.; Bisk, Y.; Schwartz, R.; and Choi, Y. 2018. SWAG: A large-scale adversarial dataset for grounded commonsense infer- ence. In EMNLP.\n\nFrom recognition to cognition: Visual commonsense reasoning. R Zellers, Y Bisk, A Farhadi, Y Choi, CVPR. Zellers, R.; Bisk, Y.; Farhadi, A.; and Choi, Y. 2019a. From recog- nition to cognition: Visual commonsense reasoning. In CVPR.\n\nHellaSwag: Can a machine really finish your sentence. R Zellers, A Holtzman, Y Bisk, A Farhadi, Y Choi, ACL. Zellers, R.; Holtzman, A.; Bisk, Y.; Farhadi, A.; and Choi, Y. 2019b. HellaSwag: Can a machine really finish your sentence? In ACL.\n", "annotations": {"author": "[{\"end\":79,\"start\":67},{\"end\":92,\"start\":80},{\"end\":109,\"start\":93},{\"end\":187,\"start\":110},{\"end\":225,\"start\":188},{\"end\":265,\"start\":226}]", "publisher": null, "author_last_name": "[{\"end\":78,\"start\":74},{\"end\":91,\"start\":86},{\"end\":108,\"start\":100},{\"end\":122,\"start\":116},{\"end\":204,\"start\":195}]", "author_first_name": "[{\"end\":73,\"start\":67},{\"end\":85,\"start\":80},{\"end\":99,\"start\":93},{\"end\":115,\"start\":110},{\"end\":194,\"start\":188}]", "author_affiliation": "[{\"end\":186,\"start\":151},{\"end\":264,\"start\":227}]", "title": "[{\"end\":64,\"start\":1},{\"end\":329,\"start\":266}]", "venue": null, "abstract": "[{\"end\":1634,\"start\":331}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1839,\"start\":1817},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1857,\"start\":1839},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1891,\"start\":1857},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1914,\"start\":1891},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2193,\"start\":2170},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2312,\"start\":2295},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4894,\"start\":4876},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4917,\"start\":4894},{\"end\":5313,\"start\":5312},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5794,\"start\":5773},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6014,\"start\":5996},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6037,\"start\":6014},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6867,\"start\":6847},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6886,\"start\":6867},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7678,\"start\":7673},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7753,\"start\":7731},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8330,\"start\":8308},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8447,\"start\":8427},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8466,\"start\":8447},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8737,\"start\":8702},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8998,\"start\":8974},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9232,\"start\":9215},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9519,\"start\":9502},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9898,\"start\":9866},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9917,\"start\":9898},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9938,\"start\":9917},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10088,\"start\":10068},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10170,\"start\":10157},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10189,\"start\":10170},{\"end\":10311,\"start\":10292},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10332,\"start\":10311},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14741,\"start\":14722},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15672,\"start\":15652},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15777,\"start\":15756},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16530,\"start\":16514},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16620,\"start\":16599},{\"end\":17371,\"start\":17370},{\"end\":17435,\"start\":17430},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17484,\"start\":17465},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18518,\"start\":18500},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18541,\"start\":18518},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19197,\"start\":19165},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19219,\"start\":19197},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21705,\"start\":21686},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21724,\"start\":21705},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21744,\"start\":21724},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24287,\"start\":24266},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24375,\"start\":24342},{\"end\":25701,\"start\":25685},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28143,\"start\":28122},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28149,\"start\":28143},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28328,\"start\":28307},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28334,\"start\":28328},{\"end\":28400,\"start\":28373},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28666,\"start\":28644},{\"end\":31401,\"start\":31399},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31886,\"start\":31872},{\"end\":33206,\"start\":33190},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34397,\"start\":34359},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":40058,\"start\":40040},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":40099,\"start\":40079},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":41112,\"start\":41093}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37694,\"start\":37450},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37753,\"start\":37695},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37843,\"start\":37754},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37918,\"start\":37844},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38068,\"start\":37919},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38429,\"start\":38069},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":39071,\"start\":38430},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":39425,\"start\":39072},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":39599,\"start\":39426},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":39738,\"start\":39600},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":40955,\"start\":39739},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":41371,\"start\":40956},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":41419,\"start\":41372}]", "paragraph": "[{\"end\":2408,\"start\":1650},{\"end\":2673,\"start\":2410},{\"end\":3192,\"start\":2675},{\"end\":4074,\"start\":3194},{\"end\":4621,\"start\":4076},{\"end\":5795,\"start\":4761},{\"end\":6143,\"start\":5797},{\"end\":6779,\"start\":6145},{\"end\":7109,\"start\":6781},{\"end\":7537,\"start\":7111},{\"end\":8298,\"start\":7575},{\"end\":8567,\"start\":8300},{\"end\":9741,\"start\":8569},{\"end\":10514,\"start\":9743},{\"end\":10892,\"start\":10550},{\"end\":11493,\"start\":10977},{\"end\":12855,\"start\":11495},{\"end\":13418,\"start\":12888},{\"end\":13708,\"start\":13420},{\"end\":14492,\"start\":13724},{\"end\":15233,\"start\":14494},{\"end\":15816,\"start\":15257},{\"end\":16290,\"start\":15857},{\"end\":16391,\"start\":16319},{\"end\":16653,\"start\":16393},{\"end\":17637,\"start\":16677},{\"end\":18696,\"start\":17639},{\"end\":19076,\"start\":18734},{\"end\":19836,\"start\":19078},{\"end\":20363,\"start\":19838},{\"end\":21474,\"start\":20388},{\"end\":22191,\"start\":21476},{\"end\":22436,\"start\":22220},{\"end\":23282,\"start\":22463},{\"end\":23773,\"start\":23284},{\"end\":24209,\"start\":23775},{\"end\":24719,\"start\":24211},{\"end\":25499,\"start\":24721},{\"end\":26223,\"start\":25526},{\"end\":26626,\"start\":26262},{\"end\":27552,\"start\":26628},{\"end\":27953,\"start\":27586},{\"end\":28575,\"start\":27955},{\"end\":29204,\"start\":28598},{\"end\":29805,\"start\":29206},{\"end\":30479,\"start\":29807},{\"end\":30786,\"start\":30516},{\"end\":32274,\"start\":30788},{\"end\":32798,\"start\":32308},{\"end\":33087,\"start\":32800},{\"end\":33938,\"start\":33104},{\"end\":34213,\"start\":33954},{\"end\":35206,\"start\":34215},{\"end\":35769,\"start\":35208},{\"end\":36488,\"start\":35771},{\"end\":37195,\"start\":36503},{\"end\":37449,\"start\":37197}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":4760,\"start\":4622},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10976,\"start\":10893},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15856,\"start\":15817},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16318,\"start\":16291}]", "table_ref": "[{\"end\":4315,\"start\":4308},{\"end\":7316,\"start\":7309},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":12223,\"start\":12216},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17881,\"start\":17874},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":21020,\"start\":21013},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":21930,\"start\":21923},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22501,\"start\":22494},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27315,\"start\":27308},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":32643,\"start\":32636},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":33419,\"start\":33412},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":33844,\"start\":33837},{\"end\":33892,\"start\":33885},{\"end\":34405,\"start\":34398},{\"end\":34885,\"start\":34878}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1648,\"start\":1636},{\"attributes\":{\"n\":\"2\"},\"end\":7573,\"start\":7540},{\"attributes\":{\"n\":\"2.1\"},\"end\":10548,\"start\":10517},{\"attributes\":{\"n\":\"3\"},\"end\":12886,\"start\":12858},{\"attributes\":{\"n\":\"3.1\"},\"end\":13722,\"start\":13711},{\"attributes\":{\"n\":\"3.2\"},\"end\":15255,\"start\":15236},{\"attributes\":{\"n\":\"3.3\"},\"end\":16675,\"start\":16656},{\"attributes\":{\"n\":\"4\"},\"end\":18709,\"start\":18699},{\"attributes\":{\"n\":\"4.1\"},\"end\":18732,\"start\":18712},{\"attributes\":{\"n\":\"4.2\"},\"end\":20386,\"start\":20366},{\"attributes\":{\"n\":\"5\"},\"end\":22218,\"start\":22194},{\"attributes\":{\"n\":\"5.1\"},\"end\":22461,\"start\":22439},{\"attributes\":{\"n\":\"5.2\"},\"end\":25524,\"start\":25502},{\"attributes\":{\"n\":\"5.3\"},\"end\":26260,\"start\":26226},{\"attributes\":{\"n\":\"6\"},\"end\":27584,\"start\":27555},{\"attributes\":{\"n\":\"6.1\"},\"end\":28596,\"start\":28578},{\"attributes\":{\"n\":\"6.2\"},\"end\":30514,\"start\":30482},{\"attributes\":{\"n\":\"6.3\"},\"end\":32306,\"start\":32277},{\"attributes\":{\"n\":\"6.4\"},\"end\":33102,\"start\":33090},{\"attributes\":{\"n\":\"7\"},\"end\":33952,\"start\":33941},{\"attributes\":{\"n\":\"8\"},\"end\":36501,\"start\":36491},{\"end\":37706,\"start\":37696},{\"end\":37765,\"start\":37755},{\"end\":37855,\"start\":37845},{\"end\":38440,\"start\":38431},{\"end\":39082,\"start\":39073},{\"end\":39610,\"start\":39601},{\"end\":41382,\"start\":41373}]", "table": "[{\"end\":38068,\"start\":37983},{\"end\":38429,\"start\":38217},{\"end\":39071,\"start\":38555},{\"end\":39599,\"start\":39460},{\"end\":40955,\"start\":40164},{\"end\":41371,\"start\":41196}]", "figure_caption": "[{\"end\":37694,\"start\":37452},{\"end\":37753,\"start\":37708},{\"end\":37843,\"start\":37767},{\"end\":37918,\"start\":37857},{\"end\":37983,\"start\":37921},{\"end\":38217,\"start\":38071},{\"end\":38555,\"start\":38442},{\"end\":39425,\"start\":39084},{\"end\":39460,\"start\":39428},{\"end\":39738,\"start\":39612},{\"end\":40164,\"start\":39741},{\"end\":41196,\"start\":40958},{\"end\":41419,\"start\":41384}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3353,\"start\":3347},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3636,\"start\":3628},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12233,\"start\":12225},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32273,\"start\":32265}]", "bib_author_first_name": "[{\"end\":42833,\"start\":42832},{\"end\":42844,\"start\":42843},{\"end\":42856,\"start\":42855},{\"end\":42871,\"start\":42870},{\"end\":42881,\"start\":42880},{\"end\":42888,\"start\":42887},{\"end\":42900,\"start\":42899},{\"end\":42909,\"start\":42908},{\"end\":42928,\"start\":42927},{\"end\":42943,\"start\":42942},{\"end\":42953,\"start\":42952},{\"end\":42961,\"start\":42960},{\"end\":42977,\"start\":42976},{\"end\":43491,\"start\":43490},{\"end\":43640,\"start\":43639},{\"end\":43651,\"start\":43647},{\"end\":43661,\"start\":43657},{\"end\":43669,\"start\":43668},{\"end\":43676,\"start\":43675},{\"end\":43685,\"start\":43684},{\"end\":43895,\"start\":43894},{\"end\":43904,\"start\":43903},{\"end\":43923,\"start\":43922},{\"end\":43942,\"start\":43941},{\"end\":43955,\"start\":43954},{\"end\":43966,\"start\":43965},{\"end\":43979,\"start\":43978},{\"end\":44389,\"start\":44388},{\"end\":44398,\"start\":44397},{\"end\":44409,\"start\":44408},{\"end\":44417,\"start\":44416},{\"end\":44430,\"start\":44429},{\"end\":44441,\"start\":44440},{\"end\":44443,\"start\":44442},{\"end\":44453,\"start\":44452},{\"end\":44748,\"start\":44747},{\"end\":44757,\"start\":44756},{\"end\":44767,\"start\":44766},{\"end\":44778,\"start\":44777},{\"end\":44786,\"start\":44785},{\"end\":44799,\"start\":44798},{\"end\":44812,\"start\":44811},{\"end\":45126,\"start\":45125},{\"end\":45139,\"start\":45135},{\"end\":45148,\"start\":45147},{\"end\":45155,\"start\":45154},{\"end\":45415,\"start\":45414},{\"end\":45422,\"start\":45421},{\"end\":45430,\"start\":45429},{\"end\":45440,\"start\":45439},{\"end\":45453,\"start\":45452},{\"end\":45462,\"start\":45461},{\"end\":45662,\"start\":45661},{\"end\":45671,\"start\":45670},{\"end\":45681,\"start\":45680},{\"end\":45696,\"start\":45695},{\"end\":45708,\"start\":45707},{\"end\":46060,\"start\":46059},{\"end\":46070,\"start\":46069},{\"end\":46089,\"start\":46088},{\"end\":46101,\"start\":46100},{\"end\":46377,\"start\":46376},{\"end\":46387,\"start\":46386},{\"end\":46396,\"start\":46395},{\"end\":46408,\"start\":46407},{\"end\":46723,\"start\":46722},{\"end\":46725,\"start\":46724},{\"end\":46735,\"start\":46734},{\"end\":46749,\"start\":46748},{\"end\":46764,\"start\":46763},{\"end\":46766,\"start\":46765},{\"end\":47147,\"start\":47146},{\"end\":47436,\"start\":47435},{\"end\":47448,\"start\":47447},{\"end\":47456,\"start\":47455},{\"end\":47469,\"start\":47468},{\"end\":47478,\"start\":47477},{\"end\":47489,\"start\":47488},{\"end\":47713,\"start\":47712},{\"end\":47725,\"start\":47724},{\"end\":47733,\"start\":47732},{\"end\":47746,\"start\":47745},{\"end\":47968,\"start\":47967},{\"end\":47980,\"start\":47979},{\"end\":47994,\"start\":47993},{\"end\":48002,\"start\":48001},{\"end\":48014,\"start\":48013},{\"end\":48276,\"start\":48275},{\"end\":48288,\"start\":48287},{\"end\":48296,\"start\":48295},{\"end\":48309,\"start\":48308},{\"end\":48546,\"start\":48545},{\"end\":48558,\"start\":48557},{\"end\":48560,\"start\":48559},{\"end\":48568,\"start\":48567},{\"end\":48576,\"start\":48575},{\"end\":48589,\"start\":48588},{\"end\":48857,\"start\":48856},{\"end\":48865,\"start\":48864},{\"end\":48878,\"start\":48877},{\"end\":49091,\"start\":49090},{\"end\":49099,\"start\":49098},{\"end\":49112,\"start\":49111},{\"end\":49333,\"start\":49332},{\"end\":49340,\"start\":49339},{\"end\":49347,\"start\":49346},{\"end\":49354,\"start\":49353},{\"end\":49362,\"start\":49361},{\"end\":49364,\"start\":49363},{\"end\":49605,\"start\":49604},{\"end\":49617,\"start\":49616},{\"end\":49626,\"start\":49625},{\"end\":49634,\"start\":49633},{\"end\":49924,\"start\":49923},{\"end\":49926,\"start\":49925},{\"end\":49936,\"start\":49935},{\"end\":49945,\"start\":49944},{\"end\":49955,\"start\":49954},{\"end\":49966,\"start\":49965},{\"end\":49970,\"start\":49967},{\"end\":49979,\"start\":49978},{\"end\":49986,\"start\":49985},{\"end\":49992,\"start\":49991},{\"end\":49998,\"start\":49997},{\"end\":50512,\"start\":50511},{\"end\":50526,\"start\":50525},{\"end\":50536,\"start\":50535},{\"end\":50538,\"start\":50537},{\"end\":50744,\"start\":50743},{\"end\":50746,\"start\":50745},{\"end\":50756,\"start\":50755},{\"end\":50767,\"start\":50766},{\"end\":50776,\"start\":50775},{\"end\":50787,\"start\":50786},{\"end\":50796,\"start\":50795},{\"end\":50803,\"start\":50802},{\"end\":51029,\"start\":51028},{\"end\":51040,\"start\":51039},{\"end\":51046,\"start\":51045},{\"end\":51055,\"start\":51054},{\"end\":51063,\"start\":51062},{\"end\":51073,\"start\":51072},{\"end\":51291,\"start\":51290},{\"end\":51298,\"start\":51297},{\"end\":51304,\"start\":51303},{\"end\":51310,\"start\":51309},{\"end\":51541,\"start\":51540},{\"end\":51551,\"start\":51550},{\"end\":51755,\"start\":51754},{\"end\":51764,\"start\":51763},{\"end\":51777,\"start\":51776},{\"end\":52007,\"start\":52006},{\"end\":52017,\"start\":52016},{\"end\":52027,\"start\":52026},{\"end\":52037,\"start\":52036},{\"end\":52296,\"start\":52295},{\"end\":52304,\"start\":52303},{\"end\":52310,\"start\":52309},{\"end\":52319,\"start\":52318},{\"end\":52329,\"start\":52328},{\"end\":52331,\"start\":52330},{\"end\":52340,\"start\":52339},{\"end\":52357,\"start\":52356},{\"end\":52359,\"start\":52358},{\"end\":52370,\"start\":52369},{\"end\":52381,\"start\":52380},{\"end\":52389,\"start\":52388},{\"end\":52401,\"start\":52400},{\"end\":52813,\"start\":52812},{\"end\":52824,\"start\":52823},{\"end\":52832,\"start\":52831},{\"end\":52843,\"start\":52842},{\"end\":53040,\"start\":53039},{\"end\":53051,\"start\":53050},{\"end\":53063,\"start\":53062},{\"end\":53071,\"start\":53070},{\"end\":53082,\"start\":53081}]", "bib_author_last_name": "[{\"end\":42841,\"start\":42834},{\"end\":42853,\"start\":42845},{\"end\":42868,\"start\":42857},{\"end\":42878,\"start\":42872},{\"end\":42885,\"start\":42882},{\"end\":42897,\"start\":42889},{\"end\":42906,\"start\":42901},{\"end\":42925,\"start\":42910},{\"end\":42940,\"start\":42929},{\"end\":42950,\"start\":42944},{\"end\":42958,\"start\":42954},{\"end\":42974,\"start\":42962},{\"end\":42986,\"start\":42978},{\"end\":43499,\"start\":43492},{\"end\":43645,\"start\":43641},{\"end\":43655,\"start\":43652},{\"end\":43666,\"start\":43662},{\"end\":43673,\"start\":43670},{\"end\":43682,\"start\":43677},{\"end\":43692,\"start\":43686},{\"end\":43901,\"start\":43896},{\"end\":43920,\"start\":43905},{\"end\":43939,\"start\":43924},{\"end\":43952,\"start\":43943},{\"end\":43963,\"start\":43956},{\"end\":43976,\"start\":43967},{\"end\":43987,\"start\":43980},{\"end\":44395,\"start\":44390},{\"end\":44406,\"start\":44399},{\"end\":44414,\"start\":44410},{\"end\":44427,\"start\":44418},{\"end\":44438,\"start\":44431},{\"end\":44450,\"start\":44444},{\"end\":44462,\"start\":44454},{\"end\":44754,\"start\":44749},{\"end\":44764,\"start\":44758},{\"end\":44775,\"start\":44768},{\"end\":44783,\"start\":44779},{\"end\":44796,\"start\":44787},{\"end\":44809,\"start\":44800},{\"end\":44820,\"start\":44813},{\"end\":45133,\"start\":45127},{\"end\":45145,\"start\":45140},{\"end\":45152,\"start\":45149},{\"end\":45165,\"start\":45156},{\"end\":45419,\"start\":45416},{\"end\":45427,\"start\":45423},{\"end\":45437,\"start\":45431},{\"end\":45450,\"start\":45441},{\"end\":45459,\"start\":45454},{\"end\":45470,\"start\":45463},{\"end\":45668,\"start\":45663},{\"end\":45678,\"start\":45672},{\"end\":45693,\"start\":45682},{\"end\":45705,\"start\":45697},{\"end\":45714,\"start\":45709},{\"end\":46067,\"start\":46061},{\"end\":46086,\"start\":46071},{\"end\":46098,\"start\":46090},{\"end\":46107,\"start\":46102},{\"end\":46384,\"start\":46378},{\"end\":46393,\"start\":46388},{\"end\":46405,\"start\":46397},{\"end\":46414,\"start\":46409},{\"end\":46732,\"start\":46726},{\"end\":46746,\"start\":46736},{\"end\":46761,\"start\":46750},{\"end\":46775,\"start\":46767},{\"end\":47154,\"start\":47148},{\"end\":47445,\"start\":47437},{\"end\":47453,\"start\":47449},{\"end\":47466,\"start\":47457},{\"end\":47475,\"start\":47470},{\"end\":47486,\"start\":47479},{\"end\":47494,\"start\":47490},{\"end\":47722,\"start\":47714},{\"end\":47730,\"start\":47726},{\"end\":47743,\"start\":47734},{\"end\":47751,\"start\":47747},{\"end\":47977,\"start\":47969},{\"end\":47991,\"start\":47981},{\"end\":47999,\"start\":47995},{\"end\":48011,\"start\":48003},{\"end\":48019,\"start\":48015},{\"end\":48285,\"start\":48277},{\"end\":48293,\"start\":48289},{\"end\":48306,\"start\":48297},{\"end\":48314,\"start\":48310},{\"end\":48555,\"start\":48547},{\"end\":48565,\"start\":48561},{\"end\":48573,\"start\":48569},{\"end\":48586,\"start\":48577},{\"end\":48594,\"start\":48590},{\"end\":48862,\"start\":48858},{\"end\":48875,\"start\":48866},{\"end\":48884,\"start\":48879},{\"end\":49096,\"start\":49092},{\"end\":49109,\"start\":49100},{\"end\":49118,\"start\":49113},{\"end\":49337,\"start\":49334},{\"end\":49344,\"start\":49341},{\"end\":49351,\"start\":49348},{\"end\":49359,\"start\":49355},{\"end\":49369,\"start\":49365},{\"end\":49614,\"start\":49606},{\"end\":49623,\"start\":49618},{\"end\":49631,\"start\":49627},{\"end\":49644,\"start\":49635},{\"end\":49933,\"start\":49927},{\"end\":49942,\"start\":49937},{\"end\":49952,\"start\":49946},{\"end\":49963,\"start\":49956},{\"end\":49976,\"start\":49971},{\"end\":49983,\"start\":49980},{\"end\":49989,\"start\":49987},{\"end\":49995,\"start\":49993},{\"end\":50001,\"start\":49999},{\"end\":50523,\"start\":50513},{\"end\":50533,\"start\":50527},{\"end\":50546,\"start\":50539},{\"end\":50753,\"start\":50747},{\"end\":50764,\"start\":50757},{\"end\":50773,\"start\":50768},{\"end\":50784,\"start\":50777},{\"end\":50793,\"start\":50788},{\"end\":50800,\"start\":50797},{\"end\":50815,\"start\":50804},{\"end\":51037,\"start\":51030},{\"end\":51043,\"start\":51041},{\"end\":51052,\"start\":51047},{\"end\":51060,\"start\":51056},{\"end\":51070,\"start\":51064},{\"end\":51083,\"start\":51074},{\"end\":51295,\"start\":51292},{\"end\":51301,\"start\":51299},{\"end\":51307,\"start\":51305},{\"end\":51317,\"start\":51311},{\"end\":51548,\"start\":51542},{\"end\":51558,\"start\":51552},{\"end\":51761,\"start\":51756},{\"end\":51774,\"start\":51765},{\"end\":51784,\"start\":51778},{\"end\":52014,\"start\":52008},{\"end\":52024,\"start\":52018},{\"end\":52034,\"start\":52028},{\"end\":52045,\"start\":52038},{\"end\":52301,\"start\":52297},{\"end\":52307,\"start\":52305},{\"end\":52316,\"start\":52311},{\"end\":52326,\"start\":52320},{\"end\":52337,\"start\":52332},{\"end\":52354,\"start\":52341},{\"end\":52367,\"start\":52360},{\"end\":52378,\"start\":52371},{\"end\":52386,\"start\":52382},{\"end\":52398,\"start\":52390},{\"end\":52406,\"start\":52402},{\"end\":52821,\"start\":52814},{\"end\":52829,\"start\":52825},{\"end\":52840,\"start\":52833},{\"end\":52848,\"start\":52844},{\"end\":53048,\"start\":53041},{\"end\":53060,\"start\":53052},{\"end\":53068,\"start\":53064},{\"end\":53079,\"start\":53072},{\"end\":53087,\"start\":53083}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":44133715},\"end\":43468,\"start\":42743},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":47328136},\"end\":43591,\"start\":43470},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":34032948},\"end\":43828,\"start\":43593},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2851712},\"end\":44299,\"start\":43830},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1255845},\"end\":44665,\"start\":44301},{\"attributes\":{\"doi\":\"CoRR abs/1803.05457\",\"id\":\"b5\"},\"end\":45041,\"start\":44667},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52967399},\"end\":45326,\"start\":45043},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":67855846},\"end\":45657,\"start\":45328},{\"attributes\":{\"id\":\"b8\"},\"end\":45783,\"start\":45659},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":12416658},\"end\":45949,\"start\":45785},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2498076},\"end\":46302,\"start\":45951},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":10741411},\"end\":46613,\"start\":46304},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3623373},\"end\":47006,\"start\":46615},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":44094607},\"end\":47358,\"start\":47008},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7145148},\"end\":47669,\"start\":47360},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":33698356},\"end\":47870,\"start\":47671},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5112038},\"end\":48206,\"start\":47872},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":20638934},\"end\":48458,\"start\":48208},{\"attributes\":{\"id\":\"b18\"},\"end\":48791,\"start\":48460},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":957320},\"end\":49006,\"start\":48793},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":202712552},\"end\":49263,\"start\":49008},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6826032},\"end\":49513,\"start\":49265},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":52183757},\"end\":49810,\"start\":49515},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5019682},\"end\":50462,\"start\":49812},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1957433},\"end\":50699,\"start\":50464},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3626819},\"end\":50973,\"start\":50701},{\"attributes\":{\"id\":\"b26\"},\"end\":51215,\"start\":50975},{\"attributes\":{\"doi\":\"CoRR abs/1810.13441\",\"id\":\"b27\"},\"end\":51477,\"start\":51217},{\"attributes\":{\"id\":\"b28\"},\"end\":51676,\"start\":51479},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":9192723},\"end\":51931,\"start\":51678},{\"attributes\":{\"doi\":\"CoRR abs/1502.05698\",\"id\":\"b30\"},\"end\":52217,\"start\":51933},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":52019251},\"end\":52749,\"start\":52219},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":53734356},\"end\":52983,\"start\":52751},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":159041722},\"end\":53225,\"start\":52985}]", "bib_title": "[{\"end\":42830,\"start\":42743},{\"end\":43488,\"start\":43470},{\"end\":43637,\"start\":43593},{\"end\":43892,\"start\":43830},{\"end\":44386,\"start\":44301},{\"end\":45123,\"start\":45043},{\"end\":45412,\"start\":45328},{\"end\":45854,\"start\":45785},{\"end\":46057,\"start\":45951},{\"end\":46374,\"start\":46304},{\"end\":46720,\"start\":46615},{\"end\":47144,\"start\":47008},{\"end\":47433,\"start\":47360},{\"end\":47710,\"start\":47671},{\"end\":47965,\"start\":47872},{\"end\":48273,\"start\":48208},{\"end\":48854,\"start\":48793},{\"end\":49088,\"start\":49008},{\"end\":49330,\"start\":49265},{\"end\":49602,\"start\":49515},{\"end\":49921,\"start\":49812},{\"end\":50509,\"start\":50464},{\"end\":50741,\"start\":50701},{\"end\":51752,\"start\":51678},{\"end\":52293,\"start\":52219},{\"end\":52810,\"start\":52751},{\"end\":53037,\"start\":52985}]", "bib_author": "[{\"end\":42843,\"start\":42832},{\"end\":42855,\"start\":42843},{\"end\":42870,\"start\":42855},{\"end\":42880,\"start\":42870},{\"end\":42887,\"start\":42880},{\"end\":42899,\"start\":42887},{\"end\":42908,\"start\":42899},{\"end\":42927,\"start\":42908},{\"end\":42942,\"start\":42927},{\"end\":42952,\"start\":42942},{\"end\":42960,\"start\":42952},{\"end\":42976,\"start\":42960},{\"end\":42988,\"start\":42976},{\"end\":43501,\"start\":43490},{\"end\":43647,\"start\":43639},{\"end\":43657,\"start\":43647},{\"end\":43668,\"start\":43657},{\"end\":43675,\"start\":43668},{\"end\":43684,\"start\":43675},{\"end\":43694,\"start\":43684},{\"end\":43903,\"start\":43894},{\"end\":43922,\"start\":43903},{\"end\":43941,\"start\":43922},{\"end\":43954,\"start\":43941},{\"end\":43965,\"start\":43954},{\"end\":43978,\"start\":43965},{\"end\":43989,\"start\":43978},{\"end\":44397,\"start\":44388},{\"end\":44408,\"start\":44397},{\"end\":44416,\"start\":44408},{\"end\":44429,\"start\":44416},{\"end\":44440,\"start\":44429},{\"end\":44452,\"start\":44440},{\"end\":44464,\"start\":44452},{\"end\":44756,\"start\":44747},{\"end\":44766,\"start\":44756},{\"end\":44777,\"start\":44766},{\"end\":44785,\"start\":44777},{\"end\":44798,\"start\":44785},{\"end\":44811,\"start\":44798},{\"end\":44822,\"start\":44811},{\"end\":45135,\"start\":45125},{\"end\":45147,\"start\":45135},{\"end\":45154,\"start\":45147},{\"end\":45167,\"start\":45154},{\"end\":45421,\"start\":45414},{\"end\":45429,\"start\":45421},{\"end\":45439,\"start\":45429},{\"end\":45452,\"start\":45439},{\"end\":45461,\"start\":45452},{\"end\":45472,\"start\":45461},{\"end\":45670,\"start\":45661},{\"end\":45680,\"start\":45670},{\"end\":45695,\"start\":45680},{\"end\":45707,\"start\":45695},{\"end\":45716,\"start\":45707},{\"end\":46069,\"start\":46059},{\"end\":46088,\"start\":46069},{\"end\":46100,\"start\":46088},{\"end\":46109,\"start\":46100},{\"end\":46386,\"start\":46376},{\"end\":46395,\"start\":46386},{\"end\":46407,\"start\":46395},{\"end\":46416,\"start\":46407},{\"end\":46734,\"start\":46722},{\"end\":46748,\"start\":46734},{\"end\":46763,\"start\":46748},{\"end\":46777,\"start\":46763},{\"end\":47156,\"start\":47146},{\"end\":47447,\"start\":47435},{\"end\":47455,\"start\":47447},{\"end\":47468,\"start\":47455},{\"end\":47477,\"start\":47468},{\"end\":47488,\"start\":47477},{\"end\":47496,\"start\":47488},{\"end\":47724,\"start\":47712},{\"end\":47732,\"start\":47724},{\"end\":47745,\"start\":47732},{\"end\":47753,\"start\":47745},{\"end\":47979,\"start\":47967},{\"end\":47993,\"start\":47979},{\"end\":48001,\"start\":47993},{\"end\":48013,\"start\":48001},{\"end\":48021,\"start\":48013},{\"end\":48287,\"start\":48275},{\"end\":48295,\"start\":48287},{\"end\":48308,\"start\":48295},{\"end\":48316,\"start\":48308},{\"end\":48557,\"start\":48545},{\"end\":48567,\"start\":48557},{\"end\":48575,\"start\":48567},{\"end\":48588,\"start\":48575},{\"end\":48596,\"start\":48588},{\"end\":48864,\"start\":48856},{\"end\":48877,\"start\":48864},{\"end\":48886,\"start\":48877},{\"end\":49098,\"start\":49090},{\"end\":49111,\"start\":49098},{\"end\":49120,\"start\":49111},{\"end\":49339,\"start\":49332},{\"end\":49346,\"start\":49339},{\"end\":49353,\"start\":49346},{\"end\":49361,\"start\":49353},{\"end\":49371,\"start\":49361},{\"end\":49616,\"start\":49604},{\"end\":49625,\"start\":49616},{\"end\":49633,\"start\":49625},{\"end\":49646,\"start\":49633},{\"end\":49935,\"start\":49923},{\"end\":49944,\"start\":49935},{\"end\":49954,\"start\":49944},{\"end\":49965,\"start\":49954},{\"end\":49978,\"start\":49965},{\"end\":49985,\"start\":49978},{\"end\":49991,\"start\":49985},{\"end\":49997,\"start\":49991},{\"end\":50003,\"start\":49997},{\"end\":50525,\"start\":50511},{\"end\":50535,\"start\":50525},{\"end\":50548,\"start\":50535},{\"end\":50755,\"start\":50743},{\"end\":50766,\"start\":50755},{\"end\":50775,\"start\":50766},{\"end\":50786,\"start\":50775},{\"end\":50795,\"start\":50786},{\"end\":50802,\"start\":50795},{\"end\":50817,\"start\":50802},{\"end\":51039,\"start\":51028},{\"end\":51045,\"start\":51039},{\"end\":51054,\"start\":51045},{\"end\":51062,\"start\":51054},{\"end\":51072,\"start\":51062},{\"end\":51085,\"start\":51072},{\"end\":51297,\"start\":51290},{\"end\":51303,\"start\":51297},{\"end\":51309,\"start\":51303},{\"end\":51319,\"start\":51309},{\"end\":51550,\"start\":51540},{\"end\":51560,\"start\":51550},{\"end\":51763,\"start\":51754},{\"end\":51776,\"start\":51763},{\"end\":51786,\"start\":51776},{\"end\":52016,\"start\":52006},{\"end\":52026,\"start\":52016},{\"end\":52036,\"start\":52026},{\"end\":52047,\"start\":52036},{\"end\":52303,\"start\":52295},{\"end\":52309,\"start\":52303},{\"end\":52318,\"start\":52309},{\"end\":52328,\"start\":52318},{\"end\":52339,\"start\":52328},{\"end\":52356,\"start\":52339},{\"end\":52369,\"start\":52356},{\"end\":52380,\"start\":52369},{\"end\":52388,\"start\":52380},{\"end\":52400,\"start\":52388},{\"end\":52408,\"start\":52400},{\"end\":52823,\"start\":52812},{\"end\":52831,\"start\":52823},{\"end\":52842,\"start\":52831},{\"end\":52850,\"start\":52842},{\"end\":53050,\"start\":53039},{\"end\":53062,\"start\":53050},{\"end\":53070,\"start\":53062},{\"end\":53081,\"start\":53070},{\"end\":53089,\"start\":53081}]", "bib_venue": "[{\"end\":43057,\"start\":42988},{\"end\":43517,\"start\":43501},{\"end\":43697,\"start\":43694},{\"end\":44049,\"start\":43989},{\"end\":44468,\"start\":44464},{\"end\":44745,\"start\":44667},{\"end\":45172,\"start\":45167},{\"end\":45477,\"start\":45472},{\"end\":45860,\"start\":45856},{\"end\":46115,\"start\":46109},{\"end\":46441,\"start\":46416},{\"end\":46796,\"start\":46777},{\"end\":47176,\"start\":47156},{\"end\":47501,\"start\":47496},{\"end\":47758,\"start\":47753},{\"end\":48026,\"start\":48021},{\"end\":48320,\"start\":48316},{\"end\":48543,\"start\":48460},{\"end\":48889,\"start\":48886},{\"end\":49125,\"start\":49120},{\"end\":49376,\"start\":49371},{\"end\":49651,\"start\":49646},{\"end\":50072,\"start\":50003},{\"end\":50568,\"start\":50548},{\"end\":50822,\"start\":50817},{\"end\":51026,\"start\":50975},{\"end\":51288,\"start\":51217},{\"end\":51538,\"start\":51479},{\"end\":51790,\"start\":51786},{\"end\":52004,\"start\":51933},{\"end\":52413,\"start\":52408},{\"end\":52854,\"start\":52850},{\"end\":53092,\"start\":53089},{\"end\":43113,\"start\":43059},{\"end\":46802,\"start\":46798},{\"end\":50575,\"start\":50570}]"}}}, "year": 2023, "month": 12, "day": 17}