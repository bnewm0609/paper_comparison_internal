{"id": 216642177, "updated": "2023-10-06 16:11:39.482", "metadata": {"title": "Task-Projected Hyperdimensional Computing for Multi-Task Learning", "authors": "[{\"first\":\"Cheng-Yang\",\"last\":\"Chang\",\"middle\":[]},{\"first\":\"Yu-Chuan\",\"last\":\"Chuang\",\"middle\":[]},{\"first\":\"An-Yeu\",\"last\":\"Wu\",\"middle\":[]}]", "venue": "IFIP Advances in Information and Communication Technology", "journal": "IFIP Advances in Information and Communication Technology", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Brain-inspired Hyperdimensional (HD) computing is an emerging technique for cognitive tasks in the field of low-power design. As a fast-learning and energy-efficient computational paradigm, HD computing has shown great success in many real-world applications. However, an HD model incrementally trained on multiple tasks suffers from the negative impacts of catastrophic forgetting. The model forgets the knowledge learned from previous tasks and only focuses on the current one. To the best of our knowledge, no study has been conducted to investigate the feasibility of applying multi-task learning to HD computing. In this paper, we propose Task-Projected Hyperdimensional Computing (TP-HDC) to make the HD model simultaneously support multiple tasks by exploiting the redundant dimensionality in the hyperspace. To mitigate the interferences between different tasks, we project each task into a separate subspace for learning. Compared with the baseline method, our approach efficiently utilizes the unused capacity in the hyperspace and shows a 12.8% improvement in averaged accuracy with negligible memory overhead.", "fields_of_study": "[\"Engineering\"]", "external_ids": {"arxiv": "2004.14252", "mag": "3038077200", "acl": null, "pubmed": null, "pubmedcentral": "7256401", "dblp": "conf/ifip12/ChangCW20", "doi": "10.1007/978-3-030-49161-1_21"}}, "content": {"source": {"pdf_hash": "23995646bd9bef3fde108015731c41295b393f47", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2004.14252v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://link.springer.com/content/pdf/10.1007/978-3-030-49161-1_21.pdf", "status": "BRONZE"}}, "grobid": {"id": "b0ccecb47641c2406e473c16ced4f7e22dd6153a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/23995646bd9bef3fde108015731c41295b393f47.txt", "contents": "\nTask-Projected Hyperdimensional Computing for Multi-Task Learning\n\n\nCheng-Yang Chang \nGraduate Institute of Electronics Engineering\nNational Taiwan University\nTaipeiTaiwan\n\nYu-Chuan Chuang \nGraduate Institute of Electronics Engineering\nNational Taiwan University\nTaipeiTaiwan\n\nAn-Yeu \nGraduate Institute of Electronics Engineering\nNational Taiwan University\nTaipeiTaiwan\n\nAndy Wu andywu@ntu.edu.tw \nGraduate Institute of Electronics Engineering\nNational Taiwan University\nTaipeiTaiwan\n\nTask-Projected Hyperdimensional Computing for Multi-Task Learning\n1Hyperdimensional Computing \uff0e Multi-task Learning \uff0e Redundant Dimensionality\nBrain-inspired Hyperdimensional (HD) computing is an emerging technique for cognitive tasks in the field of low-power design. As an energyefficient and fast learning computational paradigm, HD computing has shown great success in many real-world applications. However, an HD model incrementally trained on multiple tasks suffers from the negative impacts of catastrophic forgetting. The model forgets the knowledge learned from previous tasks and only focuses on the current one. To the best of our knowledge, no study has been conducted to investigate the feasibility of applying multi-task learning to HD computing. In this paper, we propose Task-Projected Hyperdimensional Computing (TP-HDC) to make the HD model simultaneously support multiple tasks by exploiting the redundant dimensionality in the hyperspace. To mitigate the interferences between different tasks, we project each task into a separate subspace for learning. Compared with the baseline method, our approach efficiently utilizes the unused capacity in the hyperspace and shows a 12.8% improvement in averaged accuracy with negligible memory overhead.\n\nIntroduction\n\nIn the era of IoT, edge computing with energy-efficient machine learning models keeps data processing close to end-users. This brings out numerous advantages, including lower latency, user security, and cost savings [1]. Meanwhile, multi-task learning (MTL) is grabbing attention recently since a single model can accommodate multiple cognitive tasks is more desirable for the future of IoT [2].\n\nBrain-inspired Hyperdimensional (HD) computing emulates the operations of brains and handles cognitive tasks in a hyperdimensional space with well-defined vector space operations [3]. As an energy-efficient and fast-learning computational paradigm, HD computing has shown successful progress in many real-world applications such as gesture recognition [4], language recognition [5], and general bio-signal processing [6] [7]. Moreover, HD computing can operate at an ultra lowpower condition with lower latency through massively parallel bitwise operation [8]. These advantages make HD computing suitable for efficient signal processing, e.g., 2\u00d7 lower energy at iso-accuracy when compared to a highly-optimized SVM on an ARM Cortex M4 [9]. However, an HD model incrementally trained on multiple tasks forgets the knowledge learned from previous tasks and only focuses on the current one. The phenomenon is called catastrophic forgetting [10]. To the best of our knowledge, no study has been conducted to overcome this problem and investigate the feasibility of applying MTL to HD computing. This paper aims to establish a reliable MTL framework based on HD computing to minimize the negative impact of catastrophic forgetting. Over-parameterization in DNN implies that only a small subspace spanned by the optimal parameters is occupied by a given task [11]. Based on this phenomenon, [12] exploits the redundant subspace in DNN to superimpose multiple models into one. We are inspired by this concept and propose to exploit the unused capacity in the hyperspace to project each task into a separate subspace for learning. Our approach efficiently mitigates the interferences between different tasks and keeps the knowledge learned from numerous tasks stored in one HD model with minimal accuracy degradation.\n\nThe rest of the paper is organized as follows. Section 2 provides a review of HD computing. Section 3 describes the proposed Task-projected Hyperdimensional Computing (TP-HDC) for multi-task learning. Section 4 shows our experiment setting and simulation results. Finally, we conclude this paper in Section 5.\n\n\nReview of Hyperdimensional Computing\n\nHD computing is based on high-dimensional and dense binary vectors, called HD vectors. The components of HD vectors are binary with equally probable (-1)s and 1s. The processing flow chart of a general HD computing is shown in Fig. 1 and can be divided into the following four stages:\n\nNonlinear Mapping to Hyperspace: The main goal of mapping is to project a feature vector to HD vectors with dimensionality (d), where \u2208 with m components. Feature identifier (ID) is regarded as a basic field, and the actual value of the feature is the filler of the field. HD computing starts by constructing Item Memory ( ) and Continuous item Memory ( ). = { 1 , 2 , \u2026 , }, where \u2208 (\u22121,1) , \u2208 {1,2, \u2026 } corresponds to the ID of the \u210e feature component. When d is large enough, any two different HD vectors in are nearly orthogonal, implying that ( , ) \u2245 0, ( , ) \u2245 0.5, \u2260 [13].\n\n(\u2022) and (\u2022) are cosine similarity metric and normalized Hamming distance between the two vectors, respectively.\n\nContinuous item memory (CiM) serves as the look-up table for the actual value of a feature. The procedure of establishing CiM first finds the maximum value and minimum value of each feature denoted as and . The range between and is quantized to \u2113 levels, and then an HD vector 1 \u2208 (\u22121,1) is assigned to , and \u2113 \u2208 (\u22121,1) is assigned to . The HD model determines 1 and \u2113 at random, making them approximately orthogonal.\n\n= { 1 , 2 , \u2026 , \u2113 }, where \u2208 (\u22121,1) , \u2208 {1,2, \u2026 \u2113}, and every vector in corresponds to a range of actual value. The spatial relation of levels is preserved through adjusting the Hamming distance between and according to the difference of value to which the two HD vectors correspond. In other words, each value of the specific feature component will be associated with a vector proportionate to 1 and \u2113 . Mapping of each feature value to hyperspace comprises quantizing and looking up the corresponding vectors { 1 , 2 , \u2026 , } in . After mapping each feature component of data to HD vectors, a set of twovector pairs = {( 1 , 1 ), ( 2 , 2 ), \u2026 , ( , )} can readily be used in the next stage with the vector space operations.\n\nEncoding: The HD model conducts the binding operation, bitwise XOR operation (\u2295) between two HD vectors, for each two-vector pair in . After that, the resulting HD vectors in the set is accumulated by the bundling operation, bitwise addition (+) between HD vectors. Followed by binarization with sign function denoted as [\u2022], data can be encoded as (1) and represented by the resulting binary HD vector \u2208 (\u22121,1) .\n= \u2211 \u2295 =1 = [ 1 \u2295 1 + 2 \u2295 2 + \u22ef + \u2295 ](1)\nTraining: All training samples go through the previous two stages and the resulting vector is sent to the associative memory (AM) for training. Training samples of the same class denoted as for the \u210e class are bundled together to form a class HD vector, as shown in (2). means the number of training samples of the \u210e class. For a -class classification task, AM comprises class HD vectors, denoted as { 1 , 2 , \u2026 , }.\n= \u2211 = [ 1 + 2 + \u22ef + ](2)\nClassification: In the inference phase, an unseen testing data would go through the same processing flow of mapping and encoding in the training phase and be encoded as a query vector Q \u2208 (\u22121,1) . To perform classification, the HD model checks the similarity between Q and all class HD vectors stored in AM by the Hamming distance metric. Finally, the HD model outputs the class with the minimum distance as the prediction. \n\n\nProposed Task-Projected Hyperdimensional Computing\n\nIn this section, we propose Task-projected Hyperdimensional Computing (TP-HDC) to realize multi-task learning in an HD model. We are inspired by [12], which exploits the over-parameterization in DNN to superimpose multiple models into one. This implies that only a small subspace spanned by the optimal parameters is occupied by a given task. We observe that HD computing shows a similar phenomenon, where only a small subspace spanned by class HD vectors in AM is relevant to a given task. Based on this observation, MTL can be feasible if the massive hyperspace is partitioned appropriately for each task.\n\n\nAM Table for Multi-task Learning\n\nBefore diving into the illustration of the proposed scheme, we first introduce the definition of AM table supporting multiple tasks and its notation. Following the training flow described in Section 2, each task in task sequence { 1 , 2 , \u2026 , } generates its own AM. A total of AM are present and form a 2-dimensional AM table, as shown in Fig. 2(a). Each column of the table comprises class HD vectors. We notate the \u210e class vector of the \u210e task as . If an original HD model needs to support multiple tasks, the memory requirement of storing the AM table grows linearly with the number of tasks. For resource-constrained edge devices, the memory overhead could hinder HD computing from MTL. As a result, it is more desirable to store a compressed AM with a size that is independent of the number of tasks, as shown in Fig. 2\n\n\n(b).\n\nBaseline Method: Considering the \u210e class in Fig. 2(a), the baseline method bundles the class HD vectors of the same class from all involved tasks. As shown in (3), HD vectors in the \u210e class { 1 , 2 , \u2026 , } are bundled together and form the vector , which is shared across all tasks. Compressed AM comprises { 1 , 2 , . . . , }, where represents the \u210e class HD vector used by all tasks. That is, the baseline method na\u00efvely finds the most representative vector in the hyperspace regardless of the spatial relation between tasks.\n= [ 1 + 2 + \u22ef + ], \u2208 {1,2, . . . }(3)\nFor the baseline method, the memory overhead is times less than that of AM table. However, we discover that the baseline method causes HD vectors of different tasks to occupy overlapping subspace. This induces interference between tasks and significant accuracy degradation. In the next section, we concretize the proposed TP-HDC to efficiently realize MTL in an HD model with a lower accuracy drop.\n\n(a) (b) Fig. 2 (a) AM table for original HD computing to support multiple tasks.\n\n(b) Compressed AM.\n\n\nOrthogonalization with Task-Oriented Keys\n\nThe TP-HDC consists of the following three parts, including generation of taskoriented keys, composition with task-oriented projection, and decomposition:\n\n\nGeneration of Task-Oriented Keys:\n\nWe propose to leverage the peculiar property in the Hamming space, the normalized Hamming distance from any given point in the hyperspace to a randomly drawn point highly concentrates at 0.5 [3]. Namely, two random HD vectors are approximately orthogonal (unrelated) due to hyper-dimensionality. Based on this fact, each task is assigned a task-oriented key generated at random, denoted as { 1 , 2 , . . . , }. These keys can be used for projection to achieve a division of the hyperspace in the following step of TP-HDC.\n\n\nComposition with Task-Oriented Projection:\n\nTo utilize the unused capacity of the HD model more efficiently, orthogonalization of class HD vectors of the same class, e.g., { 1 , 2 , . . . , } for the \u210e class is required. With task-oriented keys generated in the previous step, we bind the keys and the class HD vectors for each task. The effect of binding projects originally close HD vectors { 1 , 2 , . . . , } to different zones of the hyperspace since pseudo-randomly generated keys are approximately orthogonal. The new class HD vector ( ) is formed by bundling the  (4). By projecting the class HD vectors of different tasks into near-orthogonal hyperspaces, TP-HDC can mitigate the information loss caused by directly bundling class HD vectors, as implemented by the baseline method.\n= \u2211 \u2295 = 1 = [ 1 \u2295 1 + 2 \u2295 2 + \u22ef + \u2295 ], \u2208 {1,2, \u2026 }(4)\nDecomposition: Retrieval of class HD vector of the \u210e class in \u210e task , is ensured by binding with , as shown in (5). The resulting vector consists of and noise because the vectors are stored in superposition. Despite the presence of , TP-HDC can still be reliable because HD computing is robust against noise [3].\n\u0302 = \u2295 = [ 1 \u2295 1 + 2 \u2295 2 + \u22ef + \u2295 ] \u2295 = [ 1 \u2295 1 \u2295 + \u22ef + \u2295 \u2295 + \u22ef + \u2295 \u2295 ] = [ + ], \u2208 {1,2, \u2026 }(5)\n\nTraining and Inference in TP-HDC\n\nThe framework of TP-HDC is depicted in Fig. 3, and the procedure of training and inference is summarized in Algorithm 1: Training: Given a task sequence, T = { 1 , 2 , \u2026 , } each with classes, s different AMs are updated using the general HD computing training flow described in Section 2. \u2208 \u211d \u00d7 \u00d7 is a three-dimensional matrix. The first two axes of represent the number of classes and tasks, respectively, and the last axis of represents the dimensionality of HD vectors. We first generate task-oriented projection keys and denoted them as P = { 1 , 2 , \u2026 , }. P helps achieve a division of space and project class HD vectors of different tasks to separate subspaces with equation (4). The compressed AM is \u2208 \u211d \u00d7 , whose size is independent of the number of tasks. Inference: Given a task, the HD model produces the query HD vector in the inference phase by processing a testing sample with the same mapping and encoding modules used in the training phase. After retrieving all class HD vectors of the specific task \u0302= {\u03021,\u03022, \u2026 ,\u0302} with equation (5), the classification result is the class in which the corresponding class HD vector has the smallest Hamming distance with , see equation (6). Fig. 3 The framework of the proposed task-projected HD computing (TP-HDC). \n= (\u0302, )(6)\n\nAlgorithm 1 Task-projected HD Computing\n\n\nExperimental Settings and Simulation Results\n\n\nComparisons\n\nWe compare the proposed TP-HDC with two different approaches tackling the multi-task learning problem in HD computing, including the baseline method and the ideal method.\n\nBaseline method: As mentioned in Section 3.1, the baseline method na\u00efvely finds the most representative vector among all tasks with bundling operation, causing severe interference between different tasks. Therefore, the baseline method can be regarded as the model telling us what happens if we do nothing to explicitly retain information from the previous tasks.\n\n\nIdeal Method:\n\nThe ideal benchmark considers the case where computing resources are unconstrained so that all tasks can have their own AM. Therefore, the performance of the ideal method can be viewed as the upper bound of our evaluation since the class HD vectors are stored without any information loss.\n\n\nDataset and Experimental Setup\n\nWe evaluate the effectiveness of our proposed TP-HDC on Split MNIST, a standard benchmark for multi-task learning [14]. Following the experiment setting of [14] with minor modifications, we split ten digits into disjoint sets. Each set corresponds to a specific task in = { 1 , 2 , \u2026 , }, where aims at discriminating between digits { 1 , 2 , \u2026 , }. We fix the dimensionality of HD computing at d = 5000, where the performances of all HD computing models saturate. Mapping and encoding modules are shared across all tasks, meeting the expectations of MTL. Moreover, we vectorize the gray images of digits in MNIST to form 784dimensional feature vectors and pre-process pixel values using min-max normalization. All experiments are conducted on 100 independent runs to get the final averaged simulation results.\n\n\nPerformance Analysis\n\nFirst, we evaluate our proposed TP-HDC with a three-task MTL configuration. Each of the tasks, namely task A, task B, and task C contains three digits different from those of the other two tasks. HD models are sequentially trained on task A, task B, and task C. We observe that 100 training samples are enough for the convergence of all HD models in each task. Therefore, we train each task for 100 steps, and a training sample is randomly drawn to update AM in each step. Fig. 4 illustrates the learning curve of the different methods on split MNIST. Compared with the ideal method, the baseline method suffers from catastrophic forgetting, resulting in around 20% accuracy drop on task A and task B. Furthermore, task A has occupied the subspace that task B and task C need to learn for classification, causing information loss for task B and task C and bringing about a 15% accuracy drop. On the other hand, the accuracy of the proposed TP-HDC only drops by 3.6%, 3.9%, 2.9% on task A, task B, and task C, respectively.\n\nTo validate the generalization ability of our model, we also evaluate TP-HDC on the five-task case. The experimental setup is almost the same as the three-task case except that each task contained two digits. For the baseline method, Fig. 5 shows that the five tasks tend to interfere with each other severely like that in the threetask case, leading to a 16.5% accuracy drop. In comparison, TP-HDC provides around 12.8% improvement in averaged accuracy compared with the baseline method and performs closely to the ideal benchmark consistently, with a slight 3.7% accuracy drop on average. By efficiently separating the subspaces, TP-HDC mitigates the effect of interference between tasks and improves the performance of sequential training on multiple tasks.  Table 1 shows the performance of TP-HDC on split MNIST in different cases. The high standard deviation of the accuracy of the baseline method implies instability. By contrast, the results demonstrate both effectiveness (< 4% accuracy drop compared with the ideal benchmark) and stability (lower variance compared with the baseline method) of TP-HDC.  \n\n\nMemory Footprint Analysis\n\nThe memory requirement for the ideal method to store AM is ( \u00d7 ), where s and k are the number of tasks and classes, respectively. Since TP-HDC just needs to store the projection keys of each task for decomposing class HD vectors in the inference phase, the memory footprint for TP-HDC only requires ( + ) . Moreover, instead of storing all the projection keys in the memory, linear feedback shift register (LFSR) can be utilized to generate pseudo-random patterns as a hardware-friendly approach for implementation. This makes TP-HDC more efficient for multi-task learning with negligible memory overhead.\n\n\nConclusions\n\nTo the best of our knowledge, in this paper, we first investigate the feasibility of applying multi-task learning to HD computing. To avoid catastrophic forgetting, we propose TP-HDC to exploit redundant dimensionality in the hyperspace. By separating subspaces for each task with task-oriented keys, the information loss caused by the interference between tasks is effectively reduced. Based on our experimental results, TP-HDC outperforms the baseline method on split MNIST by 12.8% accuracy on average and can be implemented with negligible memory overhead.\n\nFig. 1\n1The processing flow chart of general HD computing.\n\n\nInput: T = { 1 , 2 , 3 , \u2026 , } -task sequence, each with classes mode -training phase or inference phase of the \u210e -dimensionality of the HD model Output: P = { 1 , 2 , 3 , \u2026 , } -task-oriented projection keys M -compressed AM -prediction 1: Initialize \uf0df 0 , , ; M \uf0df 0 , ; \u0302 \uf0df 0 , 2: if mode = training do 3:for each task in T do 4:for each training data do 5if mode = inference do 10:for ( = 1: )\n\nFig. 4\n4The learning curves of different methods trained on split MNIST. Tasks A, B, and C are assigned with disjoint sets of MNIST digits. The vertical dashed lines imply the transitions of the training procedure of different tasks. The top plot shows the accuracy of task A. The middle plot and bottom plot indicate the accuracy of tasks B and C, respectively.\n\nFig. 5\n5Classification accuracy of the five-task case of split MNIST. TP-HDC provides around 12.8% improvement in averaged accuracy compared with the baseline method.\n\nTable 1\n1Performance of different methods on split MNIST of different numbers of tasks.Accuracy \u00b1 Std. (%) \n\nAcknowledgments. This work is supported by the Ministry of Science and Technology of Taiwan (MOST 106-2221-E-002-205-MY3 and MOST 109-2622-8-002-012-TA), National Taiwan University, and Pixart Imaging Inc.\nMobile Edge Computing: A Survey. N Abbas, Y Zhang, A Taherkordi, T Skeie, IEEE Internet of Things Journal. 51Abbas, N., Zhang, Y., Taherkordi, A., Skeie, T.: Mobile Edge Computing: A Survey. IEEE Internet of Things Journal, vol. 5, no. 1, pp. 450-465 (2018)\n\nMultitask Learning of Deep Neural Network-Based Keyword Spotting for IoT Devices. S G Leem, I C Yoo, D Yook, IEEE Transactions on Consumer Electronics. 652Leem, S. G., Yoo, I. C., Yook, D.: Multitask Learning of Deep Neural Network-Based Keyword Spotting for IoT Devices. In: IEEE Transactions on Consumer Electronics, vol. 65, no. 2, pp. 188-194 (2019)\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cognitive computation. 12Kanerva, P.: Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Cognitive computation 1.2, pp.139-159 (2009)\n\nHyperdimensional biosignal processing: A case study for EMG-based hand gesture recognition. A Rahimi, S Benatti, P Kanerva, L Benini, J M Rabaey, IEEE International Conference on Rebooting Computing (ICRC). San Diego, CARahimi, A., Benatti, S., Kanerva, P., Benini, L., Rabaey, J. M.: Hyperdimensional biosignal processing: A case study for EMG-based hand gesture recognition. In: IEEE International Conference on Rebooting Computing (ICRC), San Diego, CA, pp. 1-8 (2016)\n\nLow-Power Sparse Hyperdimensional Encoder for Language Recognition. M Imani, J Hwang, T Rosing, A Rahimi, J M Rabaey, IEEE Design & Test. 346Imani, M., Hwang, J., Rosing, T., Rahimi, A., Rabaey, J.M.: Low-Power Sparse Hyperdimensional Encoder for Language Recognition. In: IEEE Design & Test, vol. 34, no. 6, pp. 94-101 (2017)\n\nEfficient Biosignal Processing Using Hyperdimensional Computing: Network Templates for Combined Learning and Classification of ExG Signals. A Rahimi, P Kanerva, L Benini, J M Rabaey, Proceedings of the IEEE. the IEEE107Rahimi, A., Kanerva, P., Benini, L., Rabaey, J. M.: Efficient Biosignal Processing Using Hyperdimensional Computing: Network Templates for Combined Learning and Classification of ExG Signals. In: Proceedings of the IEEE, vol. 107, no. 1, pp. 123-143 (2018)\n\nWu: Hyperdimensional Computingbased Multimodality Emotion Recognition with Physiological Signals. E J Chang, A Rahimi, L Benini, A Y , IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS). Chang, E. J., Rahimi, A., Benini, L., A. Y., Wu: Hyperdimensional Computing- based Multimodality Emotion Recognition with Physiological Signals. In: IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS), pp. 137-141 (2019)\n\nF5-hd: Fast flexible fpgabased framework for refreshing hyperdimensional computing. S Salamat, M Imani, B Khaleghi, T Rosing, Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate ArraysSalamat, S., Imani, M., Khaleghi, B., Rosing, T.: F5-hd: Fast flexible fpga- based framework for refreshing hyperdimensional computing. In: Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. (2019)\n\nPULP-HD: Accelerating brain-inspired high-dimensional computing on a parallel ultralow power platform. F Montagna, A Rahimi, S Benatti, D Rossi, L Benini, 55th ACM/ESDA/IEEE Design Automation Conference (DAC). Montagna, F., Rahimi, A., Benatti, S., Rossi, D., Benini, L.: PULP-HD: Accelerating brain-inspired high-dimensional computing on a parallel ultra- low power platform. In: 55th ACM/ESDA/IEEE Design Automation Conference (DAC), pp. 1-6 (2018)\n\nCatastrophic forgetting in connectionist networks. R M French, Trends in cognitive sciences. 34French, R. M.: Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4), pp. 128-135 (1999)\n\nA convergence theory for deep learning via over-parameterization. Z Allen-Zhu, Y Li, Z Song, arXiv:1811.03962.arXiv preprintAllen-Zhu, Z., Li, Y., Song, Z.: A convergence theory for deep learning via over-parameterization. arXiv preprint arXiv:1811.03962. (2018)\n\nSuperposition of many models into one. B Cheung, A Terekhov, Y Chen, P Agrawal, B Olshausen, Advances in Neural Information Processing Systems. Cheung, B., Terekhov, A., Chen, Y., Agrawal, P., Olshausen, B.: Superposition of many models into one. In: Advances in Neural Information Processing Systems, pp. 10867-10876 (2019)\n\nHolographic reduced representations. T A Plate, IEEE Transactions on Neural Networks. 63Plate, T. A.: Holographic reduced representations. In: IEEE Transactions on Neural Networks, vol. 6, no. 3, pp. 623-641 (1995)\n\nContinual learning through synaptic intelligence. F Zenke, B Poole, S Ganguli, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Zenke, F., Poole, B., Ganguli, S.: Continual learning through synaptic intelligence. In: Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, pp. 3987-3995 (2017)\n", "annotations": {"author": "[{\"end\":173,\"start\":69},{\"end\":277,\"start\":174},{\"end\":372,\"start\":278},{\"end\":486,\"start\":373}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":80},{\"end\":189,\"start\":183},{\"end\":380,\"start\":378}]", "author_first_name": "[{\"end\":79,\"start\":69},{\"end\":182,\"start\":174},{\"end\":284,\"start\":278},{\"end\":377,\"start\":373}]", "author_affiliation": "[{\"end\":172,\"start\":87},{\"end\":276,\"start\":191},{\"end\":371,\"start\":286},{\"end\":485,\"start\":400}]", "title": "[{\"end\":66,\"start\":1},{\"end\":552,\"start\":487}]", "venue": null, "abstract": "[{\"end\":1751,\"start\":630}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1986,\"start\":1983},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2161,\"start\":2158},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2346,\"start\":2343},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2519,\"start\":2516},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2545,\"start\":2542},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2584,\"start\":2581},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2588,\"start\":2585},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2723,\"start\":2720},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2903,\"start\":2900},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3106,\"start\":3102},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3522,\"start\":3518},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3554,\"start\":3550},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5190,\"start\":5186},{\"end\":6775,\"start\":6772},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7174,\"start\":7171},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7975,\"start\":7971},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10803,\"start\":10800},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12290,\"start\":12287},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13107,\"start\":13104},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13614,\"start\":13611},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14801,\"start\":14797},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14843,\"start\":14839}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":18927,\"start\":18868},{\"attributes\":{\"id\":\"fig_2\"},\"end\":19326,\"start\":18928},{\"attributes\":{\"id\":\"fig_3\"},\"end\":19690,\"start\":19327},{\"attributes\":{\"id\":\"fig_4\"},\"end\":19858,\"start\":19691},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":19968,\"start\":19859}]", "paragraph": "[{\"end\":2162,\"start\":1767},{\"end\":3974,\"start\":2164},{\"end\":4285,\"start\":3976},{\"end\":4610,\"start\":4326},{\"end\":5191,\"start\":4612},{\"end\":5304,\"start\":5193},{\"end\":5723,\"start\":5306},{\"end\":6449,\"start\":5725},{\"end\":6864,\"start\":6451},{\"end\":7321,\"start\":6905},{\"end\":7771,\"start\":7347},{\"end\":8433,\"start\":7826},{\"end\":9295,\"start\":8470},{\"end\":9831,\"start\":9304},{\"end\":10269,\"start\":9870},{\"end\":10351,\"start\":10271},{\"end\":10371,\"start\":10353},{\"end\":10571,\"start\":10417},{\"end\":11130,\"start\":10609},{\"end\":11923,\"start\":11177},{\"end\":12291,\"start\":11978},{\"end\":13691,\"start\":12421},{\"end\":13976,\"start\":13806},{\"end\":14341,\"start\":13978},{\"end\":14648,\"start\":14359},{\"end\":15493,\"start\":14683},{\"end\":16540,\"start\":15518},{\"end\":17655,\"start\":16542},{\"end\":18291,\"start\":17685},{\"end\":18867,\"start\":18307}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6904,\"start\":6865},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7346,\"start\":7322},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9869,\"start\":9832},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11977,\"start\":11924},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12385,\"start\":12292},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13702,\"start\":13692}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17311,\"start\":17304}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1765,\"start\":1753},{\"attributes\":{\"n\":\"2\"},\"end\":4324,\"start\":4288},{\"attributes\":{\"n\":\"3\"},\"end\":7824,\"start\":7774},{\"attributes\":{\"n\":\"3.1\"},\"end\":8468,\"start\":8436},{\"end\":9302,\"start\":9298},{\"attributes\":{\"n\":\"3.2\"},\"end\":10415,\"start\":10374},{\"end\":10607,\"start\":10574},{\"end\":11175,\"start\":11133},{\"attributes\":{\"n\":\"3.3\"},\"end\":12419,\"start\":12387},{\"end\":13743,\"start\":13704},{\"attributes\":{\"n\":\"4\"},\"end\":13790,\"start\":13746},{\"attributes\":{\"n\":\"4.1\"},\"end\":13804,\"start\":13793},{\"end\":14357,\"start\":14344},{\"attributes\":{\"n\":\"4.2\"},\"end\":14681,\"start\":14651},{\"attributes\":{\"n\":\"4.3\"},\"end\":15516,\"start\":15496},{\"attributes\":{\"n\":\"4.4\"},\"end\":17683,\"start\":17658},{\"attributes\":{\"n\":\"5\"},\"end\":18305,\"start\":18294},{\"end\":18875,\"start\":18869},{\"end\":19334,\"start\":19328},{\"end\":19698,\"start\":19692},{\"end\":19867,\"start\":19860}]", "table": "[{\"end\":19968,\"start\":19947}]", "figure_caption": "[{\"end\":18927,\"start\":18877},{\"end\":19326,\"start\":18930},{\"end\":19690,\"start\":19336},{\"end\":19858,\"start\":19700},{\"end\":19947,\"start\":19869}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4559,\"start\":4553},{\"end\":8816,\"start\":8810},{\"end\":9295,\"start\":9289},{\"end\":9357,\"start\":9348},{\"end\":10285,\"start\":10279},{\"end\":12466,\"start\":12460},{\"end\":13622,\"start\":13616},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15997,\"start\":15991},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16782,\"start\":16776}]", "bib_author_first_name": "[{\"end\":20209,\"start\":20208},{\"end\":20218,\"start\":20217},{\"end\":20227,\"start\":20226},{\"end\":20241,\"start\":20240},{\"end\":20517,\"start\":20516},{\"end\":20519,\"start\":20518},{\"end\":20527,\"start\":20526},{\"end\":20529,\"start\":20528},{\"end\":20536,\"start\":20535},{\"end\":20915,\"start\":20914},{\"end\":21227,\"start\":21226},{\"end\":21237,\"start\":21236},{\"end\":21248,\"start\":21247},{\"end\":21259,\"start\":21258},{\"end\":21269,\"start\":21268},{\"end\":21271,\"start\":21270},{\"end\":21676,\"start\":21675},{\"end\":21685,\"start\":21684},{\"end\":21694,\"start\":21693},{\"end\":21704,\"start\":21703},{\"end\":21714,\"start\":21713},{\"end\":21716,\"start\":21715},{\"end\":22076,\"start\":22075},{\"end\":22086,\"start\":22085},{\"end\":22097,\"start\":22096},{\"end\":22107,\"start\":22106},{\"end\":22109,\"start\":22108},{\"end\":22511,\"start\":22510},{\"end\":22513,\"start\":22512},{\"end\":22522,\"start\":22521},{\"end\":22532,\"start\":22531},{\"end\":22542,\"start\":22541},{\"end\":22544,\"start\":22543},{\"end\":22975,\"start\":22974},{\"end\":22986,\"start\":22985},{\"end\":22995,\"start\":22994},{\"end\":23007,\"start\":23006},{\"end\":23530,\"start\":23529},{\"end\":23542,\"start\":23541},{\"end\":23552,\"start\":23551},{\"end\":23563,\"start\":23562},{\"end\":23572,\"start\":23571},{\"end\":23930,\"start\":23929},{\"end\":23932,\"start\":23931},{\"end\":24162,\"start\":24161},{\"end\":24175,\"start\":24174},{\"end\":24181,\"start\":24180},{\"end\":24399,\"start\":24398},{\"end\":24409,\"start\":24408},{\"end\":24421,\"start\":24420},{\"end\":24429,\"start\":24428},{\"end\":24440,\"start\":24439},{\"end\":24723,\"start\":24722},{\"end\":24725,\"start\":24724},{\"end\":24952,\"start\":24951},{\"end\":24961,\"start\":24960},{\"end\":24970,\"start\":24969}]", "bib_author_last_name": "[{\"end\":20215,\"start\":20210},{\"end\":20224,\"start\":20219},{\"end\":20238,\"start\":20228},{\"end\":20247,\"start\":20242},{\"end\":20524,\"start\":20520},{\"end\":20533,\"start\":20530},{\"end\":20541,\"start\":20537},{\"end\":20923,\"start\":20916},{\"end\":21234,\"start\":21228},{\"end\":21245,\"start\":21238},{\"end\":21256,\"start\":21249},{\"end\":21266,\"start\":21260},{\"end\":21278,\"start\":21272},{\"end\":21682,\"start\":21677},{\"end\":21691,\"start\":21686},{\"end\":21701,\"start\":21695},{\"end\":21711,\"start\":21705},{\"end\":21723,\"start\":21717},{\"end\":22083,\"start\":22077},{\"end\":22094,\"start\":22087},{\"end\":22104,\"start\":22098},{\"end\":22116,\"start\":22110},{\"end\":22519,\"start\":22514},{\"end\":22529,\"start\":22523},{\"end\":22539,\"start\":22533},{\"end\":22983,\"start\":22976},{\"end\":22992,\"start\":22987},{\"end\":23004,\"start\":22996},{\"end\":23014,\"start\":23008},{\"end\":23539,\"start\":23531},{\"end\":23549,\"start\":23543},{\"end\":23560,\"start\":23553},{\"end\":23569,\"start\":23564},{\"end\":23579,\"start\":23573},{\"end\":23939,\"start\":23933},{\"end\":24172,\"start\":24163},{\"end\":24178,\"start\":24176},{\"end\":24186,\"start\":24182},{\"end\":24406,\"start\":24400},{\"end\":24418,\"start\":24410},{\"end\":24426,\"start\":24422},{\"end\":24437,\"start\":24430},{\"end\":24450,\"start\":24441},{\"end\":24731,\"start\":24726},{\"end\":24958,\"start\":24953},{\"end\":24967,\"start\":24962},{\"end\":24978,\"start\":24971}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":31429854},\"end\":20432,\"start\":20175},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":86693451},\"end\":20787,\"start\":20434},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":733980},\"end\":21132,\"start\":20789},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":12008695},\"end\":21605,\"start\":21134},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":8038292},\"end\":21933,\"start\":21607},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":57365377},\"end\":22410,\"start\":21935},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":155774532},\"end\":22888,\"start\":22412},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":67872077},\"end\":23424,\"start\":22890},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":49291228},\"end\":23876,\"start\":23426},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2691726},\"end\":24093,\"start\":23878},{\"attributes\":{\"doi\":\"arXiv:1811.03962.\",\"id\":\"b10\"},\"end\":24357,\"start\":24095},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":61153603},\"end\":24683,\"start\":24359},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2352281},\"end\":24899,\"start\":24685},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10409742},\"end\":25305,\"start\":24901}]", "bib_title": "[{\"end\":20206,\"start\":20175},{\"end\":20514,\"start\":20434},{\"end\":20912,\"start\":20789},{\"end\":21224,\"start\":21134},{\"end\":21673,\"start\":21607},{\"end\":22073,\"start\":21935},{\"end\":22508,\"start\":22412},{\"end\":22972,\"start\":22890},{\"end\":23527,\"start\":23426},{\"end\":23927,\"start\":23878},{\"end\":24396,\"start\":24359},{\"end\":24720,\"start\":24685},{\"end\":24949,\"start\":24901}]", "bib_author": "[{\"end\":20217,\"start\":20208},{\"end\":20226,\"start\":20217},{\"end\":20240,\"start\":20226},{\"end\":20249,\"start\":20240},{\"end\":20526,\"start\":20516},{\"end\":20535,\"start\":20526},{\"end\":20543,\"start\":20535},{\"end\":20925,\"start\":20914},{\"end\":21236,\"start\":21226},{\"end\":21247,\"start\":21236},{\"end\":21258,\"start\":21247},{\"end\":21268,\"start\":21258},{\"end\":21280,\"start\":21268},{\"end\":21684,\"start\":21675},{\"end\":21693,\"start\":21684},{\"end\":21703,\"start\":21693},{\"end\":21713,\"start\":21703},{\"end\":21725,\"start\":21713},{\"end\":22085,\"start\":22075},{\"end\":22096,\"start\":22085},{\"end\":22106,\"start\":22096},{\"end\":22118,\"start\":22106},{\"end\":22521,\"start\":22510},{\"end\":22531,\"start\":22521},{\"end\":22541,\"start\":22531},{\"end\":22547,\"start\":22541},{\"end\":22985,\"start\":22974},{\"end\":22994,\"start\":22985},{\"end\":23006,\"start\":22994},{\"end\":23016,\"start\":23006},{\"end\":23541,\"start\":23529},{\"end\":23551,\"start\":23541},{\"end\":23562,\"start\":23551},{\"end\":23571,\"start\":23562},{\"end\":23581,\"start\":23571},{\"end\":23941,\"start\":23929},{\"end\":24174,\"start\":24161},{\"end\":24180,\"start\":24174},{\"end\":24188,\"start\":24180},{\"end\":24408,\"start\":24398},{\"end\":24420,\"start\":24408},{\"end\":24428,\"start\":24420},{\"end\":24439,\"start\":24428},{\"end\":24452,\"start\":24439},{\"end\":24733,\"start\":24722},{\"end\":24960,\"start\":24951},{\"end\":24969,\"start\":24960},{\"end\":24980,\"start\":24969}]", "bib_venue": "[{\"end\":21354,\"start\":21341},{\"end\":22151,\"start\":22143},{\"end\":23185,\"start\":23109},{\"end\":25103,\"start\":25050},{\"end\":20280,\"start\":20249},{\"end\":20584,\"start\":20543},{\"end\":20946,\"start\":20925},{\"end\":21339,\"start\":21280},{\"end\":21743,\"start\":21725},{\"end\":22141,\"start\":22118},{\"end\":22632,\"start\":22547},{\"end\":23107,\"start\":23016},{\"end\":23634,\"start\":23581},{\"end\":23969,\"start\":23941},{\"end\":24159,\"start\":24095},{\"end\":24501,\"start\":24452},{\"end\":24769,\"start\":24733},{\"end\":25048,\"start\":24980}]"}}}, "year": 2023, "month": 12, "day": 17}