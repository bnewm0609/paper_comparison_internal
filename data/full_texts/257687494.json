{"id": 257687494, "updated": "2023-12-05 14:42:10.96", "metadata": {"title": "FedGH: Heterogeneous Federated Learning with Generalized Global Header", "authors": "[{\"first\":\"Liping\",\"last\":\"Yi\",\"middle\":[]},{\"first\":\"Gang\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Xiaoguang\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Zhuan\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Han\",\"last\":\"Yu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Federated learning (FL) is an emerging machine learning paradigm that allows multiple parties to train a shared model collaboratively in a privacy-preserving manner. Existing horizontal FL methods generally assume that the FL server and clients hold the same model structure. However, due to system heterogeneity and the need for personalization, enabling clients to hold models with diverse structures has become an important direction. Existing model-heterogeneous FL approaches often require publicly available datasets and incur high communication and/or computational costs, which limit their performances. To address these limitations, we propose a simple but effective Federated Global prediction Header (FedGH) approach. It is a communication and computation-efficient model-heterogeneous FL framework which trains a shared generalized global prediction header with representations extracted by heterogeneous extractors for clients' models at the FL server. The trained generalized global prediction header learns from different clients. The acquired global knowledge is then transferred to clients to substitute each client's local prediction header. We derive the non-convex convergence rate of FedGH. Extensive experiments on two real-world datasets demonstrate that FedGH achieves significantly more advantageous performance in both model-homogeneous and -heterogeneous FL scenarios compared to seven state-of-the-art personalized FL models, beating the best-performing baseline by up to 8.87% (for model-homogeneous FL) and 1.83% (for model-heterogeneous FL) in terms of average test accuracy, while saving up to 85.53% of communication overhead.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mm/YiWLSY23", "doi": "10.1145/3581783.3611781"}}, "content": {"source": {"pdf_hash": "6d4a520935e17aca0763630c271293cac63edbf5", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2303.13137v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ef741bf7c0e00998cabcd7dcabf6a1dfc0c17453", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6d4a520935e17aca0763630c271293cac63edbf5.txt", "contents": "\nFedGH: Heterogeneous Federated Learning with Generalized Global Header\nACMCopyright ACMOctober 29-November 3, 2023. October 29-November 3, 2023\n\nLiping Yi yiliping@nbjl.nankai.edu.cn \nGang Wang \nXiaoguang Liu \nZhuan Shi zhuanshi@mail.ustc.edu.cn \nHan Yu han.yu@ntu.edu.sg \nLiping Yi \nGang Wang \nXiaoguang Liu \nZhuan Shi \nHan Yu \n\nCollege of C.S\nGTIISC\nDISSec\n\n\nCollege of C.S., DISSec, GTIISC\nNankai University Tianjin\nChina\n\n\nCollege of C.S., DISSec, GTIISC\nNankai University Tianjin\nChina\n\n\nSchool of Computer Science and Technology\nNankai University Tianjin\nChina\n\n\nSchool of Computer Science and Engineering\nand Technology of China (USTC) Hefei Anhui\nUniversity of Science\nChina\n\n\nACM Reference Format\nNanyang Technological University (NTU)\nSingapore\n\nFedGH: Heterogeneous Federated Learning with Generalized Global Header\n\nCanada Proceedings of the 31st ACM International Conference on Multimedia (MM '23)\nOttawa, ON; Ottawa, ON, Canada; New York, NY, USAACM11October 29-November 3, 2023. October 29-November 3, 202310.1145/3581783.3611781. 2023. FedGH: Heterogeneous Federated Learning with Generalized Global Header. InCCS CONCEPTS \u2022 Computing methodologies \u2192 Distributed artificial intel- ligenceComputer vision tasksComputer vision representationsSupervised learning by classification KEYWORDS federated learningmodel heterogeneity\nFederated learning (FL) is an emerging machine learning paradigm that allows multiple parties to train a shared model collaboratively in a privacy-preserving manner. Existing horizontal FL methods generally assume that the FL server and clients hold the same model structure. However, due to system heterogeneity and the need for personalization, enabling clients to hold models with diverse structures has become an important direction. Existing model-heterogeneous FL approaches often require publicly available datasets and incur high communication and/or computational costs, which limit their performances. To address these limitations, we propose a simple but effective Federated Global prediction Header (FedGH) approach. It is a communication and computation-efficient model-heterogeneous FL framework which trains a shared generalized global prediction header with representations extracted by heterogeneous extractors for clients' models at the FL server. The trained generalized global prediction header learns from different clients. The acquired global knowledge is then transferred to clients to substitute each client's local prediction header. We derive the non-convex convergence rate of FedGH. Extensive experiments on two real-world datasets demonstrate that FedGH achieves significantly more advantageous performance in both model-homogeneous and -heterogeneous FL scenarios compared to seven state-of-the-art personalized FL models, beating the best-performing baseline by up to 8.87% (for model-homogeneous FL) and 1.83% (for model-heterogeneous FL) in terms of average test accuracy, while saving up to 85.53% of communication overhead. * Corresponding authors.\n\nINTRODUCTION\n\nFederated learning (FL) [39] has become a widely adopted approach for collaborative model training involving multiple participants with decentralized data under the premise of privacy preservation. Horizontal FL methods, such as FedAvg [27], generally involve a central FL server coordinating multiple FL clients. In each round of distributed model training, the server broadcasts the global model to selected clients. The clients then train the received global model on their respective local datasets and send the updated local models back to the server. The server then updates the global model by aggregating the received local models. The above steps are iteratively executed until the global model converges. Since only the model parameters are transmitted between the server and clients without exposing the raw data, privacy protection is enhanced. Nevertheless, the above paradigm requires all clients to train models with the same structures (i.e., model homogeneity) in order to work.\n\nHowever, in practical cross-device FL scenarios, the clients participating in FL are mostly mobile edge devices with heterogeneous and constrained system resources (e.g., computing power, network bandwidth, memory, storage, and battery capacity) [35,36,40,42,[44][45][46]. This is also referred to as system heterogeneity in FL. Modelhomogeneous FL methods face three limitations in this scenario:\n\n\u2022 Device: when training a large global model, some low-end clients may never be able to join in FL since their limited arXiv:2303.13137v2 [cs.\n\nLG] 1 Aug 2023 system resources preclude them from training large models. As a result, the accuracy of the final global model may be degraded due to the lack of information from these clients. \u2022 Data: the data held by different devices are often not identically and independently distributed (Non-IID), also known as statistical heterogeneity in FL [24,34]. \u2022 Model: if all clients join FL, the capacity of the trained homogeneous models must match the weakest client's system configurations. Unfortunately, training models with a small capacity not only reduces their performance but also wastes high-end clients' system resources due to long idle time.\n\nAlthough model-heterogeneous FL approaches have emerged to address the aforementioned challenges facing model-homogeneous FL, they still have the following limitations. During learning, the high-level design intuition is to separate the training of the homogeneous portion and the heterogeneous portion of the FL model structure into unrelated processes. This not only results in limited performance improvement but also incurs high computation and communication costs [21,33,38]. In addition, some approaches even rely on the availability of suitable public datasets closely related to the learning task in order to leverage knowledge distillation to achieve model-heterogeneous FL [19,22]. However, this is not always viable in practice. Therefore, enabling FL clients to train heterogeneous FL models with the capacity adaptive to system resource limitations and diverse data distributions in an efficient manner remains open.\n\nTo bridge the aforementioned gaps in the model-heterogeneous FL literature, we propose the Federated Global prediction Header (FedGH) approach. It is a novel model-heterogeneous FL framework capable of achieving low communication and computation costs. Under FedGH, each client's local model consists of a heterogeneous feature extractor and a homogeneous prediction header. It leverages the representations extracted by clients' feature extractors to train a global generalized prediction header at the server for all clients to share. The updated global header captures all-class knowledge among multiple clients. The generalized global prediction header replaces each client's local prediction header to transfer global knowledge to clients. In this way, FedGH enables information interaction across heterogeneous clients' models through a shared generalized global prediction header.\n\nBy communicating only the representations and the global prediction header's parameters between clients and the server, FedGH reduces communication costs. By computing local class-averaged representations on FL clients, it reduces computational costs to a level tolerable for mobile edge devices. By not relying on a public dataset, its operation is not limited by the availability of such datasets. By only sending representations which are high-level abstractions of local data, it protects data privacy. We prove the nonconvex convergence rate of FedGH. Extensive experiments on two real-world datasets demonstrate that FedGH achieves significantly more advantageous performance in both model-homogeneous and -heterogeneous FL scenarios compared to seven state-of-the-art personalized FL models, beating the best-performing baseline by up to 8.87% (for model-homogeneous FL) and 1.83% (for modelheterogeneous FL) in terms of average test accuracy, while saving up to 85.53% of communication overhead.\n\n\nRELATED WORK\n\nExisting model-heterogeneous FL methods can be divided into two main categories: 1) each client's local model is a heterogeneous subnet of the server model, and 2) different clients hold completely heterogeneous local models. The former (such as HeteroFL [10], FjORD [12], HFL [25], FedResCuE [49], FedRolex [3] and Fed2 [41]) allows clients to train heterogeneous subnets matching system resources to tackle system and statistical heterogeneity simultaneously, but the strong assumption of subnets constrains its applications. Our work is more closely related to the latter category, which can be further divided into two groups based on whether they rely on the availability of public datasets or not.\n\nPublic Data-Dependent. This category of methods achieves collaborative training across clients with heterogeneous models by knowledge distillation on public datasets. According to the site at which knowledge distillation is performed, these methods can be further divided into three groups.\n\nKnowledge distillation on the clients. In each communication round, FedMD [19] and FSFL [13] let clients compute the logits of the trained local heterogeneous model on a public dataset, and uploads them to the server. The server then aggregates these logits to generate the global logits, and broadcasts them to clients. The clients calculate the distance between the local logits and the global logits belonging to one public data sample as the knowledge loss. Finally, the distilled local model is fine-tuned on private data. To speed up convergence or enhance robustness to adversarial attacks of the above approach, Cronus [5], DS-FL [15] and FedAUX [30] proposed new aggregation rules for logits. Instead of communicating logits, FedHeNN [26] extracts representations in the above distillation process.\n\nKnowledge distillation on server. FedDF [22], FCCL [14], FedKT [20], Fed-ET [8] and FedKEMF [43] train each client's heterogeneous model via ensemble distillation on a public dataset at the server.\n\nKnowledge distillation on both the clients and the server. Upon distillation at the client side, FedGEMS [7] and CFD [31] include one additional step of distillation on the server's model to mitigate forgetfulness due to dropout.\n\nHowever, the public datasets essential for the above approaches to work may not always be available in practice. Furthermore, only public data following similar distributions with clients' private data can obtain acceptable model performance, which makes them even harder to find. Besides, distillation on each sample of public data incurs non-trivial computation costs if the public data size is large. These facts limit the applicability of these approaches.\n\nPublic Data-Independent. It involves three lines: model mixup, mutual learning and data-free knowledge distillation.\n\nModel mixup: there are many studies that split each client's local model into two parts: a feature extractor and a classifier. Only one part is shared during FL model aggregation, while the other part containing personalized parameters or even heterogeneous structures is held locally. FedRep [9], FedMatch [6], FedBABU [28] and FedAlt/FedSim [29] share the homogeneous feature extractor while LG-FedAvg [21], CHFL [23] and FedClassAvg [16] share the homogeneous classifier header. Since only part of a complete model is shared, model performance tends to degrade compared with sharing the complete model (e.g., FedAvg). Besides, a feature extractor has more parameters than a classifier header. Thus, allowing different clients to use heterogeneous extractors boosts FL model heterogeneity. Hence, we choose to allow clients to hold personalized heterogeneous feature extractors and share their homogeneous classifier headers via FL global training.\n\nMutual learning: FML [33] and FedKD [38] enable each client to train a large heterogeneous model and a small homogeneous model via mutual learning, and the small homogeneous models are aggregated on the server. Since each client is required to train two models simultaneously, the extra computation overhead may not be tolerable for mobile edge devices.\n\nData-free knowledge distillation: FedGen [48] trains a generator with clients' local data distribution on the server to learn the overall distribution. The trained generator produces extra representation with the overall distribution for each client to enhance local model generalization. However, uploading local data distributions from clients to the server risks exposing data privacy. In FedZKT [47], the server trains a generative model and a global model in an adversarial manner to transfer local knowledge to the global model. It uses the trained generative model to produce synthetic data for distilling the global knowledge to local models. The computation-intensive adversarial training and knowledge distillation are time-consuming. FedGKT [11] communicates features, logits and labels of clients' local data with the server to distil small clients' classifiers and a large server's classifier bidirectionally. Since the server and clients exchange information for each private sample, the communication cost is high when the private dataset is large. FD [17] aggregates logits by class on the server, and clients calculate the distance of each local sample logits and the aggregated global logits as distillation loss to train local models. Since logits carry similar information with hard labels, no extra knowledge is supplemented, which tends to degrade performance. To improve FD, HFD [1,2] allows clients to upload averaged samples by class, which increases the risk of privacy leakage. Different from FD, FedProto [37] utilizes representations rather than logits by class. The server in FedProto aggregates the received representations with class distributions as weights instead of averaging the received logits like FD. This potentially risks privacy leakage. Both FD and FedProto need to compute the distillation loss between each private sample logits/representations and global logits/representations with the corresponding class, which incurs high computation costs at client sides. In addition, each client can only learn about classes it already knows from the server, which hinders generalization to unseen classes.\n\nUnlike FedProto, FedGH utilizes local representations and the corresponding classes (labels), rather than class distributions, to train a homogeneous shared global prediction header at the server, and then uses it to replace local model headers to achieve global knowledge transfer. The shared global header captures all-class information across different clients whose local models consist of heterogeneous extractors and homogeneous prediction headers, thereby enhancing the generalization of local models. By not requiring class distributions, FedGH reduces privacy leakage.\n\nLG-FedAvg directly aggregates homogeneous local headers on the server, which can also support heterogeneous clients' extractors. However, the simple weighted averaging of headers by data size is ineffective in the face of non-IID data. In FedGH, each client only provides the local averaged representation (one embedding vector) about each seen class to train a global generalized header, which can better accommodate non-IID data.\n\n\nTHE PROPOSED FEDGH APPROACH\n\nIn this section, we first describe the formulation of a typical FL algorithm -FedAvg, and then define the problem FedGH addresses. We then explain how FedGH works for model-heterogeneous FL, and discuss its strengths in cost reduction and privacy preservation.\n\n\nPreliminaries\n\nTypical FL. Under FedAvg, a central FL server coordinates FL clients to collaboratively train a global model. Specifically, in each training round , the server samples a fraction of all the clients, , to join training (i.e., the set of sampled clients joining in the -th round of FL, |S | = \u00b7 = ). Then, the server broadcasts the global model to the selected clients. They then train the received global model on their respective local data \u223c ( obeys the distribution , i.e., the local data of different clients are non-IID) to obtain through \u2190 \u2212 \u2207\u2113 ( ; , ) , ( , ) \u2208 . The -th client uploads the trained local model to the server. The server then aggregates them to update the global model as =\n\u22121 =0\n. In short, FedAvg aims to minimize the average loss of the global model on all clients' local data:\nmin \u2208R L( ) = \u22121 \u2211\ufe01 =0 L ( ),(1)\nwhere = | | is the number of samples held by the -th client. is the number of samples held by all clients. L ( ) = \u2113 ( ; ) is the loss of the global model with dimensions on the -th client's local data .\n\nThe above steps iterate until the global model converges. Since the server averages the received local models, the structures of all clients' local models must be the same (homogeneous).\n\nProblem Definition for FedGH. We aim to perform FL across clients with heterogeneous models in the same supervised classification tasks. Each client's local model can be split into two parts:\n( ) = F ( ) \u2022 H ( ), i.e., = ( , ), where \u2022 denotes model splicing. F ( ; ): R \u2192 R R is a feature ex-\ntractor, which maps local samples from the input feature to the representation embedding R. H ( ; F ( ; )): R R \u2192 R is the prediction header. All clients have the same , R , . We assume that F is heterogeneous across different clients (i.e., clients can customize the sizes and structures of local feature extractors to match their system resources and data volume), and all clients share the homogeneous global header H( ) (i.e., all clients carry out the same tasks). That is, ( ) = F ( ) \u2022 H( ). So the loss of the -th client's local model is formulated as L ( ; , ) = L sup (H ( ; F ( ; )) , ) , ( , ) \u2208 . In representation learning [4], representations are the latent feature embedding vectors extracted by feature extractors from input samples. It is hard to infer the original data from the representations without knowing the model parameters [37]. Therefore, we utilize the representations with the same dimension extracted by different clients' heterogeneous feature extractors and the corresponding labels (classes) to train a shared global prediction header on the server.\n\nIt acquires knowledge across all clients and all classes. Clients with homogeneous local models are the special cases of this scenario. We define the training goal of FedGH as minimizing the sum of the losses of all clients' local heterogeneous models { 0 , . . . ,\n\u22121 } with dimensions { 0 , . . . , \u22121 }: min 0 ,..., \u22121 \u2208R 0 ,..., \u22121 \u22121 \u2211\ufe01 =0 L ( ) , = \u2022 .(2)\n\nFederated Global Header (FedGH) Algorithm\n\nThe workflow of FedGH is displayed in Figure 1. In the -th FL training round, the -th client uses its feature extractor of the local heterogeneous model after local training to extract the representations R , of each local training sample ( , ) in . Then, it calculates the average representation of samples within the same class as the local averaged representation R , (abbr. LAR) of the corresponding class:\nR , = 1 \u2211\ufe01 \u2208 R , = 1 \u2211\ufe01 \u2208 F ; .(3)\nThe -th client uploads the LARs R , for each of its local classes and the corresponding class label to the server. As stated in Tan et al. [37], the representations are latent feature embedding vectors extracted from the data. Thus, it is hard to infer original data inversely with only extracted representations and without the parameters of the feature extractors. Since each client uploads LARs (i.e., class-wise averaged representations), the risk of privacy leakage is reduced further.\n\nThe server inputs all the received LARs R , from participating clients into the global prediction header H to produce the prediction. The hard loss (e.g., cross-entropy loss) between the output prediction and the true class label is used to update the global header parameters \u22121 via gradient descent:\n\u2190 \u22121 \u2212 \u2207\u2113 \u22121 ; R , , ,(4)\nwhere is the learning rate of the global prediction header. To improve the efficiency of training the global prediction header, we allow the server to train the global header once a client's LARs are received. After the LARs from all participating clients are fed into the global header for training, the global prediction header is updated in the current round. The updated global header acquires all-class knowledge across different clients. Thus, it has a stronger generalization capability than local headers with partialclass knowledge.\n\nThe server broadcasts the updated global header to the clients selected for the next training round. In the ( + 1)-th round, the -th client replaces its local prediction header with the received global header . In this way, its complete local model becomes:\n+1 = \u2022 .(5)\nIntuitively, clients' local models can converge faster with the generalized global header. Besides, the spliced complete local model obtains the old local knowledge from the personalized heterogeneous feature extractor and the new global knowledge from the  shared global header, which enables it to better deal with statistical heterogeneity.\n\nThe assembled complete local model +1 is trained on local data to obtain the updated local model +1 :\n+1 \u2190 +1 \u2212 \u2207\u2113 +1 ; ,(6)\nwhere is the local model learning rate. The above steps iterate until all local heterogeneous models converge. The pseudocode for FedGH can be found in Algorithm 1.\n\n\nDiscussion\n\nHere, we analyze the strength of FedGH in cost reduction and privacy preservation.\n\nComputation Cost. Under FedGH, clients are required to compute the representation for each local training data sample and the averaged representation for samples belonging to the same class. Extracting the representation for one sample is a forward inference of the local model on this sample. Thus, extracting representations only consumes half the computation cost of local training (forward and backwards) in one epoch. Generally, the epochs of local training are set to be larger than 1 in order to avoid frequent communications during FL model training [27]. Therefore, extracting representations consumes acceptable computation cost. Besides, since one representation is an \u00d7 1 vector, to calculate the average of representations belonging to each class held by a client, we can first use a \"variant\" to stack the sum of the representations of each class, and then calculate the average. Therefore, when calculating local average representation (LAR), each client incurs a storage cost and the computational complexity is O( ), which are negligible compared to the cost of local model training.\n\nOn the server side, the computation cost of using LARs to train a shared global header is much lower than training a complete model as the global header is part of a complete model and the number of LARs is far fewer than local data samples. Besides, since the server often has sufficient computation power, training a global header consumes an acceptable portion of its computation resources.\n\nOverall, due to negligible computation cost on both the client and server, FedGH is suitable for both cross-device FL scenarios with resource-constrained mobile edge devices and cross-silo FL scenarios with more powerful participants.\n\nCommunication Cost. During the client-to-server uplink communication, clients upload the LAR and the class label for each class to the server. The class label is an integer-type value and the LAR is an \u00d7 1 vector. If each client has classes, FedGH incurs ( + \u00d7 ) \u00d7 32 bits of communication cost, which can be negligible compared to uploading the complete local model in FedAvg.\n\nDuring server-to-client downlink communication, the server broadcasts the updated global header parameters to clients. This incurs lower communication costs than broadcasting the complete global model to clients in FedAvg. Thus, FedGH is communication-efficient.\n\nPrivacy Preservation. During the client-to-server uplink communication, clients upload the LAR and the class label for each class to the server. As stated above, the representation for a sample is an embedding vector mapped by the feature extractor from the original feature space to the embedding space. Thus, it is hard to infer the original data by stealing only representations without knowing the parameters of the feature extractor. Moreover, the uploaded LAR is a mixup of representations within the same class, which further enhances privacy protection.\n\nDuring the server-to-client downlink communication, the server broadcasts the global prediction header to clients. Since it is part of a complete model, it is also difficult to infer original data by just knowing the global prediction header. Hence, FedGH achieves a high level of privacy preservation. It can be combined with existing privacy protection mechanisms to further enhance FL security.\n\n\nCONVERGENCE ANALYSIS\n\nTo analyze the convergence of FedGH, we first introduce some additional notations. indicates the current communication round, \u2208 {0, 1, ..., } is a local iteration, with up to iterations being executed. ( + ) is the -th iteration in the ( +1)-th round. ( +0) indicates that at the beginning of the ( +1)-th round, clients replace their local prediction header with the global header trained in the -th round. ( + 1) is the first iteration in the ( + 1)-th round. ( + ) denotes the last iteration in the ( + 1)-th round.  From Eq. (7), we can further derive:\nL 1 \u2212 L 2 \u2a7d \u2207L 2 , 1 \u2212 2 + 1 2 1 \u2212 2 2 2 .(8)\nAssumption 4.2. Unbiased Gradient and Bounded Variance. The random gradient = \u2207L ; B (B is a batch of local data) of each client's local model is unbiased, i.e.,\nE B \u2286 = \u2207L ,(9)\nand the variance of random gradient is bounded by: \nE B \u2286 \u2207L ; B \u2212 \u2207L 2 2 \u2a7d 2 .(10)E L ( +1) +0 \u2a7d E L ( +1) + 1 2 2 .(12)\nThe detailed proof can be found in Appendix A. Based on Lemma 4.1 and Lemma 4.2, we can further derive the following theorems. Theorem 1. One-round deviation. Based on the above assumptions, the expectation of the loss of an arbitrary client's local model before the start of a round of local iteration satisfies\nE L ( +1) +0 \u2a7dL +0 \u2212 \u2212 1 2 2 \u2211\ufe01 =0 \u2225L + \u2225 2 2 + 1 2 + 2 2 .(13)\nThe proof can be found in Appendix B.\n\nTheorem 2. Non-convex convergence rate of FedGH. The above assumptions, for an arbitrary client and any > 0, the following inequality holds:\n1 \u22121 \u2211\ufe01 =0 \u2211\ufe01 =0 E \u2225L + \u2225 2 2 \u2a7d 2 (L =0 \u2212 L * ) (2 \u2212 1 ) + 1 2 + 2 2 \u2212 1 \u2a7d , s.t. < 2 \u2212 1 2 1 + 2 .(14)\nTherefore, under FedGH, an arbitrary client's local model can converge at the non-convex convergence rate \u223c O 1 . The detailed proof can be found in Appendix C.\n\n\nEXPERIMENTAL EVALUATION\n\nIn this section, we experimentally compare FedGH 1 with seven existing approaches on two real-world datasets. We implement FedGH and all baselines with PyTorch and simulate the FL processes on NVIDIA GeForce RTX 3090 GPUs with 24G memory.\n\n\nExperiment Setup\n\nDatasets and Models. We evaluate FedGH and baselines on two image classification datasets: CIFAR-10 and CIFAR-100 2 [18], which are manually divided into non-IID datasets following the method in Shamsian et al. [32]. Specifically, for CIFAR-10, we assign only data from 2 out of the 10 classes to each client (non-IID: 2/10). For CIFAR-100, we assign only data from 10 out of the 100 classes to each client (non-IID: 10/100). Then, each client's local data are further divided into the training set, the evaluation set and the testing set following the ratio of 8:1:1. In this way, the testing set is stored locally by each client which follows the same distribution as the local training set. For the CIFAR-10 and CIFAR-100 datasets, each client trains a CNN model and a ResNet-18 model, respectively. The dimensions of the output layer (i.e., the last fully-connected layer) are 10 and 100, and the dimensions of the representation layer (i.e., the second last layer) are set to be 500. Baselines. We compare FedGH with the following methods. Standalone, each client trains its local model independently, which serves as a lower bound of model performance. FedAvg [27], a popular FL algorithm that only supports homogeneous local models. The public-data independent model-heterogeneous FL methods include FML [33], FedKD [38] with mutual learning, LG-FedAvg [21] with model mixup, FD [17] with knowledge distillation on logits within the same class, and FedProto [37] with knowledge distillation on representations within the same class.\n\nEvaluation Metrics. Accuracy: we measure the accuracy (%) of each client's local model and report the average test accuracy of all clients' local models. Communication Overhead (CO): We record the communication overhead ( ) incurred upto the point in time when the FL model reaches the target accuracy, which is calculated as (number of rounds required \u00d7 the number of clients in each round \u00d7 number of floating point data transmitted in the uplink and downlink per round per client \u00d7 32 bits).\n\nTraining Strategy. We tune optimal FL settings for all methods via grid search. The epochs of local training: \u2208 {1, 10, 30, 50, 100} and the batch size of local training: \u2208 {32, 64, 128, 256, 512}. The optimizer of local training is SGD with learning rate = 0.01. We also tune special hyperparameters for baselines and report the optimal results. Note that FedGH introduces no additional hyperparameters except the global prediction header learning rate . We set = = 0.01 by default. To compare FedGH with the baselines fairly, we set the total number of communication rounds \u2208 {100, 500} to guarantee that all algorithms converge. Training process of FedGH. Client: On the CIFAR-10 (non-IID: 2/10) dataset, each client uses its local heterogeneous feature extractor after local training with learning rate = 0.01 to extract the representation embedding of each data sample and compute the local averaged representation (LAR) for each class. Then each client uploads LARs and labels of its held 2 classes to the server. Similarly, on CIFAR-100 (non-IID: 10/100) dataset, each client sends 10 classes' LARs and labels to the server. Server: In the order of client id, the server inputs the LAR of a class from one client into the global header once, then computes the hard loss between the global header output and the label to update the global header via gradient descent with a learning rate = 0.01. After LARs and labels from all participating clients have been processed, the global header updating in a given round is finished. Furthermore, to accelerate training the global header, we can regard the LARs and the corresponding labels from one client as a batch and allow the server to execute mini-batch gradient descent, which is necessary for the FL scenarios with a large number of clients or classes held by each client.  \n\n\nResults and Discussion\n\nModel-homogeneous FL can be regarded as a special case of modelheterogeneous FL. Thus, we first evaluate the approaches under the model-homogeneous FL setting before evaluating them under the model-heterogeneous FL setting.\n\n\nModel-Homogeneity FL Setting.\n\nTo compare FedGH with baselines with different total numbers of clients and client participating rates , we design three settings: {( = 10, = 100%), ( = 50, = 20%), ( = 100, = 10%)}. For a fair comparison, we ensure that the number of clients participating in each round is the same (i.e., = \u00d7 = 10). The results are illustrated in Tab. 1. It can be observed that FedGH consistently achieves the highest model accuracy across experimental conditions. On average, it outperforms the best baseline FedProto by 0.54% and 8.87% under CIFAR-10 and CIFAR-100, respectively. Since most algorithms achieve high accuracy when the batch size is set to 512 on CIFAR-10, the accuracy improvement of FedGH is still significant. In addition, the obvious accuracy improvement of FedGH on CIFAR-100 further demonstrates its effectiveness in tackling statistical heterogeneity (non-IID issue). Fig. 2 shows that FedGH converges to the highest accuracy at the fastest rate, demonstrating its high efficiency.\n\n\nModel-Heterogeneity FL Setting.\n\nIn this setting, we vary the number of filters in the convolutional layers and the dimension of fully-connected layers in CNN model to obtain 5 heterogeneous models: CNN-{1, 2, ..., 5}, the detailed model structures and sizes are reported in Tab. 2. We distribute them evenly among the clients (it is still possible for different clients to have models with the same structure). In FML and FedKD, we let CNN-{1, 2, ..., 5} be clients' heterogeneous large models, and CNN-5 with the smallest model size be clients' homogeneous small models for aggregation at server.  Table 3: Comparison of average test accuracy and communication overhead (CO) under the model-heterogeneous FL setting. CO/c/r denotes the CO per client per round. Rounds (X) denotes the number of training rounds required to reach target accuracy X, and CO is the total communication traffic consumed for the target accuracy. = 10 and = 100%. \"-\" indicates that the algorithm fails to converge or reach the target accuracy.  The results are shown in Tab. 3. It can be observed that FedGH consistently achieves the highest model accuracy. It outperforms the best baseline FedProto by 1.17% and 1.83% under CIFAR-10 and CIFAR-100, respectively. Meanwhile, FedGH requires the fewest communication rounds to reach the target accuracy, thereby achieving convergence the fastest. It achieves moderate CO under CIFAR-10. However, under the more challenging CIFAR-100 dataset, it incurs the lowest CO, reducing it by 85.53% compared to the bestperforming baseline FedProto.\n0 FML - - - - - - - - FedKD 80.16 - - - 52.70 - - - LG-\nTab. 3 also shows that FML fails to converge and FedKD converges with obviously lower accuracy. The reason for the results may be that training the heterogeneous large model and the homogeneous small model locally only requires the hard loss and distillation loss of the output logits of the two models in FML, which incurs less information interaction between the two models. And in the initial training rounds, the immature shared homogeneous small model may hinder the convergence of the local heterogeneous large model. FedKD designs an adaptive hidden loss of the two models' hidden states and an adaptive mutual distillation loss based on FML, the increase of interacted knowledge between the two models benefits their convergence.\n\n\nCase Studies\n\nIn this section, we evaluate the robustness of the approaches to Non-IIDness and client participation rates, and we also test whether Test Accuracy\n\n\nCIFAR-100\n\nLG-FedAvg FedProto FedGH FedGH is sensitive to the only hyperparameter (the learning rate of the global prediction header).  Figure 3 shows that FedGH consistently achieves the highest model accuracy across different Non-IID degrees on both CIFAR-10 and CIFAR-100, which demonstrates its robustness to Non-IIDness. In addition, it can also be observed that the model accuracy degrades as the number of classes increases (i.e., more IID) as personalization of local models is less advantageous as data heterogeneity decreases (which corroborates findings in [33]).\n\n\nRobustness to\n\n\nRobustness to Partial Participation.\n\nWe test FedGH and stateof-the-art model-heterogeneous baselines:\n\nLG-FedAvg and FedProto on CIFAR-10 and CIFAR-100 with different client participation rates. Specifically, we set = 100 and vary \u2208 {0.1, 0.3, 0.5, 0.7, 0.9, 1} under CIFAR-10 (Non-IID:2/10) and CIFAR-100 (Non-IID:10/100). Figure 4 shows that FedGH consistently achieves the highest model accuracy under different client participation rates on both CIFAR-10 and CIFAR-100. This demonstrates its robustness to client participation rate. It can also be observed that the model accuracy decreases as the client participation rate increases. As more clients participate in one round of FL model training, generalization is enhanced but personalization becomes more challenging.   Fig. 5 shows that the learning rate of the global prediction header has no influence on the performance of FedGH, indicating that FedGH is not sensitive to this hyperparameter. The reason is that there are few local average representations (LARs) from all client classes used for training the global prediction header. This training process is relatively easier than training local large complete models. Thus, the learning rate has no influence on it.\n\n\nSensitivity to\n\n\nCONCLUSIONS AND FUTURE WORK\n\nIn this paper, we proposed a model-heterogeneous FL framework -FedGH. It utilizes the same-dimension representations extracted by clients' local heterogeneous feature extractors to train a homogeneous global prediction header shared by all clients, which can transfer all-class knowledge to clients by replacing clients' local headers. Theoretical derivations prove the non-convex convergence rate of FedGH. Extensive experiments demonstrate its superiority in terms of model performance and communication efficiency in both model-homogeneous and model-heterogeneous FL settings.\n\nThere are two promising directions in future work: a) since computing the local averaged representation (LAR) of each class for each client may incur information distortion, especially when one class has a large number of data samples, exploring an integrated representation containing as much local data information as possible benefits boosting the performance of the global classification header. b) Considering the fusion of the generalized global header and the personalized local header may improve the generalization and personalization of each client's final classification header. \u2a7d L ( +1) + \u2207L +1 , +1 , +1 , +1 \u2212 +1 , +1 \u27e9 +\n\nFigure 1 :\n1The workflow of the proposed FedGH approach. In each communication round: \u2780 Clients train local heterogeneous models on local data. \u2781 Clients' feature extractors output representations of all local data samples and calculate the average of representations belonging to the same class. Then, the local averaged representation and label for each class are uploaded to the server; \u2782 The server uses the received local-averaged representations and class labels to train the global prediction header, then broadcasts it to the clients. \u2783 Clients replace their local prediction header with the received shared global header. Steps \u2780-\u2783 are repeated until all clients' local models converge. After federated training, heterogeneous local models are used for inference.\n\nAssumption 4 . 1 .\n41Lipschitz Smoothness. The -th client's local model gradient is 1-Lipschitz smooth, i.e., \u2207L 1 1 ; , \u2212 \u2207L 2 2 ; , \u2a7d 1 1 \u2212 2 , \u2200 1 , 2 > 0, \u2208 {0, 1, . . . , \u2212 1}, ( , ) \u2208 .\n\n( 7 )\n7Algorithm 1: FedGH Input: , total number of clients; , number of selected clients in one round; , number of rounds; , learning rate of local models; , learning rate of global header. Randomly initialize the heterogeneous local models 0 0 , . . . , 0 \u22121 and global header 0 . for = 0 to \u2212 1 do S \u2190 Randomly select \u2a7d clients to join FL. // Clients Side (each client \u2208 S ): Receive the global header \u22121 broadcast by the server; Update the local model: representation R , of each private training sample \u2208 on the trained local model ; Calculate the average representation for each local class: averaged local class representation R , and the corresponding class label to the server. // Server Side: Receive the averaged local class representation R , and corresponding class label from the selected clients; // Train the global header: for \u2208 S do \u2190 \u22121 \u2212 \u2207\u2113 \u22121 ; R , , ; end Broadcast the trained global header to the clients selected in the next round of training. end Return Personalized heterogeneous private models for all clients:\n\nAssumption 4. 3 .. 2 .\n32Bounded Variance of the Prediction Header. The variance of the local prediction header H ( ) for the local model trained on the client 's local data , and the global prediction header H( ) trained on the global data indirectly through LAR are bounded, i.e., parameter bounded: E \u2225 \u2212 \u2225 2 2 \u2a7d 2 , gradient bounded: E \u2225\u2207L ( ) \u2212 \u2207L( )\u2225 2 2 \u2a7d 2 . Based on the above assumptions, since FedGH makes no change to the local model training process, Lemma 4.1 derived by Tan et al. [37] still holds. Lemma 4.1. Based on Assumptions 4.1 and 4.2, during the {0, 1, ..., } local iterations of the ( + 1)-th FL training round, the loss of an arbitrary client's local model is bounded by: E L ( +1) \u2a7d L Based on Assumption 4.3, the loss of an arbitrary client's local model (the local prediction header of which is replaced with the latest global prediction header) is bounded by:\n\nFigure 2 :\n2Test accuracy varies with the communication rounds when = 10, = 100%.\n\nFigure 3 :Figure 4 :\n34Robustness Robustness to client participation rate.\n\nFigure 5 :\n5Sensitivity to the global header learning rate . prediction header on the server) on CIFAR-10 (Non-IID:2/10) and CIFAR-100 (Non-IID:10/100) datasets with the following settings: = 10, = 100%, SGD optimizer with the global header's learning rate = {0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1} and the local model's learning rate = 0.1.\n\n=\nL ( +1) + L +1 , +1 ; , \u2212 L +1 , +1 ; , ( )\n\nTable 1 :\n1Comparison of average test accuracy (%) under the model-homogeneous FL setting, with different total numbers of clients and client participating rate . \"-\" indicates that the algorithm fails to converge.= 10, = 100% \n= 50, = 20% \n= 100, = 10% \nMethod \nCIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100 \nStandalone \n93.13 \n62.80 \n95.39 \n62.38 \n92.92 \n55.47 \nFedAvg \n94.34 \n64.63 \n95.68 \n62.95 \n93.39 \n56.23 \nFML \n92.39 \n61.58 \n94.55 \n56.80 \n90.36 \n50.16 \nFedKD \n92.65 \n58.35 \n93.93 \n57.36 \n91.07 \n51.90 \nLG-FedAvg \n93.54 \n63.30 \n95.29 \n63.06 \n92.96 \n54.89 \nFD \n93.63 \n-\n-\n-\n-\n-\nFedProto \n95.99 \n62.51 \n95.38 \n61.15 \n92.75 \n55.53 \nFedGH \n96.33 \n73.62 \n95.69 \n65.02 \n93.65 \n56.44 \n\n0 \n20 \n40 \n60 \n80 \n100 \nCommunication Round \n\n60 \n\n70 \n\n80 \n\n90 \n\nTest Accuracy \n\nCIFAR-10 (Non-IID:2/10) \n\nStandalone \nFedAvg \nFML \nFedKD \nLG-FedAvg \nFD \nFedProto \nFedGH(ours) \n\n0 \n100 \n200 \n300 \n400 \n500 \nCommunication Round \n\n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\nTest Accuracy \n\nCIFAR-100 (Non-IID:10/100) \n\nStandalone \nFedAvg \nFML \nFedKD \nLG-FedAvg \nFD \nFedProto \nFedGH(ours) \n\n\n\nTable 2 :\n2Structures of five heterogeneous CNN models. In the convolutional layers, the kernel size is 5 \u00d7 5, the number of filters is 16 or 32, and the dimensions of fc3 are consistent with the classes in CIFAR-10 or CIFAR-100 datasets.layer name \nCNN-1 \nCNN-2 \nCNN-3 \nCNN-4 \nCNN-5 \nconv1 \n5\u00d75, 16 \n5\u00d75, 16 5\u00d75, 16 5\u00d75, 16 5\u00d75, 16 \nconv2 \n5\u00d75, 32 \n5\u00d75, 16 5\u00d75, 32 5\u00d75, 32 5\u00d75, 32 \nfc1 \n2000 \n2000 \n1000 \n800 \n500 \nfc2 \n500 \n500 \n500 \n500 \n500 \nfc3 \n10/100 \n10/100 \n10/100 \n10/100 \n10/100 \nmodel size 10.00 MB 6.92 MB 5.04 MB 3.81 MB 2.55 MB \n\n\n\n\nNon-IIDness. We test FedGH and state-of-theart model-heterogeneous baselines: LG-FedAvg and FedProto on CIFAR-10 and CIFAR-100 with different Non-IID degrees. Specifically, we set = 10 and = 100%. Then, we distribute {2, 4, 6, 8, 10} classes of samples into each client under CIFAR-10, and we allocate {10, 30, 50, 70, 90, 100} classes of samples into each client under CIFAR-100. The more classes of samples a client has, the lower the Non-IID degree.\n\n\nHyperparameter . We test the sensitivity of FedGH to its only hyperparameter (the learning rate of the global CIFAR-10 (Non-IID:2/10) CIFAR-100 (Non-IID:10/100)0 \n25 \n50 \n75 100 \nCommunication Round \n\n80 \n\n85 \n\n90 \n\n95 \n\nTest Accuracy \n\neta=0.001 \neta=0.003 \neta=0.01 \neta=0.03 \neta=0.1 \neta=0.3 \neta=1 \n\n0 \n25 \n50 \n75 100 \nCommunication Round \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\nTest Accuracy \n\neta=0.001 \neta=0.003 \neta=0.01 \neta=0.03 \neta=0.1 \neta=0.3 \neta=1 \n\n\nhttps://github.com/LipingYi/FedGH 2 https://www.cs.toronto.edu/%7Ekriz/cifar.html\nACKNOWLEDGMENTS\nWireless Federated Distillation for Distributed Edge Learning with Heterogeneous Data. Jin-Hyun Ahn, Proc. PIMRC. IEEE. PIMRC. IEEEIstanbul, TurkeyJin-Hyun Ahn et al. 2019. Wireless Federated Distillation for Distributed Edge Learning with Heterogeneous Data. In Proc. PIMRC. IEEE, Istanbul, Turkey, 1-6.\n\nCooperative Learning VIA Federated Distillation OVER Fading Channels. Jin-Hyun Ahn, Proc. ICASSP. IEEE. ICASSP. IEEEBarcelona, SpainJin-Hyun Ahn et al. 2020. Cooperative Learning VIA Federated Distillation OVER Fading Channels. In Proc. ICASSP. IEEE, Barcelona, Spain, 8856-8860.\n\nFedRolex: Model-Heterogeneous Federated Learning with Rolling Sub-Model Extraction. Samiul Alam, Proc. NeurIPS. , virtual. NeurIPS. , virtualSamiul Alam et al. 2022. FedRolex: Model-Heterogeneous Federated Learning with Rolling Sub-Model Extraction. In Proc. NeurIPS. , virtual.\n\nRepresentation Learning: A Review and New Perspectives. Yoshua Bengio, IEEE Trans. Pattern Anal. Mach. Intell. 35Yoshua Bengio et al. 2013. Representation Learning: A Review and New Perspec- tives. IEEE Trans. Pattern Anal. Mach. Intell. 35, 8 (2013), 1798-1828.\n\nCronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer. Hongyan Chang, Proc. NeurIPS Workshop. NeurIPS WorkshopHongyan Chang et al. 2021. Cronus: Robust and Heterogeneous Collabora- tive Learning with Black-Box Knowledge Transfer. In Proc. NeurIPS Workshop. , virtual.\n\nFedMatch: Federated Learning Over Heterogeneous Question Answering Data. Jiangui Chen, Proc. CIKM. ACM, virtual. CIKM. ACM, virtualJiangui Chen et al. 2021. FedMatch: Federated Learning Over Heterogeneous Question Answering Data. In Proc. CIKM. ACM, virtual, 181-190.\n\nFedGEMS: Federated Learning of Larger Server Models via Selective Knowledge Fusion. Sijie Cheng, CoRR abs/2110.11027Sijie Cheng et al. 2021. FedGEMS: Federated Learning of Larger Server Models via Selective Knowledge Fusion. CoRR abs/2110.11027 (2021).\n\nHeterogeneous Ensemble Knowledge Transfer for Training Large Models in Federated Learning. Yae Jee Cho, Proc. IJCAI. ijcai.org, virtual. IJCAI. ijcai.org, virtualYae Jee Cho et al. 2022. Heterogeneous Ensemble Knowledge Transfer for Train- ing Large Models in Federated Learning. In Proc. IJCAI. ijcai.org, virtual, 2881- 2887.\n\nExploiting Shared Representations for Personalized Federated Learning. Liam Collins, Proc. ICML. ICML139Liam Collins et al. 2021. Exploiting Shared Representations for Personalized Federated Learning. In Proc. ICML, Vol. 139. PMLR, virtual, 2089-2099.\n\nHeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients. Enmao Diao, Proc. ICLR. OpenReview.net, virtual. ICLR. OpenReview.net, virtualEnmao Diao et al. 2021. HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients. In Proc. ICLR. OpenReview.net, virtual.\n\nGroup Knowledge Transfer: Federated Learning of Large CNNs at the Edge. Chaoyang He, Proc. NeurIPS. , virtual. NeurIPS. , virtualChaoyang He et al. 2020. Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge. In Proc. NeurIPS. , virtual.\n\nFjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout. Samuel Horv\u00e1th, Proc. NeurIPS. OpenReview.net, virtual. NeurIPS. OpenReview.net, virtualSamuel Horv\u00e1th et al. 2021. FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout. In Proc. NeurIPS. OpenReview.net, virtual, 12876-12889.\n\nFew-Shot Model Agnostic Federated Learning. Wenke Huang, Proc. MM. ACM. MM. ACMLisboa, PortugalWenke Huang et al. 2022. Few-Shot Model Agnostic Federated Learning. In Proc. MM. ACM, Lisboa, Portugal, 7309-7316.\n\nLearn from Others and Be Yourself in Heterogeneous Federated Learning. Wenke Huang, Proc. CVPR. IEEE. CVPR. IEEEWenke Huang et al. 2022. Learn from Others and Be Yourself in Heterogeneous Federated Learning. In Proc. CVPR. IEEE, virtual, 10133-10143.\n\nDistillation-Based Semi-Supervised Federated Learning for Communication-Efficient Collaborative Training With Non-IID Private Data. Sohei Itahara, IEEE Trans. Mob. Comput. 22Sohei Itahara et al. 2023. Distillation-Based Semi-Supervised Federated Learning for Communication-Efficient Collaborative Training With Non-IID Private Data. IEEE Trans. Mob. Comput. 22, 1 (2023), 191-205.\n\nFedClassAvg: Local Representation Learning for Personalized Federated Learning on Heterogeneous Neural Networks. Jaehee Jang, Proc. ICPP. ACM, virtual. ICPP. ACM, virtual76Jaehee Jang et al. 2022. FedClassAvg: Local Representation Learning for Per- sonalized Federated Learning on Heterogeneous Neural Networks. In Proc. ICPP. ACM, virtual, 76:1-76:10.\n\nCommunication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data. Eunjeong Jeong, Proc. NeurIPS Workshop on Machine Learning on the Phone and other Consumer Devices. , virtual. NeurIPS Workshop on Machine Learning on the Phone and other Consumer Devices. , virtualEunjeong Jeong et al. 2018. Communication-Efficient On-Device Machine Learn- ing: Federated Distillation and Augmentation under Non-IID Private Data. In Proc. NeurIPS Workshop on Machine Learning on the Phone and other Consumer Devices. , virtual.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Toronto, ON, CanadaAlex Krizhevsky et al. 2009. Learning multiple layers of features from tiny images. Toronto, ON, Canada, .\n\nFedMD: Heterogenous Federated Learning via Model Distillation. Daliang Li, Junpu Wang, Proc. NeurIPS Workshop. NeurIPS WorkshopDaliang Li and Junpu Wang. 2019. FedMD: Heterogenous Federated Learning via Model Distillation. In Proc. NeurIPS Workshop. , virtual.\n\nPractical One-Shot Federated Learning for Cross-Silo Setting. Qinbin Li, Proc. IJCAI. ijcai.org, virtual. IJCAI. ijcai.org, virtualQinbin Li et al. 2021. Practical One-Shot Federated Learning for Cross-Silo Setting. In Proc. IJCAI. ijcai.org, virtual, 1484-1490.\n\nPaul Pu Liang, arXiv:2001.01523Think locally, act globally: Federated learning with local and global representations. 11arXiv preprintPaul Pu Liang et al. 2020. Think locally, act globally: Federated learning with local and global representations. arXiv preprint arXiv:2001.01523 1, 1 (2020).\n\nEnsemble Distillation for Robust Model Fusion in Federated Learning. Tao Lin, Proc. NeurIPS. , virtual. NeurIPS. , virtualTao Lin et al. 2020. Ensemble Distillation for Robust Model Fusion in Federated Learning. In Proc. NeurIPS. , virtual.\n\nCompletely Heterogeneous Federated Learning. Chang Liu, CoRR abs/2210.15865Chang Liu et al. 2022. Completely Heterogeneous Federated Learning. CoRR abs/2210.15865 (2022).\n\nGTG-Shapley: Efficient and Accurate Participant Contribution Evaluation in Federated Learning. Zelei Liu, ACM Trans. Intell. Syst. Technol. 1321Zelei Liu et al. 2022. GTG-Shapley: Efficient and Accurate Participant Contri- bution Evaluation in Federated Learning. ACM Trans. Intell. Syst. Technol. 13, 4 (2022), 60:1-60:21.\n\nHeterogeneous Model Fusion Federated Learning Mechanism Based on Model Mapping. Xiaofeng Lu, IEEE Internet Things J. 9Xiaofeng Lu et al. 2022. Heterogeneous Model Fusion Federated Learning Mech- anism Based on Model Mapping. IEEE Internet Things J. 9, 8 (2022), 6058-6068.\n\nArchitecture Agnostic Federated Learning for Neural Networks. Disha Makhija, Proc. ICML. ICML162Disha Makhija et al. 2022. Architecture Agnostic Federated Learning for Neural Networks. In Proc. ICML, Vol. 162. PMLR, virtual, 14860-14870.\n\nCommunication-Efficient Learning of Deep Networks from Decentralized Data. Brendan Mcmahan, PMLRProc. AISTATS. AISTATSFort Lauderdale, FL, USA54Brendan McMahan et al. 2017. Communication-Efficient Learning of Deep Net- works from Decentralized Data. In Proc. AISTATS, Vol. 54. PMLR, Fort Lauderdale, FL, USA, 1273-1282.\n\nFedBABU: Toward Enhanced Representation for Federated Image Classification. Jaehoon Oh, Proc. ICLR. OpenReview.net, virtual. ICLR. OpenReview.net, virtualJaehoon Oh et al. 2022. FedBABU: Toward Enhanced Representation for Federated Image Classification. In Proc. ICLR. OpenReview.net, virtual.\n\nFederated Learning with Partial Model Personalization. Krishna Pillutla, Proc. ICML. ICML162Krishna Pillutla et al. 2022. Federated Learning with Partial Model Personalization. In Proc. ICML, Vol. 162. PMLR, virtual, 17716-17758.\n\nFEDAUX: Leveraging Unlabeled Auxiliary Data in Federated Learning. Felix Sattler, IEEE Trans. Neural Networks Learn. Syst. 1Felix Sattler et al. 2021. FEDAUX: Leveraging Unlabeled Auxiliary Data in Federated Learning. IEEE Trans. Neural Networks Learn. Syst. 1, 1 (2021), 1-13.\n\nCFD: Communication-Efficient Federated Distillation via Soft-Label Quantization and Delta Coding. Felix Sattler, IEEE Trans. Netw. Sci. Eng. 9Felix Sattler et al. 2022. CFD: Communication-Efficient Federated Distillation via Soft-Label Quantization and Delta Coding. IEEE Trans. Netw. Sci. Eng. 9, 4 (2022), 2025-2038.\n\nPersonalized Federated Learning using Hypernetworks. Aviv Shamsian, Proc. ICML. ICML139Aviv Shamsian et al. 2021. Personalized Federated Learning using Hypernetworks. In Proc. ICML, Vol. 139. PMLR, virtual, 9489-9502.\n\nFederated Mutual Learning. CoRR abs. Tao Shen, 16765Tao Shen et al. 2020. Federated Mutual Learning. CoRR abs/2006.16765 (2020).\n\nTowards fairness-aware federated learning. Yuxin Shi, IEEE Transactions on Neural Networks and Learning Systems. 11Yuxin Shi et al. 2023. Towards fairness-aware federated learning. IEEE Transac- tions on Neural Networks and Learning Systems 1, 1 (2023), 1.\n\nFedFAIM: A model performance-based fair incentive mechanism for federated learning. Zhuan Shi, IEEE Transactions on Big Data. 11Zhuan Shi et al. 2022. FedFAIM: A model performance-based fair incentive mechanism for federated learning. IEEE Transactions on Big Data 1, 1 (2022), 1.\n\nFedWM: Federated Crowdsourcing Workforce Management Service for Productive Laziness. Zhuan Shi, Proc. ICWS. IEEE. ICWS. IEEEChicago, USA, 1Zhuan Shi et al. 2023. FedWM: Federated Crowdsourcing Workforce Management Service for Productive Laziness. In Proc. ICWS. IEEE, Chicago, USA, 1.\n\nFedProto: Federated Prototype Learning across Heterogeneous Clients. Yue Tan, Proc. AAAI. AAAI Press, virtual. AAAI. AAAI Press, virtualYue Tan et al. 2022. FedProto: Federated Prototype Learning across Heteroge- neous Clients. In Proc. AAAI. AAAI Press, virtual, 8432-8440.\n\nCommunication-efficient federated learning via knowledge distillation. Chuhan Wu, Nature Communications. 132032Chuhan Wu et al. 2022. Communication-efficient federated learning via knowl- edge distillation. Nature Communications 13, 1 (2022), 2032.\n\nFederated Learning. Qiang Yang, Yang Liu, Yong Cheng, Yan Kang, Tianjian Chen, Han Yu, Morgan & Claypool Publishers207 pagesQiang Yang, Yang Liu, Yong Cheng, Yan Kang, Tianjian Chen, and Han Yu. 2019. Federated Learning. Morgan & Claypool Publishers, . 207 pages.\n\nQSFL: A Two-Level Uplink Communication Optimization Framework for Federated Learning. Liping Yi, Proc. ICML. ICML162Liping Yi et al. 2022. QSFL: A Two-Level Uplink Communication Optimization Framework for Federated Learning. In Proc. ICML, Vol. 162. PMLR, Virtual, 25501- 25513.\n\nFed2: Feature-Aligned Federated Learning. Fuxun Yu, Proc. KDD. ACM, virtual. KDD. ACM, virtualFuxun Yu et al. 2021. Fed2: Feature-Aligned Federated Learning. In Proc. KDD. ACM, virtual, 2066-2074.\n\nAlgorithmic Management for Improving Collective Productivity in Crowdsourcing. Han Yu, Scientific Reports. 11Han Yu et al. 2017. Algorithmic Management for Improving Collective Produc- tivity in Crowdsourcing. Scientific Reports 1, 1 (2017), 1.\n\nResource-aware Federated Learning using Knowledge Extraction and Multi-model Fusion. Sixing Yu, CoRR abs/2208.07978Sixing Yu et al. 2022. Resource-aware Federated Learning using Knowledge Extraction and Multi-model Fusion. CoRR abs/2208.07978 (2022).\n\nD2D-LSTM: LSTM-Based Path Prediction of Content Diffusion Tree in Device-to-Device Social Networks. Heng Zhang, Proc. AAAI. AAAIOrlando, FL, USAAAAI PressHeng Zhang et al. 2020. D2D-LSTM: LSTM-Based Path Prediction of Content Diffusion Tree in Device-to-Device Social Networks. In Proc. AAAI. AAAI Press, Orlando, FL, USA, 295-302.\n\nHow Far Have Edge Clouds Gone? A Spatial-Temporal Analysis of Edge Network Latency In the Wild. Heng Zhang, Proc. IWQoS. IWQoSNew York, USA, 1IEEEHeng Zhang et al. 2023. How Far Have Edge Clouds Gone? A Spatial-Temporal Analysis of Edge Network Latency In the Wild. In Proc. IWQoS. IEEE, New York, USA, 1.\n\nA Measurement-Driven Analysis and Prediction of Content Propagation in the Device-to-Device Social Networks. Heng Zhang, IEEE Trans. Knowl. Data Eng. 35Heng Zhang et al. 2023. A Measurement-Driven Analysis and Prediction of Content Propagation in the Device-to-Device Social Networks. IEEE Trans. Knowl. Data Eng. 35, 8 (2023), 7651-7664.\n\nFedZKT: Zero-Shot Knowledge Transfer towards Resource-Constrained Federated Learning with Heterogeneous On-Device Models. Lan Zhang, Proc. ICDCS. IEEE. ICDCS. IEEELan Zhang et al. 2022. FedZKT: Zero-Shot Knowledge Transfer towards Resource- Constrained Federated Learning with Heterogeneous On-Device Models. In Proc. ICDCS. IEEE, virtual, 928-938.\n\nData-Free Knowledge Distillation for Heterogeneous Federated Learning. Zhuangdi Zhu, Proc. ICML. ICML139Zhuangdi Zhu et al. 2021. Data-Free Knowledge Distillation for Heterogeneous Federated Learning. In Proc. ICML, Vol. 139. PMLR, virtual, 12878-12889.\n\nResilient and Communication Efficient Learning for Heterogeneous Federated Systems. Zhuangdi Zhu, Proc. ICML. ICML162Zhuangdi Zhu et al. 2022. Resilient and Communication Efficient Learning for Heterogeneous Federated Systems. In Proc. ICML, Vol. 162. PMLR, virtual, 27504-27526.\n", "annotations": {"author": "[{\"end\":184,\"start\":146},{\"end\":195,\"start\":185},{\"end\":210,\"start\":196},{\"end\":247,\"start\":211},{\"end\":273,\"start\":248},{\"end\":284,\"start\":274},{\"end\":295,\"start\":285},{\"end\":310,\"start\":296},{\"end\":321,\"start\":311},{\"end\":329,\"start\":322},{\"end\":360,\"start\":330},{\"end\":426,\"start\":361},{\"end\":492,\"start\":427},{\"end\":568,\"start\":493},{\"end\":684,\"start\":569},{\"end\":756,\"start\":685}]", "publisher": "[{\"end\":75,\"start\":72},{\"end\":964,\"start\":961}]", "author_last_name": "[{\"end\":155,\"start\":153},{\"end\":194,\"start\":190},{\"end\":209,\"start\":206},{\"end\":220,\"start\":217},{\"end\":254,\"start\":252},{\"end\":283,\"start\":281},{\"end\":294,\"start\":290},{\"end\":309,\"start\":306},{\"end\":320,\"start\":317},{\"end\":328,\"start\":326}]", "author_first_name": "[{\"end\":152,\"start\":146},{\"end\":189,\"start\":185},{\"end\":205,\"start\":196},{\"end\":216,\"start\":211},{\"end\":251,\"start\":248},{\"end\":280,\"start\":274},{\"end\":289,\"start\":285},{\"end\":305,\"start\":296},{\"end\":316,\"start\":311},{\"end\":325,\"start\":322}]", "author_affiliation": "[{\"end\":359,\"start\":331},{\"end\":425,\"start\":362},{\"end\":491,\"start\":428},{\"end\":567,\"start\":494},{\"end\":683,\"start\":570},{\"end\":755,\"start\":686}]", "title": "[{\"end\":71,\"start\":1},{\"end\":827,\"start\":757}]", "venue": "[{\"end\":911,\"start\":829}]", "abstract": "[{\"end\":3026,\"start\":1342}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3070,\"start\":3066},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3282,\"start\":3278},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4289,\"start\":4285},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4292,\"start\":4289},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4295,\"start\":4292},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4298,\"start\":4295},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4302,\"start\":4298},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4306,\"start\":4302},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4310,\"start\":4306},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4935,\"start\":4931},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4938,\"start\":4935},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5711,\"start\":5707},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5714,\"start\":5711},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5717,\"start\":5714},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5925,\"start\":5921},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5928,\"start\":5925},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8337,\"start\":8333},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8349,\"start\":8345},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8359,\"start\":8355},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8375,\"start\":8371},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8389,\"start\":8386},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8403,\"start\":8399},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9153,\"start\":9149},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9167,\"start\":9163},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9705,\"start\":9702},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9717,\"start\":9713},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9733,\"start\":9729},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9822,\"start\":9818},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9928,\"start\":9924},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9939,\"start\":9935},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9951,\"start\":9947},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9963,\"start\":9960},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9980,\"start\":9976},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10191,\"start\":10188},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10204,\"start\":10200},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11190,\"start\":11187},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11204,\"start\":11201},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11218,\"start\":11214},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11241,\"start\":11237},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11302,\"start\":11298},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11313,\"start\":11309},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11334,\"start\":11330},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11871,\"start\":11867},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11886,\"start\":11882},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12246,\"start\":12242},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12604,\"start\":12600},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12957,\"start\":12953},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13272,\"start\":13268},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13606,\"start\":13603},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13608,\"start\":13606},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13738,\"start\":13734},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17829,\"start\":17826},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":18044,\"start\":18040},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19270,\"start\":19266},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22055,\"start\":22051},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":26985,\"start\":26981},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27080,\"start\":27076},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28035,\"start\":28031},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28180,\"start\":28176},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":28192,\"start\":28188},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28229,\"start\":28225},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28255,\"start\":28251},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28334,\"start\":28330},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":35108,\"start\":35104}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38399,\"start\":37626},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38592,\"start\":38400},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39631,\"start\":38593},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40522,\"start\":39632},{\"attributes\":{\"id\":\"fig_4\"},\"end\":40605,\"start\":40523},{\"attributes\":{\"id\":\"fig_7\"},\"end\":40681,\"start\":40606},{\"attributes\":{\"id\":\"fig_8\"},\"end\":41024,\"start\":40682},{\"attributes\":{\"id\":\"fig_9\"},\"end\":41071,\"start\":41025},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":42154,\"start\":41072},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":42701,\"start\":42155},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":43156,\"start\":42702},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":43614,\"start\":43157}]", "paragraph": "[{\"end\":4037,\"start\":3042},{\"end\":4436,\"start\":4039},{\"end\":4580,\"start\":4438},{\"end\":5236,\"start\":4582},{\"end\":6167,\"start\":5238},{\"end\":7056,\"start\":6169},{\"end\":8061,\"start\":7058},{\"end\":8781,\"start\":8078},{\"end\":9073,\"start\":8783},{\"end\":9882,\"start\":9075},{\"end\":10081,\"start\":9884},{\"end\":10312,\"start\":10083},{\"end\":10774,\"start\":10314},{\"end\":10892,\"start\":10776},{\"end\":11844,\"start\":10894},{\"end\":12199,\"start\":11846},{\"end\":14344,\"start\":12201},{\"end\":14923,\"start\":14346},{\"end\":15356,\"start\":14925},{\"end\":15648,\"start\":15388},{\"end\":16361,\"start\":15666},{\"end\":16468,\"start\":16368},{\"end\":16705,\"start\":16502},{\"end\":16893,\"start\":16707},{\"end\":17086,\"start\":16895},{\"end\":18273,\"start\":17189},{\"end\":18540,\"start\":18275},{\"end\":19091,\"start\":18681},{\"end\":19617,\"start\":19127},{\"end\":19920,\"start\":19619},{\"end\":20488,\"start\":19947},{\"end\":20747,\"start\":20490},{\"end\":21103,\"start\":20760},{\"end\":21206,\"start\":21105},{\"end\":21394,\"start\":21230},{\"end\":21491,\"start\":21409},{\"end\":22593,\"start\":21493},{\"end\":22988,\"start\":22595},{\"end\":23224,\"start\":22990},{\"end\":23603,\"start\":23226},{\"end\":23867,\"start\":23605},{\"end\":24430,\"start\":23869},{\"end\":24829,\"start\":24432},{\"end\":25410,\"start\":24854},{\"end\":25618,\"start\":25457},{\"end\":25686,\"start\":25635},{\"end\":26069,\"start\":25757},{\"end\":26171,\"start\":26134},{\"end\":26313,\"start\":26173},{\"end\":26578,\"start\":26418},{\"end\":26844,\"start\":26606},{\"end\":28404,\"start\":26865},{\"end\":28900,\"start\":28406},{\"end\":30734,\"start\":28902},{\"end\":30984,\"start\":30761},{\"end\":32008,\"start\":31018},{\"end\":33575,\"start\":32044},{\"end\":34369,\"start\":33632},{\"end\":34533,\"start\":34386},{\"end\":35110,\"start\":34547},{\"end\":35231,\"start\":35167},{\"end\":36359,\"start\":35233},{\"end\":36987,\"start\":36408},{\"end\":37625,\"start\":36989}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16367,\"start\":16362},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16501,\"start\":16469},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17188,\"start\":17087},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18636,\"start\":18541},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19126,\"start\":19092},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19946,\"start\":19921},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20759,\"start\":20748},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21229,\"start\":21207},{\"attributes\":{\"id\":\"formula_8\"},\"end\":25456,\"start\":25411},{\"attributes\":{\"id\":\"formula_9\"},\"end\":25634,\"start\":25619},{\"attributes\":{\"id\":\"formula_10\"},\"end\":25718,\"start\":25687},{\"attributes\":{\"id\":\"formula_11\"},\"end\":25756,\"start\":25718},{\"attributes\":{\"id\":\"formula_12\"},\"end\":26133,\"start\":26070},{\"attributes\":{\"id\":\"formula_13\"},\"end\":26417,\"start\":26314},{\"attributes\":{\"id\":\"formula_14\"},\"end\":33631,\"start\":33576}]", "table_ref": "[{\"end\":32618,\"start\":32611}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3040,\"start\":3028},{\"attributes\":{\"n\":\"2\"},\"end\":8076,\"start\":8064},{\"attributes\":{\"n\":\"3\"},\"end\":15386,\"start\":15359},{\"attributes\":{\"n\":\"3.1\"},\"end\":15664,\"start\":15651},{\"attributes\":{\"n\":\"3.2\"},\"end\":18679,\"start\":18638},{\"attributes\":{\"n\":\"3.3\"},\"end\":21407,\"start\":21397},{\"attributes\":{\"n\":\"4\"},\"end\":24852,\"start\":24832},{\"attributes\":{\"n\":\"5\"},\"end\":26604,\"start\":26581},{\"attributes\":{\"n\":\"5.1\"},\"end\":26863,\"start\":26847},{\"attributes\":{\"n\":\"5.2\"},\"end\":30759,\"start\":30737},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":31016,\"start\":30987},{\"attributes\":{\"n\":\"5.2.2\"},\"end\":32042,\"start\":32011},{\"attributes\":{\"n\":\"5.3\"},\"end\":34384,\"start\":34372},{\"end\":34545,\"start\":34536},{\"attributes\":{\"n\":\"5.3.1\"},\"end\":35126,\"start\":35113},{\"attributes\":{\"n\":\"5.3.2\"},\"end\":35165,\"start\":35129},{\"attributes\":{\"n\":\"5.3.3\"},\"end\":36376,\"start\":36362},{\"attributes\":{\"n\":\"6\"},\"end\":36406,\"start\":36379},{\"end\":37637,\"start\":37627},{\"end\":38419,\"start\":38401},{\"end\":38599,\"start\":38594},{\"end\":39655,\"start\":39633},{\"end\":40534,\"start\":40524},{\"end\":40627,\"start\":40607},{\"end\":40693,\"start\":40683},{\"end\":41027,\"start\":41026},{\"end\":41082,\"start\":41073},{\"end\":42165,\"start\":42156}]", "table": "[{\"end\":42154,\"start\":41287},{\"end\":42701,\"start\":42394},{\"end\":43614,\"start\":43319}]", "figure_caption": "[{\"end\":38399,\"start\":37639},{\"end\":38592,\"start\":38422},{\"end\":39631,\"start\":38601},{\"end\":40522,\"start\":39658},{\"end\":40605,\"start\":40536},{\"end\":40681,\"start\":40630},{\"end\":41024,\"start\":40695},{\"end\":41071,\"start\":41028},{\"end\":41287,\"start\":41084},{\"end\":42394,\"start\":42167},{\"end\":43156,\"start\":42704},{\"end\":43319,\"start\":43159}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18727,\"start\":18719},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31901,\"start\":31895},{\"end\":34680,\"start\":34672},{\"end\":35462,\"start\":35454},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":35913,\"start\":35907}]", "bib_author_first_name": "[{\"end\":43808,\"start\":43800},{\"end\":44097,\"start\":44089},{\"end\":44390,\"start\":44384},{\"end\":44642,\"start\":44636},{\"end\":44942,\"start\":44935},{\"end\":45229,\"start\":45222},{\"end\":45507,\"start\":45502},{\"end\":45770,\"start\":45763},{\"end\":46076,\"start\":46072},{\"end\":46355,\"start\":46350},{\"end\":46669,\"start\":46661},{\"end\":46945,\"start\":46939},{\"end\":47255,\"start\":47250},{\"end\":47494,\"start\":47489},{\"end\":47807,\"start\":47802},{\"end\":48171,\"start\":48165},{\"end\":48534,\"start\":48526},{\"end\":49032,\"start\":49028},{\"end\":49242,\"start\":49235},{\"end\":49252,\"start\":49247},{\"end\":49502,\"start\":49496},{\"end\":49705,\"start\":49698},{\"end\":50064,\"start\":50061},{\"end\":50284,\"start\":50279},{\"end\":50506,\"start\":50501},{\"end\":50819,\"start\":50811},{\"end\":51072,\"start\":51067},{\"end\":51326,\"start\":51319},{\"end\":51648,\"start\":51641},{\"end\":51922,\"start\":51915},{\"end\":52163,\"start\":52158},{\"end\":52473,\"start\":52468},{\"end\":52747,\"start\":52743},{\"end\":52949,\"start\":52946},{\"end\":53087,\"start\":53082},{\"end\":53386,\"start\":53381},{\"end\":53669,\"start\":53664},{\"end\":53937,\"start\":53934},{\"end\":54218,\"start\":54212},{\"end\":54416,\"start\":54411},{\"end\":54427,\"start\":54423},{\"end\":54437,\"start\":54433},{\"end\":54448,\"start\":54445},{\"end\":54463,\"start\":54455},{\"end\":54473,\"start\":54470},{\"end\":54748,\"start\":54742},{\"end\":54983,\"start\":54978},{\"end\":55216,\"start\":55213},{\"end\":55471,\"start\":55465},{\"end\":55736,\"start\":55732},{\"end\":56065,\"start\":56061},{\"end\":56385,\"start\":56381},{\"end\":56737,\"start\":56734},{\"end\":57041,\"start\":57033},{\"end\":57309,\"start\":57301}]", "bib_author_last_name": "[{\"end\":43812,\"start\":43809},{\"end\":44101,\"start\":44098},{\"end\":44395,\"start\":44391},{\"end\":44649,\"start\":44643},{\"end\":44948,\"start\":44943},{\"end\":45234,\"start\":45230},{\"end\":45513,\"start\":45508},{\"end\":45774,\"start\":45771},{\"end\":46084,\"start\":46077},{\"end\":46360,\"start\":46356},{\"end\":46672,\"start\":46670},{\"end\":46953,\"start\":46946},{\"end\":47261,\"start\":47256},{\"end\":47500,\"start\":47495},{\"end\":47815,\"start\":47808},{\"end\":48176,\"start\":48172},{\"end\":48540,\"start\":48535},{\"end\":49043,\"start\":49033},{\"end\":49245,\"start\":49243},{\"end\":49257,\"start\":49253},{\"end\":49505,\"start\":49503},{\"end\":49711,\"start\":49706},{\"end\":50068,\"start\":50065},{\"end\":50288,\"start\":50285},{\"end\":50510,\"start\":50507},{\"end\":50822,\"start\":50820},{\"end\":51080,\"start\":51073},{\"end\":51334,\"start\":51327},{\"end\":51651,\"start\":51649},{\"end\":51931,\"start\":51923},{\"end\":52171,\"start\":52164},{\"end\":52481,\"start\":52474},{\"end\":52756,\"start\":52748},{\"end\":52954,\"start\":52950},{\"end\":53091,\"start\":53088},{\"end\":53390,\"start\":53387},{\"end\":53673,\"start\":53670},{\"end\":53941,\"start\":53938},{\"end\":54221,\"start\":54219},{\"end\":54421,\"start\":54417},{\"end\":54431,\"start\":54428},{\"end\":54443,\"start\":54438},{\"end\":54453,\"start\":54449},{\"end\":54468,\"start\":54464},{\"end\":54476,\"start\":54474},{\"end\":54751,\"start\":54749},{\"end\":54986,\"start\":54984},{\"end\":55219,\"start\":55217},{\"end\":55474,\"start\":55472},{\"end\":55742,\"start\":55737},{\"end\":56071,\"start\":56066},{\"end\":56391,\"start\":56386},{\"end\":56743,\"start\":56738},{\"end\":57045,\"start\":57042},{\"end\":57313,\"start\":57310}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":195820327},\"end\":44017,\"start\":43713},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":211020827},\"end\":44298,\"start\":44019},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":254247279},\"end\":44578,\"start\":44300},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":393948},\"end\":44842,\"start\":44580},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":209460621},\"end\":45147,\"start\":44844},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":236976053},\"end\":45416,\"start\":45149},{\"attributes\":{\"doi\":\"CoRR abs/2110.11027\",\"id\":\"b6\"},\"end\":45670,\"start\":45418},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":248406165},\"end\":45999,\"start\":45672},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":231924497},\"end\":46252,\"start\":46001},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":222133374},\"end\":46587,\"start\":46254},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":225091697},\"end\":46843,\"start\":46589},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":232068701},\"end\":47204,\"start\":46845},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":252783066},\"end\":47416,\"start\":47206},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":250210682},\"end\":47668,\"start\":47418},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":221135969},\"end\":48050,\"start\":47670},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":253116772},\"end\":48404,\"start\":48052},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":53820846},\"end\":48971,\"start\":48406},{\"attributes\":{\"id\":\"b17\"},\"end\":49170,\"start\":48973},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":203951869},\"end\":49432,\"start\":49172},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":235078788},\"end\":49696,\"start\":49434},{\"attributes\":{\"doi\":\"arXiv:2001.01523\",\"id\":\"b20\"},\"end\":49990,\"start\":49698},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":219636007},\"end\":50232,\"start\":49992},{\"attributes\":{\"doi\":\"CoRR abs/2210.15865\",\"id\":\"b22\"},\"end\":50404,\"start\":50234},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":237420512},\"end\":50729,\"start\":50406},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":239759357},\"end\":51003,\"start\":50731},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":246867098},\"end\":51242,\"start\":51005},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b26\",\"matched_paper_id\":14955348},\"end\":51563,\"start\":51244},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":235417313},\"end\":51858,\"start\":51565},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":248069201},\"end\":52089,\"start\":51860},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":231801970},\"end\":52368,\"start\":52091},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":236330611},\"end\":52688,\"start\":52370},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":232147378},\"end\":52907,\"start\":52690},{\"attributes\":{\"id\":\"b32\"},\"end\":53037,\"start\":52909},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":249494399},\"end\":53295,\"start\":53039},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":249802814},\"end\":53577,\"start\":53297},{\"attributes\":{\"id\":\"b35\"},\"end\":53863,\"start\":53579},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":247292268},\"end\":54139,\"start\":53865},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":237353469},\"end\":54389,\"start\":54141},{\"attributes\":{\"id\":\"b38\"},\"end\":54654,\"start\":54391},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":250360874},\"end\":54934,\"start\":54656},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":236980041},\"end\":55132,\"start\":54936},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":26658272},\"end\":55378,\"start\":55134},{\"attributes\":{\"doi\":\"CoRR abs/2208.07978\",\"id\":\"b42\"},\"end\":55630,\"start\":55380},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":212806413},\"end\":55963,\"start\":55632},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":260255261},\"end\":56270,\"start\":55965},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":253354885},\"end\":56610,\"start\":56272},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":247996828},\"end\":56960,\"start\":56612},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":235125689},\"end\":57215,\"start\":56962},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":250360879},\"end\":57496,\"start\":57217}]", "bib_title": "[{\"end\":43798,\"start\":43713},{\"end\":44087,\"start\":44019},{\"end\":44382,\"start\":44300},{\"end\":44634,\"start\":44580},{\"end\":44933,\"start\":44844},{\"end\":45220,\"start\":45149},{\"end\":45761,\"start\":45672},{\"end\":46070,\"start\":46001},{\"end\":46348,\"start\":46254},{\"end\":46659,\"start\":46589},{\"end\":46937,\"start\":46845},{\"end\":47248,\"start\":47206},{\"end\":47487,\"start\":47418},{\"end\":47800,\"start\":47670},{\"end\":48163,\"start\":48052},{\"end\":48524,\"start\":48406},{\"end\":49233,\"start\":49172},{\"end\":49494,\"start\":49434},{\"end\":50059,\"start\":49992},{\"end\":50499,\"start\":50406},{\"end\":50809,\"start\":50731},{\"end\":51065,\"start\":51005},{\"end\":51317,\"start\":51244},{\"end\":51639,\"start\":51565},{\"end\":51913,\"start\":51860},{\"end\":52156,\"start\":52091},{\"end\":52466,\"start\":52370},{\"end\":52741,\"start\":52690},{\"end\":53080,\"start\":53039},{\"end\":53379,\"start\":53297},{\"end\":53662,\"start\":53579},{\"end\":53932,\"start\":53865},{\"end\":54210,\"start\":54141},{\"end\":54740,\"start\":54656},{\"end\":54976,\"start\":54936},{\"end\":55211,\"start\":55134},{\"end\":55730,\"start\":55632},{\"end\":56059,\"start\":55965},{\"end\":56379,\"start\":56272},{\"end\":56732,\"start\":56612},{\"end\":57031,\"start\":56962},{\"end\":57299,\"start\":57217}]", "bib_author": "[{\"end\":43814,\"start\":43800},{\"end\":44103,\"start\":44089},{\"end\":44397,\"start\":44384},{\"end\":44651,\"start\":44636},{\"end\":44950,\"start\":44935},{\"end\":45236,\"start\":45222},{\"end\":45515,\"start\":45502},{\"end\":45776,\"start\":45763},{\"end\":46086,\"start\":46072},{\"end\":46362,\"start\":46350},{\"end\":46674,\"start\":46661},{\"end\":46955,\"start\":46939},{\"end\":47263,\"start\":47250},{\"end\":47502,\"start\":47489},{\"end\":47817,\"start\":47802},{\"end\":48178,\"start\":48165},{\"end\":48542,\"start\":48526},{\"end\":49045,\"start\":49028},{\"end\":49247,\"start\":49235},{\"end\":49259,\"start\":49247},{\"end\":49507,\"start\":49496},{\"end\":49713,\"start\":49698},{\"end\":50070,\"start\":50061},{\"end\":50290,\"start\":50279},{\"end\":50512,\"start\":50501},{\"end\":50824,\"start\":50811},{\"end\":51082,\"start\":51067},{\"end\":51336,\"start\":51319},{\"end\":51653,\"start\":51641},{\"end\":51933,\"start\":51915},{\"end\":52173,\"start\":52158},{\"end\":52483,\"start\":52468},{\"end\":52758,\"start\":52743},{\"end\":52956,\"start\":52946},{\"end\":53093,\"start\":53082},{\"end\":53392,\"start\":53381},{\"end\":53675,\"start\":53664},{\"end\":53943,\"start\":53934},{\"end\":54223,\"start\":54212},{\"end\":54423,\"start\":54411},{\"end\":54433,\"start\":54423},{\"end\":54445,\"start\":54433},{\"end\":54455,\"start\":54445},{\"end\":54470,\"start\":54455},{\"end\":54478,\"start\":54470},{\"end\":54753,\"start\":54742},{\"end\":54988,\"start\":54978},{\"end\":55221,\"start\":55213},{\"end\":55476,\"start\":55465},{\"end\":55744,\"start\":55732},{\"end\":56073,\"start\":56061},{\"end\":56393,\"start\":56381},{\"end\":56745,\"start\":56734},{\"end\":57047,\"start\":57033},{\"end\":57315,\"start\":57301}]", "bib_venue": "[{\"end\":43831,\"start\":43814},{\"end\":44121,\"start\":44103},{\"end\":44421,\"start\":44397},{\"end\":44689,\"start\":44651},{\"end\":44972,\"start\":44950},{\"end\":45260,\"start\":45236},{\"end\":45500,\"start\":45418},{\"end\":45807,\"start\":45776},{\"end\":46096,\"start\":46086},{\"end\":46397,\"start\":46362},{\"end\":46698,\"start\":46674},{\"end\":46993,\"start\":46955},{\"end\":47276,\"start\":47263},{\"end\":47518,\"start\":47502},{\"end\":47840,\"start\":47817},{\"end\":48202,\"start\":48178},{\"end\":48635,\"start\":48542},{\"end\":49026,\"start\":48973},{\"end\":49281,\"start\":49259},{\"end\":49538,\"start\":49507},{\"end\":49814,\"start\":49729},{\"end\":50094,\"start\":50070},{\"end\":50277,\"start\":50234},{\"end\":50544,\"start\":50512},{\"end\":50846,\"start\":50824},{\"end\":51092,\"start\":51082},{\"end\":51353,\"start\":51340},{\"end\":51688,\"start\":51653},{\"end\":51943,\"start\":51933},{\"end\":52212,\"start\":52173},{\"end\":52509,\"start\":52483},{\"end\":52768,\"start\":52758},{\"end\":52944,\"start\":52909},{\"end\":53150,\"start\":53093},{\"end\":53421,\"start\":53392},{\"end\":53691,\"start\":53675},{\"end\":53974,\"start\":53943},{\"end\":54244,\"start\":54223},{\"end\":54409,\"start\":54391},{\"end\":54763,\"start\":54753},{\"end\":55011,\"start\":54988},{\"end\":55239,\"start\":55221},{\"end\":55463,\"start\":55380},{\"end\":55754,\"start\":55744},{\"end\":56084,\"start\":56073},{\"end\":56420,\"start\":56393},{\"end\":56762,\"start\":56745},{\"end\":57057,\"start\":57047},{\"end\":57325,\"start\":57315},{\"end\":43860,\"start\":43833},{\"end\":44151,\"start\":44123},{\"end\":44441,\"start\":44423},{\"end\":44990,\"start\":44974},{\"end\":45280,\"start\":45262},{\"end\":45834,\"start\":45809},{\"end\":46102,\"start\":46098},{\"end\":46428,\"start\":46399},{\"end\":46718,\"start\":46700},{\"end\":47027,\"start\":46995},{\"end\":47301,\"start\":47278},{\"end\":47530,\"start\":47520},{\"end\":48222,\"start\":48204},{\"end\":48724,\"start\":48637},{\"end\":49299,\"start\":49283},{\"end\":49565,\"start\":49540},{\"end\":50114,\"start\":50096},{\"end\":51098,\"start\":51094},{\"end\":51386,\"start\":51355},{\"end\":51719,\"start\":51690},{\"end\":51949,\"start\":51945},{\"end\":52774,\"start\":52770},{\"end\":53718,\"start\":53693},{\"end\":54001,\"start\":53976},{\"end\":54769,\"start\":54765},{\"end\":55030,\"start\":55013},{\"end\":55776,\"start\":55756},{\"end\":56107,\"start\":56086},{\"end\":56775,\"start\":56764},{\"end\":57063,\"start\":57059},{\"end\":57331,\"start\":57327}]"}}}, "year": 2023, "month": 12, "day": 17}