{"id": 257767148, "updated": "2023-10-05 02:43:34.445", "metadata": {"title": "Spatially Adaptive Self-Supervised Learning for Real-World Image Denoising", "authors": "[{\"first\":\"Junyi\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Zhilu\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Xiaoyu\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Chaoyu\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Xiaotao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Lei\",\"middle\":[]},{\"first\":\"Wangmeng\",\"last\":\"Zuo\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Significant progress has been made in self-supervised image denoising (SSID) in the recent few years. However, most methods focus on dealing with spatially independent noise, and they have little practicality on real-world sRGB images with spatially correlated noise. Although pixel-shuffle downsampling has been suggested for breaking the noise correlation, it breaks the original information of images, which limits the denoising performance. In this paper, we propose a novel perspective to solve this problem, i.e., seeking for spatially adaptive supervision for real-world sRGB image denoising. Specifically, we take into account the respective characteristics of flat and textured regions in noisy images, and construct supervisions for them separately. For flat areas, the supervision can be safely derived from non-adjacent pixels, which are much far from the current pixel for excluding the influence of the noise-correlated ones. And we extend the blind-spot network to a blind-neighborhood network (BNN) for providing supervision on flat areas. For textured regions, the supervision has to be closely related to the content of adjacent pixels. And we present a locally aware network (LAN) to meet the requirement, while LAN itself is selectively supervised with the output of BNN. Combining these two supervisions, a denoising network (e.g., U-Net) can be well-trained. Extensive experiments show that our method performs favorably against state-of-the-art SSID methods on real-world sRGB photographs. The code is available at https://github.com/nagejacob/SpatiallyAdaptiveSSID.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2303.14934", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LiZ0FWLZ23", "doi": "10.1109/cvpr52729.2023.00956"}}, "content": {"source": {"pdf_hash": "de5281c799d1077d4a51ad3114cca08a98d8addc", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2303.14934v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "de950950dcceb04eab6fc4f14a1092616b5f43a2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/de5281c799d1077d4a51ad3114cca08a98d8addc.txt", "contents": "\nSpatially Adaptive Self-Supervised Learning for Real-World Image Denoising\n\n\nJunyi Li \nSchool of Computer Science and Technology\nHarbin Institute of Technology\nChina\n\nZhilu Zhang cszlzhang@outlook.com \nSchool of Computer Science and Technology\nHarbin Institute of Technology\nChina\n\nXiaoyu Liu liuxiaoyu1104@gmail.com \nSchool of Computer Science and Technology\nHarbin Institute of Technology\nChina\n\nChaoyu Feng \nXiaotao Wang \nLei Lei \nWangmeng Zuo wmzuo@hit.edu.cn \nSchool of Computer Science and Technology\nHarbin Institute of Technology\nChina\n\nPeng Cheng Laboratory\nChina\n\nSpatially Adaptive Self-Supervised Learning for Real-World Image Denoising\n\nSignificant progress has been made in self-supervised image denoising (SSID) in the recent few years. However, most methods focus on dealing with spatially independent noise, and they have little practicality on real-world sRGB images with spatially correlated noise. Although pixelshuffle downsampling has been suggested for breaking the noise correlation, it breaks the original information of images, which limits the denoising performance. In this paper, we propose a novel perspective to solve this problem, i.e., seeking for spatially adaptive supervision for real-world sRGB image denoising. Specifically, we take into account the respective characteristics of flat and textured regions in noisy images, and construct supervisions for them separately. For flat areas, the supervision can be safely derived from non-adjacent pixels, which are much far from the current pixel for excluding the influence of the noise-correlated ones. And we extend the blind-spot network to a blindneighborhood network (BNN) for providing supervision on flat areas. For textured regions, the supervision has to be closely related to the content of adjacent pixels. And we present a locally aware network (LAN) to meet the requirement, while LAN itself is selectively supervised with the output of BNN. Combining these two supervisions, a denoising network (e.g., U-Net) can be well-trained. Extensive experiments show that our method performs favorably against state-of-the-art SSID methods on real-world sRGB photographs. The code is available at https://github. com/nagejacob/SpatiallyAdaptiveSSID. ] have been proposed, which can be trained merely on noisy images. However, the noise model assumptions of a large amount of SSID methods do not match the characteristics of real-world noise in sRGB space. For instance,  improves the denoising performance with posterior inference, but requires explicit noise probability density. Noise2Score [20] and its extension[19]propose a closed-form image denoising schema with score matching followed by noise model and noise level estimation, but the noise is bounded to Tweedie distribution. Although some methods[17,42]are designed for distribution agnostic noise, they can only deal with spatially independent noise.Recently, a few attempts have been explored to remove spatially correlated noise in a self-supervised manner. CVF-SID [35] disentangles the image and noise components from noisy images, but the difficulty of optimization limits its performance. Some methods[44,57]break the spatial noise correlation with pixel-shuffle downsampling (PD), then utilize spatially independent denoisers (e.g., blind-spot network[6,26,44]) to remove the uncorrelated noise. However, PD breaks the original information of the images and leads to aliasing artifacts, which largely degrade the image quality. AP-BSN [27] applies asymmetric PD factors and post-refinement processing to seek for a better trade-off between noise removal and aliasing artifacts, but it is timeconsuming during inference.\n\nIntroduction\n\nImage denoising aims to restore clean images from noisy observations [5,11,14], and it has achieved noticeable improvement with the advances in deep networks [2, 10, 21, 29-31, 33, 38, 40, 41, 46-48, 50, 51]. However, the models trained with synthetic noise usually perform poorly in real-world scenarios in which noise is complex and change-(a) Noisy Input (b) CVF-SID [35] (c) AP-BSN+R 3 [27] (d) Ours Figure 1. Visual comparison between self-supervised denoising methods on the DND dataset [37]. PSNR (dB) and SSIM with respect to the ground-truth are marked on the result for quantitative comparison. Our method performs better in removing spatially correlated noise from real-world sRGB photographs. able. A feasible solution is to collect real-world cleannoisy image pairs [1,37] and take them for model training [2,15,21,46,47]. But building such datasets generally requires strictly controlled environment as well as complicated photographing and post-processing, which is timeconsuming and labor-intensive. Moreover, the noise statistics vary under different cameras and illuminating conditions [43,54], and it is impractical to capture pairs for every device and scenario.\n\nTo circumvent the limitations of noisy-clean pairs collection, self-supervised image denoising (SSID) ap-In this paper, we present a novel perspective for SSID by considering the respective characteristics of flat and textured regions in noisy images, resulting in a spatially adaptive SSID method for real-world sRGB images. Instead of utilizing pixel-shuffle downsampling and blind-spot network to learn denoising results directly, we seek for spatially adaptive supervision for a denoising network (e.g., U-Net [39]). Concretely, for flat areas, the supervision can be safely derived from non-adjacent pixels, which are much far from the current pixel for excluding the influence of noise correlation. We achieve it by extending the blind-spot network (BSN) [26] to a blind-neighborhood network (BNN). BNN modifies the architecture of BSN to expand the size of blind region, and takes the same self-supervised training schema as BSN. Note that it is difficult to determine whether an area is flat or not from the noisy images, so we directly apply BNN to the whole image and it has little effect on the handling of flat areas. Moreover, such an operation can give us a chance to detect textured areas from the output of BNN, whose variance is usually higher. For textured areas, neighboring pixels are essential for predicting the details and they can not be ignored. To this end, we present a locally aware network (LAN), which focuses on recovering the texture details solely from adjacent pixels. LAN is supervised by flat areas of BNN output. When training is done, LAN will be applied to textured areas to generate supervision information for these areas.\n\nCombining the learned supervisions for flat and textured areas, a denoising network can be readily trained. During inference, BNN and LAN can be detached, only the ultimate denoising network is used to restore clean images. Extensive experiments are conducted on SIDD [1] and DND [37] datasets. The results demonstrate our method is not only effective but also efficient. In comparison to stateof-the-art self-supervised denoising methods, our method behaves favorably in terms of both quantitative metrics and perceptual quality. The contributions of this paper can be summarized as follows:\n\n\u2022 We propose a novel perspective for self-supervised real-world image denoising, i.e., learning spatially adaptive supervision for a denoising network according to the image characteristics.\n\n\u2022 For flat areas, we extend the blind-spot network to a blind-neighborhood network (BNN) for providing supervision information. For texture areas, we present a locally aware network (LAN) to learn that from neighboring pixels.\n\n\u2022 Extensive experiments show our method has superior performance and inference efficiency against state-ofthe-art SSID methods on real-world sRGB noise removal.\n\n\nRelated Work\n\n\nDeep Image Denoising\n\nThe development of convolutional neural networks (CNNs) has led to great improvement on deep image denoising [50,51]. DnCNN [50] outperforms traditional patch-based methods [5,11,14] on Gaussian denoising. FFDNet [51] takes a noise level map as input, which can handle various noise levels with a single model. Other learning based methods, such as RED30 [33], Mem-Net [40], and MWCNN [31], are also developed with advanced architectures. Nevertheless, models trained on AWGN generalize poorly to real scenarios due to the domain discrepancy between synthetic and real noise.\n\nIn order to mitigate the gap between synthetic and real noise, one feasible way is to simulate realistic noise as much as possible and introduce it during the network training [15,49]. For example, CBDNet [15] inverses the demosaicing and gamma correction steps in image signal processing (ISP), then synthesizes signal-dependent Poisson-Gaussian noise [12] in raw space. Zhou et al. [57] break spatial-correlated noise into pixel-independent one with pixel-shuffle downsampling, then handle it with AWGNbased denoiser. Another way is to capture paired noisyclean images for constructing real-world datasets [1,37].\n\nTaking such datasets for training, models have better potential to generalize to the corresponding real noise [2,10,21,29,38,41,[46][47][48]. However, conducting well-aligned training pairs generally requires a controlled environment and much human labor. Moreover, noise statistics vary under different cameras and different illuminating conditions [43,54]. It is impractical to collect datasets for every device and scenario.\n\n\nUnpaired Image Denoising\n\nUnpaired methods [8,16,18,44] mitigate the data collecting issue by training networks with unpaired noisy and clean datasets. Most of which [8,16,18] learn to model the noise statistics with adversarial training. A noise generator is first trained to match the noise distribution of the noisy images, then used to map the clean images to pseudonoisy ones. Finally, the denoising network is trained with synthetic pseudo-noisy and clean image pairs. In addition, Wu et al. [44] learn the noise distribution by jointly training a denoising network and a noise estimator. And the ultimate denoising network is trained with pseudo-noisy and clean pairs as well as noisy and denoised pairs. However, as the noise distribution in sRGB space is very complex and difficult to model [23], the performance of unpaired methods is still limited when facing real-world photographs.\n\n\nSelf-Supervised Image Denoising\n\nIn order to get rid of dependence on clean images, selfsupervised methods are proposed, which are trained with noisy images only. Noise2Noise [28] suggests to learn a model from paired noisy images, which remains limited in practice. Subsequently, Noise2Void [24] and Noise2Self [3] separate noisy images into input and target pairs with a mask strategy. Blind-spot networks take a step further by excluding the corresponding input noisy pixel from the receptive field for each output pixel, which can be implemented with multiple network branches [7,26] or dilated and masked convolutions [6,44]. Moreover, probabilistic inference [25,26] and regular loss functions [42] are proposed to alleviate the information loss issue at the blind spot. In addition, Noisier2Noise [34] and NAC [45] take the noisy images as target, and synthesize and add new noise to the noisy images for model training. More recently, Noise2Score [20] and its extension [19] propose a closedform denoising for Tweedie distributions with score matching and posterior inference. SelfIR [55] introduce blurry images for self-supervised denoising task. Nevertheless, the above self-supervised methods can only handle noisy images with spatially independent noise.\n\nA few attempts have been done to remove the spatially correlated noise in a self-supervised manner. R2R [36] reverses the noisy images to raw space to synthesize training pairs, then renders the synthetic raw images back to sRGB space. But it requires several priors, such as the parameters of the camera ISP and noise model. StructN2V [4] adapts the blind mask from a single pixel [24] to match the structure of the noise. CVF-SID [35] disentangles noisy images into clean images and noise components. Among the self-supervised approaches for real-world sRGB noise, AP-BSN [27] propose asymmetric PD factors and post-refinement processing to make a better trade-off between noise removal and aliasing artifacts, but it is timeconsuming during inference.\n\n\nMethod\n\nSelf-supervised denoising aims to predict the denoised imagesx from the noisy observations y, without the supervision from clean images x. In this paper, we propose a new perspective for self-supervised denoising. Specifically, we extract appropriate supervisions from noisy images according to the image characters (i.e., flatness) for denoising network training. In this section, we first introduce how to learn the supervision for flat (Sec. 3.1) and textured (Sec. 3.2) areas, respectively. Then we discuss how to utilize the supervisions for the denoising network in Sec. 3.3.\n\n\nSupervision for Flat Areas\n\nFor a noisy image, denoising of flat areas is generally easier than that of textured areas whose edges and details need to be preserved well [56]. And the same goes for our extraction of supervision information. Thus, we handle these two areas separately with the consideration of their different characteristics. In this subsection, we first learn the supervision for flat areas.\n\nFor removing spatially correlated noise in real-world sRGB images, pixel-shuffle downsampling (PD) has been applied to break the noise correlation [27,44,57]. Then blind-spot network (BSN) [6,26,44] can be applied to the downsampled sub-images to remove the spatially independent noise. However, the high-frequency information is distorted during downsampling [13], leading to aliasing artifacts. Moreover, apart from noise-correlated neighbor pixels, a large amount of non-neighboring pixels are removed, which degrades the denoising performance.\n\nIn terms of flat areas in noisy images, a pixel of the underlying clean images is similar to a large number of surrounding pixels, not only the neighboring pixels but also non-neighboring ones. The non-neighboring pixels can be far enough from the current pixel that exceeds the noise correlation range. Thus, instead of PD, we can enlarge the blind-spot of BSN into a blind-neighborhood until covering all noise-correlated pixels. We present a blindneighborhood network (BNN) to achieve it, as shown in Figure 2(a). Different from PD, BNN can make full use of the uncorrelated noisy pixels and preserve the original information as much as possible. Similar to BSN, the loss function of BNN is as follows:\n\u0ddc \u2112 = \u2212 1 \u2112 = ( \u2212 ) \u2022 ( ) \u2212 \u0ddc 1 + \u2022 ( ) \u2212 \u0ddc 1 \u2112 = ( \u2212 ) \u2022 ( ) \u2212 1 (a)L BN N = \u2225x 1 \u2212 y\u2225 1 ,(1)\nwherex 1 is the output of BNN, y is the noisy image. As the random noise can not be predicted, the output of BNN will converge to the clean image. In addition, the network architecture of BNN is modified from the BSN used in HQ-SSL [26]. The BSN applies four network branches whose receptive fields are restricted in different directions. At the end of each branch, a single pixel shift is applied to the features to create the blindspot. We increase the pixel shift size from 1 to k to create a (2k \u22121)\u00d7(2k \u22121) blind-neighborhood. Referring to the analysis of spatial correlation on real-world noise in AP-BSN [27], we set k = 5 for 9\u00d79 blind-neighborhood. More experimental analysis on the blind-neighborhood size can be seen in Sec. 5.3. The details of network architecture are provided in the supplementary material.\n\n\nSupervision for Texture Areas\n\nNote that we apply BNN to the whole image. On the one hand, it is difficult to predetermine whether an area is flat or not from noisy images. Such an operation has little effect on the supervision prediction of flat areas. On the other hand, textured areas can be easily detected from the full output of BNN for their supervision extraction. In this subsection, we illustrate how to determine and extract supervision for textured areas.\n\nDetermination of Textured Areas. In flat areas, BNN sufficiently removes the noise, and the corresponding variance is low. In texture areas, although BNN is ambiguous in reserving details, the output is still not flat and the variance is high. With the above observations, we can infer the flatness from the local variance of BNN outputx 1 . Specifically, we densely extract image patches for each spatial location (i, j), then calculate the standard deviation map \u03c3,\n\u03c3(i, j) = std(x 1 (i\u2212 n\u22121 2 : i+ n\u22121 2 , j \u2212 n\u22121 2 : j + n\u22121 2 )),(2)\nwhere std(\u00b7) represents the standard deviation function and is measured on 1-channel patches by averaging RGB values. n denotes the local window size, which we empirically set n = 7. Binarization \u03c3(i, j) to determine the flatness (textured or flat) is rough and sometimes incorrect. Instead, we convert the standard deviation map \u03c3 into soft coefficients \u03b1 which are normalized to [0, 1]. In our experiments, \u03c3 is a reliable indicator that it is usually high in the textured areas (e.g., edges, texts) and is low in flat areas. Io generate the coefficient \u03b1 that indicating flatness, we convert \u03c3 to \u03b1 with a piecewise function,\n\u03b1(i, j) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 S(\u03c3(i, j) \u2212 1), \u03c3(i, j) \u2264 l 0.5, l < \u03c3(i, j) \u2264 u S(\u03c3(i, j) \u2212 5), \u03c3(i, j) > u (3)\nwhere S(\u00b7) denotes the Sigmoid function. We empirically set l = 1 and u = 5 respectively. Higher \u03b1(i, j) means the local area is more textured. Locally Aware Network. Blind-neighborhood network performs poorly in texture areas, as it ignores the adjacent pixels which are essential for details prediction. To extract proper supervision for texture areas, we present a locally aware network (LAN). LAN is carefully designed to have a local receptive field, which focuses on recovering the texture details from neighboring pixels. Nonetheless, LAN is a supervised network to be well-trained that requires clean supervision. Fortunately, the flat areas of BNN output give a chance to supervise LAN. On the one hand, the flat areas of BNN are well-denoised to approximate clean signal, and LAN can be trained safely. On the other hand, The small receptive field of LAN makes it able to preserve texture details. The loss function for training LAN is as follows:\nL LAN = (1 \u2212 \u03b1) \u00b7 \u2225sg(x 1 ) \u2212x 2 \u2225 1 ,(4)\nwherex 1 andx 2 is the output of BNN and LAN, respectively. sg(\u00b7) denotes stop gradient operation [9]. When training is done, LAN will have a transfer ability to predict the supervision for textured areas.\n\nThe network structure of LAN is simple yet delicate. We stack 3\u00d73 convolution layers to create the local receptive field, specifically, k 3\u00d73 layers can make up (2k+1)\u00d7(2k+1) receptive field. In order to further refine the color information, we additionally add several 1\u00d71 convolution blocks with channel attention mechanism [53]. More details are provided in the supplementary material.\n\n\nDenoising with Learned Supervisions\n\nThe learned images of BNN and LAN can be leveraged in two ways: image-level fusion or use as supervisions to train a denoising network. We note that imagelevel fusion suffers from performance and efficiency issues. From the performance perspective, although BNN and LAN are designed to denoise flat and texture areas respectively, they still have their weaknesses. BNN may generate oversmooth results in flat areas, while LAN can not completely remove the noise in texture areas. Image-level fusion can not avoid these drawbacks and only provides limited improvement. Instead, utilizing the results as supervision to train a denoising network achieves a better trade-off between detail preserving and noise removal, which yields better performance. From the efficiency perspective, BNN is computationally complex due to the special network design for blind-neighborhood. Training additional denoising network provides more flexibility on the network complexity. Thus, we tend to train a denoising network with learned spatially adaptive supervisions. More experimental analysis about the two ways can be seen in Sec. 5.2.\n\nWe choose a common and representative network structure, i.e., U-Net [39], as our denoising network. As shown in Figure 2(c), the U-Net is trained with the following loss,\nL D = (1 \u2212 \u03b1) \u00b7 \u2225sg(x 1 ) \u2212x\u2225 1 + \u03b1 \u00b7 \u2225sg(x 2 ) \u2212x\u2225 1 , (5)\nwherex is the output of U-Net,x 1 andx 2 are the output of BNN and LAN, respectively. sg(\u00b7) is stop gradient operation [9] and \u03b1 is the adaptive coefficients mentioned above.\n\n\nExperiments\n\n\nExperimental Settings\n\nDatasets. We conduct experiments on SIDD [1] and DND [37], which are widely used datasets for real-world image denoising. SIDD dataset captures noisy-clean pairs with smartphone cameras. Each noisy image is captured multiple times, and the mean image is served as the groundtruth. It provides 320 image pairs (SIDD-Medium) for training, 1,280 patches for validation, and 1280 patches for benchmark testing. DND [37] dataset provides 50 image pairs for testing only. Its noisy-clean pairs are conducted by shooting the same scene twice with different ISO settings. The high-ISO images are taken as noisy inputs, while the corresponding low-ISO images are nearly noise-free and can serve as ground-truth. The benchmark testing results (i.e., PSNR and SSIM) can be achieved by uploading the denoised patches to their official websites. We note that SIDD and DND benchmarks provide evaluations in both the raw space and sRGB space. As our method is developed to remove the spatially correlated noise of sRGB images, we do not compare with the self-supervised denoising methods [6,17,42] for raw images.\n\nImplementation Details. We only leverage the noisy images of SIDD Medium dataset [1] to train our denoising framework. Our BNN, LAN and the denoising network are trained successively with the same training settings. We crop the training images into patches of size 256\u00d7256, and augment the image patches with random flipping and rotation. The augmented patches are formed as mini-batches of size 8 to facilitate network training. Each network is trained with Adam optimizer [22] for 400k iterations, for a total of 1200k iterations. The learning rate is initially set to 3 \u00d7 10 \u22124 , and decreased to zero with cosine annealing scheduler [32]. Table 1. Quantitative comparison of PSNR (dB), SSIM and LPIPS on SIDD [1] and DND [37] datasets. LPIPS is calculated on SIDD validation dataset only as the ground-truth images on benchmark datasets are not available. Hereinafter, red and blue indicate the best and the second best results among unpaired and self-supervised methods, respectively.  \n\n\nResults for Real-World Denoising\n\nQuantitative Comparison. The quantitative comparison results of SIDD [1] and DND [37] datasets can be seen in Table 1. Our method outperforms all unpaired and selfsupervised approaches. In terms of PSNR and SSIM metrics, our method is comparable to DnCNN [50] trained with real-world pairs. It demonstrates our method further mitigates the performance gap between self-supervised methods and supervised ones. Among self-supervised methods, Noise2Void [24] and Noise2Self [3] fail to handle the noise in sRGB images due to their spatially independent noise assumption. NAC [45] and R2R [36] depend on the parameters of the camera ISP to simulate training pairs, which (a) Noisy Input (b) Baseline, N2C [39] (c) NAC [45] (d) CVF-SID [35] (e) AP-BSN+R 3 [27] (f) Ours are usually unavailable. CVF-SID [35], AP-BSN [27] and our method are directly trained on noisy images, while our method achieves the best performance. We also provide LPIPS [52] results on SIDD validation dataset to measure the perceptual quality. Our method shows the lowest LPIPS score in all competing denoising methods. It shows that our denoised images are most perceptually similar to the ground-truth. Qualitative Comparison. The visual comparison of state-of-the-art self-supervised methods on the benchmark datasets is shown in Figure 3 and Figure 4. As the clean ground-truth image is not provided, we train a supervised counterpart of our denoising network for reference, which is called baseline N2C. From Figure 3, our method removes most spatially correlated noise and recovers the texture details better. Other methods hardly generate visually pleasure results. Noise2Void [24] has little denoising effect and seems to converge into the noisy image. CVF-SID [35] can not decouple the noisy and clean components well from the noisy observations, leading to partially denoised results. AP-BSN [27] suffers from aliasing artifacts due to pixelshuffle downsampling, e.g., the texture details on the fabric are missed. The qualitative comparison on DND benchmark testing dataset [37] is shown in Figure 4. In comparison with other competitive methods, our result is cleaner and more photo-realistic.\n\n\nAblation Study\n\nWe conduct extensive ablation studies on SIDD validation dataset [1] to analyze the effectiveness of the proposed framework. Table 2 shows that each component of the proposed model is essential for the denoising network training. From the table,x 1 andx 2 are not ideal supervisions and have their pros and cons. Supervision byx 1 clearly removes the noise but at the risk of smoothing-out fine details. Supervision byx 2 preserves the texture details but exhibits inferior denoising effect. The network trained with both supervisions makes a tradeoff between noise removal and texture preserving, thus showing better results than separate ones. Each one of them alone is not sufficient enough to facilitate the denoising network training. Specifically, using BNN output as supervision alone leads to 0.46dB performance drop, and using LAN output as supervision alone widens the gap to 1.35dB. This demonstrates that both BNN and LAN provide indispensable supervisions for training the denoising network. Adaptive coefficients \u03b1 is used for balancing the supervision strength of BNN and LAN outputs according to the local flatness. We apply \u03b1 to the loss function (i.e., Eqn. (5)) to emphasize the BNN output in flat areas, as well as the LAN output in texture areas. It provides 0.09dB performance boost, which demonstrates the effectiveness of learning spatially adaptive supervision for self-supervised denoising.\n\n\nEffects of Supervision Components\n\n\nComparison with Image-Level Fusion\n\nApart from as supervisions, the outputs of BNN and LAN can also serve as denoised images directly, and further fused to get better results. However, as illustrated in Sec. 3.3, the way of image-level fusion have performance and efficiency limits. We try spatially adaptive fusion strategy that the outputs of BNN and LAN are weighted averaged according to the adaptive coefficients \u03b1, the result is shown in Table 3. Although the fusion strategy has improvement over individual images, it still falls 0.55dB from our proposed method. Besides, due to the complexity of BNN, the inference time of the fusion strategy is much higher. Figure 5 provides visual comparison between the fused image and our denoising network outputx.x shows better denoising effects. It further demonstrates the effectiveness of taking BNN and LAN outputs as supervisions.  \n(a)x 1 (b)x 2 (c) \u03b1 (d) Fused (e)x\n\nBlind-Neighborhood Size\n\nAs illustrated in Sec. 3.1, the blind-neighborhood of BNN should cover the spatially correlated noisy pixels accurately. Figure 6(a) analyses the effects of blindneightborhood size of BNN. For 1\u00d71 blind-neighborhood size, BNN degrades to BSN which does not work for spatially correlated noise. When gradually increasing the blind-neighborhood size, BNN shows better denoising effect as more noise-correlated pixels are excluded from the receptive field. The max performance of BNN is achieved at 9\u00d79 blind-neighborhood size, which means all the noise correlation pixels have lain in this range. It is consistent with the observation in AP-BSN [27]. Further increasing the blind-neighborhood size excludes noise-independent pixels from the receptive field, which is harmful to denoising performance. The visualization of denoised images in Figure 7(a) also supports the 9\u00d79 blind-neighborhood size. In addition, the calculation of adaptive coefficients \u03b1 and the training of LAN are highly depend on the denoising quality of BNN on flat areas. Thus, the performance of the denoising network shows the same trend as BNN.    But at the same time, the output of LAN tends to be more blurry. The outputs of LAN should serve as good supervision of texture areas for the denoising network training. The best performance of our denoising framework is achieved when LAN has 3\u00d73 local receptive size.\n\n\nLocal Receptive Size\n\n\nConclusion\n\nIn this work, we propose a novel perspective to solve real-world image self-supervised denoising, i.e., seeking for spatially adaptive supervision for a denoising network according to the image characteristics. The clean signal in flat areas can be inferred from non-neighboring noiseindependent pixels, while the texture details should come from neighboring ones. Thus, blind-neighborhood network (BNN) is proposed to learn the supervision for flat areas. Adaptive coefficients and locally aware network (LAN) are proposed to determine the flatness and learn the supervision for texture areas, respectively. The denoising network trained by learned supervisions outperforms state-of-the-art self-supervised and unpaired methods with better efficiency.\n\nFigure 3 .\n3Qualitative comparison on SIDD benchmark dataset[1]. Sub-figure (b) denotes the result of our denoising network trained on SIDD dataset in a supervised manner. Sub-figures (c)-(f) are from self-supervised denoising methods.\n\nFigure 4 .\n4Qualitative comparison on DND benchmark dataset[37]. Sub-figures (c)-(f) are from self-supervised denoising methods.\n\nFigure 5 .\n5Visual comparison with image-level fusion strategy. (d) denotes the spatially adaptive fusion result (1 \u2212 \u03b1) \u00b7x1 + \u03b1 \u00b7x2.\n\nFigure 6 (\n6b) and7(b)  show the effects of local receptive size of LAN. For 1\u00d71 local receptive size, LAN has little denoising effect due to no neighboring pixels are leveraged. For larger local receptive sizes, LAN can effectively predict the favorable signal from neighbor pixels with good details.\n\nFigure 6 .\n6Effects of the blind-neighborhood size and local receptive size. (a) BNN achieves best performance with blindneighborhood size 9\u00d79. (b) LAN achieves better performance as the local receptive size increases, but the final denoising network performs best with local receptive size 3\u00d73. ) BNN output with different blind-neighborhood sizes.(b) LAN output with different local receptive sizes.\n\nFigure 7 .\n7Visual comparison of different blind-neighborhood sizes and local receptive sizes.\n\n\nFigure 2. Overview of our self-supervised denoising framework. (a) In training stage one, the blind-neighborhood network (BNN) learns to remove the spatially correlated noise for flat areas in a self-supervised manner. (b) In training stage two, the locally aware network (LAN) is supervised by flat areas of BAN outputs. When training is done, LAN will be applied to texture areas to generate texture details. (c) In training stage three, the denoising network is supervised by the first two stages' results with adaptive coefficients. (d) During inference, the 3rd-stage denoising network can be deployed to denoise real-world photographs.Training Stage 1: Blind-Neighborhood Network \n(b) Training Stage 2: Locally Aware Network \n\n(c) Training Stage 3: Denoising Network \n(d) Inference with the 3rd-Stage Denoising Network \n\nNoisy Image \nDenoising Result \n\nTextured \nor Flat ? \n\n\n\nTable 2 .\n2Ablation study of supervision components. The notation ofx1,x2 and \u03b1 follows Eqn.(5).Supervision ofx1 \n\u2713 \n\u2713 \n\u2713 \nSupervision ofx2 \n\u2713 \n\u2713 \n\u2713 \nAdaptive Coefficients \u03b1 \n\u2713 \n\nPSNR ofx \n36.84 35.95 37.30 37.39 \n\nTable 3. Comparison between image-level fusion strategy and our \nmethod. \n\nImagex1x2 \n(1 \u2212 \u03b1) \u00b7x1 + \u03b1 \u00b7x2x \n\nPSNR \n36.37 35.00 \n36.84 \n37.39 \nTime (ms) \n16.7 \n5.9 \n22.9 \n4.8 \n\n\nAcknowledgement. This work is partially supported by the National Natural Science Foundation of China(NSFC) under Grant No. U19A2073.\nA high-quality denoising dataset for smartphone cameras. Abdelrahman Abdelhamed, Stephen Lin, Michael S Brown, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition67Abdelrahman Abdelhamed, Stephen Lin, and Michael S Brown. A high-quality denoising dataset for smartphone cameras. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, pages 1692-1700, 2018. 1, 2, 5, 6, 7\n\nReal image denoising with feature attention. Saeed Anwar, Nick Barnes, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision13Saeed Anwar and Nick Barnes. Real image denoising with feature attention. In Proceedings of the IEEE/CVF inter- national conference on computer vision, pages 3155-3164, 2019. 1, 3\n\nNoise2self: Blind denoising by self-supervision. Joshua Batson, Loic Royer, PMLRInternational Conference on Machine Learning. 26Joshua Batson and Loic Royer. Noise2self: Blind denoising by self-supervision. In International Conference on Machine Learning, pages 524-533. PMLR, 2019. 2, 3, 6\n\nRemoving structured noise with self-supervised blind-spot networks. Coleman Broaddus, Alexander Krull, Martin Weigert, Uwe Schmidt, Gene Myers, 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI). IEEEColeman Broaddus, Alexander Krull, Martin Weigert, Uwe Schmidt, and Gene Myers. Removing structured noise with self-supervised blind-spot networks. In 2020 IEEE 17th In- ternational Symposium on Biomedical Imaging (ISBI), pages 159-163. IEEE, 2020. 3\n\nA non-local algorithm for image denoising. Antoni Buades, Bartomeu Coll, J-M Morel, IEEE computer society conference on computer vision and pattern recognition (CVPR'05). Ieee2Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local algorithm for image denoising. In 2005 IEEE computer so- ciety conference on computer vision and pattern recognition (CVPR'05), volume 2, pages 60-65. Ieee, 2005. 1, 2\n\nFbidenoiser: Fast blind image denoiser for poisson-gaussian noise. Jaeseok Byun, Sungmin Cha, Taesup Moon, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition25Jaeseok Byun, Sungmin Cha, and Taesup Moon. Fbi- denoiser: Fast blind image denoiser for poisson-gaussian noise. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 5768-5777, 2021. 2, 3, 5\n\nFully convolutional pixel adaptive image denoiser. Sungmin Cha, Taesup Moon, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionSungmin Cha and Taesup Moon. Fully convolutional pixel adaptive image denoiser. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4160- 4169, 2019. 3\n\nImage blind denoising with generative adversarial network based noise modeling. Jingwen Chen, Jiawei Chen, Hongyang Chao, Ming Yang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition36Jingwen Chen, Jiawei Chen, Hongyang Chao, and Ming Yang. Image blind denoising with generative adversarial net- work based noise modeling. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 3155-3164, 2018. 3, 6\n\nExploring simple siamese representation learning. Xinlei Chen, Kaiming He, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXinlei Chen and Kaiming He. Exploring simple siamese rep- resentation learning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 15750-15758, 2021. 5\n\nNbnet: Noise basis learning for image denoising with subspace projection. Shen Cheng, Yuzhi Wang, Haibin Huang, Donghao Liu, Haoqiang Fan, Shuaicheng Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition13Shen Cheng, Yuzhi Wang, Haibin Huang, Donghao Liu, Haoqiang Fan, and Shuaicheng Liu. Nbnet: Noise basis learning for image denoising with subspace projection. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 4896-4906, 2021. 1, 3\n\nImage denoising by sparse 3-d transformdomain collaborative filtering. Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, Karen Egiazarian, IEEE Transactions on image processing. 1686Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse 3-d transform- domain collaborative filtering. IEEE Transactions on image processing, 16(8):2080-2095, 2007. 1, 2, 6\n\nPractical poissonian-gaussian noise modeling and fitting for single-image raw-data. Alessandro Foi, Mejdi Trimeche, Vladimir Katkovnik, Karen Egiazarian, IEEE Transactions on Image Processing. 1710Alessandro Foi, Mejdi Trimeche, Vladimir Katkovnik, and Karen Egiazarian. Practical poissonian-gaussian noise mod- eling and fitting for single-image raw-data. IEEE Transac- tions on Image Processing, 17(10):1737-1754, 2008. 2\n\nDigital image processing. Pearson education india. C Rafael, Gonzalez, Rafael C Gonzalez. Digital image processing. Pearson edu- cation india, 2009. 3\n\nWeighted nuclear norm minimization with application to image denoising. Shuhang Gu, Lei Zhang, Wangmeng Zuo, Xiangchu Feng, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition6Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm minimization with applica- tion to image denoising. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 2862-2869, 2014. 1, 2, 6\n\nToward convolutional blind denoising of real photographs. Zifei Shi Guo, Kai Yan, Wangmeng Zhang, Lei Zuo, Zhang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition6Shi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo, and Lei Zhang. Toward convolutional blind denoising of real pho- tographs. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1712-1722, 2019. 1, 2, 6\n\nEnd-to-end unpaired image denoising with conditional adversarial networks. Zhiwei Hong, Xiaocheng Fan, Tao Jiang, Jianxing Feng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence346Zhiwei Hong, Xiaocheng Fan, Tao Jiang, and Jianxing Feng. End-to-end unpaired image denoising with conditional ad- versarial networks. In Proceedings of the AAAI Confer- ence on Artificial Intelligence, volume 34, pages 4140-4149, 2020. 3, 6\n\nNeighbor2neighbor: Self-supervised denoising from single noisy images. Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, Jianzhuang Liu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition25Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, and Jianzhuang Liu. Neighbor2neighbor: Self-supervised de- noising from single noisy images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14781-14790, 2021. 2, 5\n\nC2n: Practical generative noise modeling for real-world denoising. Geonwoon Jang, Wooseok Lee, Sanghyun Son, Kyoung Mu Lee, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision36Geonwoon Jang, Wooseok Lee, Sanghyun Son, and Ky- oung Mu Lee. C2n: Practical generative noise modeling for real-world denoising. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision, pages 2350-2359, 2021. 3, 6\n\nNoise distribution adaptive self-supervised image denoising using tweedie distribution and score matching. Kwanyoung Kim, Taesung Kwon, Jong Chul Ye, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition23Kwanyoung Kim, Taesung Kwon, and Jong Chul Ye. Noise distribution adaptive self-supervised image denoising using tweedie distribution and score matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2008-2016, 2022. 2, 3\n\nNoise2score: tweedie's approach to self-supervised image denoising without clean images. Kwanyoung Kim, Jong Chul Ye, Advances in Neural Information Processing Systems. 343Kwanyoung Kim and Jong Chul Ye. Noise2score: tweedie's approach to self-supervised image denoising without clean images. Advances in Neural Information Processing Sys- tems, 34:864-874, 2021. 2, 3\n\nTransfer learning from synthetic to real-noise denoising with adaptive instance normalization. Yoonsik Kim, Jae Woong Soh, Yong Gu, Nam Ik Park, Cho, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition13Yoonsik Kim, Jae Woong Soh, Gu Yong Park, and Nam Ik Cho. Transfer learning from synthetic to real-noise denois- ing with adaptive instance normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3482-3492, 2020. 1, 3\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5\n\nModeling srgb camera noise with normalizing flows. Shayan Kousha, Ali Maleky, S Michael, Marcus A Brown, Brubaker, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022Shayan Kousha, Ali Maleky, Michael S Brown, and Mar- cus A Brubaker. Modeling srgb camera noise with normal- izing flows. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17463- 17471, 2022. 3\n\nNoise2void-learning denoising from single noisy images. Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition67Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2void-learning denoising from single noisy images. In Proceedings of the IEEE/CVF conference on computer vi- sion and pattern recognition, pages 2129-2137, 2019. 2, 3, 6, 7\n\nProbabilistic noise2void: Unsupervised content-aware denoising. Alexander Krull, Tom\u00e1\u0161 Vi\u010dar, Mangal Prakash, Manan Lalit, Florian Jug, Frontiers in Computer Science. 25Alexander Krull, Tom\u00e1\u0161 Vi\u010dar, Mangal Prakash, Manan Lalit, and Florian Jug. Probabilistic noise2void: Unsuper- vised content-aware denoising. Frontiers in Computer Sci- ence, 2:5, 2020. 3\n\nHigh-quality self-supervised deep image denoising. Samuli Laine, Tero Karras, Jaakko Lehtinen, Timo Aila, Advances in Neural Information Processing Systems. 324Samuli Laine, Tero Karras, Jaakko Lehtinen, and Timo Aila. High-quality self-supervised deep image denoising. Ad- vances in Neural Information Processing Systems, 32, 2019. 2, 3, 4\n\nApbsn: Self-supervised denoising for real-world images via asymmetric pd and blind-spot network. Wooseok Lee, Sanghyun Son, Kyoung Mu Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition7Wooseok Lee, Sanghyun Son, and Kyoung Mu Lee. Ap- bsn: Self-supervised denoising for real-world images via asymmetric pd and blind-spot network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17725-17734, 2022. 1, 2, 3, 4, 6, 7, 8\n\nNoise2noise: Learning image restoration without clean data. Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, Timo Aila, International Conference on Machine Learning. 23Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila. Noise2noise: Learning image restoration without clean data. In International Conference on Machine Learning, pages 4620-4631, 2018. 2, 3\n\nSwinir: Image restoration using swin transformer. Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, Radu Timofte, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision13Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration us- ing swin transformer. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision, pages 1833-1844, 2021. 1, 3\n\nNon-local recurrent network for image restoration. Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, Thomas S Huang, Advances in neural information processing systems. 31Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, and Thomas S Huang. Non-local recurrent network for image restoration. Advances in neural information processing sys- tems, 31, 2018. 1\n\nMulti-level wavelet-cnn for image restoration. Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, Wangmeng Zuo, Proceedings of the IEEE conference on computer vision and pattern recognition workshops. the IEEE conference on computer vision and pattern recognition workshops1Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, and Wangmeng Zuo. Multi-level wavelet-cnn for image restora- tion. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 773-782, 2018. 1, 2\n\nSgdr: Stochastic gradient descent with warm restarts. Ilya Loshchilov, Frank Hutter, arXiv:1608.03983arXiv preprintIlya Loshchilov and Frank Hutter. Sgdr: Stochas- tic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. 5\n\nImage restoration using very deep convolutional encoder-decoder networks with symmetric skip connections. Xiaojiao Mao, Chunhua Shen, Yu-Bin Yang, Advances in neural information processing systems. 29Xiaojiao Mao, Chunhua Shen, and Yu-Bin Yang. Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections. Advances in neu- ral information processing systems, 29, 2016. 1, 2\n\nNoisier2noise: Learning to denoise from unpaired noisy data. Nick Moran, Dan Schmidt, Yu Zhong, Patrick Coady, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition23Nick Moran, Dan Schmidt, Yu Zhong, and Patrick Coady. Noisier2noise: Learning to denoise from unpaired noisy data. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 12064-12072, 2020. 2, 3\n\nCvf-sid: Cyclic multi-variate function for self-supervised image denoising by disentangling noise from image. Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, Kyoung Mu Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition67Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, and Kyoung Mu Lee. Cvf-sid: Cyclic multi-variate function for self-supervised image denoising by disentangling noise from image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17583- 17591, 2022. 1, 2, 3, 6, 7\n\nRecorrupted-to-recorrupted: unsupervised deep learning for image denoising. Tongyao Pang, Huan Zheng, Yuhui Quan, Hui Ji, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition26Tongyao Pang, Huan Zheng, Yuhui Quan, and Hui Ji. Recorrupted-to-recorrupted: unsupervised deep learning for image denoising. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, pages 2043-2052, 2021. 2, 3, 6\n\nBenchmarking denoising algorithms with real photographs. Tobias Plotz, Stefan Roth, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition67Tobias Plotz and Stefan Roth. Benchmarking denoising algorithms with real photographs. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 1586-1595, 2017. 1, 2, 5, 6, 7\n\nAdaptive consistency prior based deep network for image denoising. Xiaohai Chao Ren, Chuncheng He, Zhibo Wang, Zhao, proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition13Chao Ren, Xiaohai He, Chuncheng Wang, and Zhibo Zhao. Adaptive consistency prior based deep network for image denoising. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8596-8606, 2021. 1, 3\n\nUnet: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical image computing and computer-assisted intervention. Springer67Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmen- tation. In International Conference on Medical image com- puting and computer-assisted intervention, pages 234-241. Springer, 2015. 2, 5, 6, 7\n\nMemnet: A persistent memory network for image restoration. Ying Tai, Jian Yang, Xiaoming Liu, Chunyan Xu, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision1Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Mem- net: A persistent memory network for image restoration. In Proceedings of the IEEE international conference on com- puter vision, pages 4539-4547, 2017. 1, 2\n\nUformer: A general u-shaped transformer for image restoration. Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, Houqiang Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition13Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: A general u-shaped transformer for image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 17683-17693, 2022. 1, 3\n\nBlind2unblind: Self-supervised image denoising with visible blind spots. Zejin Wang, Jiazheng Liu, Guoqing Li, Hua Han, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition25Zejin Wang, Jiazheng Liu, Guoqing Li, and Hua Han. Blind2unblind: Self-supervised image denoising with visi- ble blind spots. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2027- 2036, 2022. 2, 3, 5\n\nA physics-based noise formation model for extreme low-light raw denoising. Kaixuan Wei, Ying Fu, Jiaolong Yang, Hua Huang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition13Kaixuan Wei, Ying Fu, Jiaolong Yang, and Hua Huang. A physics-based noise formation model for extreme low-light raw denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2758- 2767, 2020. 1, 3\n\nUnpaired learning of deep image denoising. Xiaohe Wu, Ming Liu, Yue Cao, Dongwei Ren, Wangmeng Zuo, European conference on computer vision. Springer26Xiaohe Wu, Ming Liu, Yue Cao, Dongwei Ren, and Wang- meng Zuo. Unpaired learning of deep image denoising. In European conference on computer vision, pages 352-368. Springer, 2020. 2, 3, 6\n\nNoisy-as-clean: Learning selfsupervised denoising from corrupted image. Jun Xu, Yuan Huang, Ming-Ming Cheng, Li Liu, Fan Zhu, Zhou Xu, Ling Shao, IEEE Transactions on Image Processing. 297Jun Xu, Yuan Huang, Ming-Ming Cheng, Li Liu, Fan Zhu, Zhou Xu, and Ling Shao. Noisy-as-clean: Learning self- supervised denoising from corrupted image. IEEE Transac- tions on Image Processing, 29:9316-9329, 2020. 2, 3, 6, 7\n\nVariational denoising network: Toward blind noise modeling and removal. Zongsheng Yue, Hongwei Yong, Qian Zhao, Deyu Meng, Lei Zhang, Advances in neural information processing systems. 326Zongsheng Yue, Hongwei Yong, Qian Zhao, Deyu Meng, and Lei Zhang. Variational denoising network: Toward blind noise modeling and removal. Advances in neural information processing systems, 32, 2019. 1, 3, 6\n\nDual adversarial network: Toward real-world noise removal and noise generation. Zongsheng Yue, Qian Zhao, Lei Zhang, Deyu Meng, European Conference on Computer Vision. Springer13Zongsheng Yue, Qian Zhao, Lei Zhang, and Deyu Meng. Dual adversarial network: Toward real-world noise removal and noise generation. In European Conference on Computer Vision, pages 41-58. Springer, 2020. 1, 3\n\nRestormer: Efficient transformer for high-resolution image restoration. Aditya Syed Waqas Zamir, Salman Arora, Munawar Khan, Hayat, Ming-Hsuan Fahad Shahbaz Khan, Yang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition16Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu- nawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5728- 5739, 2022. 1, 3, 6\n\nCycleisp: Real image restoration via improved data synthesis. Aditya Syed Waqas Zamir, Salman Arora, Munawar Khan, Hayat, Ming-Hsuan Fahad Shahbaz Khan, Ling Yang, Shao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSyed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Cycleisp: Real image restoration via improved data synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2696- 2705, 2020. 2\n\nBeyond a gaussian denoiser: Residual learning of deep cnn for image denoising. Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang, IEEE transactions on image processing. 2676Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE transactions on image processing, 26(7):3142-3155, 2017. 1, 2, 6\n\nFfdnet: Toward a fast and flexible solution for cnn-based image denoising. Kai Zhang, Wangmeng Zuo, Lei Zhang, IEEE Transactions on Image Processing. 279Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward a fast and flexible solution for cnn-based image denoising. IEEE Transactions on Image Processing, 27(9):4608-4622, 2018. 1, 2\n\nThe unreasonable effectiveness of deep features as a perceptual metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht- man, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 586-595, 2018. 7\n\nImage super-resolution using very deep residual channel attention networks. Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In Proceedings of the European conference on computer vision (ECCV), pages 286-301, 2018. 5\n\nRethinking noise synthesis and modeling in raw denoising. Yi Zhang, Hongwei Qin, Xiaogang Wang, Hongsheng Li, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision13Yi Zhang, Hongwei Qin, Xiaogang Wang, and Hongsheng Li. Rethinking noise synthesis and modeling in raw denois- ing. In Proceedings of the IEEE/CVF International Confer- ence on Computer Vision, pages 4593-4601, 2021. 1, 3\n\nSelf-supervised image restoration with blurry and noisy pairs. Zhilu Zhang, Rongjian Xu, Ming Liu, Zifei Yan, Wangmeng Zuo, Advances in Neural Information Processing Systems. 2022Zhilu Zhang, RongJian Xu, Ming Liu, Zifei Yan, and Wang- meng Zuo. Self-supervised image restoration with blurry and noisy pairs. In Advances in Neural Information Processing Systems, 2022. 3\n\nTexture variation adaptive image denoising with nonlocal pca. Wenzhao Zhao, Qiegen Liu, Yisong Lv, Binjie Qin, IEEE Transactions on Image Processing. 2811Wenzhao Zhao, Qiegen Liu, Yisong Lv, and Binjie Qin. Tex- ture variation adaptive image denoising with nonlocal pca. IEEE Transactions on Image Processing, 28(11):5537-5551, 2019. 3\n\nWhen awgn-based denoiser meets real noises. Yuqian Zhou, Jianbo Jiao, Haibin Huang, Yang Wang, Jue Wang, Honghui Shi, Thomas Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence346Yuqian Zhou, Jianbo Jiao, Haibin Huang, Yang Wang, Jue Wang, Honghui Shi, and Thomas Huang. When awgn-based denoiser meets real noises. In Proceedings of the AAAI Con- ference on Artificial Intelligence, volume 34, pages 13074- 13081, 2020. 2, 3, 6\n", "annotations": {"author": "[{\"end\":167,\"start\":78},{\"end\":282,\"start\":168},{\"end\":398,\"start\":283},{\"end\":411,\"start\":399},{\"end\":425,\"start\":412},{\"end\":434,\"start\":426},{\"end\":574,\"start\":435}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":84},{\"end\":179,\"start\":174},{\"end\":293,\"start\":290},{\"end\":410,\"start\":406},{\"end\":424,\"start\":420},{\"end\":433,\"start\":430},{\"end\":447,\"start\":444}]", "author_first_name": "[{\"end\":83,\"start\":78},{\"end\":173,\"start\":168},{\"end\":289,\"start\":283},{\"end\":405,\"start\":399},{\"end\":419,\"start\":412},{\"end\":429,\"start\":426},{\"end\":443,\"start\":435}]", "author_affiliation": "[{\"end\":166,\"start\":88},{\"end\":281,\"start\":203},{\"end\":397,\"start\":319},{\"end\":544,\"start\":466},{\"end\":573,\"start\":546}]", "title": "[{\"end\":75,\"start\":1},{\"end\":649,\"start\":575}]", "venue": null, "abstract": "[{\"end\":3678,\"start\":651}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3766,\"start\":3763},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3769,\"start\":3766},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3772,\"start\":3769},{\"end\":3901,\"start\":3852},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4068,\"start\":4064},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4088,\"start\":4084},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4191,\"start\":4187},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4476,\"start\":4473},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4479,\"start\":4476},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4516,\"start\":4513},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4519,\"start\":4516},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4522,\"start\":4519},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4525,\"start\":4522},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":4528,\"start\":4525},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4802,\"start\":4798},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":4805,\"start\":4802},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5396,\"start\":5392},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5643,\"start\":5639},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6814,\"start\":6811},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6827,\"start\":6823},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7870,\"start\":7866},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7873,\"start\":7870},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7885,\"start\":7881},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7933,\"start\":7930},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7936,\"start\":7933},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7939,\"start\":7936},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7974,\"start\":7970},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8116,\"start\":8112},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8130,\"start\":8126},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8146,\"start\":8142},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8514,\"start\":8510},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8517,\"start\":8514},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8543,\"start\":8539},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8691,\"start\":8687},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8722,\"start\":8718},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8945,\"start\":8942},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8948,\"start\":8945},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9064,\"start\":9061},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9067,\"start\":9064},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9070,\"start\":9067},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9073,\"start\":9070},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9076,\"start\":9073},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9079,\"start\":9076},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9083,\"start\":9079},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9087,\"start\":9083},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9091,\"start\":9087},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9305,\"start\":9301},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9308,\"start\":9305},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9427,\"start\":9424},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9430,\"start\":9427},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9433,\"start\":9430},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9436,\"start\":9433},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9550,\"start\":9547},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9553,\"start\":9550},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9556,\"start\":9553},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9883,\"start\":9879},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10185,\"start\":10181},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10457,\"start\":10453},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10574,\"start\":10570},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10593,\"start\":10590},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10862,\"start\":10859},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10865,\"start\":10862},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10904,\"start\":10901},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10907,\"start\":10904},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10947,\"start\":10943},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10950,\"start\":10947},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10982,\"start\":10978},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11086,\"start\":11082},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11099,\"start\":11095},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11237,\"start\":11233},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11260,\"start\":11256},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":11374,\"start\":11370},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11655,\"start\":11651},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11886,\"start\":11883},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11933,\"start\":11929},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11983,\"start\":11979},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12125,\"start\":12121},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":13069,\"start\":13065},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13457,\"start\":13453},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13460,\"start\":13457},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":13463,\"start\":13460},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13498,\"start\":13495},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13501,\"start\":13498},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13504,\"start\":13501},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13670,\"start\":13666},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14892,\"start\":14888},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15271,\"start\":15267},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18317,\"start\":18314},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":18753,\"start\":18749},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20047,\"start\":20043},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20328,\"start\":20325},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20464,\"start\":20461},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20477,\"start\":20473},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20835,\"start\":20831},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21496,\"start\":21493},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21499,\"start\":21496},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21502,\"start\":21499},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21604,\"start\":21601},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21998,\"start\":21994},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22161,\"start\":22157},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22236,\"start\":22233},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22249,\"start\":22245},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22620,\"start\":22617},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22633,\"start\":22629},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22807,\"start\":22803},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23003,\"start\":22999},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23022,\"start\":23019},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":23124,\"start\":23120},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23137,\"start\":23133},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":23253,\"start\":23249},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":23266,\"start\":23262},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":23283,\"start\":23279},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23303,\"start\":23299},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":23350,\"start\":23346},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23363,\"start\":23359},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23491,\"start\":23487},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24206,\"start\":24202},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24291,\"start\":24287},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24424,\"start\":24420},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24607,\"start\":24603},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24810,\"start\":24807},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":27791,\"start\":27787},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":29389,\"start\":29386},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":29626,\"start\":29622},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31609,\"start\":31606}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29561,\"start\":29325},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29691,\"start\":29562},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29826,\"start\":29692},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30129,\"start\":29827},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30532,\"start\":30130},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30628,\"start\":30533},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31512,\"start\":30629},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31905,\"start\":31513}]", "paragraph": "[{\"end\":4876,\"start\":3694},{\"end\":6541,\"start\":4878},{\"end\":7135,\"start\":6543},{\"end\":7327,\"start\":7137},{\"end\":7555,\"start\":7329},{\"end\":7717,\"start\":7557},{\"end\":8332,\"start\":7757},{\"end\":8949,\"start\":8334},{\"end\":9378,\"start\":8951},{\"end\":10275,\"start\":9407},{\"end\":11545,\"start\":10311},{\"end\":12301,\"start\":11547},{\"end\":12893,\"start\":12312},{\"end\":13304,\"start\":12924},{\"end\":13853,\"start\":13306},{\"end\":14560,\"start\":13855},{\"end\":15476,\"start\":14656},{\"end\":15946,\"start\":15510},{\"end\":16415,\"start\":15948},{\"end\":17114,\"start\":16486},{\"end\":18173,\"start\":17216},{\"end\":18421,\"start\":18216},{\"end\":18811,\"start\":18423},{\"end\":19972,\"start\":18851},{\"end\":20145,\"start\":19974},{\"end\":20380,\"start\":20206},{\"end\":21518,\"start\":20420},{\"end\":22511,\"start\":21520},{\"end\":24723,\"start\":22548},{\"end\":26158,\"start\":24742},{\"end\":27082,\"start\":26233},{\"end\":28534,\"start\":27144},{\"end\":29324,\"start\":28572}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14630,\"start\":14561},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14655,\"start\":14630},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16485,\"start\":16416},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17215,\"start\":17115},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18215,\"start\":18174},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20205,\"start\":20146},{\"attributes\":{\"id\":\"formula_6\"},\"end\":27117,\"start\":27083}]", "table_ref": "[{\"end\":22170,\"start\":22163},{\"end\":22665,\"start\":22658},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24874,\"start\":24867},{\"end\":26648,\"start\":26641}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":3692,\"start\":3680},{\"attributes\":{\"n\":\"2.\"},\"end\":7732,\"start\":7720},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7755,\"start\":7735},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9405,\"start\":9381},{\"attributes\":{\"n\":\"2.3.\"},\"end\":10309,\"start\":10278},{\"attributes\":{\"n\":\"3.\"},\"end\":12310,\"start\":12304},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12922,\"start\":12896},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15508,\"start\":15479},{\"attributes\":{\"n\":\"3.3.\"},\"end\":18849,\"start\":18814},{\"attributes\":{\"n\":\"4.\"},\"end\":20394,\"start\":20383},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20418,\"start\":20397},{\"attributes\":{\"n\":\"4.2.\"},\"end\":22546,\"start\":22514},{\"attributes\":{\"n\":\"5.\"},\"end\":24740,\"start\":24726},{\"attributes\":{\"n\":\"5.1.\"},\"end\":26194,\"start\":26161},{\"attributes\":{\"n\":\"5.2.\"},\"end\":26231,\"start\":26197},{\"attributes\":{\"n\":\"5.3.\"},\"end\":27142,\"start\":27119},{\"attributes\":{\"n\":\"5.4.\"},\"end\":28557,\"start\":28537},{\"attributes\":{\"n\":\"6.\"},\"end\":28570,\"start\":28560},{\"end\":29336,\"start\":29326},{\"end\":29573,\"start\":29563},{\"end\":29703,\"start\":29693},{\"end\":29838,\"start\":29828},{\"end\":30141,\"start\":30131},{\"end\":30544,\"start\":30534},{\"end\":31523,\"start\":31514}]", "table": "[{\"end\":31512,\"start\":31272},{\"end\":31905,\"start\":31610}]", "figure_caption": "[{\"end\":29561,\"start\":29338},{\"end\":29691,\"start\":29575},{\"end\":29826,\"start\":29705},{\"end\":30129,\"start\":29840},{\"end\":30532,\"start\":30143},{\"end\":30628,\"start\":30546},{\"end\":31272,\"start\":30631},{\"end\":31610,\"start\":31525}]", "figure_ref": "[{\"end\":4106,\"start\":4098},{\"end\":14367,\"start\":14359},{\"end\":20098,\"start\":20087},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23859,\"start\":23851},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23872,\"start\":23864},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24040,\"start\":24032},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24628,\"start\":24620},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26872,\"start\":26864},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27273,\"start\":27265},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27991,\"start\":27983}]", "bib_author_first_name": "[{\"end\":32108,\"start\":32097},{\"end\":32128,\"start\":32121},{\"end\":32143,\"start\":32134},{\"end\":32581,\"start\":32576},{\"end\":32593,\"start\":32589},{\"end\":32969,\"start\":32963},{\"end\":32982,\"start\":32978},{\"end\":33281,\"start\":33274},{\"end\":33301,\"start\":33292},{\"end\":33315,\"start\":33309},{\"end\":33328,\"start\":33325},{\"end\":33342,\"start\":33338},{\"end\":33724,\"start\":33718},{\"end\":33741,\"start\":33733},{\"end\":33751,\"start\":33748},{\"end\":34151,\"start\":34144},{\"end\":34165,\"start\":34158},{\"end\":34177,\"start\":34171},{\"end\":34626,\"start\":34619},{\"end\":34638,\"start\":34632},{\"end\":35044,\"start\":35037},{\"end\":35057,\"start\":35051},{\"end\":35072,\"start\":35064},{\"end\":35083,\"start\":35079},{\"end\":35541,\"start\":35535},{\"end\":35555,\"start\":35548},{\"end\":35983,\"start\":35979},{\"end\":35996,\"start\":35991},{\"end\":36009,\"start\":36003},{\"end\":36024,\"start\":36017},{\"end\":36038,\"start\":36030},{\"end\":36054,\"start\":36044},{\"end\":36566,\"start\":36558},{\"end\":36584,\"start\":36574},{\"end\":36598,\"start\":36590},{\"end\":36615,\"start\":36610},{\"end\":36983,\"start\":36973},{\"end\":36994,\"start\":36989},{\"end\":37013,\"start\":37005},{\"end\":37030,\"start\":37025},{\"end\":37366,\"start\":37365},{\"end\":37545,\"start\":37538},{\"end\":37553,\"start\":37550},{\"end\":37569,\"start\":37561},{\"end\":37583,\"start\":37575},{\"end\":38041,\"start\":38036},{\"end\":38054,\"start\":38051},{\"end\":38068,\"start\":38060},{\"end\":38079,\"start\":38076},{\"end\":38561,\"start\":38555},{\"end\":38577,\"start\":38568},{\"end\":38586,\"start\":38583},{\"end\":38602,\"start\":38594},{\"end\":39038,\"start\":39035},{\"end\":39055,\"start\":39046},{\"end\":39062,\"start\":39060},{\"end\":39075,\"start\":39068},{\"end\":39090,\"start\":39080},{\"end\":39577,\"start\":39569},{\"end\":39591,\"start\":39584},{\"end\":39605,\"start\":39597},{\"end\":39620,\"start\":39611},{\"end\":40110,\"start\":40101},{\"end\":40123,\"start\":40116},{\"end\":40139,\"start\":40130},{\"end\":40662,\"start\":40653},{\"end\":40672,\"start\":40668},{\"end\":41036,\"start\":41029},{\"end\":41045,\"start\":41042},{\"end\":41051,\"start\":41046},{\"end\":41061,\"start\":41057},{\"end\":41069,\"start\":41066},{\"end\":41072,\"start\":41070},{\"end\":41550,\"start\":41549},{\"end\":41566,\"start\":41561},{\"end\":41782,\"start\":41776},{\"end\":41794,\"start\":41791},{\"end\":41804,\"start\":41803},{\"end\":41822,\"start\":41814},{\"end\":42295,\"start\":42286},{\"end\":42313,\"start\":42303},{\"end\":42331,\"start\":42324},{\"end\":42795,\"start\":42786},{\"end\":42808,\"start\":42803},{\"end\":42822,\"start\":42816},{\"end\":42837,\"start\":42832},{\"end\":42852,\"start\":42845},{\"end\":43137,\"start\":43131},{\"end\":43149,\"start\":43145},{\"end\":43164,\"start\":43158},{\"end\":43179,\"start\":43175},{\"end\":43526,\"start\":43519},{\"end\":43540,\"start\":43532},{\"end\":43555,\"start\":43546},{\"end\":44054,\"start\":44048},{\"end\":44070,\"start\":44065},{\"end\":44084,\"start\":44081},{\"end\":44103,\"start\":44097},{\"end\":44115,\"start\":44111},{\"end\":44129,\"start\":44124},{\"end\":44143,\"start\":44139},{\"end\":44499,\"start\":44492},{\"end\":44515,\"start\":44507},{\"end\":44527,\"start\":44521},{\"end\":44536,\"start\":44533},{\"end\":44547,\"start\":44544},{\"end\":44562,\"start\":44558},{\"end\":45001,\"start\":44997},{\"end\":45012,\"start\":45007},{\"end\":45024,\"start\":45018},{\"end\":45034,\"start\":45030},{\"end\":45041,\"start\":45035},{\"end\":45055,\"start\":45047},{\"end\":45356,\"start\":45350},{\"end\":45369,\"start\":45362},{\"end\":45380,\"start\":45377},{\"end\":45393,\"start\":45388},{\"end\":45407,\"start\":45399},{\"end\":45868,\"start\":45864},{\"end\":45886,\"start\":45881},{\"end\":46171,\"start\":46163},{\"end\":46184,\"start\":46177},{\"end\":46197,\"start\":46191},{\"end\":46542,\"start\":46538},{\"end\":46553,\"start\":46550},{\"end\":46565,\"start\":46563},{\"end\":46580,\"start\":46573},{\"end\":47091,\"start\":47083},{\"end\":47110,\"start\":47104},{\"end\":47131,\"start\":47123},{\"end\":47146,\"start\":47137},{\"end\":47696,\"start\":47689},{\"end\":47707,\"start\":47703},{\"end\":47720,\"start\":47715},{\"end\":47730,\"start\":47727},{\"end\":48195,\"start\":48189},{\"end\":48209,\"start\":48203},{\"end\":48642,\"start\":48635},{\"end\":48662,\"start\":48653},{\"end\":48672,\"start\":48667},{\"end\":49140,\"start\":49136},{\"end\":49161,\"start\":49154},{\"end\":49177,\"start\":49171},{\"end\":49601,\"start\":49597},{\"end\":49611,\"start\":49607},{\"end\":49626,\"start\":49618},{\"end\":49639,\"start\":49632},{\"end\":50052,\"start\":50044},{\"end\":50067,\"start\":50059},{\"end\":50080,\"start\":50073},{\"end\":50093,\"start\":50086},{\"end\":50110,\"start\":50100},{\"end\":50124,\"start\":50116},{\"end\":50629,\"start\":50624},{\"end\":50644,\"start\":50636},{\"end\":50657,\"start\":50650},{\"end\":50665,\"start\":50662},{\"end\":51149,\"start\":51142},{\"end\":51159,\"start\":51155},{\"end\":51172,\"start\":51164},{\"end\":51182,\"start\":51179},{\"end\":51633,\"start\":51627},{\"end\":51642,\"start\":51638},{\"end\":51651,\"start\":51648},{\"end\":51664,\"start\":51657},{\"end\":51678,\"start\":51670},{\"end\":51998,\"start\":51995},{\"end\":52007,\"start\":52003},{\"end\":52024,\"start\":52015},{\"end\":52034,\"start\":52032},{\"end\":52043,\"start\":52040},{\"end\":52053,\"start\":52049},{\"end\":52062,\"start\":52058},{\"end\":52417,\"start\":52408},{\"end\":52430,\"start\":52423},{\"end\":52441,\"start\":52437},{\"end\":52452,\"start\":52448},{\"end\":52462,\"start\":52459},{\"end\":52821,\"start\":52812},{\"end\":52831,\"start\":52827},{\"end\":52841,\"start\":52838},{\"end\":52853,\"start\":52849},{\"end\":53198,\"start\":53192},{\"end\":53223,\"start\":53217},{\"end\":53238,\"start\":53231},{\"end\":53262,\"start\":53252},{\"end\":53802,\"start\":53796},{\"end\":53827,\"start\":53821},{\"end\":53842,\"start\":53835},{\"end\":53866,\"start\":53856},{\"end\":53891,\"start\":53887},{\"end\":54422,\"start\":54419},{\"end\":54438,\"start\":54430},{\"end\":54450,\"start\":54444},{\"end\":54461,\"start\":54457},{\"end\":54471,\"start\":54468},{\"end\":54814,\"start\":54811},{\"end\":54830,\"start\":54822},{\"end\":54839,\"start\":54836},{\"end\":55151,\"start\":55144},{\"end\":55166,\"start\":55159},{\"end\":55180,\"start\":55174},{\"end\":55182,\"start\":55181},{\"end\":55193,\"start\":55190},{\"end\":55211,\"start\":55205},{\"end\":55700,\"start\":55695},{\"end\":55715,\"start\":55708},{\"end\":55723,\"start\":55720},{\"end\":55734,\"start\":55728},{\"end\":55747,\"start\":55741},{\"end\":55758,\"start\":55755},{\"end\":56179,\"start\":56177},{\"end\":56194,\"start\":56187},{\"end\":56208,\"start\":56200},{\"end\":56224,\"start\":56215},{\"end\":56651,\"start\":56646},{\"end\":56667,\"start\":56659},{\"end\":56676,\"start\":56672},{\"end\":56687,\"start\":56682},{\"end\":56701,\"start\":56693},{\"end\":57024,\"start\":57017},{\"end\":57037,\"start\":57031},{\"end\":57049,\"start\":57043},{\"end\":57060,\"start\":57054},{\"end\":57342,\"start\":57336},{\"end\":57355,\"start\":57349},{\"end\":57368,\"start\":57362},{\"end\":57380,\"start\":57376},{\"end\":57390,\"start\":57387},{\"end\":57404,\"start\":57397},{\"end\":57416,\"start\":57410}]", "bib_author_last_name": "[{\"end\":32119,\"start\":32109},{\"end\":32132,\"start\":32129},{\"end\":32149,\"start\":32144},{\"end\":32587,\"start\":32582},{\"end\":32600,\"start\":32594},{\"end\":32976,\"start\":32970},{\"end\":32988,\"start\":32983},{\"end\":33290,\"start\":33282},{\"end\":33307,\"start\":33302},{\"end\":33323,\"start\":33316},{\"end\":33336,\"start\":33329},{\"end\":33348,\"start\":33343},{\"end\":33731,\"start\":33725},{\"end\":33746,\"start\":33742},{\"end\":33757,\"start\":33752},{\"end\":34156,\"start\":34152},{\"end\":34169,\"start\":34166},{\"end\":34182,\"start\":34178},{\"end\":34630,\"start\":34627},{\"end\":34643,\"start\":34639},{\"end\":35049,\"start\":35045},{\"end\":35062,\"start\":35058},{\"end\":35077,\"start\":35073},{\"end\":35088,\"start\":35084},{\"end\":35546,\"start\":35542},{\"end\":35558,\"start\":35556},{\"end\":35989,\"start\":35984},{\"end\":36001,\"start\":35997},{\"end\":36015,\"start\":36010},{\"end\":36028,\"start\":36025},{\"end\":36042,\"start\":36039},{\"end\":36058,\"start\":36055},{\"end\":36572,\"start\":36567},{\"end\":36588,\"start\":36585},{\"end\":36608,\"start\":36599},{\"end\":36626,\"start\":36616},{\"end\":36987,\"start\":36984},{\"end\":37003,\"start\":36995},{\"end\":37023,\"start\":37014},{\"end\":37041,\"start\":37031},{\"end\":37373,\"start\":37367},{\"end\":37383,\"start\":37375},{\"end\":37548,\"start\":37546},{\"end\":37559,\"start\":37554},{\"end\":37573,\"start\":37570},{\"end\":37588,\"start\":37584},{\"end\":38049,\"start\":38042},{\"end\":38058,\"start\":38055},{\"end\":38074,\"start\":38069},{\"end\":38083,\"start\":38080},{\"end\":38090,\"start\":38085},{\"end\":38566,\"start\":38562},{\"end\":38581,\"start\":38578},{\"end\":38592,\"start\":38587},{\"end\":38607,\"start\":38603},{\"end\":39044,\"start\":39039},{\"end\":39058,\"start\":39056},{\"end\":39066,\"start\":39063},{\"end\":39078,\"start\":39076},{\"end\":39094,\"start\":39091},{\"end\":39582,\"start\":39578},{\"end\":39595,\"start\":39592},{\"end\":39609,\"start\":39606},{\"end\":39624,\"start\":39621},{\"end\":40114,\"start\":40111},{\"end\":40128,\"start\":40124},{\"end\":40142,\"start\":40140},{\"end\":40666,\"start\":40663},{\"end\":40680,\"start\":40673},{\"end\":41040,\"start\":41037},{\"end\":41055,\"start\":41052},{\"end\":41064,\"start\":41062},{\"end\":41077,\"start\":41073},{\"end\":41082,\"start\":41079},{\"end\":41559,\"start\":41551},{\"end\":41573,\"start\":41567},{\"end\":41577,\"start\":41575},{\"end\":41789,\"start\":41783},{\"end\":41801,\"start\":41795},{\"end\":41812,\"start\":41805},{\"end\":41828,\"start\":41823},{\"end\":41838,\"start\":41830},{\"end\":42301,\"start\":42296},{\"end\":42322,\"start\":42314},{\"end\":42335,\"start\":42332},{\"end\":42801,\"start\":42796},{\"end\":42814,\"start\":42809},{\"end\":42830,\"start\":42823},{\"end\":42843,\"start\":42838},{\"end\":42856,\"start\":42853},{\"end\":43143,\"start\":43138},{\"end\":43156,\"start\":43150},{\"end\":43173,\"start\":43165},{\"end\":43184,\"start\":43180},{\"end\":43530,\"start\":43527},{\"end\":43544,\"start\":43541},{\"end\":43559,\"start\":43556},{\"end\":44063,\"start\":44055},{\"end\":44079,\"start\":44071},{\"end\":44095,\"start\":44085},{\"end\":44109,\"start\":44104},{\"end\":44122,\"start\":44116},{\"end\":44137,\"start\":44130},{\"end\":44148,\"start\":44144},{\"end\":44505,\"start\":44500},{\"end\":44519,\"start\":44516},{\"end\":44531,\"start\":44528},{\"end\":44542,\"start\":44537},{\"end\":44556,\"start\":44548},{\"end\":44570,\"start\":44563},{\"end\":45005,\"start\":45002},{\"end\":45016,\"start\":45013},{\"end\":45028,\"start\":45025},{\"end\":45045,\"start\":45042},{\"end\":45061,\"start\":45056},{\"end\":45360,\"start\":45357},{\"end\":45375,\"start\":45370},{\"end\":45386,\"start\":45381},{\"end\":45397,\"start\":45394},{\"end\":45411,\"start\":45408},{\"end\":45879,\"start\":45869},{\"end\":45893,\"start\":45887},{\"end\":46175,\"start\":46172},{\"end\":46189,\"start\":46185},{\"end\":46202,\"start\":46198},{\"end\":46548,\"start\":46543},{\"end\":46561,\"start\":46554},{\"end\":46571,\"start\":46566},{\"end\":46586,\"start\":46581},{\"end\":47102,\"start\":47092},{\"end\":47121,\"start\":47111},{\"end\":47135,\"start\":47132},{\"end\":47150,\"start\":47147},{\"end\":47701,\"start\":47697},{\"end\":47713,\"start\":47708},{\"end\":47725,\"start\":47721},{\"end\":47733,\"start\":47731},{\"end\":48201,\"start\":48196},{\"end\":48214,\"start\":48210},{\"end\":48651,\"start\":48643},{\"end\":48665,\"start\":48663},{\"end\":48677,\"start\":48673},{\"end\":48683,\"start\":48679},{\"end\":49152,\"start\":49141},{\"end\":49169,\"start\":49162},{\"end\":49182,\"start\":49178},{\"end\":49605,\"start\":49602},{\"end\":49616,\"start\":49612},{\"end\":49630,\"start\":49627},{\"end\":49642,\"start\":49640},{\"end\":50057,\"start\":50053},{\"end\":50071,\"start\":50068},{\"end\":50084,\"start\":50081},{\"end\":50098,\"start\":50094},{\"end\":50114,\"start\":50111},{\"end\":50127,\"start\":50125},{\"end\":50634,\"start\":50630},{\"end\":50648,\"start\":50645},{\"end\":50660,\"start\":50658},{\"end\":50669,\"start\":50666},{\"end\":51153,\"start\":51150},{\"end\":51162,\"start\":51160},{\"end\":51177,\"start\":51173},{\"end\":51188,\"start\":51183},{\"end\":51636,\"start\":51634},{\"end\":51646,\"start\":51643},{\"end\":51655,\"start\":51652},{\"end\":51668,\"start\":51665},{\"end\":51682,\"start\":51679},{\"end\":52001,\"start\":51999},{\"end\":52013,\"start\":52008},{\"end\":52030,\"start\":52025},{\"end\":52038,\"start\":52035},{\"end\":52047,\"start\":52044},{\"end\":52056,\"start\":52054},{\"end\":52067,\"start\":52063},{\"end\":52421,\"start\":52418},{\"end\":52435,\"start\":52431},{\"end\":52446,\"start\":52442},{\"end\":52457,\"start\":52453},{\"end\":52468,\"start\":52463},{\"end\":52825,\"start\":52822},{\"end\":52836,\"start\":52832},{\"end\":52847,\"start\":52842},{\"end\":52858,\"start\":52854},{\"end\":53215,\"start\":53199},{\"end\":53229,\"start\":53224},{\"end\":53243,\"start\":53239},{\"end\":53250,\"start\":53245},{\"end\":53281,\"start\":53263},{\"end\":53287,\"start\":53283},{\"end\":53819,\"start\":53803},{\"end\":53833,\"start\":53828},{\"end\":53847,\"start\":53843},{\"end\":53854,\"start\":53849},{\"end\":53885,\"start\":53867},{\"end\":53896,\"start\":53892},{\"end\":53902,\"start\":53898},{\"end\":54428,\"start\":54423},{\"end\":54442,\"start\":54439},{\"end\":54455,\"start\":54451},{\"end\":54466,\"start\":54462},{\"end\":54477,\"start\":54472},{\"end\":54820,\"start\":54815},{\"end\":54834,\"start\":54831},{\"end\":54845,\"start\":54840},{\"end\":55157,\"start\":55152},{\"end\":55172,\"start\":55167},{\"end\":55188,\"start\":55183},{\"end\":55203,\"start\":55194},{\"end\":55216,\"start\":55212},{\"end\":55706,\"start\":55701},{\"end\":55718,\"start\":55716},{\"end\":55726,\"start\":55724},{\"end\":55739,\"start\":55735},{\"end\":55753,\"start\":55748},{\"end\":55761,\"start\":55759},{\"end\":56185,\"start\":56180},{\"end\":56198,\"start\":56195},{\"end\":56213,\"start\":56209},{\"end\":56227,\"start\":56225},{\"end\":56657,\"start\":56652},{\"end\":56670,\"start\":56668},{\"end\":56680,\"start\":56677},{\"end\":56691,\"start\":56688},{\"end\":56705,\"start\":56702},{\"end\":57029,\"start\":57025},{\"end\":57041,\"start\":57038},{\"end\":57052,\"start\":57050},{\"end\":57064,\"start\":57061},{\"end\":57347,\"start\":57343},{\"end\":57360,\"start\":57356},{\"end\":57374,\"start\":57369},{\"end\":57385,\"start\":57381},{\"end\":57395,\"start\":57391},{\"end\":57408,\"start\":57405},{\"end\":57422,\"start\":57417}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":52059988},\"end\":32529,\"start\":32040},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":118713138},\"end\":32912,\"start\":32531},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b2\",\"matched_paper_id\":59523708},\"end\":33204,\"start\":32914},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218894622},\"end\":33673,\"start\":33206},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":11206708},\"end\":34075,\"start\":33675},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":235166372},\"end\":34566,\"start\":34077},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":49904981},\"end\":34955,\"start\":34568},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":51989956},\"end\":35483,\"start\":34957},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":227118869},\"end\":35903,\"start\":35485},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":229923112},\"end\":36485,\"start\":35905},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1475121},\"end\":36887,\"start\":36487},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9044077},\"end\":37312,\"start\":36889},{\"attributes\":{\"id\":\"b12\"},\"end\":37464,\"start\":37314},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1663191},\"end\":37976,\"start\":37466},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":49672261},\"end\":38478,\"start\":37978},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":213655883},\"end\":38962,\"start\":38480},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":231419143},\"end\":39500,\"start\":38964},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":244426949},\"end\":39992,\"start\":39502},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":244920784},\"end\":40562,\"start\":39994},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":235422658},\"end\":40932,\"start\":40564},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":211506288},\"end\":41503,\"start\":40934},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b21\"},\"end\":41723,\"start\":41505},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":249282520},\"end\":42228,\"start\":41725},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":53751136},\"end\":42720,\"start\":42230},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":173990717},\"end\":43078,\"start\":42722},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":173990648},\"end\":43420,\"start\":43080},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":247596985},\"end\":43986,\"start\":43422},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3846544},\"end\":44440,\"start\":43988},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":237266491},\"end\":44944,\"start\":44442},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":47007607},\"end\":45301,\"start\":44946},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":29151865},\"end\":45808,\"start\":45303},{\"attributes\":{\"doi\":\"arXiv:1608.03983\",\"id\":\"b31\"},\"end\":46055,\"start\":45810},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":10987457},\"end\":46475,\"start\":46057},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":204904999},\"end\":46971,\"start\":46477},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":247627958},\"end\":47611,\"start\":46973},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":235719899},\"end\":48130,\"start\":47613},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":9715523},\"end\":48566,\"start\":48132},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":235702920},\"end\":49070,\"start\":48568},{\"attributes\":{\"id\":\"b38\"},\"end\":49536,\"start\":49072},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":8550762},\"end\":49979,\"start\":49538},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":235358213},\"end\":50549,\"start\":49981},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":247447122},\"end\":51065,\"start\":50551},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":214714493},\"end\":51582,\"start\":51067},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":221376798},\"end\":51921,\"start\":51584},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":222154570},\"end\":52334,\"start\":51923},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":201667906},\"end\":52730,\"start\":52336},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":220495900},\"end\":53118,\"start\":52732},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":244346144},\"end\":53732,\"start\":53120},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":212737191},\"end\":54338,\"start\":53734},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":996788},\"end\":54734,\"start\":54340},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":10514149},\"end\":55070,\"start\":54736},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":4766599},\"end\":55617,\"start\":55072},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":49657846},\"end\":56117,\"start\":55619},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":238583414},\"end\":56581,\"start\":56119},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":253510104},\"end\":56953,\"start\":56583},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":53079586},\"end\":57290,\"start\":56955},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":102351079},\"end\":57784,\"start\":57292}]", "bib_title": "[{\"end\":32095,\"start\":32040},{\"end\":32574,\"start\":32531},{\"end\":32961,\"start\":32914},{\"end\":33272,\"start\":33206},{\"end\":33716,\"start\":33675},{\"end\":34142,\"start\":34077},{\"end\":34617,\"start\":34568},{\"end\":35035,\"start\":34957},{\"end\":35533,\"start\":35485},{\"end\":35977,\"start\":35905},{\"end\":36556,\"start\":36487},{\"end\":36971,\"start\":36889},{\"end\":37536,\"start\":37466},{\"end\":38034,\"start\":37978},{\"end\":38553,\"start\":38480},{\"end\":39033,\"start\":38964},{\"end\":39567,\"start\":39502},{\"end\":40099,\"start\":39994},{\"end\":40651,\"start\":40564},{\"end\":41027,\"start\":40934},{\"end\":41774,\"start\":41725},{\"end\":42284,\"start\":42230},{\"end\":42784,\"start\":42722},{\"end\":43129,\"start\":43080},{\"end\":43517,\"start\":43422},{\"end\":44046,\"start\":43988},{\"end\":44490,\"start\":44442},{\"end\":44995,\"start\":44946},{\"end\":45348,\"start\":45303},{\"end\":46161,\"start\":46057},{\"end\":46536,\"start\":46477},{\"end\":47081,\"start\":46973},{\"end\":47687,\"start\":47613},{\"end\":48187,\"start\":48132},{\"end\":48633,\"start\":48568},{\"end\":49134,\"start\":49072},{\"end\":49595,\"start\":49538},{\"end\":50042,\"start\":49981},{\"end\":50622,\"start\":50551},{\"end\":51140,\"start\":51067},{\"end\":51625,\"start\":51584},{\"end\":51993,\"start\":51923},{\"end\":52406,\"start\":52336},{\"end\":52810,\"start\":52732},{\"end\":53190,\"start\":53120},{\"end\":53794,\"start\":53734},{\"end\":54417,\"start\":54340},{\"end\":54809,\"start\":54736},{\"end\":55142,\"start\":55072},{\"end\":55693,\"start\":55619},{\"end\":56175,\"start\":56119},{\"end\":56644,\"start\":56583},{\"end\":57015,\"start\":56955},{\"end\":57334,\"start\":57292}]", "bib_author": "[{\"end\":32121,\"start\":32097},{\"end\":32134,\"start\":32121},{\"end\":32151,\"start\":32134},{\"end\":32589,\"start\":32576},{\"end\":32602,\"start\":32589},{\"end\":32978,\"start\":32963},{\"end\":32990,\"start\":32978},{\"end\":33292,\"start\":33274},{\"end\":33309,\"start\":33292},{\"end\":33325,\"start\":33309},{\"end\":33338,\"start\":33325},{\"end\":33350,\"start\":33338},{\"end\":33733,\"start\":33718},{\"end\":33748,\"start\":33733},{\"end\":33759,\"start\":33748},{\"end\":34158,\"start\":34144},{\"end\":34171,\"start\":34158},{\"end\":34184,\"start\":34171},{\"end\":34632,\"start\":34619},{\"end\":34645,\"start\":34632},{\"end\":35051,\"start\":35037},{\"end\":35064,\"start\":35051},{\"end\":35079,\"start\":35064},{\"end\":35090,\"start\":35079},{\"end\":35548,\"start\":35535},{\"end\":35560,\"start\":35548},{\"end\":35991,\"start\":35979},{\"end\":36003,\"start\":35991},{\"end\":36017,\"start\":36003},{\"end\":36030,\"start\":36017},{\"end\":36044,\"start\":36030},{\"end\":36060,\"start\":36044},{\"end\":36574,\"start\":36558},{\"end\":36590,\"start\":36574},{\"end\":36610,\"start\":36590},{\"end\":36628,\"start\":36610},{\"end\":36989,\"start\":36973},{\"end\":37005,\"start\":36989},{\"end\":37025,\"start\":37005},{\"end\":37043,\"start\":37025},{\"end\":37375,\"start\":37365},{\"end\":37385,\"start\":37375},{\"end\":37550,\"start\":37538},{\"end\":37561,\"start\":37550},{\"end\":37575,\"start\":37561},{\"end\":37590,\"start\":37575},{\"end\":38051,\"start\":38036},{\"end\":38060,\"start\":38051},{\"end\":38076,\"start\":38060},{\"end\":38085,\"start\":38076},{\"end\":38092,\"start\":38085},{\"end\":38568,\"start\":38555},{\"end\":38583,\"start\":38568},{\"end\":38594,\"start\":38583},{\"end\":38609,\"start\":38594},{\"end\":39046,\"start\":39035},{\"end\":39060,\"start\":39046},{\"end\":39068,\"start\":39060},{\"end\":39080,\"start\":39068},{\"end\":39096,\"start\":39080},{\"end\":39584,\"start\":39569},{\"end\":39597,\"start\":39584},{\"end\":39611,\"start\":39597},{\"end\":39626,\"start\":39611},{\"end\":40116,\"start\":40101},{\"end\":40130,\"start\":40116},{\"end\":40144,\"start\":40130},{\"end\":40668,\"start\":40653},{\"end\":40682,\"start\":40668},{\"end\":41042,\"start\":41029},{\"end\":41057,\"start\":41042},{\"end\":41066,\"start\":41057},{\"end\":41079,\"start\":41066},{\"end\":41084,\"start\":41079},{\"end\":41561,\"start\":41549},{\"end\":41575,\"start\":41561},{\"end\":41579,\"start\":41575},{\"end\":41791,\"start\":41776},{\"end\":41803,\"start\":41791},{\"end\":41814,\"start\":41803},{\"end\":41830,\"start\":41814},{\"end\":41840,\"start\":41830},{\"end\":42303,\"start\":42286},{\"end\":42324,\"start\":42303},{\"end\":42337,\"start\":42324},{\"end\":42803,\"start\":42786},{\"end\":42816,\"start\":42803},{\"end\":42832,\"start\":42816},{\"end\":42845,\"start\":42832},{\"end\":42858,\"start\":42845},{\"end\":43145,\"start\":43131},{\"end\":43158,\"start\":43145},{\"end\":43175,\"start\":43158},{\"end\":43186,\"start\":43175},{\"end\":43532,\"start\":43519},{\"end\":43546,\"start\":43532},{\"end\":43561,\"start\":43546},{\"end\":44065,\"start\":44048},{\"end\":44081,\"start\":44065},{\"end\":44097,\"start\":44081},{\"end\":44111,\"start\":44097},{\"end\":44124,\"start\":44111},{\"end\":44139,\"start\":44124},{\"end\":44150,\"start\":44139},{\"end\":44507,\"start\":44492},{\"end\":44521,\"start\":44507},{\"end\":44533,\"start\":44521},{\"end\":44544,\"start\":44533},{\"end\":44558,\"start\":44544},{\"end\":44572,\"start\":44558},{\"end\":45007,\"start\":44997},{\"end\":45018,\"start\":45007},{\"end\":45030,\"start\":45018},{\"end\":45047,\"start\":45030},{\"end\":45063,\"start\":45047},{\"end\":45362,\"start\":45350},{\"end\":45377,\"start\":45362},{\"end\":45388,\"start\":45377},{\"end\":45399,\"start\":45388},{\"end\":45413,\"start\":45399},{\"end\":45881,\"start\":45864},{\"end\":45895,\"start\":45881},{\"end\":46177,\"start\":46163},{\"end\":46191,\"start\":46177},{\"end\":46204,\"start\":46191},{\"end\":46550,\"start\":46538},{\"end\":46563,\"start\":46550},{\"end\":46573,\"start\":46563},{\"end\":46588,\"start\":46573},{\"end\":47104,\"start\":47083},{\"end\":47123,\"start\":47104},{\"end\":47137,\"start\":47123},{\"end\":47152,\"start\":47137},{\"end\":47703,\"start\":47689},{\"end\":47715,\"start\":47703},{\"end\":47727,\"start\":47715},{\"end\":47735,\"start\":47727},{\"end\":48203,\"start\":48189},{\"end\":48216,\"start\":48203},{\"end\":48653,\"start\":48635},{\"end\":48667,\"start\":48653},{\"end\":48679,\"start\":48667},{\"end\":48685,\"start\":48679},{\"end\":49154,\"start\":49136},{\"end\":49171,\"start\":49154},{\"end\":49184,\"start\":49171},{\"end\":49607,\"start\":49597},{\"end\":49618,\"start\":49607},{\"end\":49632,\"start\":49618},{\"end\":49644,\"start\":49632},{\"end\":50059,\"start\":50044},{\"end\":50073,\"start\":50059},{\"end\":50086,\"start\":50073},{\"end\":50100,\"start\":50086},{\"end\":50116,\"start\":50100},{\"end\":50129,\"start\":50116},{\"end\":50636,\"start\":50624},{\"end\":50650,\"start\":50636},{\"end\":50662,\"start\":50650},{\"end\":50671,\"start\":50662},{\"end\":51155,\"start\":51142},{\"end\":51164,\"start\":51155},{\"end\":51179,\"start\":51164},{\"end\":51190,\"start\":51179},{\"end\":51638,\"start\":51627},{\"end\":51648,\"start\":51638},{\"end\":51657,\"start\":51648},{\"end\":51670,\"start\":51657},{\"end\":51684,\"start\":51670},{\"end\":52003,\"start\":51995},{\"end\":52015,\"start\":52003},{\"end\":52032,\"start\":52015},{\"end\":52040,\"start\":52032},{\"end\":52049,\"start\":52040},{\"end\":52058,\"start\":52049},{\"end\":52069,\"start\":52058},{\"end\":52423,\"start\":52408},{\"end\":52437,\"start\":52423},{\"end\":52448,\"start\":52437},{\"end\":52459,\"start\":52448},{\"end\":52470,\"start\":52459},{\"end\":52827,\"start\":52812},{\"end\":52838,\"start\":52827},{\"end\":52849,\"start\":52838},{\"end\":52860,\"start\":52849},{\"end\":53217,\"start\":53192},{\"end\":53231,\"start\":53217},{\"end\":53245,\"start\":53231},{\"end\":53252,\"start\":53245},{\"end\":53283,\"start\":53252},{\"end\":53289,\"start\":53283},{\"end\":53821,\"start\":53796},{\"end\":53835,\"start\":53821},{\"end\":53849,\"start\":53835},{\"end\":53856,\"start\":53849},{\"end\":53887,\"start\":53856},{\"end\":53898,\"start\":53887},{\"end\":53904,\"start\":53898},{\"end\":54430,\"start\":54419},{\"end\":54444,\"start\":54430},{\"end\":54457,\"start\":54444},{\"end\":54468,\"start\":54457},{\"end\":54479,\"start\":54468},{\"end\":54822,\"start\":54811},{\"end\":54836,\"start\":54822},{\"end\":54847,\"start\":54836},{\"end\":55159,\"start\":55144},{\"end\":55174,\"start\":55159},{\"end\":55190,\"start\":55174},{\"end\":55205,\"start\":55190},{\"end\":55218,\"start\":55205},{\"end\":55708,\"start\":55695},{\"end\":55720,\"start\":55708},{\"end\":55728,\"start\":55720},{\"end\":55741,\"start\":55728},{\"end\":55755,\"start\":55741},{\"end\":55763,\"start\":55755},{\"end\":56187,\"start\":56177},{\"end\":56200,\"start\":56187},{\"end\":56215,\"start\":56200},{\"end\":56229,\"start\":56215},{\"end\":56659,\"start\":56646},{\"end\":56672,\"start\":56659},{\"end\":56682,\"start\":56672},{\"end\":56693,\"start\":56682},{\"end\":56707,\"start\":56693},{\"end\":57031,\"start\":57017},{\"end\":57043,\"start\":57031},{\"end\":57054,\"start\":57043},{\"end\":57066,\"start\":57054},{\"end\":57349,\"start\":57336},{\"end\":57362,\"start\":57349},{\"end\":57376,\"start\":57362},{\"end\":57387,\"start\":57376},{\"end\":57397,\"start\":57387},{\"end\":57410,\"start\":57397},{\"end\":57424,\"start\":57410}]", "bib_venue": "[{\"end\":32292,\"start\":32230},{\"end\":32731,\"start\":32675},{\"end\":34333,\"start\":34267},{\"end\":34774,\"start\":34718},{\"end\":35231,\"start\":35169},{\"end\":35709,\"start\":35643},{\"end\":36209,\"start\":36143},{\"end\":37731,\"start\":37669},{\"end\":38241,\"start\":38175},{\"end\":38718,\"start\":38672},{\"end\":39245,\"start\":39179},{\"end\":39755,\"start\":39699},{\"end\":40293,\"start\":40227},{\"end\":41233,\"start\":41167},{\"end\":41989,\"start\":41923},{\"end\":42486,\"start\":42420},{\"end\":43710,\"start\":43644},{\"end\":44701,\"start\":44645},{\"end\":45574,\"start\":45502},{\"end\":46737,\"start\":46671},{\"end\":47301,\"start\":47235},{\"end\":47884,\"start\":47818},{\"end\":48357,\"start\":48295},{\"end\":48834,\"start\":48768},{\"end\":49765,\"start\":49713},{\"end\":50278,\"start\":50212},{\"end\":50820,\"start\":50754},{\"end\":51339,\"start\":51273},{\"end\":53438,\"start\":53372},{\"end\":54053,\"start\":53987},{\"end\":55359,\"start\":55297},{\"end\":55878,\"start\":55829},{\"end\":56358,\"start\":56302},{\"end\":57533,\"start\":57487},{\"end\":32228,\"start\":32151},{\"end\":32673,\"start\":32602},{\"end\":33038,\"start\":32994},{\"end\":33417,\"start\":33350},{\"end\":33844,\"start\":33759},{\"end\":34265,\"start\":34184},{\"end\":34716,\"start\":34645},{\"end\":35167,\"start\":35090},{\"end\":35641,\"start\":35560},{\"end\":36141,\"start\":36060},{\"end\":36665,\"start\":36628},{\"end\":37080,\"start\":37043},{\"end\":37363,\"start\":37314},{\"end\":37667,\"start\":37590},{\"end\":38173,\"start\":38092},{\"end\":38670,\"start\":38609},{\"end\":39177,\"start\":39096},{\"end\":39697,\"start\":39626},{\"end\":40225,\"start\":40144},{\"end\":40731,\"start\":40682},{\"end\":41165,\"start\":41084},{\"end\":41547,\"start\":41505},{\"end\":41921,\"start\":41840},{\"end\":42418,\"start\":42337},{\"end\":42887,\"start\":42858},{\"end\":43235,\"start\":43186},{\"end\":43642,\"start\":43561},{\"end\":44194,\"start\":44150},{\"end\":44643,\"start\":44572},{\"end\":45112,\"start\":45063},{\"end\":45500,\"start\":45413},{\"end\":45862,\"start\":45810},{\"end\":46253,\"start\":46204},{\"end\":46669,\"start\":46588},{\"end\":47233,\"start\":47152},{\"end\":47816,\"start\":47735},{\"end\":48293,\"start\":48216},{\"end\":48766,\"start\":48685},{\"end\":49270,\"start\":49184},{\"end\":49711,\"start\":49644},{\"end\":50210,\"start\":50129},{\"end\":50752,\"start\":50671},{\"end\":51271,\"start\":51190},{\"end\":51722,\"start\":51684},{\"end\":52106,\"start\":52069},{\"end\":52519,\"start\":52470},{\"end\":52898,\"start\":52860},{\"end\":53370,\"start\":53289},{\"end\":53985,\"start\":53904},{\"end\":54516,\"start\":54479},{\"end\":54884,\"start\":54847},{\"end\":55295,\"start\":55218},{\"end\":55827,\"start\":55763},{\"end\":56300,\"start\":56229},{\"end\":56756,\"start\":56707},{\"end\":57103,\"start\":57066},{\"end\":57485,\"start\":57424}]"}}}, "year": 2023, "month": 12, "day": 17}