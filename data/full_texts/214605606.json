{"id": 214605606, "updated": "2023-10-06 17:34:16.463", "metadata": {"title": "DIDFuse: Deep Image Decomposition for Infrared and Visible Image Fusion", "authors": "[{\"first\":\"Zixiang\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Shuang\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Chunxia\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Junmin\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Pengfei\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Jiangshe\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence", "journal": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence", "publication_date": {"year": 2020, "month": 3, "day": 20}, "abstract": "Infrared and visible image fusion, a hot topic in the field of image processing, aims at obtaining fused images keeping the advantages of source images. This paper proposes a novel auto-encoder (AE) based fusion network. The core idea is that the encoder decomposes an image into background and detail feature maps with low- and high-frequency information, respectively, and that the decoder recovers the original image. To this end, the loss function makes the background/detail feature maps of source images similar/dissimilar. In the test phase, background and detail feature maps are respectively merged via a fusion module, and the fused image is recovered by the decoder. Qualitative and quantitative results illustrate that our method can generate fusion images containing highlighted targets and abundant detail texture information with strong robustness and meanwhile surpass state-of-the-art (SOTA) approaches.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2003.09210", "mag": "3104771364", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ijcai/ZhaoXZLZL20", "doi": "10.24963/ijcai.2020/135"}}, "content": {"source": {"pdf_hash": "bb8b568310a77b1b7bea20a9443c6dca4278cdc2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2003.09210v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://www.ijcai.org/proceedings/2020/0135.pdf", "status": "BRONZE"}}, "grobid": {"id": "ac7a42f6cdce6cca4c56e9f083a64553faf55191", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bb8b568310a77b1b7bea20a9443c6dca4278cdc2.txt", "contents": "\nDIDFuse: Deep Image Decomposition for Infrared and Visible Image Fusion\n\n\nZixiang Zhao \nSchool of Mathematics and Statistics\nXi'an Jiaotong University\nChina\n\nShuang Xu shuangxu@stu.xjtu.edu.cn \nSchool of Mathematics and Statistics\nXi'an Jiaotong University\nChina\n\nChunxia Zhang \nSchool of Mathematics and Statistics\nXi'an Jiaotong University\nChina\n\nJunmin Liu \nSchool of Mathematics and Statistics\nXi'an Jiaotong University\nChina\n\nPengfei Li lipengfei27@hikvision.com \nHikvisionChina\n\nJiangshe Zhang jszhang@mail.xjtu.edu.cn \nSchool of Mathematics and Statistics\nXi'an Jiaotong University\nChina\n\nDIDFuse: Deep Image Decomposition for Infrared and Visible Image Fusion\n\nInfrared and visible image fusion, a hot topic in the field of image processing, aims at obtaining fused images keeping the advantages of source images. This paper proposes a novel auto-encoder (AE) based fusion network. The core idea is that the encoder decomposes an image into background and detail feature maps with low-and high-frequency information, respectively, and that the decoder recovers the original image. To this end, the loss function makes the background/detail feature maps of source images similar/dissimilar. In the test phase, background and detail feature maps are respectively merged via a fusion module, and the fused image is recovered by the decoder. Qualitative and quantitative results illustrate that our method can generate fusion images containing highlighted targets and abundant detail texture information with strong robustness and meanwhile surpass state-of-the-art (SOTA) approaches. arXiv:2003.09210v1 [eess.IV]\n\nIntroduction\n\nImage fusion is an image processing technique for information enhancement. The principle is to preserve the complementary and redundant information from source images containing the same scene without artifacts [Meher et al., 2019]. In image fusion, the infrared and visible image fusion, a.k.a. IVIF, can be applied to many domains, such as surveillance [Bhatnagar and Liu, 2015], modern military and fire rescue tasks [Lahoud and Susstrunk, 2018;Hu et al., 2017], face recognition  and so on.\n\nProverbially, infrared images can avoid visually cognitive obstacles caused by illumination changes and artifacts, but they are with low spatial resolution and poor texture detail information. Conversely, visible images are with high spatial resolution and rich information of appearance and gradient, while they are easily affected by obstructions and light reflections. Therefore, making the fusion image retain both thermal * Contact Author \u2020 Corresponding Author radiation information of the infrared images and gradient information of the visible images will be conducive to target recognition and tracking. In general, the IVIF algorithms can be divided into two groups: traditional methods and deep learning methods. Specifically, representative traditional methods includes image multi-scale transformation , sparse representation [Zong and Qiu, 2017], subspace learning [Patil and Mudengudi, 2011] and the saliency based method [Zhang et al., 2017].\n\nCurrently, deep learning (DL) has emerged as a prevalent tool in the field of IVIF. DL based methods can be categorized into three groups. The first group is based on Generative Adversarial Networks (GANs). In FusionGAN , a generator creates fused images with infrared thermal radiation and visible gradient information and a discriminator forces the fused images to have more details from the visible images. In the light of Conditional GANs [Mirza and Osindero, 2014], detail preserving GAN [Ma et al., 2020] changes the loss function of FusionGAN for improving the quality of detail information and sharpening the target boundary. The second group Lahoud and S\u00fcsstrunk, 2019] is an extension of image multiscale transformation. Generally speaking, they transform images from the spatial domain to background and detail domains by means of filters or optimization based methods. Background images are simply averaged. Since there are high-frequency textures in detail images, they fuse feature maps of detail images extracted from a pre-trained network (for example, VGG [Simonyan and Zisserman, 2014]). At last, a fusion image is recovered by merging the fused background and detail images. The third group consists of AE based methods . In the training phase, an AE network is trained. In the test phase, they fuse feature maps of source images, which then pass through the decoder to recover a fusion image. In summary, in DL based methods, deep neural networks (DNNs) are often employed to extract features of input images and then a certain fusion strategy is exploited to combine features to complete the image fusion task.\n\nIt is worth to point out a shortcoming of the second group, i.e. DL is used only in the fusion stage and they employ filters or optimization based methods in the decomposition stage. To overcome this shortcoming, by combining princi-ples of the second and third groups, we propose a novel IVIF network, called deep image decomposition based IVIF (DID-Fuse). Our contributions are two-fold:\n\n(1) To the best of our knowledge, this is the first deep image decomposition model for IVIF task, where both fusion and decomposition are accomplished via an AE network. The encoder and the decoder are responsible for image decomposition and reconstruction, respectively. In training phase, for decomposition stage, the loss function forces background and detail feature maps of two source images similar/dissimilar. Simultaneously, for reconstruction stage, the loss function maintains pixel intensities between source and reconstructed images, and gradient details of the visible image. In the test phase, background and detail feature maps of test pairs are separately fused according to a specific fusion strategy, and then the fused image can be acquired through the decoder.\n\n(2) As far as we know, the performance of existing IVIF methods Zhang et al., 2017; is only verified on a limited number of hand-picked examples in TNO dataset. However, we test our model on three datasets, including TNO, FLIR and NIR. In total, there are 132 test images with indoor and outdoor scenes, and with daylight and nightlight illuminations. Compared with SOTA methods, our method can robustly create fusion images with brighter targets and richer details. It can be potentially utilized in target recognition and tracking.\n\nThe remaining article is arranged as follows. Related work is introduced in section 2. The mechanism of the proposed network is described in section 3. Then, experimental results are reported in section 4. At last, some conclusions are drawn in section 5.\n\n\nRelated Work\n\nSince our network structure is closely related with U-Net, we introduce U-Net in section 2.1. Then, traditional two-scale image decomposition methods are briefly reviewed in section 2.2.\n\n\nU-Net and skip connection\n\nU-Net is applied to biomedical image segmentation [Ronneberger et al., 2015], Similar to AE network, U-Net consists of a contracting path for feature extraction and an expanding path for precise localization. Compared with AE, there is a channel-wise concatenation of corresponding feature maps from contracting and expanding paths in U-Net. In this manner, it can extract thicker features that help preserve image texture details during downsampling. In literature [Mao et al., 2016], a U-Net-like symmetric network is used for image restoration. It employs skip connection technique, where feature maps of convolution layers are added to corresponding deconvolution layers to enhance the information extraction capability of the neural network and to accelerate convergence.\n\n\nTwo-scale decomposition\n\nAs a subset of multi-scale transformation, two-scale decomposition in IVIF decomposes an original image into background and detail images with background and target information, respectively. In , given an image I, they obtained the background image I b by solving the following optimization problem,\nI b = arg min ||I \u2212 I b || 2 F + \u03bb(||g x * I b || 2 F + ||g y * I b || 2 F ),\nwhere * denotes a convolution operator, and g x = [\u22121, 1] and g y = [\u22121, 1] T are gradient kernels. Then, the detail image is acquired by I d = I \u2212 I b . Similarly, a box filter is used to get the background image in [Lahoud and S\u00fcsstrunk, 2019], and the method of obtaining the detail image is the same as that of . After decomposition, background and detail images are separately fused with different criteria. At last, the fused image is reconstructed by combining fused background and detail images.\n\n\nMethod\n\nIn this section, we will introduce our DIDFuse algorithm and the proposed network structure. In addition, details of training and testing phases are also illustrated.\n\n\nMotivation\n\nAs described in section 2.2, two-scale decomposition decomposes the input image into a background image containing low-frequency information with large-scale pixel intensity changes and a detail image embodying high-frequency information with small-scale pixel intensity changes. Currently, most algorithms incorporate certain prior knowledge, and employ filters or optimization based methods to decompose images. Hence, they are manually designed decomposition algorithm. We highlight that image decomposition algorithms are intrinsically feature extractors. Formally, they transform source images from spatial domain into feature domain. It is well known that the DNN is a promising datadriven feature extractor and has great superiority over traditional manually-designed methods. Unfortunately, it lacks a DL based image decomposition algorithm for IVIF task.\n\nConsequently, we present a novel deep image decomposition network in which an encoder is exploited to perform two-scale decomposition and extract different types of information, and a decoder is used to recover original images.\n\n\nNetwork architecture\n\nOur neural network consists of an encoder and a decoder. As illustrated in Figure 1, the encoder is fed with an infrared or a visible image and generates background and detail feature maps. Then, the network concatenates two kinds of layers along channels. At last, concatenated feature maps pass through decoder to recover the original image. To prevent the detail information of the feature maps from being lost after multiple convolutions and to speed up the convergence, we add the feature maps from the first and second convolutions to the inputs of the last and penultimate convolutions, and the adding strategy is concatenating the corresponding feature maps along channels. As a consequence, the pixel intensity and gradient information of the source images can be better retained in the reconstructed image. Table 1 lists the network configuration. Encoder and decoder contain four and three convolutional layers, respectively. Each layer consists of a padding, a 3 \u00d7 3 convolution, ( , )  Figure 1: Neural network framework of DIDFuse.  PReLU  conv3  3  64  64  Zero  Tanh  conv4  3  64  64  Zero  Tanh  conv5  3  128  64  Zero  PReLU  conv6  3  64  64  Zero  PReLU  conv7  3  64  1 Reflection Sigmoid a batch normalization and an activation function. The first and the last layers utilize reflection padding to prevent artifacts at the edges of the fused image. Activation functions of conv3 and conv4 are set to the hyperbolic tangent function (tanh) since they output background and detail feature maps. As for conv7, it is activated by sigmoid function since it reconstructs original images. Other layers are followed by parametric rectified linear units (PReLU).\nf I I f V V V V \uf061 \uf061 \uf061 + + + \uf0d1 \u2212 \uf0d1128\n\nLoss function\n\nIn the training phase, we aim to obtain an encoder that performs two-scale decomposition on the source images, and at the same time, acquire a decoder that can fuse the images and preserve the information of source images well. The training process is shown in Figure 1(a). Image decomposition Background feature maps are used to extract the common features of source images, while detail feature maps are used to capture the distinct characteristics from infrared and visible images. Therefore, we should make the gap of background feature maps small. In contrast, the gap of detail feature maps should be great. To this end, the loss function of image decomposition is defined as follow,\nL 1 = \u03a6 B V \u2212 B I 2 2 \u2212 \u03b1 1 \u03a6 D V \u2212 D I 2 2 ,(1)\nwhere B V , D V are the background and detail feature maps of the visible image V , and B I , D I are those of the infrared image I. \u03a6 (\u00b7) is tanh function that is used to bound gap into interval (\u22121, 1).\n\nImage reconstruction As for image reconstruction, to successfully retain the pixel intensity and detailed texture information of input images, the reconstruction loss function is given by\nL 2 = \u03b1 2 f (I,\u00ce) + \u03b1 3 f (V,V ) + \u03b1 4 \u2207V \u2212 \u2207V 1 ,(2)\nwhere I and\u00ce, V andV represent the input and reconstructed images of infrared and visible images, respectively. \u2207 denotes the gradient operator, and f (X,X) = X \u2212X 2 2 + \u03bbL SSIM (X,X),\n\nwhere X andX represent the above input image and the reconstructed image, and \u03bb is the hyperparameter. SSIM is the structural similarity index [Wang et al., 2004], which is a measure of the similarity between two pictures. Then L SSIM can be described as\nL SSIM (X,X) = 1 \u2212 SSIM (X,X) 2 .\nRemark that L 2 -norm measures the pixel intensity agreement between original and reconstructed images, and that L SSIM computes image dissimilarity in terms of brightness, contrast and structure. Specially, since visible images are with enriched textures, the reconstruction of visible images is regularized by gradient sparsity penalty to guarantee texture agreement.\n\nCombining Eqs.\n\n(1) and (2), the total loss L total can be expressed as\nL total =L 1 + L 2 =\u03a6 B V \u2212 B I 2 2 \u2212 \u03b1 1 \u03a6 D V \u2212 D I 2 2 +\u03b1 2 f (I,\u00ce) + \u03b1 3 f (V,V ) + \u03b1 4 \u2207V \u2212 \u2207V 1 ,(4)\nwhere \u03b1 1 , \u03b1 2 , \u03b1 3 , \u03b1 4 are the tuning parameters.\n\n\nFusion strategy\n\nIn the above subsections, we have proposed network structure and loss function. After training, we will acquire a decomposer (or say, encoder) and a decoder. In the test phase, we aim to fuse infrared and visible images. The workflow is shown in Figure1(b). Different from training, a fusion layer is inserted in the test phase. It fuses background and detail feature maps separately. In formula, there is\nB F = Fusion(B I , B V ), D F = Fusion(D I , D V ),(5)\nwhere B F and D F denote the fused background and detail feature maps, respectively. In this paper, three fusion strategies are considered as follows:\n\n\u2022 Summation method:\nB F = B I \u2295 B V , D F = D I \u2295 D V ,\nwhere the symbol \u2295 means element-wise addition.\n\n\u2022 Weighted average method:\nB F = \u03b3 1 B I \u2295 \u03b3 2 B V , D F = \u03b3 3 D I \u2295 \u03b3 4 D V ,\nwhere \u03b3 1 + \u03b3 2 = \u03b3 3 + \u03b3 4 = 1 and the default settings for \u03b3 i (i = 1, \u00b7 \u00b7 \u00b7, 4) are all equal to 0.5.\n\n\u2022 L 1 -norm method: Referring to , we use the L 1 -norm as a measure of activity, combining with the softmax operator. In simple, we can obtain the activity level map of the fused background and detail feature maps by B i (x, y) 1 and D i (x, y) 1 (i = 1, 2), where B 1 , B 2 , D 1 and D 2 represent B I , B V , D I and D V , and (x, y) represents the corresponding coordinates of the feature maps and the fused feature map. Then the adding weights can be calculated by:\n\u03b7 B i (x, y) = \u03c8 ( B i (x, y) 1 ) 2 i=1 \u03c8 ( B i (x, y) 1 ) , \u03b7 D i (x, y) = \u03c8 ( D i (x, y) 1 ) 2 i=1 \u03c8 ( D i (x, y) 1 ) ,\nwhere \u03c8(\u00b7) is a 3 \u00d7 3 box blur (also known as a mean filter operator). Consequently, we have\nB F = (\u03b7 B 1 \u2297 B I ) \u2295 (\u03b7 B 2 \u2297 B V ), D F = (\u03b7 D 1 \u2297 D I ) \u2295 (\u03b7 D 2 \u2297 D V ).\nwhere \u2297 means element-wise multiplication.\n\n\nExperiment\n\nThe aim of this section is to study the performance of our proposed model and compare it with other SOTA models, including FusionGAN , Densefuse , ImageFuse , DeepFuse [Prabhakar et al., 2017], TSIFVS [Bavirisetti and Dhuli, 2016], TVADMM [Guo et al., 2017], CSR [Liu et al., 2016] and ADF [Bavirisetti and Dhuli, 2015]. All experiments were conducted with Pytorch on a computer with Intel Core i7-9750H CPU@2.60GHz and RTX2070 GPU. We employ six metrics to evaluate the quality of a fused image, that is, entropy (EN), mutual information (MI), standard deviation (SD), spatial frequency (SF), visual information fidelity (VIF) and average gradient (AG).  Datasets and preprocessing Our experiments are conducted on three datasets, including TNO [Toet and Hogervorst, 2012], NIR [Brown and S\u00fcsstrunk, 2011] and FLIR (available at https://github.com/jiayi-ma/RoadScene). In our experiment, we divide them into training, validation, and test sets. Table 2 shows the numbers of image pairs, illumination and scene information of the datasets. We randomly selected 180 pairs of images in the FLIR dataset as training samples. Before training, all images are transformed into grayscale. At the same time, we center-crop them with 128 \u00d7 128 pixels.\n\nHyperparameters setting The tuning parameters in loss function are empirically set as follows: \u03b1 1 = 0.5, \u03b1 2 = 2, \u03b1 3 = 2, \u03b1 4 = 10 and \u03bb = 5. In training phase, the network is optimized by Adam over 120 epochs with a batch size of 24. As for learning rate, we set it to 10 \u22122 and decrease it by 10 times every 40 epochs. Figure 2 displays loss curves versus epoch index. It is shown that all loss curves are very flat after 120 epochs. In other words, the network is able to converge with this configuration.\n\n\nExperiments on fusion strategy\n\nAs described in section 3.4, fusion strategy plays an important role in our model. We investigate the performance of three strategies on validation set. Table 3 reports numerical results of six metrics on validation set. Obviously, it is shown that summation strategy achieves higher values, especially in terms of SD, SF, VIF and AG. Hence, the following experiments adopt summation strategy. \n\n\nExperiments on image decomposition\n\nOne of our contributions is the deep image decomposition. It is interesting to study whether decomposed feature maps are able to meet our demands. In Figure 3, it displays the first channels of feature maps which are generated by conv3 and conv4. It is evident that our method can separate the backgrounds and details of infrared and visible images. For background feature maps, it is found that B I and B V are visually similar, and they reflect the background and environment of the same scene. Conversely, the gap between D I and D V is large, which illustrates the distinct characteristics contained in different source images. That is, the infrared images contain target highlight and thermal radiation information while gradient and texture information of targets are involved in the visible images. In conclusion, it to some degree verifies the rationality of our proposed network structure and image decomposition loss function.\n\n\nComparison with other models\n\nIn this subsection, we will compare our model with the other popular counterparts. Qualitative comparison Figure 4 exhibits several representative fusion images generated by different models. Visual inspection shows that, in the images containing people, other methods have problems such as weak high-lighted objects, poor contrast and less prominent contour of targets and backgrounds. Similarly, if the images are natural landscapes, others have blurred boundaries of mountains and trees, poor color contrast, and insufficient sharpness. On the contrary, our method can obtain fused images with brighter targets, sharper edge contours, and retaining richer detailed information.\n\nQuantitative comparison Subsequently, quantitative comparison results on test set are listed in Table 4. It is found that our model is the best performer on all datasets in terms of all metrics. As for competitors, they may perform well on \nI V B I B V D I D V\n\nExperiments on robustness\n\nAs is known, deep learning methods are often criticized for instability. Therefore, we test the robustness of DIDFuse in the last experiment. We repeatedly train the network 25 times and quantitatively compare the 25 parallel results. As shown in Figure 5, the black solid curves report six metrics over 25 experiments. The red dashed line and blue dotted line represent the greatest and the second greatest values in the comparison methods, respectively. Similar to the above results, our method can basically keep the first place all the time, indicating that DIDFuse can generate high-quality fused images steadily.\n\n\nConclusion\n\nTo solve the IVIF issue, we construct a new AE network in which the encoder is used for two-scale image decomposition and the decoder is responsible for image reconstruction.\n\nIn the training phase, the encoder is trained to output background and feature maps, then the decoder reconstructs original images. In the test phase, we set a fusion layer between the encoder and decoder to fuse background and detail feature maps through a specific fusion strategy. Finally, the fused image can be acquired through the decoder. We test our model on TNO, FLIR, and NIR datasets. Qualitative and quantitative results show that our model outperforms other SOTA methods, since our model can robustly obtain a fusion image of highlight targets and rich details.\n\nFigure 2 :\n2Loss curves over 120 epochs.\n\nFigure 3 :Figure 4 :Figure 5 :\n345Illustration of deep image decomposition. From left to right: infrared image, visible image, background and detail feature maps of infrared image and visible image. Qualitative fusion results for different methods. Areas marked by orange and blue boxes are amplified for ease of inspection. Test results of model robustness: quantitative results of repetitive training DIDFuse network 25 times. The red dashed and blue dotted lines represent the best and the second best results achieved by competitors, respectively. From top to bottom: image fusion dataset TNO, FLIR, and NIR. From left to right: the values of EN, MI, SD, SF, VIF and AG. a dataset in terms of part of metrics. This result demonstrates that images fused by our model are with enriched textures and satisfy human visual system.\n\nTable 1 :\n1Network configuration. Size denotes the size of convolu-\ntional kernel. InC and OutC are the numbers of input and output \nchannels, respectively. \n\nLayers Size InC OutC \nPadding \nActivation \n\nconv1 \n3 \n1 \n64 \nReflection \nPReLU \nconv2 \n3 \n64 \n64 \nZero \n\n\nTable 2 :\n2Dataset used in this paper.Dataset(pairs) \nIllumination \n\nTraining \nFLIR-Train(180) Daylight&Nightlight \n\nValidation \nNIR-Urban(58) \nDaylight \nNIR-Street(50) \nDaylight \n\nTest \n\nTNO (40) \nNightlight \nFLIR-Test(40) \nDaylight&Nightlight \nNIR-Country(52) \nDaylight \n\n\n\nTable 3 :\n3Results of validation set for choosing the addition strategy.Dataset: NIR Dataset. Scene: Street \nMethod \nSummation \nAverage \nL 1 -norm \n\nEN \n7.17 \u00b1 0.10 \n6.85 \u00b1 0.03 \n6.87 \u00b1 0.03 \nMI \n4.69 \u00b1 0.06 \n4.68 \u00b1 0.04 \n4.68 \u00b1 0.03 \nSD \n55.51 \u00b1 1.74 36.51 \u00b1 0.59 36.88 \u00b1 0.65 \nSF \n24.28 \u00b1 1.12 16.85 \u00b1 0.23 16.80 \u00b1 0.23 \nVIF \n1.02 \u00b1 0.04 \n0.62 \u00b1 0.01 \n0.63 \u00b1 0.01 \nAG \n7.18 \u00b1 0.41 \n4.89 \u00b1 0.07 \n4.88 \u00b1 0.07 \n\nDataset: NIR Dataset. Scene: Urban \nMethod \nSummation \nAverage \nL 1 -norm \n\nEN \n7.18 \u00b1 0.14 \n7.12 \u00b1 0.03 \n7.12 \u00b1 0.03 \nMI \n6.07 \u00b1 0.07 \n6.15 \u00b1 0.04 \n6.14 \u00b1 0.03 \nSD \n61.46 \u00b1 1.55 41.64 \u00b1 0.51 41.73 \u00b1 0.50 \nSF \n29.22 \u00b1 1.14 20.20 \u00b1 0.22 20.17 \u00b1 0.22 \nVIF \n1.13 \u00b1 0.05 \n0.77 \u00b1 0.01 \n0.77 \u00b1 0.01 \nAG \n8.13 \u00b1 0.41 \n5.85 \u00b1 0.06 \n5.85 \u00b1 0.06 \n\n\n\nAn adaptive fusion algorithm for visible and infrared videos based on entropy and the cumulative distribution of gray levels. Dhuli ; Durga Prasad Bavirisetti, Ravindra Bavirisetti, Prasad Dhuli ; Durga, Ravindra Bavirisetti, Zheng Dhuli ; Gaurav Bhatnagar, Sabine Liu ; Matthew Brown, S\u00fcsstrunk ; Hanqi, Yong Guo, Xiaoguang Ma, Jiayi Ma ; Hai-Miao Mei, Jiawei Hu, Bo Wu, Qiang Li, Jin Guo, ; F Zheng, S Lahoud, Susstrunk, arXiv:1905.03590Fusion of infrared and visible sensor images based on anisotropic diffusion and karhunen-loeve transform. IEEE16arXiv preprint25th IEEE International Conference on Image Processing (ICIP). Lahoud and S\u00fcsstrunk, 2019] Fayez Lahoud and Sabine S\u00fcsstrunk. Fast and efficient zero-learning image fusionReferences [Bavirisetti and Dhuli, 2015] Durga Prasad Bavirisetti and Ravindra Dhuli. Fusion of infrared and visible sensor im- ages based on anisotropic diffusion and karhunen-loeve transform. IEEE Sensors Journal, 16(1):203-209, 2015. [Bavirisetti and Dhuli, 2016] Durga Prasad Bavirisetti and Ravindra Dhuli. Two-scale image fusion of visible and infrared images using saliency detection. Infrared Physics & Technology, 76:52-64, 2016. [Bhatnagar and Liu, 2015] Gaurav Bhatnagar and Zheng Liu. A novel image fusion framework for night-vision navigation and surveillance. Signal, Image and Video Pro- cessing, 9(1):165-175, 2015. [Brown and S\u00fcsstrunk, 2011] Matthew Brown and Sabine S\u00fcsstrunk. Multi-spectral sift for scene category recog- nition. In CVPR 2011, pages 177-184. IEEE, 2011. [Guo et al., 2017] Hanqi Guo, Yong Ma, Xiaoguang Mei, and Jiayi Ma. Infrared and visible image fusion based on total variation and augmented lagrangian. Journal of the Optical Society of America A, 34(11):1961-1968, 2017. [Hu et al., 2017] Hai-Miao Hu, Jiawei Wu, Bo Li, Qiang Guo, and Jin Zheng. An adaptive fusion algorithm for visible and infrared videos based on entropy and the cu- mulative distribution of gray levels. IEEE Transactions on Multimedia, 19(12):2706-2719, 2017. [Lahoud and Susstrunk, 2018] F. Lahoud and S. Susstrunk. Ar in vr: Simulating infrared augmented vision. In 2018 25th IEEE International Conference on Image Processing (ICIP), pages 3893-3897, Oct 2018. [Lahoud and S\u00fcsstrunk, 2019] Fayez Lahoud and Sabine S\u00fcsstrunk. Fast and efficient zero-learning image fusion. arXiv preprint arXiv:1905.03590, 2019.\n\nPerformance comparison of different multi-resolution transforms for image fusion. Wu ; Hui Li, Xiao-Jun Wu, ; Li, Densefuse: A fusion approach to infrared and visible images. IEEE Transactions on Image Processing. IEEE28Information Fusionand Wu, 2018] Hui Li and Xiao-Jun Wu. Densefuse: A fusion approach to infrared and visible images. IEEE Transactions on Image Processing, 28(5):2614-2623, 2018. [Li et al., 2011] Shutao Li, Bin Yang, and Jianwen Hu. Per- formance comparison of different multi-resolution trans- forms for image fusion. Information Fusion, 12(2):74-84, 2011. [Li et al., 2018] Hui Li, Xiao-Jun Wu, and Josef Kittler. In- frared and visible image fusion using a deep learning framework. In 2018 24th International Conference on Pat- tern Recognition (ICPR), pages 2705-2710. IEEE, 2018. [Liu et al., 2016] Yu Liu, Xun Chen, Rabab K Ward, and Z Jane Wang. Image fusion with convolutional sparse rep- resentation. IEEE Signal Processing Letters, 23(12):1882- 1886, 2016.\n\nInfrared and visible image fusion via gradient transfer and total variation minimization. Ma, 31Information FusionMa et al., 2016] Jiayi Ma, Chen Chen, Chang Li, and Jun Huang. Infrared and visible image fusion via gradient transfer and total variation minimization. Information Fu- sion, 31:100-109, 2016.\n\nImage restoration using very deep convolutional encoder-decoder networks with symmetric skip connections. Ma, Advances in neural information processing systems. Mao, Chunhua Shen, and Yu-Bin Yang54Infrared and visible image fusion via detail preserving adversarial learningMa et al., 2019] Jiayi Ma, Wei Yu, Pengwei Liang, Chang Li, and Junjun Jiang. Fusiongan: A generative adversarial [Ma et al., 2020] Jiayi Ma, Pengwei Liang, Wei Yu, Chen Chen, Xiaojie Guo, Jia Wu, and Junjun Jiang. Infrared and visible image fusion via detail preserving adversarial learning. Information Fusion, 54:85-98, 2020. [Mao et al., 2016] Xiaojiao Mao, Chunhua Shen, and Yu- Bin Yang. Image restoration using very deep convolu- tional encoder-decoder networks with symmetric skip con- nections. In Advances in neural information processing systems, pages 2802-2810, 2016.\n\nRutuparna Panda, and Ajith Abraham. A survey on region based image fusion methods. [ Meher, Bikash Meher, Sanjay Agrawal. 48Information Fusion[Meher et al., 2019] Bikash Meher, Sanjay Agrawal, Rutu- parna Panda, and Ajith Abraham. A survey on region based image fusion methods. Information Fusion, 48:119- 132, 2019.\n\nMehdi Mirza and Simon Osindero. Osindero Mirza, arXiv:1411.1784Conditional generative adversarial nets. arXiv preprint[Mirza and Osindero, 2014] Mehdi Mirza and Simon Osin- dero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.\n\nUjwala Patil and Uma Mudengudi. Image fusion using hierarchical pca. 2011 International Conference on Image Information Processing. IEEEPatil and Mudengudi[Patil and Mudengudi, 2011] Ujwala Patil and Uma Muden- gudi. Image fusion using hierarchical pca. In 2011 In- ternational Conference on Image Information Processing, pages 1-6. IEEE, 2011.\n\nDeepfuse: A deep unsupervised approach for exposure fusion with extreme exposure image pairs. [ Prabhakar, arXiv:1409.1556Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. Olaf Ronneberger, Philipp Fischer, and Thomas BroxSpringerarXiv preprintInternational Conference on Medical image computing and computerassisted intervention[Prabhakar et al., 2017] K Ram Prabhakar, V Sai Srikar, and R Venkatesh Babu. Deepfuse: A deep unsupervised ap- proach for exposure fusion with extreme exposure image pairs. In ICCV, pages 4724-4732, 2017. [Ronneberger et al., 2015] Olaf Ronneberger, Philipp Fis- cher, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer- assisted intervention, pages 234-241. Springer, 2015. [Simonyan and Zisserman, 2014] Karen Simonyan and An- drew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nInfrared and visible image fusion via saliency analysis and local edge-preserving multi-scale decomposition. Alexander Toet, A Maarten, Hogervorst, Wang, Journal of the Optical Society of America A. Zhang, Yong Ma, Fan Fan, Ying Zhang, and Jun Huang511IEEE transactions on image processingand Hogervorst, 2012] Alexander Toet and Maarten A. Hogervorst. Progress in color night vision. Optical Engi- neering, 51(1):1 -20, 2012. [Wang et al., 2004] Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli, et al. Image quality assess- ment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600-612, 2004. [Zhang et al., 2017] Xiaoye Zhang, Yong Ma, Fan Fan, Ying Zhang, and Jun Huang. Infrared and visible image fusion via saliency analysis and local edge-preserving multi-scale decomposition. Journal of the Optical Society of America A, 34(8):1400-1410, 2017.\n\nJing-jing Zong and Tian-shuang Qiu. Medical image fusion based on sparse representation of classified image patches. Biomedical Signal Processing and Control. 34and Qiu, 2017] Jing-jing Zong and Tian-shuang Qiu. Medical image fusion based on sparse representation of classified image patches. Biomedical Signal Processing and Control, 34:195-205, 2017.\n", "annotations": {"author": "[{\"end\":158,\"start\":75},{\"end\":264,\"start\":159},{\"end\":349,\"start\":265},{\"end\":431,\"start\":350},{\"end\":485,\"start\":432},{\"end\":596,\"start\":486}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":83},{\"end\":168,\"start\":166},{\"end\":278,\"start\":273},{\"end\":360,\"start\":357},{\"end\":442,\"start\":440},{\"end\":500,\"start\":495}]", "author_first_name": "[{\"end\":82,\"start\":75},{\"end\":165,\"start\":159},{\"end\":272,\"start\":265},{\"end\":356,\"start\":350},{\"end\":439,\"start\":432},{\"end\":494,\"start\":486}]", "author_affiliation": "[{\"end\":157,\"start\":89},{\"end\":263,\"start\":195},{\"end\":348,\"start\":280},{\"end\":430,\"start\":362},{\"end\":484,\"start\":470},{\"end\":595,\"start\":527}]", "title": "[{\"end\":72,\"start\":1},{\"end\":668,\"start\":597}]", "venue": null, "abstract": "[{\"end\":1618,\"start\":670}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1865,\"start\":1845},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2014,\"start\":1989},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2082,\"start\":2054},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2098,\"start\":2082},{\"end\":2989,\"start\":2969},{\"end\":3036,\"start\":3009},{\"end\":3087,\"start\":3067},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3559,\"start\":3533},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3600,\"start\":3583},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3768,\"start\":3741},{\"end\":4193,\"start\":4163},{\"end\":5979,\"start\":5960},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6995,\"start\":6969},{\"end\":7403,\"start\":7385},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8347,\"start\":8319},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13180,\"start\":13161},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15847,\"start\":15823},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15885,\"start\":15856},{\"end\":15912,\"start\":15887},{\"end\":15936,\"start\":15914},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15974,\"start\":15945},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16428,\"start\":16401}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":21242,\"start\":21201},{\"attributes\":{\"id\":\"fig_2\"},\"end\":22073,\"start\":21243},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":22338,\"start\":22074},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":22614,\"start\":22339},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":23365,\"start\":22615}]", "paragraph": "[{\"end\":2128,\"start\":1634},{\"end\":3088,\"start\":2130},{\"end\":4721,\"start\":3090},{\"end\":5112,\"start\":4723},{\"end\":5894,\"start\":5114},{\"end\":6429,\"start\":5896},{\"end\":6686,\"start\":6431},{\"end\":6889,\"start\":6703},{\"end\":7695,\"start\":6919},{\"end\":8023,\"start\":7723},{\"end\":8605,\"start\":8102},{\"end\":8782,\"start\":8616},{\"end\":9660,\"start\":8797},{\"end\":9889,\"start\":9662},{\"end\":11591,\"start\":9914},{\"end\":12334,\"start\":11645},{\"end\":12588,\"start\":12384},{\"end\":12777,\"start\":12590},{\"end\":13016,\"start\":12832},{\"end\":13272,\"start\":13018},{\"end\":13676,\"start\":13307},{\"end\":13692,\"start\":13678},{\"end\":13749,\"start\":13694},{\"end\":13911,\"start\":13857},{\"end\":14336,\"start\":13931},{\"end\":14542,\"start\":14392},{\"end\":14563,\"start\":14544},{\"end\":14647,\"start\":14600},{\"end\":14675,\"start\":14649},{\"end\":14832,\"start\":14728},{\"end\":15304,\"start\":14834},{\"end\":15519,\"start\":15427},{\"end\":15640,\"start\":15598},{\"end\":16897,\"start\":15655},{\"end\":17409,\"start\":16899},{\"end\":17838,\"start\":17444},{\"end\":18813,\"start\":17877},{\"end\":19526,\"start\":18846},{\"end\":19768,\"start\":19528},{\"end\":20435,\"start\":19817},{\"end\":20624,\"start\":20450},{\"end\":21200,\"start\":20626}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8101,\"start\":8024},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11628,\"start\":11592},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12383,\"start\":12335},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12831,\"start\":12778},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13306,\"start\":13273},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13856,\"start\":13750},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14391,\"start\":14337},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14599,\"start\":14564},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14727,\"start\":14676},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15426,\"start\":15305},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15597,\"start\":15520},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19788,\"start\":19769}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":10738,\"start\":10731},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":11106,\"start\":10961},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":16608,\"start\":16601},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":17604,\"start\":17597},{\"end\":19631,\"start\":19624}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1632,\"start\":1620},{\"attributes\":{\"n\":\"2\"},\"end\":6701,\"start\":6689},{\"attributes\":{\"n\":\"2.1\"},\"end\":6917,\"start\":6892},{\"attributes\":{\"n\":\"2.2\"},\"end\":7721,\"start\":7698},{\"attributes\":{\"n\":\"3\"},\"end\":8614,\"start\":8608},{\"attributes\":{\"n\":\"3.1\"},\"end\":8795,\"start\":8785},{\"attributes\":{\"n\":\"3.2\"},\"end\":9912,\"start\":9892},{\"attributes\":{\"n\":\"3.3\"},\"end\":11643,\"start\":11630},{\"attributes\":{\"n\":\"3.4\"},\"end\":13929,\"start\":13914},{\"attributes\":{\"n\":\"4\"},\"end\":15653,\"start\":15643},{\"attributes\":{\"n\":\"4.1\"},\"end\":17442,\"start\":17412},{\"attributes\":{\"n\":\"4.2\"},\"end\":17875,\"start\":17841},{\"attributes\":{\"n\":\"4.3\"},\"end\":18844,\"start\":18816},{\"attributes\":{\"n\":\"4.4\"},\"end\":19815,\"start\":19790},{\"attributes\":{\"n\":\"5\"},\"end\":20448,\"start\":20438},{\"end\":21212,\"start\":21202},{\"end\":21274,\"start\":21244},{\"end\":22084,\"start\":22075},{\"end\":22349,\"start\":22340},{\"end\":22625,\"start\":22616}]", "table": "[{\"end\":22338,\"start\":22086},{\"end\":22614,\"start\":22378},{\"end\":23365,\"start\":22688}]", "figure_caption": "[{\"end\":21242,\"start\":21214},{\"end\":22073,\"start\":21278},{\"end\":22378,\"start\":22351},{\"end\":22688,\"start\":22627}]", "figure_ref": "[{\"end\":9997,\"start\":9989},{\"end\":10921,\"start\":10913},{\"end\":11917,\"start\":11906},{\"end\":14187,\"start\":14177},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17230,\"start\":17222},{\"end\":18035,\"start\":18027},{\"end\":18960,\"start\":18952},{\"end\":20072,\"start\":20064}]", "bib_author_first_name": "[{\"end\":23513,\"start\":23493},{\"end\":23535,\"start\":23527},{\"end\":23555,\"start\":23549},{\"end\":23579,\"start\":23571},{\"end\":23598,\"start\":23593},{\"end\":23631,\"start\":23625},{\"end\":23676,\"start\":23672},{\"end\":23691,\"start\":23682},{\"end\":23715,\"start\":23696},{\"end\":23727,\"start\":23721},{\"end\":23734,\"start\":23732},{\"end\":23744,\"start\":23739},{\"end\":23752,\"start\":23749},{\"end\":23759,\"start\":23758},{\"end\":23761,\"start\":23760},{\"end\":23770,\"start\":23769},{\"end\":25820,\"start\":25812},{\"end\":25833,\"start\":25825},{\"end\":25839,\"start\":25838},{\"end\":27966,\"start\":27965},{\"end\":28240,\"start\":28232},{\"end\":28900,\"start\":28899},{\"end\":29959,\"start\":29950},{\"end\":29967,\"start\":29966}]", "bib_author_last_name": "[{\"end\":23525,\"start\":23514},{\"end\":23547,\"start\":23536},{\"end\":23569,\"start\":23556},{\"end\":23591,\"start\":23580},{\"end\":23623,\"start\":23599},{\"end\":23651,\"start\":23632},{\"end\":23670,\"start\":23653},{\"end\":23680,\"start\":23677},{\"end\":23694,\"start\":23692},{\"end\":23719,\"start\":23716},{\"end\":23730,\"start\":23728},{\"end\":23737,\"start\":23735},{\"end\":23747,\"start\":23745},{\"end\":23756,\"start\":23753},{\"end\":23767,\"start\":23762},{\"end\":23777,\"start\":23771},{\"end\":23788,\"start\":23779},{\"end\":25823,\"start\":25821},{\"end\":25836,\"start\":25834},{\"end\":25842,\"start\":25840},{\"end\":26811,\"start\":26809},{\"end\":27135,\"start\":27133},{\"end\":27972,\"start\":27967},{\"end\":28246,\"start\":28241},{\"end\":28910,\"start\":28901},{\"end\":29964,\"start\":29960},{\"end\":29975,\"start\":29968},{\"end\":29987,\"start\":29977},{\"end\":29993,\"start\":29989}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1905.03590\",\"id\":\"b0\",\"matched_paper_id\":206753134},\"end\":25728,\"start\":23367},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":13273826},\"end\":26717,\"start\":25730},{\"attributes\":{\"id\":\"b2\"},\"end\":27025,\"start\":26719},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":10987457},\"end\":27880,\"start\":27027},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":126786100},\"end\":28198,\"start\":27882},{\"attributes\":{\"doi\":\"arXiv:1411.1784\",\"id\":\"b5\"},\"end\":28457,\"start\":28200},{\"attributes\":{\"id\":\"b6\"},\"end\":28803,\"start\":28459},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b7\",\"matched_paper_id\":216738},\"end\":29839,\"start\":28805},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":37093748},\"end\":30747,\"start\":29841},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":26169126},\"end\":31101,\"start\":30749}]", "bib_title": "[{\"end\":23491,\"start\":23367},{\"end\":25810,\"start\":25730},{\"end\":27131,\"start\":27027},{\"end\":27963,\"start\":27882},{\"end\":28230,\"start\":28200},{\"end\":28526,\"start\":28459},{\"end\":28897,\"start\":28805},{\"end\":29948,\"start\":29841},{\"end\":30864,\"start\":30749}]", "bib_author": "[{\"end\":23527,\"start\":23493},{\"end\":23549,\"start\":23527},{\"end\":23571,\"start\":23549},{\"end\":23593,\"start\":23571},{\"end\":23625,\"start\":23593},{\"end\":23653,\"start\":23625},{\"end\":23672,\"start\":23653},{\"end\":23682,\"start\":23672},{\"end\":23696,\"start\":23682},{\"end\":23721,\"start\":23696},{\"end\":23732,\"start\":23721},{\"end\":23739,\"start\":23732},{\"end\":23749,\"start\":23739},{\"end\":23758,\"start\":23749},{\"end\":23769,\"start\":23758},{\"end\":23779,\"start\":23769},{\"end\":23790,\"start\":23779},{\"end\":25825,\"start\":25812},{\"end\":25838,\"start\":25825},{\"end\":25844,\"start\":25838},{\"end\":26813,\"start\":26809},{\"end\":27137,\"start\":27133},{\"end\":27974,\"start\":27965},{\"end\":28248,\"start\":28232},{\"end\":28912,\"start\":28899},{\"end\":29966,\"start\":29950},{\"end\":29977,\"start\":29966},{\"end\":29989,\"start\":29977},{\"end\":29995,\"start\":29989}]", "bib_venue": "[{\"end\":23910,\"start\":23806},{\"end\":25942,\"start\":25844},{\"end\":26807,\"start\":26719},{\"end\":27186,\"start\":27137},{\"end\":28002,\"start\":27974},{\"end\":28302,\"start\":28263},{\"end\":28589,\"start\":28528},{\"end\":29030,\"start\":28927},{\"end\":30038,\"start\":29995},{\"end\":30906,\"start\":30866}]"}}}, "year": 2023, "month": 12, "day": 17}