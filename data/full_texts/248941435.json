{"id": 248941435, "updated": "2023-07-21 13:28:46.373", "metadata": {"title": "Embodied Multi-Agent Task Planning from Ambiguous Instruction", "authors": "[{\"first\":\"Xinzhu\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Xinghang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Di\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Sinan\",\"last\":\"Tan\",\"middle\":[]},{\"first\":\"Huaping\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Fuchun\",\"last\":\"Sun\",\"middle\":[]}]", "venue": "Robotics: Science and Systems XVIII", "journal": "Robotics: Science and Systems XVIII", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "\u2014In human-robots collaboration scenarios, a human would give robots an instruction that is intuitive for the human himself to accomplish. However, the instruction given to robots is likely ambiguous for them to understand as some information is implicit in the instruction. Therefore, it is necessary for the robots to jointly reason the operation details and perform the embodied multi-agent task planning given the ambiguous instruction. This problem exhibits significant challenges in both language understanding and dynamic task planning with the perception information. In this work, an embodied multi-agent task planning framework is proposed to utilize external knowledge sources and dynamically perceived visual information to resolve the high-level instructions, and dynamically allocate the decomposed tasks to multiple agents. Furthermore, we utilize the semantic information to perform environment perception and generate sub-goals to achieve the navigation motion. This model effectively bridges the difference between the simulation environment and the physical environment, thus it can be simultaneously applied in both simulation and physical scenarios and avoid the notori- ous sim2real problem. Finally, we build a benchmark dataset to validate the embodied multi-agent task planning problem, which includes three types of high-level instructions in which some target objects are implicit in instructions. We perform the evaluation experiments on the simulation platform and in physical scenarios, demonstrating that the proposed model can achieve promising results for multi-agent collaborative tasks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/rss/LiuLGTL022", "doi": "10.15607/rss.2022.xviii.032"}}, "content": {"source": {"pdf_hash": "a2b2ff265ebe112626892703a233203348b3936e", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "c70733b0546ab354f48a36b52046e3326641ca9d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a2b2ff265ebe112626892703a233203348b3936e.txt", "contents": "\nRobotics: Science and Systems Embodied Multi-Agent Task Planning from Ambiguous Instruction\n\n\nNY, USAYork City \nDepartment of Computer Science and Technology\nBNRist\nTsinghua University\nBeijingChina\n\nJune -July \nDepartment of Computer Science and Technology\nBNRist\nTsinghua University\nBeijingChina\n\nXinzhu Liu \nDepartment of Computer Science and Technology\nBNRist\nTsinghua University\nBeijingChina\n\nXinghang Li \nDepartment of Computer Science and Technology\nBNRist\nTsinghua University\nBeijingChina\n\nDi Guo \nDepartment of Computer Science and Technology\nBNRist\nTsinghua University\nBeijingChina\n\nSinan Tan \nDepartment of Computer Science and Technology\nBNRist\nTsinghua University\nBeijingChina\n\nHuaping Liu \nDepartment of Computer Science and Technology\nBNRist\nTsinghua University\nBeijingChina\n\nFuchun Sun \nDepartment of Computer Science and Technology\nBNRist\nTsinghua University\nBeijingChina\n\nRobotics: Science and Systems Embodied Multi-Agent Task Planning from Ambiguous Instruction\n\nIn human-robots collaboration scenarios, a human would give robots an instruction that is intuitive for the human himself to accomplish. However, the instruction given to robots is likely ambiguous for them to understand as some information is implicit in the instruction. Therefore, it is necessary for the robots to jointly reason the operation details and perform the embodied multi-agent task planning given the ambiguous instruction. This problem exhibits significant challenges in both language understanding and dynamic task planning with the perception information. In this work, an embodied multi-agent task planning framework is proposed to utilize external knowledge sources and dynamically perceived visual information to resolve the high-level instructions, and dynamically allocate the decomposed tasks to multiple agents. Furthermore, we utilize the semantic information to perform environment perception and generate sub-goals to achieve the navigation motion. This model effectively bridges the difference between the simulation environment and the physical environment, thus it can be simultaneously applied in both simulation and physical scenarios and avoid the notorious sim2real problem. Finally, we build a benchmark dataset to validate the embodied multi-agent task planning problem, which includes three types of high-level instructions in which some target objects are implicit in instructions. We perform the evaluation experiments on the simulation platform and in physical scenarios, demonstrating that the proposed model can achieve promising results for multi-agent collaborative tasks.\n\nI. INTRODUCTION\n\nIn real life, a group leader may release an ambiguous instruction, which contains his intention but lacks the implementation details. Nevertheless, intelligent group members may analyze the instruction to extract the intention and utilize their knowledge or shared-mental-mind with the leader to execute the necessary operational details to accomplish the task. Such a collaboration mechanism is also highly expected for humanrobot collaboration. For example, a human would give robots an instruction in which the process of completing the task is obvious to the human himself. However, the overall instruction given to robots is likely ambiguous for them to understand as some information is implicit in the instruction, such as \"Put the book and newspaper away\". Although human knows that the book and newspaper are most likely to be put on the bookshelf, or the drawer if there is no bookshelf found, robots may not know where to put the book and newspaper directly from the instruction, let alone collaborating to complete this task. Therefore, it is necessary for the robots to jointly reason the operation details and perform the embodied multi-agent task planning given the ambiguous instruction (Fig. 1). Fig. 1. An overview of the embodied multi-agent task planning from ambiguous instruction. Given a high-level instruction, several sub-tasks are generated and allocated to a group of agents. The agents explore the environment and implement the sub-tasks. With the change of the visual observation during the exploration process, the task decomposition and task allocation processes are also adjusted dynamically.\n\nIn the multi-agent task planning scenario, a complex task can be decomposed in multiple possible ways, and the decomposed sub-tasks are allocated to multiple agents for the execution [24]. Therefore, the task planning includes task decomposition, which focuses on the problem of what to do [21], task allocation which focuses on the problem of who does what [3], and task scheduling which focuses on the problem of how to arrange tasks in time [43]. Among them, the task decomposition is the problem of decomposing a complex task into simpler ones, down to the level of actionable tasks [21,28]; task allocation is the problem of determining which robot should execute which task in order to achieve the overall system goal [3], and task scheduling is the problem of sequencing tasks for execution [17]. The above problems have been extensively investigated in diversified works of literature, most of which transform the given task into a well-defined optimization problem that requires a clear, structured, and complete instruction [2,16]. In practical scenarios, task planning is highly coupled with the human-robot interaction and the perception of the environment. It should be dynamically adjusted due to the vagueness of the interaction and dynamics of the environment. In this work, we formulate such a problem as embodied multi-agent task planning from ambiguous instruction, which exhibits the following key challenges: 1) Ambiguous Instruction Due to the incompleteness and ambiguity of the given instruction, it is necessary to use external knowledge sources (such as domain knowledge in a specific field, knowledge graph, and industry rules) to reason and clarify the instruction. Based on the clarified instruction, combined with the characteristics of the robot, the given task is required to be decomposed into specific sub-tasks that the robot can perform. For example, the task \"Put the book and the newspaper away\" should be clarified to be \"Put the book and the newspaper on the bookshelf\" with the visual perceptions of robots and the knowledge that books and newspapers are always placed on the bookshelf. Then it should be initially decomposed into the sub-tasks of \"Find the book\", \"Find the newspaper\" and \"Find the bookshelf\".\n\n2) Dynamic Task Decomposition Since the initial visual perceptions of multiple robots are limited, the reasoning information based on the initial state may not be correct. Therefore, it is necessary to dynamically adjust the instruction reasoning and task decomposition with updated visual perceptions during the continuous execution process of agents. For example, the task \"Put the book and the newspaper away\" is clarified to be \"Put the book and the newspaper on the bookshelf\" with the initial visual perceptions. After several steps of exploration, agents find that there is no bookshelf but a drawer in the current scene based on their newly obtained visual perceptions. The book and newspaper can also be put in the drawer. Then agents need to re-reason the implicit information in the given instruction and obtain the new clarified instruction \"Put the book and the newspaper in the drawer\". Afterward, the subsequent decomposition and allocation processes are performed based on the clarified results.\n\n3) Dynamic Task Allocation Based on the specific decomposed sub-tasks, the sub-tasks need to be allocated to multiple robots considering the robots' perception and motion abilities so that each robot is assigned to a corresponding sub-task. More importantly, in the specific execution process, because of the ambiguity of instructions and the dynamic nature of the environment, robots are required to dynamically adjust their allocated sub-tasks according to the environment perception information. For example, the three decomposed tasks \"Find the book\", \"Find the newspaper\" and \"Find the bookshelf\" are allocated to Agent 1, Agent 2 and Agent 3 respectively based on their initial visual environment information. After several steps, if Agent 1 finds that it is actually closer to the bookshelf based on its obtained observations, they need to reallocate the sub-tasks and Agent 1 would change to perform \"Find the bookshelf\", and Agent 2 would change to \"Find the book\" accordingly.\n\nTo tackle the above issues, we propose an embodied multi-agent task planning framework demonstrated in Fig.1 which utilizes external knowledge sources, and dynamically perceives environment information to parse the high-level ambiguous instructions, dynamically allocates the decomposed sub-tasks and completes the distributed navigation tasks. In this framework, multiple agents are able to leverage the advantages of their embodiment attribute to dynamically and automatically adjust the instruction parsing results and efficiently complete the task. The main contributions are summarized as follows:\n\n1) Multi-agent embodied task planning framework: A multi-agent task planning framework is proposed to solve the multi-agent collaborative mission, which utilizes external knowledge sources, and dynamically perceived visual information to resolve the high-level instructions, and dynamically allocates the decomposed tasks.\n\n2) Sim&Real learning method for embodied task planning: A dynamic task allocation model is developed based on multi-agent collaboration. We utilize the semantic information to perform the environment perception and generate sub-goals to achieve navigation motion. This model effectively bridges the difference between the simulation environment and the physical environment, thus it can be simultaneously applied in both simulation and physical scenarios and avoid the notorious sim2real problem.\n\n3) Evaluation and validation: We build a benchmark dataset to validate the embodied multi-agent task planning problem, which includes three types of high-level instructions in which some target objects are implicit. We perform the evaluation experiments both in the AI2-THOR [22] platform and physical scenarios including Easy and Hard settings, which demonstrate that the proposed model can achieve promising results for multi-agent collaborative tasks.\n\n\nII. RELATED WORK\n\n\nA. Embodied Multi-Agent Collaboration\n\nRecently, embodied intelligence tasks which integrate perception and action (sometimes language) to perform navigation [41,25], exploration [7], object search [39], question answering [12], remote embodied visual referring expression [29], task completion with language instructions [4] etc., are progressively proposed and accelerate the fusion of communities of machine learning, computer vision and robotics. Most of them are validated in simulation environment and some realworld experiments also emerges [1].\n\nOn the other hand, in multi-agent collaboration scenarios, the complicated collaborative mission, in which multiple agents are required to decompose a specific task and execute the sub-tasks in a distributed manner, has attracted a surge of attentions in domains of search and rescue [38,18], exploration [10,40], as well as industrial manufacturing [15]. Furthermore, multi-agent systems also witness remarkable progress in planning, perception, localization, and control [36].\n\nInspired by the success of the multi-agent system, there have been some researches on multi-agent embodied tasks in visual simulation environments. FurnLift [19] and FurnMove [20] tasks have been proposed to learn a multi-agent decision policy for two agents to collaboratively lift or move large objects at the same time. A centralized 3D reconstruction method is developed to solve the multi-agent question answering task, in which multiple agents explore the scene jointly to answer the given question [34]. Modified multi-agent reinforcement learning and memory-augmented communication module are \u2713 HLSM-ALFRED [4] \u2713 \u2713 Virtualhome [28] \u2713 \u2713 VGP-WV [21] \u2713 \u2713 ProScript [30] \u2713\n\u2713 LMCR [8] \u2713 \u2713 \u2713 MA-EQA [34] \u2713 \u2713 FurnMove [20] \u2713 \u2713 CollaVN [37] \u2713 \u2713 Ours \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nutilized to solve the multi-agent visual navigation task [37]. A centralized spatial coordination planner is developed to solve the multi-agent exploration tasks to improve the exploration efficiency [40]. In all multi-agent tasks mentioned above, the task each agent needs to complete is determined without the process of dynamic decomposition and allocation. In our work, the given task is required to be dynamically decomposed and assigned according to the visual perception of each agent. More importantly, our work provides a promising method to bridge the difference between the simulation and real-world.\n\n\nB. Dynamic Task Decomposition and Task Allocation\n\nIn terms of task decomposition, the given task is required to decompose into executable sub-tasks [32,27].\n\nParticularly, in some tasks with instructions as input, the language feature is utilized to decompose the task. A commonsense instruction reasoning approach is proposed to help the agent to reason the missing information in the instruction with the environment observation and complete the instruction [8]. Pre-trained language models are utilized to decompose the abstract instruction into sub-tasks [21] or sub-programs [28] and predict their temporal orders [30] with language instructions.\n\nIn terms of task allocation, multiple tasks need to be allocated to multiple agents to obtain maximum benefit. Negotiation-based methods including time-constraint negotiation [23] and game theory-based negotiation [11] are classical methods to solve the task allocation problem. Auction-based algorithm [5], consensus-based bundle algorithm [42] as well as cross-entropy temporal logic optimization [3] have also been proposed to solve this problem. Besides, dynamic task allocation extends the general task allocation task to the dynamic environment setting, and the allocation results need to be modified with the change of the environment. The policy search algorithm of reinforcement learning is used to study the autonomous task sequencing challenge, in which the agent swarm can sequence tasks whose execution orders are unknown [17]. Stochastic conflict-based allocation model [9] and adaptive allocation approach for heterogeneous agents [13,14] are proposed to solve the challenge of dynamic multi-agent task assignment under environment uncertainty and temporal constraints. All the above methods regard task allocation as an optimization problem that do not consider the impact of the visual perception of different agents. Our work is different from other relevant works that have language instructions as input, and the comparison between these works is illustrated in Table I.\n\n\nIII. PROBLEM FORMULATION\n\nIn this work, we focus on multiple embodied agents which aim to decompose the task indicated by the high-level ambiguous instruction into several sub-tasks, collaboratively allocate sub-tasks and cooperate to complete the entire task with egocentric visual observations.\n\nConcretely speaking, we consider embodied agents {D (1) , D (2) , \u00b7 \u00b7 \u00b7 , D (N ) }, where N is the number of agents. All of the agents share an action space A, from which each agent can take an action to perform at each time instant. The state representation of the i-th agent is represented S\n(i) t .\nGiven a high-level language instruction L, which contains the main intention of the person but lacks necessary operation details to ensure the task to be performed, what we hope to address is to analyze the intention embedding in the instruction, complete the operation details using external knowledge source, and perform the task planning for practice execution. So, we should first transform the original instruction L as structural representation P, which may contain incomplete or missing items due to the ambiguity of the instruction L.\n\nTherefore, at each time instant t, we have to resort to the available external knowledge source K and the agents' perception information to complete the necessary operation details and decompose the whole task into sub-tasks T t = {T\n(1) t , T (2) t , \u00b7 \u00b7 \u00b7 , T (K) t } with temporal ordering constrains, where T (k) t\ndenotes a specific sub-task, and K is the number of sub-tasks. This procedure can be represented as\nT t = TD(P, S t ) where S t = {S (1) t , S (2) t , \u00b7 \u00b7 \u00b7 , S (N ) t\n} are the collected state information from the agents. Obviously, the above representations formulate a perception-aware task decomposition problem, and it results in a list of sub-tasks that should be performed. The next step is to allocate them to the embodied agents. The task allocation results can be represented as\nV t = {V (1) t , V (2) t , \u00b7 \u00b7 \u00b7 , V (N ) t }, which can be derived as V t = TA(T t , S t ).\nOnce the goal of each agent at time t is determined, a multiagent search strategy can be used to perform as\na (i) t = \u03c0 (i) (S (i) t , S (i \u2212 ) t , V (i) t )(1)for i = 1, 2, \u00b7 \u00b7 \u00b7 , N , where S (i \u2212 ) t\nis the state information the other agents which can be transferred via communication.\n\nAccording to Eq.(1), the main concern in this work is essentially a multi-agent visual semantic search problem. Different from existing VSN work such as [37,26], in our work, the goal of each agent V (i) t is time-varying, and it is determined from an ambiguous instruction, which makes this problem more challenging.\n\n\nIV. DATASET CONSTRUCTION\n\nTo evaluate the embodied task planning in the general environment is extremely difficult, and a specific scenario benchmark simulation dataset is desired to validate the proposed methods. In this work, we restrict ourselves to deal with the ambiguous instructions from which we may infer the receptacles for some objects. We use such practical scenarios to construct a benchmark dataset.\n\nThere exist lots of simulation environments such as Mat-terport3D [6] and Habitat [31] simulator. Though existing datasets in these simulators could provide photo-realistic visual perception for the navigation, none of them supports manipulating actions that are required to evaluate the embodied multi-agent task planning from ambiguous instruction. To develop a benchmark, we resort to ALFRED [33] dataset in AI2-THOR simulator [22]. ALFRED is a recently developed benchmark for interpreting grounded instructions for everyday tasks. Based on ALFRED dataset, reasonable and abstract high-level instructions can be generated in which the target object is implicit and need to be inferred. However, ALFRED does not provide ambiguous instructions, and it cannot be directly used for multi-agent exploration.\n\nIn this work, we use the instructions in ALFRED to extract necessary information and construct a new benchmark with AI2-THOR, which supports multi-agent collaboration. Concretely speaking, we extract five fundamental types of behaviors heat, cool, clean, put, and throw in the high-level instructions. In addition, we extract the relationships between object properties and behaviors in ALFRED, such as bread can be heated and book cannot be heated or cleaned. We also preserve the placement constraints in ALFRED between small objects and receptacles, which imposes the constraints on whether a certain type of objects could be placed on or inside another type of receptacle objects. For example, the reasonable receptacles of book can be bookshelf, desk and countertop rather than garbage can. This provides solutions to design reasonable ambiguous instructions.\n\nThen, we use the AI2-THOR environment to design our tasks. The constructed dataset consists of three types of multiagent tasks indicated by high-level instructions, in which the target objects and operation information may be implicit and need to be reasoned during the process of task execution. As illustrated in Fig.2, the three types of tasks are generated based on the fundamental actions, object properties, and placement constraints. When generating high-level instructions for each type of tasks, we select each type of action, preserve objects meeting object properties constraints for the specific action, and obtain the most likely reasonable receptacles for the specific pairs of action and object to form the triple (action, object, receptacle). In particular, we sort the reasonable receptacles in a descending order according to the frequency of the triples with the specific action and object pair appeared in ALFRED instructions. For the fixed action and object, the candidate triples are utilized to generate the high-level instructions.\n\nFor the task Type I, the high-level instructions contain one action and one object. Instructions are generated directly based on obtained triplets as well as several language rules, and implicit receptacles do not appear explicitly in instructions. For instance, in the instruction \"Heat the bread\", the receptacle microwave needs to be reasoned.\n\nFor the task Type II, the high-level instructions include one action and two objects that can be executed by that action. We select two objects with a common receptacle for a specific action from candidate triples and generate high-level instructions in a similar way, such as \"Heat the tomato and the bread\".\n\nFor the task Type III, the high-level instructions contain two actions and two objects, in which two actions have a temporal order, and the object processed by the first action is also the target object of the second action put. After the first action is completed, the operated object needs to be placed on another receptacle, so we filter to get receptacles of the second action according to the placement constraints. The instructions are generated using \"and\" to connect two action instructions and the receptacle of the first action is implicit. For instance, in the instruction \"Heat the bread and put it on the countertop\", the bread needs to be put on the counterTop after heated in the microwave.\n\nIt is noted that the receptacles are implicit in all three types of tasks, and they need to be inferred when the task is processed. Meanwhile, all three types of tasks can be decomposed into sub-tasks that are executed by multiple agents.\n\nTo facilitate evaluating the dynamic task allocation strategy, we utilize the object mask to perform data argumentation. For the sorted receptacles, we mask out the top three objects in the scene respectively and select another object of the highest frequency to form new triples and generate new tasks of the three types. The correct inferred receptacles are different from the task without masked objects, but the generated high-level instructions are the same since the receptacles are implicit in instructions. When the agent cannot see the top 1 masked receptacle during the task process, it needs to dynamically adjust the inferred results to find other possible receptacles. For example, in the instruction \"Put the book away\", the most likely receptacle Drawer is masked, the agent needs to re-reason the receptacle to be Cabinet after finding no Drawer in the scene. Meanwhile, it is noted that to make the generated dataset more natural and flexible, we extend the original dataset to a larger one, in which we utilize natural language processing methods to generate richer daily activity instructions through synonym generation, syntax analysis, and sentence pattern conversion.\n\nThe expert demonstrations are also generated, which consist of the task decomposition results, sub-task allocation results, and actions to be taken at each step for every agent. The task decomposition results are generated with specific language rules and the ground-truth task triples. We consider the situation where three agents are in the environment, and the locations of the three agents are initialized under the constrains that the distance between multiple agents is greater than 1m and the distance between agents and the same target object is also greater than 1m, ensuring allocation results reasonable. The allocation results are generated with the task decomposition results and agents' initialization locations. The navigation actions are generated by the shortest path algorithm in the sub-goal level with the rotation.\n\nFrom the above description, we can see that though the developed benchmark is dependent on AI2-THOR, it is significantly different with ALFRED. Finally, We divide the full dataset, including 120 scenes, into three non-overlapping splits, namely Train, Validation, and Test. Train split contains 80 scenes, while Validation and Test split contain 20 unseen scenes each. The corresponding information is shown in Table  II. In addition, more details of the dataset is presented in the Appendix A and we will release this dataset soon.\n\n\nV. MODELS\n\nThe Embodied Multi-Agent Task planning requires the agents to reason a set of sub-tasks from the high-level instruction and to dynamically correct and re-allocate the subtasks in the exploring process. Considering these challenges, we propose a hierarchical multi-agent framework to solve the task. Our framework consists of four essential modules: scene encoder module, task planning module which concludes the task decomposition and task allocation, action module and communication module, as is shown in Fig. 3.\n\n\nA. Scene Encoder\n\nThe scene encoder module firstly takes the RGB and depth observations as input to obtain the semantic map, and a pretrained method [35] is utilized to generate the semantic map feature vector. The Swin Transformer is used to obtain the semantic point cloud from the agent's current RGB and depth observations. Then we voxelize the semantic point cloud with a voxel size of 0.125m \u00d7 0.125m and a semantic map of C \u00d7 M \u00d7 M is obtained, where C indicates the number of object types, M denotes the length of the semantic map. We maintain the egocentric semantic map that is within four meters around the agent, so the size of i-th agent's semantic map mp  Then, a scene encoder ScEr is trained with a pre-training method to generate the feature vector sm t . The feature vector is supposed to retain the region information in the semantic map. The scene encoder is composed of a series of convolution layers. To train the scene encoder, we give a query to see if a specific type of object exists in a sub-region of the map. The scene encoder is trained to answer this query correctly to get the reasonable semantic map feature vector. With the pre-trained scene encoder ScEr, we could have the semantic map feature vectors as sm\n(i) t = ScEr(M erge(mp (i) t\u22121 , lp (i) t ))\n, which are the input for the task planning and action modules.\n\n\nB. Task Planning\n\nThe task planning module is composed of task decomposition part and task allocation part. The task decomposition part takes the high-level instruction and the semantic map feature vectors as input for receptacle reasoning and sub-task sequences generation. The task allocation part allocates the sub-task sequence to each agent conditioned on its semantic map information.\n\n1) Task decomposition: Given a high-level instruction, the task decomposition part firstly extracts the key actions and objects in the instruction, and the implicit receptacles in the instruction are predicted based on the semantic map feature vectors. At step t, we concatenate the encoding of the action and objects from the instruction to generate the task embedding k t . And then the task embedding k t and the current semantic map feature vectors sm (i) t , i = 1, 2, \u00b7 \u00b7 \u00b7 , N from all agents are fed into an attention layer and the attention attd t is obtained. We use a LSTM network to predict the implicit receptacle as recp t = F d (LST M (k t , attd t )), where F d denotes the receptacle prediction network. Having the receptacle recp t , the given incomplete instruction can be completed. Then, the decomposed sub-tasks as well as their corresponding temporal order are generated based on the type of the instruction.\n\n2) Task allocation: The task allocation part predicts the distance from the agents' current location to target objects in the scene given its egocentric semantic map feature vector sm (i) t . At step t, given the embedding of the target objects o t , we firstly calculate the attention atta from the agent's current location to target objects is predicted as follows,\ndis (i) t = DisP (sm (i) t , LST M (atta (i) t ), o t ) dis (i) t = {dis (i) t (1), dis (i) t (2), \u00b7 \u00b7 \u00b7 , dis (i) t (H)}(2)\nwhere H is the number of target objects, DisP denotes the distance predictor which consists of linear layers. After the distances are predicted, the task allocation part utilizes the method similar to the bipartite graph matching algorithm to allocate each sub-task to each agent based on their predicted distances so that the total length executed when multiple agents complete the task is the smallest. And the result of task allocation is denoted as V = F a (dis (1) t , dis (2) t , \u00b7 \u00b7 \u00b7 , dis\n(N ) t ),\nwhere F a denotes the task allocation model. The task allocation method requires that each sub-task is assigned to at least one agent for execution.\n\nIt is noted that the task decomposition and task allocation results can be adjusted dynamically. After each step t, new visual observations can be obtained after the agents execute actions based on their assigned sub-tasks. Then with their updated semantic map feature vectors, the task planning module will also be updated correspondingly.\n\n\nC. Communication\n\nHaving the sub-tasks allocated, the communication module exchanges the information among agents. The agent can utilize the message from other agents to facilitate itself for the following interaction module.\n\nFor each agent D (i) , i = 1, 2, ..., N , we utilize the triple (action, object, receptacle) to represent the corresponding sub-task for each agent, and \"blank\" is used to denote a vacant item in the sub-task, such as (f ind, bread, blank) for the subtask \"Find the bread\". Then we encode the triple to get the sub-task embedding s (i) t assigned to agent D i at step t. The sub-task s       is obtained. If we view the multi-agents as a sequence of agents, then the communication between each pair of agents is similar to self-attention mechanism, which is widely used in transformerbased methods. Therefore, we use the multi-head attention layer for the information communication, which takes the fused feature f (i) t from multiple agents as input, and calculates the attention of each agent's state information on other agents' states separately to generate the communication embedding as\n(c (1) t , c (2) t \u00b7 \u00b7 \u00b7 , c (N ) t ) = F mha (f (i) t , f (2) t \u00b7 \u00b7 \u00b7 , f (N ) t )\nwhere F mha denotes the multi-head attention network. Through attention mechanism, each agent can selectively utilize the beneficial information from others for the following action module. Meanwhile, we modify the traditional multihead attention structure with the Softmax activation function to make the communication module more convenient to extend to the multi-agent system with a larger number of agents.\n\n\nD. Action Module\n\nThe action module leverages the communication information, semantic information, sub-task information and previous sub-goal to predict the next sub-goal for the agent. After obtaining the sub-goal, the low-level action generation policy is employed to generate the navigation actions for the agent to execute in the environment.\n\nWe train a sub-goal predictor F p to generate the next subgoal a (i) t , which includes the egocentric movement distance in the x and y axis, namely \u2206x \n(i) t . Given f (i) t , s (i) t , a (i) t\u22121 , atts (i) t and c i t , the next sub-goal a (i) t = F p (LST M (f (i) t , s (i) t , a (i) t\u22121 , atts (i) t , c (i) t ) is generated.\nIf the agent performs \"stop\" action, the model judges whether the target object is found. If it is found, a specific manipulation action such as \"Pick\" is performed and the current sub-task is switched to the next one. If the current sub-task is the last one, then the entire task is completed.\n\nTo navigate to the sub-goal, in the simulation environments, a low-level action strategy based on the shortest path algorithm is utilized to make agents navigate from the current sub-goal to the next sub-goal. In real-world experiments, agents can navigate from the current location to the predicted sub-goal based on their own localization, obstacle avoidance, and path planning system. In this way, we can bridge the difference between the simulation and real-word to the most.\n\n\nVI. PERFORMANCE VALIDATION\n\n\nA. Experiment Settings\n\nTo comprehensively evaluate the proposed method, we design two settings Easy and Hard using the developed dataset. The main difference lies in the layout of the room.\n\n1) Easy: All of the target objects and agents are in one single room. In this setting, multiple agents are initialized in random positions.\n\n2) Hard: The target objects and multiple agents are distributed in different rooms, and some agents need to go from one room to another to complete the specified task. This is an extension to AI2-THOR environment. In this setting, multiple target objects are distributed in different rooms and multiple agents are initialized in random rooms.\n\nWe train the models only in Easy setting and evaluate the performance in both Easy and Hard settings to assess the generalization ability of models.\n\n\nB. Evaluation Metrics\n\nWe utilize two metrics to compare the performance of all models: Success Rate (SR), Success weight by Path Length (SPL). If all sub-tasks are completed successfully under the temporal order constrains, the given task is considered successful. SR is the ratio of successful tasks, which is defined as SR = 1\nN task N task i=1 R i where R i = 1\nif the i-th task is successful, otherwise R i = 0, and N task is the number of tasks. SPL is denoted as SP L = 1\nN task N task i=1 R i Li max(Di,Li) ,\nwhere L i denotes the minimum number of steps for multiagent to complete the task, and D i is the actual steps.\n\n\nC. Comparison of Task Planning\n\nWe conduct experiments to evaluate the effectiveness of our task planning module, including task decomposition and task allocation parts.\n\nIn the task decomposition part, we are interested in the difference between Fixed Task Decomposition (FTD) and Dynamic Task Decomposition (DTD). The former provides task decomposition results directly from the external knowledge base, while DTD will further dynamically adjust the results using the perception information. In addition, to show the performance gap to the upper bound, we also introduce the Oracle Task Decomposition (Oracle-TD) method, which uses the ground truth of the task decomposition.\n\nIn the task allocation part, we first design two baseline methods: For Random method, the agents randomly choose the task allocation for execution. For Init. method, the agents initially use some simple rules which satisfy the allocation constrictions to form the allocation results and keep them unchanged during the running period. We consider two cases of our proposed task allocation method: Fixed Task Allocation (FTA) and Dynamic Task Allocation (DTA): The former calculates the allocation results for only once and the latter dynamically adjust the allocation using the perception information. Finally, we replace the calculation method in Fixed Task Allocation (FTA) and Dynamic Task Allocation (DTA) with the ground truth results to form the ceiling methods Oracle-FTA and Oracle-DTA.\n\nTherefore, there are a total of 3 \u00d7 6 = 18 combinations considering different task decomposition and allocation ways, and we evaluate them in two types of difficulty setting with three types of tasks, respectively. The results are shown in Table III, from which we draw the following conclusions: 1) Dynamic task decomposition significantly performs better than fixed task decomposition: The reason is that FTD simply reasons receptacles based on natural language without considering visual perceptions, while DTD dynamically updates the decomposition results according to agents' perception information. An exception appears in Type III, in which the receptacles required for inference in the type two action & two objects are relatively simple, and can be accurately reasoned  Task Allocation   Task Decomposition  Type I  Type II  Type III  Oracle-TD  FTD  DTD  Oracle-TD  FTD  DTD  Oracle-TD  FTD  DTD     directly based on fixed decomposition. Therefore, the success rates of different decomposition models in this type of tasks are the same. We also observe that the oracle decomposition method Oracle-TD performs best in each type of tasks because it uses the true decomposition results for the subsequent process. This reflects that the task decomposition significantly affect the subsequent task allocation and navigation process, so it has great influence on the success rate of the whole tasks. The results also show that there still exists gap between Oracle-TD and our DTD, which means the embodied task decomposition is challenging and still has large space for further research.\n\n2) Dynamic task allocation significantly performs better than fixed task allocation: It is found that the dynamic allocation methods always have advantages over the specific fixed ones. Due to the mutual influence of task allocation and navigation, agents perform navigation based on the allocated subtasks, and agents would obtain new distance prediction results with the movement and new visual observations. Dynamic allocation methods can dynamically update the allocation results based on new perceptions at each step, which is more reasonable than the fixed strategy.\n\n3) The embodied task planning achieves the best results except Oracle: Agents have less visual perceptions about the scene at the beginning of the task, then the receptacle reasoning and distance prediction become more accurate with the continuous interaction and perception with the scene, which can generate more accurate allocation results and improve the accuracy of navigation and task completion. The results demonstrate the importance and effectiveness of the embodied task planning module.\n\n\nD. Qualitative Results\n\nWe demonstrate the successful sample of multi-agent task planning in hard settings, which is shown in Fig.4. Agents perform \"Clean the plate and put it on the desk\" in different rooms. At 3-th step, Agent 1 and Agent 3 dynamically adjust the task allocation results with the updated perceptions of the scene. After Agent 2 finishing its sub-tasks Clean the plate in the sink, it switches to Put the plate on the desk, goes into the bedroom with the help of the effective information from Agent 3, and finally completes the task successfully. This successful sample demonstrates that the embodied dynamic task allocation is important and effective in the multi-agent task planning process. Some more success and failure examples are illustrated in Appendix C (in simulator) and D (in real scenario). \n\n\nE. Multi-Agent v.s. Single-Agent\n\nBefore closing this section, we present results to show the advantages of utilizing multi-agent in these scenarios, and therefore further verify the roles of the embodied task planning since it is only useful for multi-agent. To this end, we compare the results of our multi-agent task planning framework in three types of tasks with the following methods.\n\nFor single-agent cases, we consider three methods. The most typical one is VSN-SP in which scene prior is incorporated into this model except for the RGB observations and the previous action as input [39]. This method reduces to VSN if we remove the scene prior module. Further, we set the number of the agent   to one in our method and form the method Single-Agent which does not have the task planning and communication module. The core difference between Single-Agent and VSN-SP, VSN is that the former uses the sub-goal planning while the latter ones use the low action generation policy.\n\nFor multi-agent cases, we consider two methods. The first one is named Multi-Agent w/o com, which does not have the communication module for the agents to exchange information during the collaborative navigation, and its rest is the same as our multi-agent framework. The other one Multi-Agent is our standard and complete version.\n\nTo perform a fair comparison between multi-agent and single-agent baselines, we use the oracle task decomposition and allocation results, and the SPL is calculated based on the low-level actions. The comparison results are shown in Table  IV, from which we make the following observations: 1) Even for single-agent case, the proposed method Single-Agent performs better than VSN and VSN-SP. This illustrates the roles of the semantic map coding for the visual perception and sub-goal planning for the action generation.\n\n2) The Multi-Agent method performs much better than\n\nSingle-Agent methods. This shows the advantages of the multiagent in such a scenario and the roles of the task planning.\n\n3) Type III has higher requirements for collaboration, and our model has significant improvement compared with the model without communication. The comparison results between Multi-Agent and Multi-Agent w/o Com. demonstrate the effectiveness of the communication module.\n\n\nVII. REAL-WORLD EXPERIMENTS\n\nAs we claim, the proposed learning method reduces the gap between simulation and real-world by using the semantic encoder to represent the visual perception and the sub-goal to facilitates navigation. To verify this point, we establish a realworld scenario in a practical office building, which is also an unseen environment but the navigation map is available. This scenario consists of Room1(30m 2 ), Room2(100m 2 ) and the corridor(35m long) connecting them. Following similar guidelines, we design the Easy(single room) and Hard settings(cross room), which are shown in Fig.5. We deploy three customized mobile robots as the embodied agents. Since we use the subgoal navigation policy, the developed method can be directly transferred to the real robots. However, it is noted that our robotic platform does not have manipulator and therefore we assume that the object is virtually manipulated when it is found by the object detector, which is consistent with the setting in [29].\n\nIn Fig.6 we show the results for Hard setting. We also show the qualitative result of the Easy setting in Appendix D. The results of both tasks in detail are shown in the attached video. From the evaluation experiments in the real-world scenes, it is found that the encoded semantic map and the sub-goal prediction can help shrink the gap in visual and action domains between simulation and real-world.\n\n\nVIII. CONCLUSIONS\n\nIn this paper, we establish the benchmark and develop methods for the challenging problem of embodied multiagent task planing from ambiguous instructions. An important merit of this work is that the multiple agents could jointly reason the implicit meaning in the ambiguous instruction and perform the autonomous task planning for navigation. The proposed method has been validated in the simulation and real-world environments and the experimental results verify that the dynamic adjustment for the task decomposition and task allocation is vital for embodied agents.\n\nDue to the limitation of the simulator, we can only investigate this problem in restricted scenarios. In the future work, we would like to extend this work to more complicated real scenarios and further improve the multi-agent collaboration capability of dealing with the ambiguous instruction.    In Fig. C.2, we show a failure case, in which the instruction is \"Clean the tomato and put it on the coffee table\". Agent 1 and Agent 2 are in the living room and Agent 3 is in the kitchen. The optimal task allocation result is that Agent 3 washes the tomato in the sink and brings it to the living room, but in the inference process, Agent 3 chooses to find the sink at the beginning resulting in its early stop. The sub-task assigned to Agent 1 is \"find the tomato\", and only when it reaches the maximum step limit in living room can it realize to go to the kitchen to find the tomato because of the lack of perception information in the kitchen causing by the early stop of Agent 3. Agent 1 needs to wash the tomato in the kitchen and then back to living room to put it on the coffee table, but it exceeds the maximum task steps, resulting in the task failure. The cases indicate that in Hard setting, the task allocation results at the first step are important for the task completion. If the allocation is reasonable at first, the overall task is more likely to be completed in fewer steps.\n\n\nD Qualitative Results in Real World\n\nWe show the qualitative performance in the Easy setting in the real-world scenario in the same room in Fig. D.1. Agents perform the task \"Freeze the bottle of water\". Agent 3 finds the bottle of water, then finds the fridge with the help of communication information from Agent 1 and Agent 2, and finally completes the task successfully.\n\n\nC \u00d7 64 \u00d7 64 at step t. Specifically, for step t, the i \u2212 th agent's semantic map mp (i) t is generated by merging its current local semantic map lp (i) t and the semantic map mp\n\nFig. 3 .\n3An overview of the hierarchical multi-agent framework for the embodied multi-agent task planning.\n\n\nthe agent's semantic map feature vector sm (i) t are firstly fed into an attention layer and the attention atts\n\n\n. The action information of the agent is represented by the sub-goal. And the sub-goal of the agent in the previous step t \u2212 1 includes the movement distance in its egocentric x and y axis \u2206x\n\n\nthe previous sub-goal to get the action embedding a\n\n\nand the sub-goal of the agent in the previous step a (i) t\u2212i are fed into a LSTM fusion layer and the fused feature f(i) t\n\n\nand the probability of \"Stop\" action stop\n\nFig. 6 .\n6Qualitative results of the Hard setting in the Real-World scenarios.\n\nFig. 5 .\n5The scenario setup in the Real-World experiments.\n\nFigure C. 1 :\n1Qualitative results of the Easy setting in the simulation.\n\nFigure C. 2 :\n2The failure case in the simulation environment.\n\nFigure D. 1 :\n1Qualitative results of the Easy setting in the real-world scenarios.\n\nTABLE I COMPARISON\nIBETWEEN OUR WORK AND OTHER RELATED WORK WITH LANGUAGE INSTRUCTIONWork \nAmbiguous Instructions Task Decomposition \nDynamic \nTask Allocation \n\nEmbodied \nMulti-Agent \nVision Perception \nReal-World \nExperiments \nS2R-VLN [1] \n\u2713 \n\u2713 \nREVERIE [29] \n\n\nTABLE II\nIIFig. 2. Three types of instructions in our dataset.DATASET SPLITS \n\nTrain \nValidation(Unseen) Test(Unseen) \n#Tasks \n4025 \n728 \n926 \n#Scenes \n80 \n20 \n20 \n#Demonstrations 75260 \n9465 \n22310 \n\n\n\nTABLE III COMPARATIVE\nIIIRESULTS IN TASK PLANNING MODULESettings \n\nTABLE IV COMPARATIVE\nIVRESULTS FOR MULTI-AGENT V.S. SINGLE-AGENTSettings \nMethods \nType I \nType II \nType III \nSR(%) \nSPL(%) \nSR(%) \nSPL(%) \nSR(%) SPL(%) \n\nEasy \n\nVSN \n13.2 \n2.9 \n2.0 \n0.2 \n4.0 \n1.3 \nVSN-SP \n15.0 \n3.0 \n4.0 \n0.5 \n9.5 \n2.5 \nSingle-Agent \n46.3 \n10.4 \n10.1 \n2.3 \n22.0 \n8.7 \nMulti-Agent w/o Com. \n77.8 \n16.0 \n29.8 \n7.0 \n26.5 \n6.0 \nMulti-Agent \n78.2 \n16.2 \n31.3 \n7.5 \n31.0 \n7.2 \n\nHard \n\nVSN \n8.1 \n3.8 \n0.0 \n0.0 \n0.0 \n0.0 \nVSN-SP \n11.0 \n5.7 \n0.0 \n0.0 \n1.5 \n0.2 \nSingle-Agent \n23.9 \n10.6 \n1.3 \n0.8 \n4.5 \n1.6 \nMulti-Agent w/o Com. \n52.7 \n26.4 \n18.7 \n8.0 \n7.5 \n1.9 \nMulti-Agent \n55.0 \n30.7 \n20.0 \n6.7 \n11.9 \n3.1 \n\n\n\n\nFig. 4. Qualitative results of the Hard setting in the Simulation environment.Instruction: bake the apple and put it on the dining tableAgent 1 \n\nAgent 2 \n\nAgent 3 \n\nfind the \nDesk \n\nfind the \nPlate \n\nfind the \nSink \n\nupdate \ntask assign \n\nupdate \ntask assign \n\nInstruction: Clean the plate and put it on the desk \n\nsub-task \nfinished \n\nfind the \nDesk \n\nfound the \nPlate \n\nfind the \nSink \n\nfind the \nDesk \n\nfind the \nSink \n\nswitch \ntask \n\nput the plate \nin the sink \n\nfound the \nDesk \n\nfound the \nSink \n\nput the plate \nin the sink \n\nsub-task \nfinished \n\nsub-task \nfinished \n\nfound the \nDesk \n\nfound the \nSink \n\nput the plate \nin the sink \n\nsub-task \nfinished \n\nsub-task \nfinished \n\nsub-task \nfinished \n\nclean the plate \nin the sink \n\nfound the \nSink \n\nfound the \nDesk \n\nput the plate \non the desk \n\nfound the \nSink \n\nfound the \nDesk \n\nswitch \ntask \n\ninform the room \nof the desk \n\ngoing to the \nbedroom \n\nput the plate \non the desk \n\nfound the \nSink \n\nfound the \nDesk \n\nput the plate \non the desk \n\nfound the \nSink \n\nfound the \nDesk \n\ngo into the \nbedroom \n\nput the plate \non the desk \n\nfound the \nSink \n\nfound the \nDesk \n\nput the plate \non the desk \n\nAgent 1 \n\nAgent 2 \n\nAgent 3 \n\nfind the apple \n\nfind the \nmicrowave \n\nfound the \nbottle \n\nfound the \nmicrowave \n\nfind the dining \ntable \n\nfind the dining \ntable \n\nput the apple in \nthe microwave \n\nfound the \nmicrowave \n\nfind the dining \ntable \n\nswitch \ntask \n\nsub-task \nfinished \n\nsub-task \nfinished \n\nbake the apple in \nthe microwave \n\nfound the \nmicrowave \n\nfound the \ndining table \n\nsub-task \nfinished \n\nput the apple on \nthe dining table \n\nfound the \nmicrowave \n\nfound the \ndining table \n\nswitch \ntask \n\ninform the room of \nthe dining table \n\ngoing to the \nliving room \n\nfound the \nmicrowave \n\nfound the \ndining table \n\nput the apple on \nthe dining table \n\nfound the \nmicrowave \n\nfound the \ndining table \n\nput the apple on \nthe dining table \n\nfound the \nmicrowave \n\nfound the \ndining table \n\nput the apple on \nthe dining table \n\n\n\n\nInstruction: clean the tomato and put it on the coffee tablefound the \nSink \n\nfound the \nTomato \n\nput the tomato \nin the sink \n\nput the plate \non the desk \n\nput the tomato on \nthe coffee table \n\nfound the \nDesk \n\nfind the \nCoffeeTable \n\nfind the \nSink \n\nfind the \nTomato \n\nfound the \nCoffeeTable \n\nsub-task \nfinished \n\nfind the \nSink \n\nfind the \nTomato \n\nsub-task \nfinished \n\nfound the \nCoffeeTable \n\nfind the \nTomato \nfind the \nTomato \n\nfound the \nCoffeeTable \n\nfound the \nSink \n\nfind the \nTomato \n\nmax attempts \nreached \n\ngoing to the \nkitchen \n\nfound the \nCoffeeTable \n\nfound the \nSink \n\ngoing to the \nkitchen \n\nfind the \nTomato \n\nfound the \nCoffeeTable \n\nfound the \nSink \n\nfound the \nCoffeeTable \n\nfound the \nSink \nfound the \nSink \n\nfound the \nCoffeeTable \n\nswitch \ntask \n\nfound the \nSink \n\nfound the \nCoffeeTable \n\nswitch \ntask \n\ninform the room of \nthe coffee table \n\nput the tomato \nin the sink \n\n\nACKNOWLEDGMENTSThis work was supported in part by the National Natural Science Fund for Distinguished Young Scholars under Grant 62025304.Appendix A Dataset DetailsThe detailed information about the three types of tasks in the constructed dataset is shown inB Training MethodologySince both the task planning module and the action module take semantic map feature vector as input, and the training process of the two modules is relatively independent, we train this two module separately. We utilize the supervised learning algorithm to train the task planning module and the imitation learning algorithm to train the action model.The task decomposition part in the task planning module predicts the implicit receptacles, and the task allocation part predicts the distance between the agent to the target objects in the scene. The ground truth of the implicit receptacles can be obtained from the true task triple. The true distance between the agent's location and all objects can be obtained from the AI2-THOR simulator. During the training process, the task allocation part is trained to predict the distance from the agent's location to all objects. During the inference process, the agents only predict the distance to target objects. The loss function of the task planning module is defined as follows, Loss task = \u03b1Loss recep + \u03b2Loss dis , Loss dis = Loss seen + \u03bbLoss unseen(1)where Loss recep denotes the entropy loss between the predicted receptacles and the true receptacles, Loss dis denotes the sum of MSE loss of distance to objects. Loss dis contains the regression loss of the predicted distance to seen objects Loss seen and the regression loss of the distance to unseen objects Loss unseen . \u03b1, \u03b2 and \u03bb are the hyper-parameters to control the training process. In our implementation, we set \u03b1 and \u03b2 to 1 and \u03bb to 0.1.\nSim-to-real transfer for vision-and-language navigation. Peter Anderson, Ayush Shrivastava, Joanne Truong, Arjun Majumdar, Devi Parikh, Dhruv Batra, Stefan Lee, Conference on Robot Learning. PMLRPeter Anderson, Ayush Shrivastava, Joanne Truong, Ar- jun Majumdar, Devi Parikh, Dhruv Batra, and Stefan Lee. Sim-to-real transfer for vision-and-language naviga- tion. In Conference on Robot Learning, pages 671-681. PMLR, 2021.\n\nDecentralized planning and control for uav-ugv cooperative teams. Barbara Arbanas, Antun Ivanovic, Marko Car, Matko Orsag, Tamara Petrovic, Stjepan Bogdan, Autonomous Robots. 428Barbara Arbanas, Antun Ivanovic, Marko Car, Matko Orsag, Tamara Petrovic, and Stjepan Bogdan. Decentral- ized planning and control for uav-ugv cooperative teams. Autonomous Robots, 42(8):1601-1618, 2018.\n\nMulti-agent task allocation using cross-entropy temporal logic optimization. Christopher Banks, Sean Wilson, Samuel Coogan, Magnus Egerstedt, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEEChristopher Banks, Sean Wilson, Samuel Coogan, and Magnus Egerstedt. Multi-agent task allocation using cross-entropy temporal logic optimization. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 7712-7718. IEEE, 2020.\n\nA persistent spatial semantic representation for high-level natural language instruction execution. Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, Yoav Artzi, arXiv:2107.05612arXiv preprintValts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, and Yoav Artzi. A persistent spatial semantic representa- tion for high-level natural language instruction execution. arXiv preprint arXiv:2107.05612, 2021.\n\nGreedy decentralized auction-based task allocation for multi-agent systems. Martin Braquet, Efstathios Bakolas, IFAC-PapersOnLine. 5420Martin Braquet and Efstathios Bakolas. Greedy de- centralized auction-based task allocation for multi-agent systems. IFAC-PapersOnLine, 54(20):675-680, 2021.\n\nAngel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang, arXiv:1709.06158Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprintAngel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017.\n\nLearning to explore using active neural slam. Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, Ruslan Salakhutdinov, arXiv:2004.05155arXiv preprintDevendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, and Ruslan Salakhutdinov. Learning to explore using active neural slam. arXiv preprint arXiv:2004.05155, 2020.\n\nEnabling robots to understand incomplete natural language instructions using commonsense reasoning. Haonan Chen, Hao Tan, Alan Kuntz, Mohit Bansal, Ron Alterovitz, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEEHaonan Chen, Hao Tan, Alan Kuntz, Mohit Bansal, and Ron Alterovitz. Enabling robots to understand incomplete natural language instructions using common- sense reasoning. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 1963-1969. IEEE, 2020.\n\nDynamic multi-robot task allocation under uncertainty and temporal constraints. Shushman Choudhury, K Jayesh, Gupta, J Mykel, Dorsa Kochenderfer, Jeannette Sadigh, Bohg, Autonomous Robots. Shushman Choudhury, Jayesh K Gupta, Mykel J Kochen- derfer, Dorsa Sadigh, and Jeannette Bohg. Dynamic multi-robot task allocation under uncertainty and tempo- ral constraints. Autonomous Robots, pages 1-17, 2021.\n\nDistributed matroidconstrained submodular maximization for multi-robot exploration: Theory and practice. Micah Corah, Nathan Michael, Autonomous Robots. 432Micah Corah and Nathan Michael. Distributed matroid- constrained submodular maximization for multi-robot exploration: Theory and practice. Autonomous Robots, 43(2):485-501, 2019.\n\nGame theory-based negotiation for multiple robots task allocation. Rongxin Cui, Ji Guo, Bo Gao, Robotica. 316Rongxin Cui, Ji Guo, and Bo Gao. Game theory-based negotiation for multiple robots task allocation. Robotica, 31(6):923-934, 2013.\n\nEmbodied question answering. Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAbhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1-10, 2018.\n\nAdaptive task allocation for heterogeneous multi-robot teams with evolving and unknown robot capabilities. Yousef Emam, Siddharth Mayya, Gennaro Notomista, Addison Bohannon, Magnus Egerstedt, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEEYousef Emam, Siddharth Mayya, Gennaro Notomista, Addison Bohannon, and Magnus Egerstedt. Adaptive task allocation for heterogeneous multi-robot teams with evolving and unknown robot capabilities. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 7719-7725. IEEE, 2020.\n\nData-driven adaptive task allocation for heterogeneous multi-robot teams using robust control barrier functions. Yousef Emam, Gennaro Notomista, Paul Glotfelter, Magnus Egerstedt, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEEYousef Emam, Gennaro Notomista, Paul Glotfelter, and Magnus Egerstedt. Data-driven adaptive task allocation for heterogeneous multi-robot teams using robust control barrier functions. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 9124-9130. IEEE, 2021.\n\nSelf-organized adaptive paths in multi-robot manufacturing: reconfigurable and patternindependent fibre deployment. Catriona Eschke, Mary Katherine Heinrich, Mostafa Wahby, Heiko Haman, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEECatriona Eschke, Mary Katherine Heinrich, Mostafa Wahby, and Heiko Haman. Self-organized adaptive paths in multi-robot manufacturing: reconfigurable and pattern- independent fibre deployment. In 2019 IEEE/RSJ Inter- national Conference on Intelligent Robots and Systems (IROS), pages 4086-4091. IEEE, 2019.\n\nDistributed mission planning of complex tasks for heterogeneous multi-robot teams. Tamara Barbara Arbanas Ferreira, Stjepan Petrovi\u0107, Bogdan, arXiv:2109.10106arXiv preprintBarbara Arbanas Ferreira, Tamara Petrovi\u0107, and Stjepan Bogdan. Distributed mission planning of complex tasks for heterogeneous multi-robot teams. arXiv preprint arXiv:2109.10106, 2021.\n\nAutonomous task sequencing in a robot swarm. Lorenzo Garattoni, Mauro Birattari, Science Robotics. 320Lorenzo Garattoni and Mauro Birattari. Autonomous task sequencing in a robot swarm. Science Robotics, 3(20), 2018.\n\nRobot inner attention modeling for task-adaptive teaming of heterogeneous multi robots. Chao Huang, Rui Liu, arXiv:2006.15482arXiv preprintChao Huang and Rui Liu. Robot inner attention modeling for task-adaptive teaming of heterogeneous multi robots. arXiv preprint arXiv:2006.15482, 2020.\n\nTwo body problem: Collaborative visual task completion. Unnat Jain, Luca Weihs, Eric Kolve, Mohammad Rastegari, Svetlana Lazebnik, Ali Farhadi, Alexander G Schwing, Aniruddha Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionUnnat Jain, Luca Weihs, Eric Kolve, Mohammad Raste- gari, Svetlana Lazebnik, Ali Farhadi, Alexander G Schwing, and Aniruddha Kembhavi. Two body problem: Collaborative visual task completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6689-6699, 2019.\n\nA cordial sync: Going beyond marginal policies for multi-agent embodied tasks. Unnat Jain, Luca Weihs, Eric Kolve, Ali Farhadi, Svetlana Lazebnik, Aniruddha Kembhavi, Alexander Schwing, European Conference on Computer Vision. SpringerUnnat Jain, Luca Weihs, Eric Kolve, Ali Farhadi, Svet- lana Lazebnik, Aniruddha Kembhavi, and Alexander Schwing. A cordial sync: Going beyond marginal poli- cies for multi-agent embodied tasks. In European Con- ference on Computer Vision, pages 471-490. Springer, 2020.\n\nVisually-grounded planning without vision: Language models infer detailed plans from highlevel instructions. A Peter, Jansen, arXiv:2009.14259arXiv preprintPeter A Jansen. Visually-grounded planning without vision: Language models infer detailed plans from high- level instructions. arXiv preprint arXiv:2009.14259, 2020.\n\nAi2-thor: An interactive 3d environment for visual ai. Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-Derbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, arXiv:1712.05474arXiv preprintEric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van- derBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474, 2017.\n\nA negotiationbased method for task allocation with time constraints in open grid environments. Yan Kong, Minjie Zhang, Dayong Ye, Concurrency and Computation: Practice and Experience. 273Yan Kong, Minjie Zhang, and Dayong Ye. A negotiation- based method for task allocation with time constraints in open grid environments. Concurrency and Computation: Practice and Experience, 27(3):735-761, 2015.\n\nA comprehensive taxonomy for multi-robot task allocation. Anthony G Ayorkor Korsah, M Bernardine Stentz, Dias, The International Journal of Robotics Research. 3212G Ayorkor Korsah, Anthony Stentz, and M Bernardine Dias. A comprehensive taxonomy for multi-robot task al- location. The International Journal of Robotics Research, 32(12):1495-1512, 2013.\n\nAdvantage of prediction and mental imagery for goal-directed behaviour in agents and robots. Tiffany Jeffrey L Krichmar, Xinyun Hwu, Todd Zou, Hylton, Cognitive Computation and Systems. 11Jeffrey L Krichmar, Tiffany Hwu, Xinyun Zou, and Todd Hylton. Advantage of prediction and mental imagery for goal-directed behaviour in agents and robots. Cognitive Computation and Systems, 1(1):12-19, 2019.\n\nMulti-agent embodied visual semantic navigation with scene prior knowledge. Xinzhu Liu, Di Guo, Huaping Liu, Fuchun Sun, 10.1109/LRA.2022.3145964IEEE Robotics and Automation Letters. Xinzhu Liu, Di Guo, Huaping Liu, and Fuchun Sun. Multi-agent embodied visual semantic navigation with scene prior knowledge. IEEE Robotics and Automa- tion Letters, pages 1-1, 2022. doi: 10.1109/LRA.2022. 3145964.\n\nMulti-robot task and motion planning with subtask dependencies. James Motes, Read Sandstr\u00f6m, Hannah Lee, Shawna Thomas, Nancy M Amato, IEEE Robotics and Automation Letters. 52James Motes, Read Sandstr\u00f6m, Hannah Lee, Shawna Thomas, and Nancy M Amato. Multi-robot task and mo- tion planning with subtask dependencies. IEEE Robotics and Automation Letters, 5(2):3338-3345, 2020.\n\nVirtualhome: Simulating household activities via programs. Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionXavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8494-8502, 2018.\n\nReverie: Remote embodied visual referring expression in real indoor environments. Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, Anton Van Den, Hengel, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, and Anton van den Hengel. Reverie: Remote embodied visual referring expression in real indoor environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9982-9991, 2020.\n\nproscript: Partially ordered scripts generation via pre-trained language models. Keisuke Sakaguchi, Chandra Bhagavatula, Niket Ronan Le Bras, Peter Tandon, Yejin Clark, Choi, arXiv:2104.08251arXiv preprintKeisuke Sakaguchi, Chandra Bhagavatula, Ronan Le Bras, Niket Tandon, Peter Clark, and Yejin Choi. pro- script: Partially ordered scripts generation via pre-trained language models. arXiv preprint arXiv:2104.08251, 2021.\n\nHabitat: A platform for embodied ai research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionManolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9339-9347, 2019.\n\nLearning task decomposition via temporal alignment for control. Kyriacos Shiarlis, Markus Wulfmeier, Sasha Salter, Shimon Whiteson, Ingmar Posner, Taco, International Conference on Machine Learning. PMLRKyriacos Shiarlis, Markus Wulfmeier, Sasha Salter, Shi- mon Whiteson, and Ingmar Posner. Taco: Learning task decomposition via temporal alignment for control. In International Conference on Machine Learning, pages 4654-4663. PMLR, 2018.\n\nAlfred: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10740-10749, 2020.\n\nMulti-agent embodied question answering in interactive environments. Sinan Tan, Weilai Xiang, Huaping Liu, Di Guo, Fuchun Sun, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerProceedings, Part XIII 16Sinan Tan, Weilai Xiang, Huaping Liu, Di Guo, and Fuchun Sun. Multi-agent embodied question answering in interactive environments. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIII 16, pages 663-678. Springer, 2020.\n\nSelf-supervised 3d semantic representation learning for vision-and-language navigation. Sinan Tan, Mengmeng Ge, Di Guo, Huaping Liu, Fuchun Sun, arXiv:2201.10788arXiv preprintSinan Tan, Mengmeng Ge, Di Guo, Huaping Liu, and Fuchun Sun. Self-supervised 3d semantic representa- tion learning for vision-and-language navigation. arXiv preprint arXiv:2201.10788, 2022.\n\nMulti-objective multi-agent planning for jointly discovering and tracking mobile objects. Hoa Van Nguyen, Hamid Rezatofighi, Ba-Ngu Vo, Damith C Ranasinghe, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Hoa Van Nguyen, Hamid Rezatofighi, Ba-Ngu Vo, and Damith C Ranasinghe. Multi-objective multi-agent plan- ning for jointly discovering and tracking mobile objects. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7227-7235, 2020.\n\nHaiyang Wang, Wenguan Wang, Xizhou Zhu, Jifeng Dai, Liwei Wang, arXiv:2107.01151Collaborative visual navigation. arXiv preprintHaiyang Wang, Wenguan Wang, Xizhou Zhu, Jifeng Dai, and Liwei Wang. Collaborative visual navigation. arXiv preprint arXiv:2107.01151, 2021.\n\nMaster-followed multiple robots cooperation slam adapted to search and rescue environment. Hongling Wang, Chengjin Zhang, Yong Song, Bao Pang, International Journal of Control, Automation and Systems. 166Hongling Wang, Chengjin Zhang, Yong Song, and Bao Pang. Master-followed multiple robots cooperation slam adapted to search and rescue environment. International Journal of Control, Automation and Systems, 16(6): 2593-2608, 2018.\n\nVisual semantic navigation using scene priors. Wei Yang, Xiaolong Wang, Ali Farhadi, Abhinav Gupta, Roozbeh Mottaghi, arXiv:1810.06543arXiv preprintWei Yang, Xiaolong Wang, Ali Farhadi, Abhinav Gupta, and Roozbeh Mottaghi. Visual semantic navigation using scene priors. arXiv preprint arXiv:1810.06543, 2018.\n\nLearning efficient multiagent cooperative visual exploration. Chao Yu, Xinyi Yang, Jiaxuan Gao, Huazhong Yang, Yu Wang, Yi Wu, arXiv:2110.05734arXiv preprintChao Yu, Xinyi Yang, Jiaxuan Gao, Huazhong Yang, Yu Wang, and Yi Wu. Learning efficient multi- agent cooperative visual exploration. arXiv preprint arXiv:2110.05734, 2021.\n\nTargetdriven visual navigation in indoor scenes using deep reinforcement learning. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, J Joseph, Abhinav Lim, Li Gupta, Ali Fei-Fei, Farhadi, 2017 IEEE international conference on robotics and automation (ICRA). IEEEYuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target- driven visual navigation in indoor scenes using deep reinforcement learning. In 2017 IEEE international conference on robotics and automation (ICRA), pages 3357-3364. IEEE, 2017.\n\nA distributed approach to the multi-robot task allocation problem using the consensus-based bundle algorithm and ant colony system. Farouq Zitouni, Saad Harous, Ramdane Maamri, IEEE Access. 8Farouq Zitouni, Saad Harous, and Ramdane Maamri. A distributed approach to the multi-robot task allocation problem using the consensus-based bundle algorithm and ant colony system. IEEE Access, 8:27479-27494, 2020.\n\nMarket-based multirobot coordination for complex tasks. Robert Zlot, Anthony Stentz, The International Journal of Robotics Research. 251Robert Zlot and Anthony Stentz. Market-based multi- robot coordination for complex tasks. The International Journal of Robotics Research, 25(1):73-101, 2006.\n", "annotations": {"author": "[{\"end\":199,\"start\":95},{\"end\":298,\"start\":200},{\"end\":397,\"start\":299},{\"end\":497,\"start\":398},{\"end\":592,\"start\":498},{\"end\":690,\"start\":593},{\"end\":790,\"start\":691},{\"end\":889,\"start\":791}]", "publisher": null, "author_last_name": "[{\"end\":111,\"start\":107},{\"end\":309,\"start\":306},{\"end\":409,\"start\":407},{\"end\":504,\"start\":501},{\"end\":602,\"start\":599},{\"end\":702,\"start\":699},{\"end\":801,\"start\":798}]", "author_first_name": "[{\"end\":106,\"start\":102},{\"end\":210,\"start\":200},{\"end\":305,\"start\":299},{\"end\":406,\"start\":398},{\"end\":500,\"start\":498},{\"end\":598,\"start\":593},{\"end\":698,\"start\":691},{\"end\":797,\"start\":791}]", "author_affiliation": "[{\"end\":198,\"start\":113},{\"end\":297,\"start\":212},{\"end\":396,\"start\":311},{\"end\":496,\"start\":411},{\"end\":591,\"start\":506},{\"end\":689,\"start\":604},{\"end\":789,\"start\":704},{\"end\":888,\"start\":803}]", "title": "[{\"end\":92,\"start\":1},{\"end\":981,\"start\":890}]", "venue": null, "abstract": "[{\"end\":2600,\"start\":983}]", "bib_ref": "[{\"end\":3838,\"start\":3832},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4432,\"start\":4428},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4539,\"start\":4535},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4606,\"start\":4603},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4693,\"start\":4689},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4836,\"start\":4832},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4839,\"start\":4836},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4972,\"start\":4969},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5047,\"start\":5043},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5282,\"start\":5279},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5285,\"start\":5282},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10205,\"start\":10201},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10564,\"start\":10560},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10567,\"start\":10564},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10584,\"start\":10581},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10604,\"start\":10600},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10629,\"start\":10625},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10679,\"start\":10675},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10727,\"start\":10724},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10953,\"start\":10950},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11244,\"start\":11240},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11247,\"start\":11244},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11265,\"start\":11261},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11268,\"start\":11265},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11310,\"start\":11306},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11433,\"start\":11429},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11597,\"start\":11593},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11615,\"start\":11611},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11945,\"start\":11941},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12054,\"start\":12051},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12075,\"start\":12071},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12091,\"start\":12087},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12110,\"start\":12106},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12259,\"start\":12255},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12402,\"start\":12398},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12965,\"start\":12961},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12968,\"start\":12965},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13276,\"start\":13273},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13376,\"start\":13372},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13397,\"start\":13393},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13436,\"start\":13432},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13645,\"start\":13641},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13684,\"start\":13680},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13772,\"start\":13769},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13811,\"start\":13807},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13868,\"start\":13865},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14305,\"start\":14301},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14353,\"start\":14350},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14416,\"start\":14412},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14419,\"start\":14416},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15220,\"start\":15217},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17351,\"start\":17347},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17354,\"start\":17351},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17998,\"start\":17995},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18015,\"start\":18011},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":18328,\"start\":18324},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18363,\"start\":18359},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":25510,\"start\":25506},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28998,\"start\":28995},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29010,\"start\":29007},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":39657,\"start\":39653},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":42360,\"start\":42356}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":45307,\"start\":45128},{\"attributes\":{\"id\":\"fig_3\"},\"end\":45416,\"start\":45308},{\"attributes\":{\"id\":\"fig_5\"},\"end\":45530,\"start\":45417},{\"attributes\":{\"id\":\"fig_6\"},\"end\":45724,\"start\":45531},{\"attributes\":{\"id\":\"fig_9\"},\"end\":45778,\"start\":45725},{\"attributes\":{\"id\":\"fig_11\"},\"end\":45903,\"start\":45779},{\"attributes\":{\"id\":\"fig_12\"},\"end\":45947,\"start\":45904},{\"attributes\":{\"id\":\"fig_15\"},\"end\":46027,\"start\":45948},{\"attributes\":{\"id\":\"fig_16\"},\"end\":46088,\"start\":46028},{\"attributes\":{\"id\":\"fig_17\"},\"end\":46163,\"start\":46089},{\"attributes\":{\"id\":\"fig_18\"},\"end\":46227,\"start\":46164},{\"attributes\":{\"id\":\"fig_19\"},\"end\":46312,\"start\":46228},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":46575,\"start\":46313},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":46778,\"start\":46576},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":46845,\"start\":46779},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":47466,\"start\":46846},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":49453,\"start\":47467},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":50360,\"start\":49454}]", "paragraph": "[{\"end\":4243,\"start\":2619},{\"end\":6497,\"start\":4245},{\"end\":7510,\"start\":6499},{\"end\":8498,\"start\":7512},{\"end\":9102,\"start\":8500},{\"end\":9426,\"start\":9104},{\"end\":9924,\"start\":9428},{\"end\":10380,\"start\":9926},{\"end\":10954,\"start\":10441},{\"end\":11434,\"start\":10956},{\"end\":12112,\"start\":11436},{\"end\":12809,\"start\":12198},{\"end\":12969,\"start\":12863},{\"end\":13464,\"start\":12971},{\"end\":14856,\"start\":13466},{\"end\":15155,\"start\":14885},{\"end\":15450,\"start\":15157},{\"end\":16001,\"start\":15459},{\"end\":16236,\"start\":16003},{\"end\":16421,\"start\":16322},{\"end\":16810,\"start\":16490},{\"end\":17011,\"start\":16904},{\"end\":17192,\"start\":17107},{\"end\":17511,\"start\":17194},{\"end\":17927,\"start\":17540},{\"end\":18735,\"start\":17929},{\"end\":19601,\"start\":18737},{\"end\":20658,\"start\":19603},{\"end\":21006,\"start\":20660},{\"end\":21317,\"start\":21008},{\"end\":22024,\"start\":21319},{\"end\":22264,\"start\":22026},{\"end\":23455,\"start\":22266},{\"end\":24292,\"start\":23457},{\"end\":24826,\"start\":24294},{\"end\":25354,\"start\":24840},{\"end\":26599,\"start\":25375},{\"end\":26708,\"start\":26645},{\"end\":27101,\"start\":26729},{\"end\":28034,\"start\":27103},{\"end\":28403,\"start\":28036},{\"end\":29026,\"start\":28529},{\"end\":29185,\"start\":29037},{\"end\":29527,\"start\":29187},{\"end\":29755,\"start\":29548},{\"end\":30649,\"start\":29757},{\"end\":31144,\"start\":30734},{\"end\":31493,\"start\":31165},{\"end\":31647,\"start\":31495},{\"end\":32120,\"start\":31826},{\"end\":32601,\"start\":32122},{\"end\":32823,\"start\":32657},{\"end\":32964,\"start\":32825},{\"end\":33308,\"start\":32966},{\"end\":33458,\"start\":33310},{\"end\":33790,\"start\":33484},{\"end\":33939,\"start\":33827},{\"end\":34089,\"start\":33978},{\"end\":34261,\"start\":34124},{\"end\":34769,\"start\":34263},{\"end\":35564,\"start\":34771},{\"end\":37159,\"start\":35566},{\"end\":37733,\"start\":37161},{\"end\":38232,\"start\":37735},{\"end\":39058,\"start\":38259},{\"end\":39451,\"start\":39095},{\"end\":40045,\"start\":39453},{\"end\":40378,\"start\":40047},{\"end\":40899,\"start\":40380},{\"end\":40952,\"start\":40901},{\"end\":41074,\"start\":40954},{\"end\":41346,\"start\":41076},{\"end\":42361,\"start\":41378},{\"end\":42765,\"start\":42363},{\"end\":43355,\"start\":42787},{\"end\":44750,\"start\":43357},{\"end\":45127,\"start\":44790}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12197,\"start\":12113},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15458,\"start\":15451},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16321,\"start\":16237},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16489,\"start\":16422},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16903,\"start\":16811},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17064,\"start\":17012},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17106,\"start\":17064},{\"attributes\":{\"id\":\"formula_7\"},\"end\":26644,\"start\":26600},{\"attributes\":{\"id\":\"formula_8\"},\"end\":28528,\"start\":28404},{\"attributes\":{\"id\":\"formula_9\"},\"end\":29036,\"start\":29027},{\"attributes\":{\"id\":\"formula_10\"},\"end\":30733,\"start\":30650},{\"attributes\":{\"id\":\"formula_11\"},\"end\":31825,\"start\":31648},{\"attributes\":{\"id\":\"formula_12\"},\"end\":33826,\"start\":33791},{\"attributes\":{\"id\":\"formula_13\"},\"end\":33977,\"start\":33940}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":14855,\"start\":14848},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24714,\"start\":24705},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":35815,\"start\":35806},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":36471,\"start\":36345},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":40621,\"start\":40612}]", "section_header": "[{\"end\":2617,\"start\":2602},{\"end\":10399,\"start\":10383},{\"end\":10439,\"start\":10402},{\"end\":12861,\"start\":12812},{\"end\":14883,\"start\":14859},{\"end\":17538,\"start\":17514},{\"end\":24838,\"start\":24829},{\"end\":25373,\"start\":25357},{\"end\":26727,\"start\":26711},{\"end\":29546,\"start\":29530},{\"end\":31163,\"start\":31147},{\"end\":32630,\"start\":32604},{\"end\":32655,\"start\":32633},{\"end\":33482,\"start\":33461},{\"end\":34122,\"start\":34092},{\"end\":38257,\"start\":38235},{\"end\":39093,\"start\":39061},{\"end\":41376,\"start\":41349},{\"end\":42785,\"start\":42768},{\"end\":44788,\"start\":44753},{\"end\":45317,\"start\":45309},{\"end\":45957,\"start\":45949},{\"end\":46037,\"start\":46029},{\"end\":46103,\"start\":46090},{\"end\":46178,\"start\":46165},{\"end\":46242,\"start\":46229},{\"end\":46332,\"start\":46314},{\"end\":46585,\"start\":46577},{\"end\":46801,\"start\":46780},{\"end\":46867,\"start\":46847}]", "table": "[{\"end\":46575,\"start\":46399},{\"end\":46778,\"start\":46639},{\"end\":46845,\"start\":46836},{\"end\":47466,\"start\":46911},{\"end\":49453,\"start\":47605},{\"end\":50360,\"start\":49516}]", "figure_caption": "[{\"end\":45307,\"start\":45130},{\"end\":45416,\"start\":45319},{\"end\":45530,\"start\":45419},{\"end\":45724,\"start\":45533},{\"end\":45778,\"start\":45727},{\"end\":45903,\"start\":45781},{\"end\":45947,\"start\":45906},{\"end\":46027,\"start\":45959},{\"end\":46088,\"start\":46039},{\"end\":46163,\"start\":46105},{\"end\":46227,\"start\":46180},{\"end\":46312,\"start\":46244},{\"end\":46399,\"start\":46334},{\"end\":46639,\"start\":46588},{\"end\":46836,\"start\":46805},{\"end\":46911,\"start\":46870},{\"end\":47605,\"start\":47469},{\"end\":49516,\"start\":49456}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":3830,\"start\":3822},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":8608,\"start\":8603},{\"attributes\":{\"ref_id\":\"fig_18\"},\"end\":19923,\"start\":19918},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25353,\"start\":25347},{\"end\":38366,\"start\":38361},{\"attributes\":{\"ref_id\":\"fig_16\"},\"end\":41957,\"start\":41952},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":42371,\"start\":42366},{\"end\":43665,\"start\":43658},{\"end\":44900,\"start\":44893}]", "bib_author_first_name": "[{\"end\":52259,\"start\":52254},{\"end\":52275,\"start\":52270},{\"end\":52295,\"start\":52289},{\"end\":52309,\"start\":52304},{\"end\":52324,\"start\":52320},{\"end\":52338,\"start\":52333},{\"end\":52352,\"start\":52346},{\"end\":52695,\"start\":52688},{\"end\":52710,\"start\":52705},{\"end\":52726,\"start\":52721},{\"end\":52737,\"start\":52732},{\"end\":52751,\"start\":52745},{\"end\":52769,\"start\":52762},{\"end\":53093,\"start\":53082},{\"end\":53105,\"start\":53101},{\"end\":53120,\"start\":53114},{\"end\":53135,\"start\":53129},{\"end\":53575,\"start\":53570},{\"end\":53589,\"start\":53584},{\"end\":53604,\"start\":53598},{\"end\":53617,\"start\":53610},{\"end\":53628,\"start\":53624},{\"end\":53960,\"start\":53954},{\"end\":53980,\"start\":53970},{\"end\":54177,\"start\":54172},{\"end\":54191,\"start\":54185},{\"end\":54203,\"start\":54197},{\"end\":54222,\"start\":54216},{\"end\":54239,\"start\":54231},{\"end\":54257,\"start\":54250},{\"end\":54271,\"start\":54265},{\"end\":54282,\"start\":54278},{\"end\":54294,\"start\":54289},{\"end\":54686,\"start\":54678},{\"end\":54708,\"start\":54702},{\"end\":54724,\"start\":54717},{\"end\":54739,\"start\":54732},{\"end\":54753,\"start\":54747},{\"end\":55086,\"start\":55080},{\"end\":55096,\"start\":55093},{\"end\":55106,\"start\":55102},{\"end\":55119,\"start\":55114},{\"end\":55131,\"start\":55128},{\"end\":55579,\"start\":55571},{\"end\":55592,\"start\":55591},{\"end\":55609,\"start\":55608},{\"end\":55622,\"start\":55617},{\"end\":55646,\"start\":55637},{\"end\":56004,\"start\":55999},{\"end\":56018,\"start\":56012},{\"end\":56304,\"start\":56297},{\"end\":56312,\"start\":56310},{\"end\":56320,\"start\":56318},{\"end\":56508,\"start\":56500},{\"end\":56520,\"start\":56514},{\"end\":56535,\"start\":56528},{\"end\":56552,\"start\":56546},{\"end\":56562,\"start\":56558},{\"end\":56576,\"start\":56571},{\"end\":57056,\"start\":57050},{\"end\":57072,\"start\":57063},{\"end\":57087,\"start\":57080},{\"end\":57106,\"start\":57099},{\"end\":57123,\"start\":57117},{\"end\":57627,\"start\":57621},{\"end\":57641,\"start\":57634},{\"end\":57657,\"start\":57653},{\"end\":57676,\"start\":57670},{\"end\":58173,\"start\":58165},{\"end\":58186,\"start\":58182},{\"end\":58196,\"start\":58187},{\"end\":58214,\"start\":58207},{\"end\":58227,\"start\":58222},{\"end\":58717,\"start\":58711},{\"end\":58751,\"start\":58744},{\"end\":59038,\"start\":59031},{\"end\":59055,\"start\":59050},{\"end\":59296,\"start\":59292},{\"end\":59307,\"start\":59304},{\"end\":59556,\"start\":59551},{\"end\":59567,\"start\":59563},{\"end\":59579,\"start\":59575},{\"end\":59595,\"start\":59587},{\"end\":59615,\"start\":59607},{\"end\":59629,\"start\":59626},{\"end\":59648,\"start\":59639},{\"end\":59650,\"start\":59649},{\"end\":59669,\"start\":59660},{\"end\":60214,\"start\":60209},{\"end\":60225,\"start\":60221},{\"end\":60237,\"start\":60233},{\"end\":60248,\"start\":60245},{\"end\":60266,\"start\":60258},{\"end\":60286,\"start\":60277},{\"end\":60306,\"start\":60297},{\"end\":60745,\"start\":60744},{\"end\":61017,\"start\":61013},{\"end\":61032,\"start\":61025},{\"end\":61049,\"start\":61043},{\"end\":61058,\"start\":61055},{\"end\":61076,\"start\":61072},{\"end\":61090,\"start\":61084},{\"end\":61107,\"start\":61101},{\"end\":61120,\"start\":61116},{\"end\":61133,\"start\":61126},{\"end\":61144,\"start\":61141},{\"end\":61523,\"start\":61520},{\"end\":61536,\"start\":61530},{\"end\":61550,\"start\":61544},{\"end\":61889,\"start\":61882},{\"end\":61920,\"start\":61908},{\"end\":62277,\"start\":62270},{\"end\":62304,\"start\":62298},{\"end\":62314,\"start\":62310},{\"end\":62656,\"start\":62650},{\"end\":62664,\"start\":62662},{\"end\":62677,\"start\":62670},{\"end\":62689,\"start\":62683},{\"end\":63041,\"start\":63036},{\"end\":63053,\"start\":63049},{\"end\":63071,\"start\":63065},{\"end\":63083,\"start\":63077},{\"end\":63097,\"start\":63092},{\"end\":63099,\"start\":63098},{\"end\":63414,\"start\":63408},{\"end\":63426,\"start\":63421},{\"end\":63436,\"start\":63431},{\"end\":63450,\"start\":63444},{\"end\":63461,\"start\":63455},{\"end\":63473,\"start\":63468},{\"end\":63489,\"start\":63482},{\"end\":63993,\"start\":63986},{\"end\":64000,\"start\":63998},{\"end\":64010,\"start\":64005},{\"end\":64024,\"start\":64021},{\"end\":64038,\"start\":64031},{\"end\":64043,\"start\":64039},{\"end\":64057,\"start\":64050},{\"end\":64069,\"start\":64064},{\"end\":64620,\"start\":64613},{\"end\":64639,\"start\":64632},{\"end\":64658,\"start\":64653},{\"end\":64679,\"start\":64674},{\"end\":64693,\"start\":64688},{\"end\":65011,\"start\":65004},{\"end\":65027,\"start\":65019},{\"end\":65045,\"start\":65036},{\"end\":65061,\"start\":65057},{\"end\":65072,\"start\":65068},{\"end\":65089,\"start\":65082},{\"end\":65102,\"start\":65096},{\"end\":65114,\"start\":65111},{\"end\":65127,\"start\":65120},{\"end\":65144,\"start\":65136},{\"end\":65654,\"start\":65646},{\"end\":65671,\"start\":65665},{\"end\":65688,\"start\":65683},{\"end\":65703,\"start\":65697},{\"end\":65720,\"start\":65714},{\"end\":66107,\"start\":66102},{\"end\":66123,\"start\":66118},{\"end\":66140,\"start\":66134},{\"end\":66156,\"start\":66149},{\"end\":66169,\"start\":66163},{\"end\":66182,\"start\":66175},{\"end\":66197,\"start\":66193},{\"end\":66217,\"start\":66211},{\"end\":66762,\"start\":66757},{\"end\":66774,\"start\":66768},{\"end\":66789,\"start\":66782},{\"end\":66797,\"start\":66795},{\"end\":66809,\"start\":66803},{\"end\":67284,\"start\":67279},{\"end\":67298,\"start\":67290},{\"end\":67305,\"start\":67303},{\"end\":67318,\"start\":67311},{\"end\":67330,\"start\":67324},{\"end\":67650,\"start\":67647},{\"end\":67668,\"start\":67663},{\"end\":67688,\"start\":67682},{\"end\":68096,\"start\":68089},{\"end\":68110,\"start\":68103},{\"end\":68123,\"start\":68117},{\"end\":68135,\"start\":68129},{\"end\":68146,\"start\":68141},{\"end\":68456,\"start\":68448},{\"end\":68471,\"start\":68463},{\"end\":68483,\"start\":68479},{\"end\":68493,\"start\":68490},{\"end\":68841,\"start\":68838},{\"end\":68856,\"start\":68848},{\"end\":68866,\"start\":68863},{\"end\":68883,\"start\":68876},{\"end\":68898,\"start\":68891},{\"end\":69167,\"start\":69163},{\"end\":69177,\"start\":69172},{\"end\":69191,\"start\":69184},{\"end\":69205,\"start\":69197},{\"end\":69214,\"start\":69212},{\"end\":69223,\"start\":69221},{\"end\":69518,\"start\":69514},{\"end\":69531,\"start\":69524},{\"end\":69546,\"start\":69542},{\"end\":69555,\"start\":69554},{\"end\":69571,\"start\":69564},{\"end\":69579,\"start\":69577},{\"end\":69590,\"start\":69587},{\"end\":70107,\"start\":70101},{\"end\":70121,\"start\":70117},{\"end\":70137,\"start\":70130},{\"end\":70438,\"start\":70432},{\"end\":70452,\"start\":70445}]", "bib_author_last_name": "[{\"end\":52268,\"start\":52260},{\"end\":52287,\"start\":52276},{\"end\":52302,\"start\":52296},{\"end\":52318,\"start\":52310},{\"end\":52331,\"start\":52325},{\"end\":52344,\"start\":52339},{\"end\":52356,\"start\":52353},{\"end\":52703,\"start\":52696},{\"end\":52719,\"start\":52711},{\"end\":52730,\"start\":52727},{\"end\":52743,\"start\":52738},{\"end\":52760,\"start\":52752},{\"end\":52776,\"start\":52770},{\"end\":53099,\"start\":53094},{\"end\":53112,\"start\":53106},{\"end\":53127,\"start\":53121},{\"end\":53145,\"start\":53136},{\"end\":53582,\"start\":53576},{\"end\":53596,\"start\":53590},{\"end\":53608,\"start\":53605},{\"end\":53622,\"start\":53618},{\"end\":53634,\"start\":53629},{\"end\":53968,\"start\":53961},{\"end\":53988,\"start\":53981},{\"end\":54183,\"start\":54178},{\"end\":54195,\"start\":54192},{\"end\":54214,\"start\":54204},{\"end\":54229,\"start\":54223},{\"end\":54248,\"start\":54240},{\"end\":54263,\"start\":54258},{\"end\":54276,\"start\":54272},{\"end\":54287,\"start\":54283},{\"end\":54300,\"start\":54295},{\"end\":54700,\"start\":54687},{\"end\":54715,\"start\":54709},{\"end\":54730,\"start\":54725},{\"end\":54745,\"start\":54740},{\"end\":54767,\"start\":54754},{\"end\":55091,\"start\":55087},{\"end\":55100,\"start\":55097},{\"end\":55112,\"start\":55107},{\"end\":55126,\"start\":55120},{\"end\":55142,\"start\":55132},{\"end\":55589,\"start\":55580},{\"end\":55599,\"start\":55593},{\"end\":55606,\"start\":55601},{\"end\":55615,\"start\":55610},{\"end\":55635,\"start\":55623},{\"end\":55653,\"start\":55647},{\"end\":55659,\"start\":55655},{\"end\":56010,\"start\":56005},{\"end\":56026,\"start\":56019},{\"end\":56308,\"start\":56305},{\"end\":56316,\"start\":56313},{\"end\":56324,\"start\":56321},{\"end\":56512,\"start\":56509},{\"end\":56526,\"start\":56521},{\"end\":56544,\"start\":56536},{\"end\":56556,\"start\":56553},{\"end\":56569,\"start\":56563},{\"end\":56582,\"start\":56577},{\"end\":57061,\"start\":57057},{\"end\":57078,\"start\":57073},{\"end\":57097,\"start\":57088},{\"end\":57115,\"start\":57107},{\"end\":57133,\"start\":57124},{\"end\":57632,\"start\":57628},{\"end\":57651,\"start\":57642},{\"end\":57668,\"start\":57658},{\"end\":57686,\"start\":57677},{\"end\":58180,\"start\":58174},{\"end\":58205,\"start\":58197},{\"end\":58220,\"start\":58215},{\"end\":58233,\"start\":58228},{\"end\":58742,\"start\":58718},{\"end\":58760,\"start\":58752},{\"end\":58768,\"start\":58762},{\"end\":59048,\"start\":59039},{\"end\":59065,\"start\":59056},{\"end\":59302,\"start\":59297},{\"end\":59311,\"start\":59308},{\"end\":59561,\"start\":59557},{\"end\":59573,\"start\":59568},{\"end\":59585,\"start\":59580},{\"end\":59605,\"start\":59596},{\"end\":59624,\"start\":59616},{\"end\":59637,\"start\":59630},{\"end\":59658,\"start\":59651},{\"end\":59678,\"start\":59670},{\"end\":60219,\"start\":60215},{\"end\":60231,\"start\":60226},{\"end\":60243,\"start\":60238},{\"end\":60256,\"start\":60249},{\"end\":60275,\"start\":60267},{\"end\":60295,\"start\":60287},{\"end\":60314,\"start\":60307},{\"end\":60751,\"start\":60746},{\"end\":60759,\"start\":60753},{\"end\":61023,\"start\":61018},{\"end\":61041,\"start\":61033},{\"end\":61053,\"start\":61050},{\"end\":61070,\"start\":61059},{\"end\":61082,\"start\":61077},{\"end\":61099,\"start\":61091},{\"end\":61114,\"start\":61108},{\"end\":61124,\"start\":61121},{\"end\":61139,\"start\":61134},{\"end\":61152,\"start\":61145},{\"end\":61528,\"start\":61524},{\"end\":61542,\"start\":61537},{\"end\":61553,\"start\":61551},{\"end\":61906,\"start\":61890},{\"end\":61927,\"start\":61921},{\"end\":61933,\"start\":61929},{\"end\":62296,\"start\":62278},{\"end\":62308,\"start\":62305},{\"end\":62318,\"start\":62315},{\"end\":62326,\"start\":62320},{\"end\":62660,\"start\":62657},{\"end\":62668,\"start\":62665},{\"end\":62681,\"start\":62678},{\"end\":62693,\"start\":62690},{\"end\":63047,\"start\":63042},{\"end\":63063,\"start\":63054},{\"end\":63075,\"start\":63072},{\"end\":63090,\"start\":63084},{\"end\":63105,\"start\":63100},{\"end\":63419,\"start\":63415},{\"end\":63429,\"start\":63427},{\"end\":63442,\"start\":63437},{\"end\":63453,\"start\":63451},{\"end\":63466,\"start\":63462},{\"end\":63480,\"start\":63474},{\"end\":63498,\"start\":63490},{\"end\":63996,\"start\":63994},{\"end\":64003,\"start\":64001},{\"end\":64019,\"start\":64011},{\"end\":64029,\"start\":64025},{\"end\":64048,\"start\":64044},{\"end\":64062,\"start\":64058},{\"end\":64077,\"start\":64070},{\"end\":64085,\"start\":64079},{\"end\":64630,\"start\":64621},{\"end\":64651,\"start\":64640},{\"end\":64672,\"start\":64659},{\"end\":64686,\"start\":64680},{\"end\":64699,\"start\":64694},{\"end\":64705,\"start\":64701},{\"end\":65017,\"start\":65012},{\"end\":65034,\"start\":65028},{\"end\":65055,\"start\":65046},{\"end\":65066,\"start\":65062},{\"end\":65080,\"start\":65073},{\"end\":65094,\"start\":65090},{\"end\":65109,\"start\":65103},{\"end\":65118,\"start\":65115},{\"end\":65134,\"start\":65128},{\"end\":65150,\"start\":65145},{\"end\":65663,\"start\":65655},{\"end\":65681,\"start\":65672},{\"end\":65695,\"start\":65689},{\"end\":65712,\"start\":65704},{\"end\":65727,\"start\":65721},{\"end\":65733,\"start\":65729},{\"end\":66116,\"start\":66108},{\"end\":66132,\"start\":66124},{\"end\":66147,\"start\":66141},{\"end\":66161,\"start\":66157},{\"end\":66173,\"start\":66170},{\"end\":66191,\"start\":66183},{\"end\":66209,\"start\":66198},{\"end\":66221,\"start\":66218},{\"end\":66766,\"start\":66763},{\"end\":66780,\"start\":66775},{\"end\":66793,\"start\":66790},{\"end\":66801,\"start\":66798},{\"end\":66813,\"start\":66810},{\"end\":67288,\"start\":67285},{\"end\":67301,\"start\":67299},{\"end\":67309,\"start\":67306},{\"end\":67322,\"start\":67319},{\"end\":67334,\"start\":67331},{\"end\":67661,\"start\":67651},{\"end\":67680,\"start\":67669},{\"end\":67691,\"start\":67689},{\"end\":67712,\"start\":67693},{\"end\":68101,\"start\":68097},{\"end\":68115,\"start\":68111},{\"end\":68127,\"start\":68124},{\"end\":68139,\"start\":68136},{\"end\":68151,\"start\":68147},{\"end\":68461,\"start\":68457},{\"end\":68477,\"start\":68472},{\"end\":68488,\"start\":68484},{\"end\":68498,\"start\":68494},{\"end\":68846,\"start\":68842},{\"end\":68861,\"start\":68857},{\"end\":68874,\"start\":68867},{\"end\":68889,\"start\":68884},{\"end\":68907,\"start\":68899},{\"end\":69170,\"start\":69168},{\"end\":69182,\"start\":69178},{\"end\":69195,\"start\":69192},{\"end\":69210,\"start\":69206},{\"end\":69219,\"start\":69215},{\"end\":69226,\"start\":69224},{\"end\":69522,\"start\":69519},{\"end\":69540,\"start\":69532},{\"end\":69552,\"start\":69547},{\"end\":69562,\"start\":69556},{\"end\":69575,\"start\":69572},{\"end\":69585,\"start\":69580},{\"end\":69598,\"start\":69591},{\"end\":69607,\"start\":69600},{\"end\":70115,\"start\":70108},{\"end\":70128,\"start\":70122},{\"end\":70144,\"start\":70138},{\"end\":70443,\"start\":70439},{\"end\":70459,\"start\":70453}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":226282073},\"end\":52620,\"start\":52197},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":53044443},\"end\":53003,\"start\":52622},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":220825778},\"end\":53468,\"start\":53005},{\"attributes\":{\"doi\":\"arXiv:2107.05612\",\"id\":\"b3\"},\"end\":53876,\"start\":53470},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":235694537},\"end\":54170,\"start\":53878},{\"attributes\":{\"doi\":\"arXiv:1709.06158\",\"id\":\"b5\"},\"end\":54630,\"start\":54172},{\"attributes\":{\"doi\":\"arXiv:2004.05155\",\"id\":\"b6\"},\"end\":54978,\"start\":54632},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":140310562},\"end\":55489,\"start\":54980},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":218900678},\"end\":55892,\"start\":55491},{\"attributes\":{\"id\":\"b9\"},\"end\":56228,\"start\":55894},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":30059524},\"end\":56469,\"start\":56230},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":35985986},\"end\":56941,\"start\":56471},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":212628332},\"end\":57506,\"start\":56943},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":226236824},\"end\":58047,\"start\":57508},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":210970554},\"end\":58626,\"start\":58049},{\"attributes\":{\"doi\":\"arXiv:2109.10106\",\"id\":\"b15\"},\"end\":58984,\"start\":58628},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":49888615},\"end\":59202,\"start\":58986},{\"attributes\":{\"doi\":\"arXiv:2006.15482\",\"id\":\"b17\"},\"end\":59493,\"start\":59204},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":109933186},\"end\":60128,\"start\":59495},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":220424783},\"end\":60633,\"start\":60130},{\"attributes\":{\"doi\":\"arXiv:2009.14259\",\"id\":\"b20\"},\"end\":60956,\"start\":60635},{\"attributes\":{\"doi\":\"arXiv:1712.05474\",\"id\":\"b21\"},\"end\":61423,\"start\":60958},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14411368},\"end\":61822,\"start\":61425},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":12515065},\"end\":62175,\"start\":61824},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":68190687},\"end\":62572,\"start\":62177},{\"attributes\":{\"doi\":\"10.1109/LRA.2022.3145964\",\"id\":\"b25\",\"matched_paper_id\":237572234},\"end\":62970,\"start\":62574},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":212703531},\"end\":63347,\"start\":62972},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":49317780},\"end\":63902,\"start\":63349},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":214264259},\"end\":64530,\"start\":63904},{\"attributes\":{\"doi\":\"arXiv:2104.08251\",\"id\":\"b29\"},\"end\":64956,\"start\":64532},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":91184540},\"end\":65580,\"start\":64958},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":3741505},\"end\":66021,\"start\":65582},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":208617407},\"end\":66686,\"start\":66023},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":227232796},\"end\":67189,\"start\":66688},{\"attributes\":{\"doi\":\"arXiv:2201.10788\",\"id\":\"b34\"},\"end\":67555,\"start\":67191},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":208263453},\"end\":68087,\"start\":67557},{\"attributes\":{\"doi\":\"arXiv:2107.01151\",\"id\":\"b36\"},\"end\":68355,\"start\":68089},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":126376999},\"end\":68789,\"start\":68357},{\"attributes\":{\"doi\":\"arXiv:1810.06543\",\"id\":\"b38\"},\"end\":69099,\"start\":68791},{\"attributes\":{\"doi\":\"arXiv:2110.05734\",\"id\":\"b39\"},\"end\":69429,\"start\":69101},{\"attributes\":{\"id\":\"b40\"},\"end\":69967,\"start\":69431},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":211209532},\"end\":70374,\"start\":69969},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":3287827},\"end\":70669,\"start\":70376}]", "bib_title": "[{\"end\":52252,\"start\":52197},{\"end\":52686,\"start\":52622},{\"end\":53080,\"start\":53005},{\"end\":53952,\"start\":53878},{\"end\":55078,\"start\":54980},{\"end\":55569,\"start\":55491},{\"end\":55997,\"start\":55894},{\"end\":56295,\"start\":56230},{\"end\":56498,\"start\":56471},{\"end\":57048,\"start\":56943},{\"end\":57619,\"start\":57508},{\"end\":58163,\"start\":58049},{\"end\":59029,\"start\":58986},{\"end\":59549,\"start\":59495},{\"end\":60207,\"start\":60130},{\"end\":61518,\"start\":61425},{\"end\":61880,\"start\":61824},{\"end\":62268,\"start\":62177},{\"end\":62648,\"start\":62574},{\"end\":63034,\"start\":62972},{\"end\":63406,\"start\":63349},{\"end\":63984,\"start\":63904},{\"end\":65002,\"start\":64958},{\"end\":65644,\"start\":65582},{\"end\":66100,\"start\":66023},{\"end\":66755,\"start\":66688},{\"end\":67645,\"start\":67557},{\"end\":68446,\"start\":68357},{\"end\":69512,\"start\":69431},{\"end\":70099,\"start\":69969},{\"end\":70430,\"start\":70376}]", "bib_author": "[{\"end\":52270,\"start\":52254},{\"end\":52289,\"start\":52270},{\"end\":52304,\"start\":52289},{\"end\":52320,\"start\":52304},{\"end\":52333,\"start\":52320},{\"end\":52346,\"start\":52333},{\"end\":52358,\"start\":52346},{\"end\":52705,\"start\":52688},{\"end\":52721,\"start\":52705},{\"end\":52732,\"start\":52721},{\"end\":52745,\"start\":52732},{\"end\":52762,\"start\":52745},{\"end\":52778,\"start\":52762},{\"end\":53101,\"start\":53082},{\"end\":53114,\"start\":53101},{\"end\":53129,\"start\":53114},{\"end\":53147,\"start\":53129},{\"end\":53584,\"start\":53570},{\"end\":53598,\"start\":53584},{\"end\":53610,\"start\":53598},{\"end\":53624,\"start\":53610},{\"end\":53636,\"start\":53624},{\"end\":53970,\"start\":53954},{\"end\":53990,\"start\":53970},{\"end\":54185,\"start\":54172},{\"end\":54197,\"start\":54185},{\"end\":54216,\"start\":54197},{\"end\":54231,\"start\":54216},{\"end\":54250,\"start\":54231},{\"end\":54265,\"start\":54250},{\"end\":54278,\"start\":54265},{\"end\":54289,\"start\":54278},{\"end\":54302,\"start\":54289},{\"end\":54702,\"start\":54678},{\"end\":54717,\"start\":54702},{\"end\":54732,\"start\":54717},{\"end\":54747,\"start\":54732},{\"end\":54769,\"start\":54747},{\"end\":55093,\"start\":55080},{\"end\":55102,\"start\":55093},{\"end\":55114,\"start\":55102},{\"end\":55128,\"start\":55114},{\"end\":55144,\"start\":55128},{\"end\":55591,\"start\":55571},{\"end\":55601,\"start\":55591},{\"end\":55608,\"start\":55601},{\"end\":55617,\"start\":55608},{\"end\":55637,\"start\":55617},{\"end\":55655,\"start\":55637},{\"end\":55661,\"start\":55655},{\"end\":56012,\"start\":55999},{\"end\":56028,\"start\":56012},{\"end\":56310,\"start\":56297},{\"end\":56318,\"start\":56310},{\"end\":56326,\"start\":56318},{\"end\":56514,\"start\":56500},{\"end\":56528,\"start\":56514},{\"end\":56546,\"start\":56528},{\"end\":56558,\"start\":56546},{\"end\":56571,\"start\":56558},{\"end\":56584,\"start\":56571},{\"end\":57063,\"start\":57050},{\"end\":57080,\"start\":57063},{\"end\":57099,\"start\":57080},{\"end\":57117,\"start\":57099},{\"end\":57135,\"start\":57117},{\"end\":57634,\"start\":57621},{\"end\":57653,\"start\":57634},{\"end\":57670,\"start\":57653},{\"end\":57688,\"start\":57670},{\"end\":58182,\"start\":58165},{\"end\":58207,\"start\":58182},{\"end\":58222,\"start\":58207},{\"end\":58235,\"start\":58222},{\"end\":58744,\"start\":58711},{\"end\":58762,\"start\":58744},{\"end\":58770,\"start\":58762},{\"end\":59050,\"start\":59031},{\"end\":59067,\"start\":59050},{\"end\":59304,\"start\":59292},{\"end\":59313,\"start\":59304},{\"end\":59563,\"start\":59551},{\"end\":59575,\"start\":59563},{\"end\":59587,\"start\":59575},{\"end\":59607,\"start\":59587},{\"end\":59626,\"start\":59607},{\"end\":59639,\"start\":59626},{\"end\":59660,\"start\":59639},{\"end\":59680,\"start\":59660},{\"end\":60221,\"start\":60209},{\"end\":60233,\"start\":60221},{\"end\":60245,\"start\":60233},{\"end\":60258,\"start\":60245},{\"end\":60277,\"start\":60258},{\"end\":60297,\"start\":60277},{\"end\":60316,\"start\":60297},{\"end\":60753,\"start\":60744},{\"end\":60761,\"start\":60753},{\"end\":61025,\"start\":61013},{\"end\":61043,\"start\":61025},{\"end\":61055,\"start\":61043},{\"end\":61072,\"start\":61055},{\"end\":61084,\"start\":61072},{\"end\":61101,\"start\":61084},{\"end\":61116,\"start\":61101},{\"end\":61126,\"start\":61116},{\"end\":61141,\"start\":61126},{\"end\":61154,\"start\":61141},{\"end\":61530,\"start\":61520},{\"end\":61544,\"start\":61530},{\"end\":61555,\"start\":61544},{\"end\":61908,\"start\":61882},{\"end\":61929,\"start\":61908},{\"end\":61935,\"start\":61929},{\"end\":62298,\"start\":62270},{\"end\":62310,\"start\":62298},{\"end\":62320,\"start\":62310},{\"end\":62328,\"start\":62320},{\"end\":62662,\"start\":62650},{\"end\":62670,\"start\":62662},{\"end\":62683,\"start\":62670},{\"end\":62695,\"start\":62683},{\"end\":63049,\"start\":63036},{\"end\":63065,\"start\":63049},{\"end\":63077,\"start\":63065},{\"end\":63092,\"start\":63077},{\"end\":63107,\"start\":63092},{\"end\":63421,\"start\":63408},{\"end\":63431,\"start\":63421},{\"end\":63444,\"start\":63431},{\"end\":63455,\"start\":63444},{\"end\":63468,\"start\":63455},{\"end\":63482,\"start\":63468},{\"end\":63500,\"start\":63482},{\"end\":63998,\"start\":63986},{\"end\":64005,\"start\":63998},{\"end\":64021,\"start\":64005},{\"end\":64031,\"start\":64021},{\"end\":64050,\"start\":64031},{\"end\":64064,\"start\":64050},{\"end\":64079,\"start\":64064},{\"end\":64087,\"start\":64079},{\"end\":64632,\"start\":64613},{\"end\":64653,\"start\":64632},{\"end\":64674,\"start\":64653},{\"end\":64688,\"start\":64674},{\"end\":64701,\"start\":64688},{\"end\":64707,\"start\":64701},{\"end\":65019,\"start\":65004},{\"end\":65036,\"start\":65019},{\"end\":65057,\"start\":65036},{\"end\":65068,\"start\":65057},{\"end\":65082,\"start\":65068},{\"end\":65096,\"start\":65082},{\"end\":65111,\"start\":65096},{\"end\":65120,\"start\":65111},{\"end\":65136,\"start\":65120},{\"end\":65152,\"start\":65136},{\"end\":65665,\"start\":65646},{\"end\":65683,\"start\":65665},{\"end\":65697,\"start\":65683},{\"end\":65714,\"start\":65697},{\"end\":65729,\"start\":65714},{\"end\":65735,\"start\":65729},{\"end\":66118,\"start\":66102},{\"end\":66134,\"start\":66118},{\"end\":66149,\"start\":66134},{\"end\":66163,\"start\":66149},{\"end\":66175,\"start\":66163},{\"end\":66193,\"start\":66175},{\"end\":66211,\"start\":66193},{\"end\":66223,\"start\":66211},{\"end\":66768,\"start\":66757},{\"end\":66782,\"start\":66768},{\"end\":66795,\"start\":66782},{\"end\":66803,\"start\":66795},{\"end\":66815,\"start\":66803},{\"end\":67290,\"start\":67279},{\"end\":67303,\"start\":67290},{\"end\":67311,\"start\":67303},{\"end\":67324,\"start\":67311},{\"end\":67336,\"start\":67324},{\"end\":67663,\"start\":67647},{\"end\":67682,\"start\":67663},{\"end\":67693,\"start\":67682},{\"end\":67714,\"start\":67693},{\"end\":68103,\"start\":68089},{\"end\":68117,\"start\":68103},{\"end\":68129,\"start\":68117},{\"end\":68141,\"start\":68129},{\"end\":68153,\"start\":68141},{\"end\":68463,\"start\":68448},{\"end\":68479,\"start\":68463},{\"end\":68490,\"start\":68479},{\"end\":68500,\"start\":68490},{\"end\":68848,\"start\":68838},{\"end\":68863,\"start\":68848},{\"end\":68876,\"start\":68863},{\"end\":68891,\"start\":68876},{\"end\":68909,\"start\":68891},{\"end\":69172,\"start\":69163},{\"end\":69184,\"start\":69172},{\"end\":69197,\"start\":69184},{\"end\":69212,\"start\":69197},{\"end\":69221,\"start\":69212},{\"end\":69228,\"start\":69221},{\"end\":69524,\"start\":69514},{\"end\":69542,\"start\":69524},{\"end\":69554,\"start\":69542},{\"end\":69564,\"start\":69554},{\"end\":69577,\"start\":69564},{\"end\":69587,\"start\":69577},{\"end\":69600,\"start\":69587},{\"end\":69609,\"start\":69600},{\"end\":70117,\"start\":70101},{\"end\":70130,\"start\":70117},{\"end\":70146,\"start\":70130},{\"end\":70445,\"start\":70432},{\"end\":70461,\"start\":70445}]", "bib_venue": "[{\"end\":52386,\"start\":52358},{\"end\":52795,\"start\":52778},{\"end\":53215,\"start\":53147},{\"end\":53568,\"start\":53470},{\"end\":54007,\"start\":53990},{\"end\":54379,\"start\":54318},{\"end\":54676,\"start\":54632},{\"end\":55212,\"start\":55144},{\"end\":55678,\"start\":55661},{\"end\":56045,\"start\":56028},{\"end\":56334,\"start\":56326},{\"end\":56661,\"start\":56584},{\"end\":57203,\"start\":57135},{\"end\":57756,\"start\":57688},{\"end\":58314,\"start\":58235},{\"end\":58709,\"start\":58628},{\"end\":59083,\"start\":59067},{\"end\":59290,\"start\":59204},{\"end\":59761,\"start\":59680},{\"end\":60354,\"start\":60316},{\"end\":60742,\"start\":60635},{\"end\":61011,\"start\":60958},{\"end\":61607,\"start\":61555},{\"end\":61981,\"start\":61935},{\"end\":62361,\"start\":62328},{\"end\":62755,\"start\":62719},{\"end\":63143,\"start\":63107},{\"end\":63577,\"start\":63500},{\"end\":64168,\"start\":64087},{\"end\":64611,\"start\":64532},{\"end\":65223,\"start\":65152},{\"end\":65779,\"start\":65735},{\"end\":66304,\"start\":66223},{\"end\":66866,\"start\":66815},{\"end\":67277,\"start\":67191},{\"end\":67775,\"start\":67714},{\"end\":68200,\"start\":68169},{\"end\":68556,\"start\":68500},{\"end\":68836,\"start\":68791},{\"end\":69161,\"start\":69101},{\"end\":69677,\"start\":69609},{\"end\":70157,\"start\":70146},{\"end\":70507,\"start\":70461},{\"end\":56725,\"start\":56663},{\"end\":59829,\"start\":59763},{\"end\":63641,\"start\":63579},{\"end\":64236,\"start\":64170},{\"end\":65281,\"start\":65225},{\"end\":66372,\"start\":66306},{\"end\":66879,\"start\":66868},{\"end\":67823,\"start\":67777}]"}}}, "year": 2023, "month": 12, "day": 17}