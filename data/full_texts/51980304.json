{"id": 51980304, "updated": "2023-10-07 10:56:06.066", "metadata": {"title": "This Time with Feeling: Learning Expressive Musical Performance", "authors": "[{\"first\":\"Sageev\",\"last\":\"Oore\",\"middle\":[]},{\"first\":\"Ian\",\"last\":\"Simon\",\"middle\":[]},{\"first\":\"Sander\",\"last\":\"Dieleman\",\"middle\":[]},{\"first\":\"Douglas\",\"last\":\"Eck\",\"middle\":[]},{\"first\":\"Karen\",\"last\":\"Simonyan\",\"middle\":[]}]", "venue": "Neural Computing and Applications", "journal": "Neural Computing and Applications", "publication_date": {"year": 2018, "month": 8, "day": 10}, "abstract": "Music generation has generally been focused on either creating scores or interpreting them. We discuss differences between these two problems and propose that, in fact, it may be valuable to work in the space of direct $\\it performance$ generation: jointly predicting the notes $\\it and$ $\\it also$ their expressive timing and dynamics. We consider the significance and qualities of the data set needed for this. Having identified both a problem domain and characteristics of an appropriate data set, we show an LSTM-based recurrent network model that subjectively performs quite well on this task. Critically, we provide generated examples. We also include feedback from professional composers and musicians about some of these examples.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "1808.03715", "mag": "2963408210", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/nca/OoreSDES20", "doi": "10.1007/s00521-018-3758-9"}}, "content": {"source": {"pdf_hash": "b1b8856137f673eed765423983d2a92e6fde74b3", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1808.03715v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://link.springer.com/content/pdf/10.1007/s00521-018-3758-9.pdf", "status": "HYBRID"}}, "grobid": {"id": "84f0dce1bbcf4164a288e3a910c498f764fd1369", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/b1b8856137f673eed765423983d2a92e6fde74b3.txt", "contents": "\nThis Time with Feeling: Learning Expressive Musical Performance\nAugust 14, 2018\n\nSageev Oore \nDalhousie University\n\n\nIan Simon \nDalhousie University\n\n\nSander Dieleman \nDalhousie University\n\n\nDouglas Eck \nDalhousie University\n\n\nKaren Simonyan \nDalhousie University\n\n\nBrain \u2020 Google \nDalhousie University\n\n\nBrain \u2021 Google \nDalhousie University\n\n\n\u00a7 Deepmind \nDalhousie University\n\n\nBrain \u00b6 Google \nDalhousie University\n\n\nDeepmind \nDalhousie University\n\n\nThis Time with Feeling: Learning Expressive Musical Performance\nAugust 14, 20189F3B94F7445A37AC83C2383D64141ED0arXiv:1808.03715v1[cs.SD]music generationdeep learningrecurrent neural networksartificial intelligence\nMusic generation has generally been focused on either creating scores or interpreting them.We discuss differences between these two problems and propose that, in fact, it may be valuable to work in the space of direct performance generation: jointly predicting the notes and also their expressive timing and dynamics.We consider the significance and qualities of the data set needed for this.Having identified both a problem domain and characteristics of an appropriate data set, we show an LSTM-based recurrent network model that subjectively performs quite well on this task.Critically, we provide generated examples.We also include feedback from professional composers and musicians about some of these examples.\n\nIntroduction\n\nIn this work, we discuss training a machine-learning system to generate music.The first two key words in the title are time and feeling: not coincidentally, our central thesis is that, given the current state of the art in music generation systems, it is effective to generate the expressive timing and dynamics information concurrently with the music.Here we do this by directly generating improvised performances rather than creating or interpreting scores.We begin with an exposition of some relevant musical concepts.\n\n\nScores, Performances and Musical Abstraction\n\nMusic exists in the audio domain, and is experienced through individuals' perceptual systems.Any \"music\" that is not in the audio domain (e.g. a text or binary file of any sort) is of course a representation of music: if it is not physically vibrating, it is not (yet) sound, and if it is not sound, it is certainly not music.The obvious implication is that for any representation, there are additional steps to transform that representation-whatever it might be-into sound.Those steps might be as local as the conversion from digital to analog waves, or as global as the human performance of written score, for example.In generating music 2 , therefore, one must be aware of which of those steps is addressed directly by their generative system, which ones must be addressed in other ways, and, importantly, the impact of all of those choices on the listener's perception of the music, where it is ultimately experienced.\n\nA defining characteristic of a representation, then, is what is omitted: what still needs to be added or done to it in order to create music from it, and the relation of that abstraction to our perceptual experience.With that consideration in mind, we now discuss some common symbolic representations.\n\n\nScores\n\nFigure 1 is an example of a musical score [8].It shows which notes to play and when to play them relative to each other.The timing in a score is aligned to an implicit and relative metrical grid.For example, quarter notes are the same duration as quarter note rests, twice the duration of eighth notes, and so on.Some scores additionally specify an absolute tempo, e.g. in quarter notes per minute.\n\nAnd yet, by the time the music is heard as audio, most of this timing information will have been intentionally not followed exactly!For example, in classical music from the 1800's onwards, rubato developed: an expressive malleability of timing that overrides metrical accuracy (i.e. can deviate very far from the grid), and this device is both frequent and essential for making perceptual sense of certain pieces.Another example of a rhythmic construct that is not written in scores is swing, a defining quality of many African American music traditions 3 .\n\nBut tempo is not the only way in which the score is not followed exactly.Dynamics refers to how the music gets louder and quieter.While scores do give information about dynamics, in this respect, too, their effectiveness relies heavily on conventions that are not written into the score.For example, where the above score says \"p\" it means to play quietly, but that does tell us how quietly, nor will all the notes be equally quiet.When there is a crescendo marking indicating to get louder, in some cases the performer will at first get momentarily quieter, creating space from which to build.Furthermore, when playing polyphonic piano music, notes played at the same time will usually be played at different dynamic levels and articulated differently from one another in order to bring out some voices over others.\n\nPhrasing includes a joint effect of both expressive timing and dynamics.For example, there is a natural correlation between the melody rising, getting louder, and speeding up.These are not rules, however; skilled performers may deliberately choose to counteract such patterns to great effect.We can think of a score as a highly abstract representation of music.The effective use of scores, i.e. the assumption by a composer that a score will subsequently be well-rendered as music, relies on the existence of conventions, traditions, and individual creativity.For example, Chopin wrote scores where the pianist's use of rubato is expected, indeed the score requires it in order to make sense.Similarly, the melodies in jazz lead sheets were written with the understanding that they will be swung and probably embellished in various ways.There are numerous other instrument-specific aspects that scores do not explicitly represent, from the vibrato imbued by a string player to the tone of a horn player.Sometimes, the score just won't really make perceptual sense without these.\n\nIn short, the mapping from score to music is full of subtlety and complexity, all of which turns out to be very important in the perceptual impact that the music will have.To get a sense of the impact of these concepts, we recommend that the reader listen:\n\n\u2022 first to a direct rendering of the above score here: https://clyp.it/jhdkghso,played according to the written grid and quantized to 16 th notes.Then,\n\n\u2022 listen to an expressive performance [33] of it here: https://clyp.it/x24hp1pq.\n\n\nMIDI\n\nMIDI is a communication protocol for digital musical instruments: a symbolic representation, transmitted serially, that indicates Note On and Note Off events and allows for a high temporal sampling rate.The loudness of each note is encoded in a discrete quantity referred to as velocity (the name is derived from how fast a piano key is pressed).While MIDI encodes note timing and duration, it does not encode qualities such as timbre; instead, MIDI events are used to trigger playback of audio samples.MIDI can be visualized as a piano roll-a digital version of the old player piano rolls.Figure 2 is an example of a MIDI piano roll corresponding to the score shown in Figure 1.Each row corresponds to one of the 128 possible MIDI pitches.Each column corresponds to a uniform time step.If note i is ON at time t and had been pressed with velocity v, then element (t, i) = v.So, at 125 Hz, six seconds of MIDI data would be represented on a grid of size 128 \u00d7 (6 \u00d7 125).Actual MIDI sampling can be faster than this, so even at 125 Hz we are still subsampling from the finest available temporal grid.\n\nWe refer to a score that has been rendered directly into a MIDI file as a MIDI Score.That is, it is rendered with no dynamics and exactly according to the written metrical grid.As given earlier, https://clyp.it/jhdkghso is an example of this.\n\nIf, instead, a score has been performed, by a musician for example, and that performance has been encoded into a MIDI stream, we refer to that as a MIDI Performance.https://clyp.it/x24hp1pq is an example (also given previously) of a MIDI performance.\n\n2 Factoring the Music Generation Process: Related Work\n\nFigure 3 shows one way of factoring the music generation process.The first stage shown in this figure is composition, which yields a score.The score is then performed.The performance is rendered as sound, and finally that sound is perceived.In the analog world, of course, performance and rendering the sound are the same on a physical instrument, but in the digital world, those steps are often separate.While other views of the process are possible, this one provides us a helpful context for considering much of the existing relevant work.Noting that sound generation and perception (the last two steps in Figure 3) are outside our scope, in the rest of this section we focus primarily on composition and performance.Perhaps it is precisely because music is so often perceived as a profoundly human endeavour that there has also been, in parallel, an ongoing fascination with automating its creation.This fascination long predates notions such as the Turing test (ostensibly for discriminating automation of the most human behaviour), and has spawned a range of efforts: from attempts at the formalization of unambiguously strict rules of composition to incorporation of complete random chance into scores and performances.The use of rules exemplifies the algorithmic (and largely deterministic) approach to music generation, one that is interesting and outside the scope of the current work; for background on this we refer the reader, for example, to the text by Nierhaus [31].Our present work, Figure 3: Factoring music generation.We can see music as starting with the composition of a score; that score gets turned into a performance (shown as a MIDI piano roll); that MIDI roll, in turn, gets rendered into sound using a synthesizer, and finally the resulting audio gets perceived as music by a human listener.\n\non the other hand, lies in a part of the spectrum that incorporates probability and sampling.\n\nAleatory refers to music or art that involve elements of randomness, derived from the Latin alea (alee), meaning \"die (dice)\".Dice were used in the 1700's to create music in a game referred to as Musikalisches W\u00fcrfelspiel [31,16,2]: the rolled numbers were used to select from pre-composed fragments of music.Some of these compositions were attributed to Mozart and Haydn, though this has not been authenticated.\n\nTwo centuries later, as the foundations of AI were being set, the notion of automatically understanding (and therefore generating) music was among the earliest applications to capture the imagination of researchers, with papers on computational approaches to perception, interpretation and generation of music by Simon, Longuet-Higgins and others [23,24,25,26,36].Since then, many interesting efforts were made [15,39,30,9,34,19], and it is clear that in recent years both interest and progress in score generation has continued to advance, e.g.Lattner et al [22], Boulanger-Lewandowski et al [3], Bretan et al [5], Herremans et al [17], Roberts et al [35], Sturm [37], to name only a few.Briot et al [6] provide a survey of generative music models that involve machine learning.Herremans et al [18] provide a comprehensive survey and satisfying taxonomy of music generation systems.McDonald [28] gives an overview highlighting some key examples of such work.\n\nCorresponding to the second step in Figure 3 is a body of work often referred to as EMP (Expressive Musical Performance) systems.For example, the work by Chacon and Grachten [7], inspired by the Linear Basis Models proposed by Grachten and Widmer [14], involves defining a set of hand-engineered features, some of which depend on having a score with dynamic expression marks, others on heuristics for musical analysis (e.g. a basis function indicating whether the note falls on the first beat of a measure of 4/4).Widmer and Goebl [43] and Kirke and Miranda [21] both present extensive and detailed surveys of work done in the field of computational EMPs.In the latter survey, the authors also provide a tabular comparison of 29 systems that they have reviewed.Out of those systems, two use neural networks (one of which also uses performance rules) and a few more use PCA, linear regression, KCCA, etc.Some of the other systems that involve some learning, do so by learning rules in some way.For example, the KTH model [11] consists of a top-down approach for predicting performance characteristics from rules based on local musical context.Bresin [4] presents two variations of a neural network-based system for learning how to add dynamics and timing to MIDI piano performance.\n\nGrachten and Krebs [13] use a variety of unsupervised learning techniques to learn features with which they then predict expressive dynamics.Building on that work, van Herwaarden et al [42] use an interesting combination of an RBMbased architecture, a note-centered input representation, and multiple datasets to-again-predict expressive dynamics.In both of these cases, the dynamics predictions appear to depend on the micro-timing rather than being predicted jointly as in the present work.\n\nTeramura et al [20], observe that many previous performance rendering systems \"often consist of many heuristic rules and tend to be complex.It makes [it] difficult to generate and select the useful rules, or perform the optimization of parameters in the rules.\"They thus present a method that uses Gaussian Processes to achieve this, where some parameters can be learned.In their ostensibly simpler system, \"for each single note, three outputs and corresponding thirteen input features are defined, and three functions each of which returns one of three outputs and receive the thirteen input features, are independently learned\".However, some of these features, too, depend on certain information, e.g. they compute the differences between successive pitches, and this only works in compositions where the voice leading is absolutely clear; in the majority of classical piano repertoire, this is not the case.In Laminae [32], Okumura et al systematize a set of context-dependent models, building a decision tree which allows rendering a performance by combining contextual information.\n\nMoulieras and Pachet [29] use a maximum entropy model to generate expressive music, but their focus is again monophonic plus simple harmonic information.They also explicitly assume that \"musical expression consists in local texture, rather than long-range correlations\".While this is fairly reasonable at this point, and indeed it is hard to say how much long-range correlation is captured by our model, we wished to choose a model which, at least in principle, allowed the possibility of modeling long-range correlation: ultimately, we believe that these correlations are of fundamental importance.Malik and Ek [27] use a neural network to learn to predict the dynamic levels of individual notes while assuming quantized and steady timing.\n\n3 Choosing Assumptions and a Problem Domain\n\n\nAssumptions\n\nIn the case of both score production and interpretation, any computational model naturally makes assumptions.Let us review potential implications of some of these when generating music, and identify some of the choices we make in our own model in these respects.\n\n\u2022 Metric Abstraction Many systems abstract rhythm in relation to an underlying grid, with metric-based units such as eighth notes and triplets.\n\nOften this is further restricted to step sizes at powers of two.Such abstraction is oblivious to many essential musical devices, including e.g.rubato and swing as described in Section 1.1.1.Some EMP systems allow for variations in the global tempo, but this would not be able to represent common performance techniques such as playing of the melody slightly staggered from accompaniment (i.e.creating an asynchrony beyond what is written in the score).\n\nWe choose a temporal representation based on absolute time intervals between events, rounded to 8ms.\n\n\u2022 No Dynamics Nearly every compositional system represents notes as ON or OFF.This binary representation ignores dynamics, which constitute an essential aspect of how music is perceived.The EMP systems do tend to focus on dynamics.While many systems do not have audio readily available, we point out that listening to, e.g. the work of Malik and Ek [27] where a binned velocity value is predicted for each note, the abstracted and static tempo is still quite noticeable.When dynamic level is treated in some EMPs as a global parameter applied equally to simultaneous notes, this defeats the ability of dynamics to differentiate between voices, or to compensate for a dense accompaniment (that is best played quietly) underneath a sparse melody.We allow each note to have its own dynamic level.\n\n\u2022 Monophony Some systems only generate monophonic sequences.Admittedly, one must start somewhere: the need to limit to monophonic output is in this sense entirely understandable.This can work very well for instruments such as voice and violin, where the performer also has sophisticated control beyond quantized pitch and the velocity of the note attack.The perceived quality of monophonic sequences may be inextricably tied to these other dimensions that are difficult to capture and usually absent from MIDI sequences.\n\nIn our experience, the leap from monophonic to polyphonic generation is a significant one.A survey of the literature shows that most systems that admit polyphony still make assumptions about its nature-either that it is separable into chords, or that it is separable into voices, or that any microvariation in tempo applies to all voices at once (as opposed to allowing one voice to come in ahead of the beat), and so forth.Each of these assumptions is correct only sometimes.We settled on a representation that turned out to be simpler and more agnostic than this, in that it does not make any of these assumptions:\n\nWe specify note events one at a time, but allow the system to predict an arbitrary number of simultaneous notes, should it be so inclined.\n\nGenerally speaking, in contrast to many of the method discussed in Section 2, our approach makes no assumptions about the features other than the information that is known to exist in MIDI files: velocity, timing and duration of each note.We do not require computing or knowing the time signature, we do not require knowing the voice leading, we do not require inferring the chord, and so on.While additional information could be both useful and interesting, given the current state of the art and available data, we are focused on showing how much can be done without defining any rules or heuristics at all; we simply try to model the distribution of the existing data.Listening to some of the examples, one hears that our system generates a variety of natural time feels, including 3/4, 4/4 and odd time signatures, and they never feel rhythmically heavy-handed.\n\n\nProblem Domain: Simultaneously Composing and Performing\n\nIn Figure 4, we show a few different possible entry points to the music generation process.For example, at one extreme, we can subsume all steps into a single mechanism so as to predict audio directly, as is done by WaveNet, with impressive results [41].Another approach is to focus only on the instrument synthesis aspect [10], which is an interesting problem outside the scope of our present work.As described in Section 2, the compositional systems generate scores that require performances, while the EMP systems require scores in order to generate performances.\n\nHere, we demonstrate that jointly predicting composition and performance with expressive timing and dynamics, as illustrated in Figure 4(d), is another effective domain for music generation given the current state of the art.Furthermore, it creates output that can be listened to without requiring additional steps beyond audio synthesis as provided by a piano sample library.\n\nWhile the primary evidence for this will be found simply by listening to the results, we mention two related discussion points about the state of the art:\n\n\u2022 Music with very long-term, fully coherent structure is still elusive.In \"real\" compositions, long-term structure spans the order of many minutes  and is coherent on many levels.There is no current system that is able to learn such structure effectively.That is, if \u2206t = 8ms, then even for just 2 minutes, P (e i+15000 |e i ) should be different from P (e i+15000 ).There is no current system that effectively achieves anywhere near this for symbolic MIDI representation.\n\n\u2022 Metrics for evaluating generated music are very limited.Theis and others [38,40] have given clear arguments about the limitations of metrics for evaluating the quality of generative models in the case of visual models, and their explanations extend naturally to the case of musical and audio models.In particular, they point out that ultimately, \"models need to be evaluated directly with respect to the application(s) they were intended for\".In the case of the generative music models that we are considering, this involves humans listening.\n\nTaken together, what this means is that systems that generate musical scores face a significant evaluation dilemma.Since by definition any listening-based evaluation must operate in the audio space, either a) the scores must be rendered directly and will lack expression entirely, or b) a human or other system must perform the scores, in which case the quality of the generated score is hard to disentangle from the quality of the performance. 4Furthermore, the lack of long-term structure compounds the difficulty of evaluation, because one of the primary qualities of a good score is precisely in its long-term structure.This implicitly bounds the potential significance of evaluating a short and contextfree compositional fragment.\n\nWith these considerations in mind, we generate directly in the domain of musical performance.A side benefit of this is that informal evaluation becomes more potentially meaningful: musicians and non-musicians alike can listen to clips of generated performances while (1) not being put off by the lack of expressiveness and (2) not needing to disentangle the different elements that contributed to what they hear, since both the notes and how they are all played were all generated by the system. 5We also note that our approach is consistent with many of the points and arguments recently made by Widmer [44].\n\n\nData\n\nIf we wish to predict expressive performance, we need to have the appropriate data.We use the International Piano-e-Competition dataset [1], which contains MIDI captures of roughly 1400 performances by skilled pianists.The pianists were playing a Disklavier, which is a real piano that also has internal sensors that record MIDI events corresponding to the performer's actions.The critical importance of good data is well-known for machine learning in general, but here we note some particular aspects of this data set that made it well-suited for our task.\n\n\nHomogeneous\n\nThe data set was homogeneous in a set of important ways.It might be easy to underestimate the importance of any of the following criteria, and so we list them all explicitly here with some discussion:\n\nFirst, it was all classical music.This helps the coherence of the output.\n\nSecond, it was all solo instrumental music.If one includes data that is for two or more instruments, then it no longer makes sense to train a generative model that is expected to generate for a solo instrument; there will be many (if not most) passages where what one instrument is doing is entirely dependent on what the other instrument is doing.The text analogy would be hoping for a system to learn to write novels by training it on only one character's dialogue from movies and plays.There will occasionally be self-sufficient monologues, but generally speaking, well-written dialogue has already been distilled by the playwright, and makes more sense when voices are not removed from it.\n\nThird, that solo instrument was consistently piano.Classical composers generally write in a way that is very specific to whichever instrument they are writing for.Each instrument has its own natural characteristics, and classical music scores (i.e. that which is captured in the MIDI representation) are very closely related to the timbre of that instrument (i.e.how those notes will be \"rendered\").One exception to this is that Bach's music tends to sound quite good on any instrument, e.g. it is OK to train a piano system on Bach vocal chorales.\n\nFourth, the piano performances were all done by humans.The system did not have to contend with learning from a dataset where some of the examples were synthesized, some were \"hand-synthesized\" to appear like human performances, etc.Each of those classes has its own patterns of micro-timing and dynamics, and each may be well-suited for a variety of music-related tasks, but for training a system on performances, it is very helpful that all the performances are indeed. . .performances.\n\nFinally, all of those humans were experts.If we wish the system to learn about human performance, that human performance must match the listener's concept of what \"human performance\" sounds like, which is usually performances by experts.The casual evaluator might find themselves slightly underwhelmed were they to listen to a system that has learned to play like a beginning pianist, even if the system has done so with remarkable fidelity to the dynamic and velocity patterns that occur in that situation.\n\n\nRealizable\n\nThe fact that the solo instrument was piano had additional advantages.Synthesizing audio from MIDI can be a challenging problem for some instruments.For example, having velocities and note durations and timing of violin music would not immediately lead to good-sounding violin audio at all.The problems are even more evident if one considers synthesizing vocals from MIDI.That the piano is a percussive instrument buys us an important benefit: synthesizing piano music from MIDI can sound quite good.Thus, when we generate data we can properly realize it in audio space and therefore have a good point of comparison.Conversely, capturing the MIDI data of piano playing provides us with a sufficiently rich set of parameters that we can later learn enough in order to be able to render audio.Note that with violin or voice, for example, we would need to capture many more parameters than those typically available in the MIDI protocol in order to get a sufficiently meaningful set of parameters for expressive performance.\n\n\nRNN Model\n\nWe modeled the performance data with an LSTM-based Recurrent Neural Network.The model consisted of three layers of 512 cells each, although the network did not seem particularly sensitive to this hyperparameter.We used a temporally non-uniform representation of the data, as described next.\n\n\nRepresentation: Time-shift\n\nA MIDI excerpt is represented as a sequence of events from the following vocabulary of 413 different events:\n\n\u2022 128 NOTE-ON events: one for each of the 128 MIDI pitches.Each one starts a new note.\n\n\u2022 128 NOTE-OFF events: one for each of the 128 MIDI pitches.Each one releases a note.\n\n\u2022 125 TIME-SHIFT events: each one moves the time step forward by increments of 8 ms up to 1 second.\n\n\u2022 32 VELOCITY events: each one changes the velocity applied to all subsequent notes (until the next velocity event).The neural network operates on a one-hot encoding over this event vocabulary.Thus, at each step, the input to the RNN is a single one-hot 413dimensional vector.For the piano-e-competition dataset, a 15-second clip typically contains 600 such one-hot vectors, although this varies considerably (and roughly linearly with the number of notes in the clip).\n\nWhile the minimal time step is a fixed absolute size (8 ms), the model can skip forward in time to the next note event.Thus, any time steps that contain rests or simply hold existing notes can be skipped with a single event.The largest possible single time shift in our case is 1 second but time shifts can be applied consecutively to allow effectively longer shifts.The combination of fine quantization and time-shift events helps maintain expressiveness in note timings while greatly reducing sequence length compared to an uncompressed representation.\n\nThis fine quantization is able to maintain expressiveness in note timings while not being as sparse as a grid-based representation.This sequence representation uses more events in sections with higher note density, which matches our intuition.Figure 5 shows an example of a small excerpt of MIDI performance data converted to our representation.Figure 6 shows a diagram of the basic RNN architecture.\n\n\nTraining and Data Augmentation\n\nWe train the models by first separating the data into 30-second clips, from which we then select shorter segments.We train using stochastic gradient descent with LSTM layer LSTM layer target output\n\n\nP( output event | input )\n\nevent from prev step\n\n\nLSTM layer\n\nFigure 6: The basic RNN architecture consists of three hidden layers of LSTMs, each layer with 512 cells.The input is a 413-dimensional one-hot vector, as is the target, and the model outputs a categorical distribution over the same dimensionality as well.For generation, the output is sampled stochastically with beam search, while teacher forcing is used for training.a mini-batch size of 64 and a learning rate of 0.001 and teacher forcing.\n\n\nAugmentation\n\nWe augment the data in two different ways, for different runs:\n\nLess augmentation:\n\n\u2022 Each example is transposed up and down all intervals up to a major third, resulting in 8 new examples plus the original.\n\n\u2022 Each example is stretched in time uniformly by \u00b12.5% and \u00b15%, resulting in 4 new examples plus the original.\n\nMore augmentation:\n\n\u2022 Each example is transposed up and down all intervals up to 5 or 6 semitones to span a full octave, resulting in 11 new examples plus the original.\n\n\u2022 Each example is stretched in time uniformly by up to \u00b110%.\n\n\nQuantization\n\nIn Section 3 we describe several forms of quantization that can be harmful to perceived musical quality.Our models also operate on quantized data; however, unlike much prior work we aim for quantization levels that are below noticeable perceptual thresholds.\n\n\nTiming\n\nFriberg and Sundberg [12] found that the just noticeable difference (JND) when temporally displacing a single tone in a sequence was generally no finer than roughly 10ms.Other studies have found that the JND for change in tempo is no finer than roughly 5%.We note that for a tempo of 120bpm, each beat lasts for 500ms, and therefore this corresponds to a change of roughly 25ms.Given that at that tempo beats will frequently still be subdivided into 2 or triplets, that would correspond to a change of roughly 8 ms per subdivided unit.We therefore assume that using a sampling rate of 125Hz (i.e.1000/8) should generally be below the typical perceptual threshold.\n\n\nDynamics\n\nWorking with piano music, we have found that 32 different \"steps\" of velocity are sufficient.Note that there are about 8 levels of common dynamic marking in classical music (from ppp to fff), so it may well be the case that we could do with fewer than 32 bins, but our objective was not to find the lower bound here.\n\n\nPredicting Pedal\n\nIn the RNN model, we experimented with predicting sustain pedal.We applied Pedal On by directly extending the lengths of the notes: for any notes on during or after a Pedal On signal, we delay their corresponding Note Off events until the next Pedal Off signal.This made it a lot easier for the system to accurately predict a whole set of Note Off events all at once, as well as to predict the corresponding delay preceding this.Doing so may have also freed up resources to focus on better prediction of other events as well.Finally, as one might expect, including pedal made a significant subjective improvement in the quality of the resulting output.\n\n\nResults\n\nWe begin with the most important indicator of performance: generated audio examples.\n\n\nExamples\n\nIn these examples, our systems generated all MIDI events: timing and duration of notes as well as note velocities.We then used freely-availably piano samples to synthesize audio from the resulting MIDI file.\n\nA small set of examples are available at https://clyp.it/user/3mdslat4.We strongly encourage the reader to listen.These examples are representative of the general output of the model.We comment on a few samples in particular, to give a sense of the kind of musical structure that we observe:\n\n\u2022 RNN Sample 4: This starts off with a slower segment that goes through a very natural harmonic progression in G minor, pauses on the dominant chord, and then breaks into a faster section that starts with a G major chord, then passes through major chords related to G minor (Bb, etc).\n\nHarmonically, this shows structural coherence even while the tempo and feel shift.At around 12s, the \"left hand\" uses dynamics to bring out an inner voice in a very natural and appropriate way.\n\n\u2022 RNN Sample 7: This excerpt begins very reminiscent of a Schubert Impromptu, although it is sufficiently different that it has clearly not memorized it.There is a small rubato at the very beginning of the phrase, especially on the first note, which is musically appropriate.The swells in the phrasing make musical sense, as do the slight pauses right before some of the isolated notes in the left hand (e.g. the E at 0:10s, the F\u266f at around 12.5 seconds).and then at around 8 seconds, the left hand mirrors that articulation pattern with a set of descending repeated notes (A\u266d, G, F).\n\n\nLog-likelihood\n\nWe begin by noting that objective evaluation of these kinds of generative is fundamentally very difficult, and measures such as log-likelihood can be quite misleading [38].Nevertheless, we provide comparisons here over several different hyperparameter configurations for the RNN.Table 1 contains the per-time-step log-loss of several RNN model variants.The baseline model is trained on 15-second performance clips, ignoring sustain pedal and with the two forms of data augmentation described in Section 5.1.\n\nNote that while RNN-NV has the best log-loss, this variant is inherently easier as the model does not need to predict velocities.In the RNN-SUS variant, sustain pedal is used to extend note durations until the pedal is lifted; this aids prediction as discussed in Section 5.1.3.\n\n\nInformal Feedback From Professional Composers and Musicians\n\nWe gave a small set of clips to professional musicians and composers for informal comments.We were not trying to do a Turing test, so we mentioned that the clips were generated by an automated system, and simply asked for any initial reactions/comments.Here is a small, representative subset of the comments we received (musical background in bold, some particularly interesting excerpts are italicized for later discussion):\n\nTV/Film composer: In terms of performance I'm quite impressed with the results.It sounds more expressive than any playback feature Ive worked with when using composition software.\nFantastic!!!!\nIn terms of composition, I think there is more room for improvement.The main issue is lack of consistency in rhythmic structure and genre or style.For example, Sample 1 starts with a phrase in Mozarts style, then continues with a phrase in Waltons style perhaps, which then turns into Scott Joplin. . .Sample 2 uses the harmonic language of a late Mahler symphony, along with the rhythmic language of a free jazz improvisation (I couldnt make a time signature out of this clip).Sample 3 starts with a phrase that could be the opening of a Romantic composition, and then takes off with a rhythmic structure that resembles a Bach composition, while keeping the Romantic harmonic language.Sample 4 is the most consistent of all.It sounds like a composition in the style of one of the Romantic piano composers (such as Liszt perhaps) and remains in that style throughout the clip.\n\nMusic Professor:\n\n[. ..]I'd guess human because of a couple of \"errors\" in there, but maybe the AI has learned to throw some in! [. ..]\n\n\nPianist, TV & Film Composer:\n\nSample 1: resembles music in the style of Robert Schumann's Kinderszenen or some early romantic salon music.I'm fond of the rest after the little initial chord and melody structure.The tempo slows down slightly before the rest which sounds really lively and realistic -almost a little rubato.Then the distinct hard attack.Nice sense of dynamics.Also nice ritardando at the end of the snippet.Not liking the somewhat messy run but this almost seems as if someone had to study a little bit harder to get it right -it seems wrong in a human way.\n\nSample 2: reminds me of some kind of Chopin waltz, rhythm is somewhat unclear.The seemingly wrong harmony at the beginning seems to be a misinterpretation of grace notes.The trill is astonishing and feels light and airy.Overall, we note that the comments were quite consistent in terms of perceiving a human quality to the performance.Indeed, even though we made an effort to explain that all aspects of the MIDI file were generated by the computer, some people still wanted to double check whether in fact these were human performances.\n\nWhile acknowledging the human quality of the performances, many of the musicians also questioned the strength of the long-term compositional structure.Indeed, creating music with long-term structure (e.g. more than several seconds of structure) is still a very challenging problem.\n\nMany musicians identified the 'style' as the mix of classical composers of which the data indeed consisted.\n\n\nConclusion\n\nWe have considered various approaches to the question of generating music, and propose that it is currently effective to generate in the space of MIDI performances.We describe the characteristics of an effective data set for doing so, and demonstrate a system that achieves this quite effectively.\n\nOur resulting system creates audio that sounds, to our ears, like a pianist who knows very well how to play, but has not yet figured out exactly what they want to play, nor is quite able remember what they just played.Professional composers and musicians have provided feedback that is consistent with the notion that the system generates music which, on one hand, does not yet demonstrate long-term structure, but where the local structure, e.g.phrasing, dynamics, is very strong.Indeed, even though we did not frame the question as a Turing test, a number of the musicians assumed that (or asked whether) the samples were performed by a human.\n\nFigure 1 :\n1\nFigure 1: Excerpt from the score of Chopin's Piano Concerto No. 1.\n\n\nFigure 2 :\n2\nFigure 2: Piano roll based on the score in Figure 1.The horizontal axis represents time; the vertical axis represents pitch; each rectangle is a note; and the length of the rectangle corresponds to the duration of the note.\n\n\nFigure 4 :\n4\nFigure 4: Here are four other entry points into the generative process.The magenta arrows represents machine-learned generators.(a) One extreme, exemplified by WaveNet [41], is to jump directly into the generation of the audio, as shown on the top.(b) The next diagram represents learning the instrument synthesizer model (e.g.NSynth[10]).(c) The third diagram represents generating scores, i.e. learning to compose.In that case, some unspecified process is still needed in order to convert the score into audio, and therein lies one of the problems with score-based generation at the moment.(d) Finally, the bottom diagram represents bypassing the generation of scores, and directly generating performances, as we propose here.In this case, a synthetic instrument is needed in order to convert the performance into audio.For an instrument such as piano, doing this quite well is feasible.\n\n\nFigure 5 :\n5\nFigure 5: Example of Representation used for PerformanceRNN.The progression illustrates how a MIDI sequence (e.g.shown as a MIDI roll consisting of a long note followed by a shorter note) is converted into a sequence of commands (on the right hand side) in our event vocabulary.Note that an arbitrary number of events can in principle occur between two time shifts.\n\n\nSample 3 :\n3\nCould be some piece by Franz Schubert.Nice loosely feeling opening structure which shifts convincingly into fierce sequence with quite static velocity.This really reminds me of Schubert because Johann Sebastian Bach shines through the harmonic structure as it would have with Schubert.Interesting effort to change the dynamic focus from the right to the left hand and back again.This is really interesting!Piano Teacher:Sample 1: Sounded almost Bach-like for about the first bar, then turned somewhat rag-timey for the rest Sample 2: Here we have a very drunken Chopin, messing around a bit with psychedelics Does that help at all?Also, what do you mean by a regular piano sample library?Did you play these clips as composed by the AI system?\n\n\n\n\n\n\n\nTable 1 :\n1\nLog-loss of RNN model variants trained on the Piano-e-competition performance dataset and evaluated on a held-out subset.\n\u2022 RNN Sample 2: This excerpt begins in a classical style (e.g. Haydn orMozart). Interestingly, the same way that one note (an F) is repeated inthe right hand in the first few seconds, after a pause, the next phrase begins\n\n\n\nHow many hours of learning [. ..] here?\nquite nicely. I think that it's not far from actually coming up witha worthwhile melody. [. . .] How does it know what \"inspirationalemotion\" to draw from? or is it mostly doing things \"in the likenessof\"?Fascinating!!Composer & Professional MusicianThis [. . .] absolutely blows the stuff I've heard online out of the solarsystem. The melodic sense is still foggy, in my view, but it's stagger-ing that it makes nice pauses with some arcing chord progressions\nThis quote has been attributed to a range of individuals from Laurie Anderson to Miles Davis, and numerous others.\nIn this text, we use the term \"generation\" to refer to computational generation, as opposed to human creation or performance.\nWhile explaining swing is outside the current scope, we do note that it is occasionally incorrectly described in terms of triplets.\nFor example, listening to the direct score and performance clips given above, it should be clear that other than perhaps very experienced musicians, it would be extremely difficult for a listener to hear the audio of the MIDI Score and intuitively understand that that same passage could sound as it does in the MIDI Performance.\nWe emphasize that these observations do not apply to the development of tools for composers, where score fragment generation might be appropriate. Also, we reiterate that this discussion is made in relation to the current state of the art.\nAcknowledgmentsWe gratefully acknowledge all of the musicians who provided feedback on the samples.We thank members and visitors at Google Brain and specifically the Magenta team for discussions, including Adam Roberts, Anna Huang, Colin Raffel, Curtis Hawthorne, David Ha, David So, Fred Bertch, George Dahl, Jesse Engel, Kory Mathewson, Kyle Kastner, Natasha Jaques and Tim Cooijmans.Finally, we thank the reviewers for their useful feedback.\nZur Theorie der offenen Form in der neuen Musik. Konrad Boehmer, 1967Edition TonosDarmstadt\n\nModeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. Nicolas Boulanger-Lewandowski, Yoshua Bengio, Pascal Vincent, Proceedings of the 29th International Conference on Machine Learning. the 29th International Conference on Machine Learning2012\n\nArtificial neural networks based models for automatic performance of musical scores. R Bresin, J. New Music Res. 271998\n\nDeep music: Towards musical dialogue. Mason Bretan, Sageev Oore, Jesse Engel, Douglas Eck, Larry Heck, Proc. AAAI. AAAI2017\n\nDeep learning techniques for music generation -A survey. Jean-Pierre Briot, Ga\u00ebtan Hadjeres, Fran\u00e7ois Pachet, CoRR, abs/1709.016202017\n\nThe basis mixer: A computational romantic pianist. C E Cancino Chacn, M Grachten, Late-Breaking Demo Session of the 17th International Society for Music Information Retrieval Conference. New York, NY2016\n\nPiano Concerto No. 1 in E minor. Fr\u00e9d\u00e9ric Chopin, 1830\n\nFinding temporal structure in music: blues improvisation with lstm recurrent networks. D Eck, J Schmidhuber, Proceedings of the 12th IEEE Workshop on Neural Networks for Signal Processing. the 12th IEEE Workshop on Neural Networks for Signal ProcessingSept 2002\n\nNeural audio synthesis of musical notes with wavenet autoencoders. Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Douglas Eck, Karen Simonyan, Mohammad Norouzi, 2017\n\nOverview of the kth rule system for musical performance. A Friberg, R Bresin, Sundberg, Advances in Cognitive Psychology. 22-32006\n\nPerception of just noticeable time displacement of a tone presented in a metrical sequence at different tempos. Anders Friberg, Johan Sundberg, 1992\n\nAn assessment of learned score features for modeling expressive dynamics in music. M Grachten, F Krebs, IEEE Transactions on Multimedia. 1652014\n\nLinear basis models for prediction and analysis of musical expression. Maarten Grachten, Gerhard Widmer, Journal of New Music Research. 4142012\n\nMusical Networks: Parallel Distributed Perception and Performance. Niall Griffith, Peter M Todd, 1999\n\nDice music in the eighteenth century. Stephen Hedges, Music and Letters. 591978\n\nMorpheus: Automatic music generation with recurrent pattern constraints and tension. D Herremans, E Chew, IEEE Transactions on Affective Computing. \n\nA functional taxonomy of music generation systems. Dorien Herremans, Ching-Hua Chuan, Elaine Chew, ACM Comput. Surv. 50530September 2017\n\nHarmonet: A neural net for harmonizing chorales in the style of j.s.bach. Hermann Hild, Johannes Feulner, Wolfram Menzel, Proceedings of the 4th International Conference on Neural Information Processing Systems, NIPS'91. the 4th International Conference on Neural Information Processing Systems, NIPS'91San Francisco, CA, USAMorgan Kaufmann Publishers Inc1991\n\nGaussian process regression for rendering music performance. Y , Taniguchi S Makimoto, K Teramura, H Okuma, S Maeda, Proceedings of the 10th International Conference on Music Perception and Cognition. the 10th International Conference on Music Perception and CognitionJapan2008ICMPC 10\n\nAn overview of computer systems for expressive music performance. A Kirke, E R Miranda, Guide to Computing for Expressive Music Performance. LondonSpringer-Verlag2013\n\nImposing higherlevel structure in polyphonic music generation using convolutional restricted boltzmann machines and constraints. Stefan Lattner, Maarten Grachten, Gerhard Widmer, 2017\n\nTowards a generative theory of melody. B Lindblom, Sundberg, Svensk Tidskrift fr Musikforskning. 521970\n\nThe perception of melodies. H C Longuet- Higgins, Nature. 2631976\n\nThe perception of music. H C Longuet- Higgins, Interdisciplinary Science Review. 31978\n\n. H C Longuet- Higgins, M Steedman, On interpreting bach. Machine Intelligence. 61971\n\nNeural translation of musical style. Iman Malik, Carl Henrik, Ek , CoRR, abs/1708.035352017\n\nKyle Mcdonald, Neural nets for generating music. Medium. 2017. -November-201715\n\nMaximum entropy models for generation of expressive music. S Moulieras, F Pachet, 2016\n\nNeural network composition by prediction: Exploring the benefits of psychophysical constraints and multiscale processing. C Michael, Mozer, 1994\n\nAlgorithmic Composition: Paradigms of Automated Music Generation. Gerhard Nierhaus, 2009SpringerVienna\n\nLaminae: A stochastic modelingbased autonomous performance rendering system that elucidates performer characteristics. K Okumura, S Sako, T Kitamura, International Computer Music Conference (ICMC). \n\n. Greece Athens, 2014\n\nRecording of Chopin Piano Concerto No. 1 in E Minor. Sageev Oore, Op. 111 st movement, 2017unreleased\n\nThe continuator: Musical interaction with style. Franois Pachet, Journal of New Music Research. 3232003\n\nInteractive musical improvisation with magenta. A Roberts, J Engel, C Hawthorne, I Simon, E Waite, S Oore, N Jaques, C Resnick, D Eck, Demonstration Track in Neural Information Processing Systems (NIPS). 2016\n\nPattern in music. H A Simon, R K Sumner, Formal Representation of Human Judgement. Kleinmuntz, New YorkJohn Wiley1968\n\nMusic transcription modelling and composition using deep learning. Bob Sturm, Joao Felipe Santos, Oded Ben-Tal, Iryna Korshunova, Proc. 1st Conf. Computer Simulation of Musical Creativity. 1st Conf. Computer Simulation of Musical CreativityHuddersfield, UKJuly 2016\n\nA note on the evaluation of generative models. Lucas Theis, Aron Van Den Oord, Matthias Bethge, ICLR2016\n\nPeter M Todd, Gareth Loy, Music and Connectionism. MIT Press1991\n\nLocally-connected transformations for deep gmms. A Van Den Oord, J Dambre, Proceedings of the 32 nd International Conference on Machine Learning. the 32 nd International Conference on Machine LearningLille, France2015\n\nWavenet: A generative model for raw audio. A\u00e4ron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W Senior, Koray Kavukcuoglu, CoRR, abs/1609.034992016\n\nPredicting expressive dynamics using neural networks. S Van Herwaarden, M Grachten, W B Haas, Proceedings of the 15th Conference of the International Society for Music Information Retrieval. the 15th Conference of the International Society for Music Information Retrieval2014\n\nComputational models of expressive music performance: The state of the art. G Widmer, W Goebl, Journal of New Music Research. 333\n\nGetting closer to the essence of music: The con espressione manifesto. Gerhard Widmer, CoRR, abs/1611.097332016\n", "annotations": {"author": "[{\"end\":117,\"start\":82},{\"end\":151,\"start\":118},{\"end\":191,\"start\":152},{\"end\":227,\"start\":192},{\"end\":266,\"start\":228},{\"end\":305,\"start\":267},{\"end\":344,\"start\":306},{\"end\":379,\"start\":345},{\"end\":418,\"start\":380},{\"end\":451,\"start\":419}]", "publisher": null, "author_last_name": "[{\"end\":93,\"start\":89},{\"end\":127,\"start\":122},{\"end\":167,\"start\":159},{\"end\":203,\"start\":200},{\"end\":242,\"start\":234},{\"end\":281,\"start\":275},{\"end\":320,\"start\":314},{\"end\":355,\"start\":347},{\"end\":394,\"start\":388},{\"end\":427,\"start\":419}]", "author_first_name": "[{\"end\":88,\"start\":82},{\"end\":121,\"start\":118},{\"end\":158,\"start\":152},{\"end\":199,\"start\":192},{\"end\":233,\"start\":228},{\"end\":272,\"start\":267},{\"end\":274,\"start\":273},{\"end\":311,\"start\":306},{\"end\":313,\"start\":312},{\"end\":346,\"start\":345},{\"end\":385,\"start\":380},{\"end\":387,\"start\":386}]", "author_affiliation": "[{\"end\":116,\"start\":95},{\"end\":150,\"start\":129},{\"end\":190,\"start\":169},{\"end\":226,\"start\":205},{\"end\":265,\"start\":244},{\"end\":304,\"start\":283},{\"end\":343,\"start\":322},{\"end\":378,\"start\":357},{\"end\":417,\"start\":396},{\"end\":450,\"start\":429}]", "title": "[{\"end\":64,\"start\":1},{\"end\":515,\"start\":452}]", "venue": null, "abstract": "[{\"end\":1381,\"start\":666}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3248,\"start\":3245},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6513,\"start\":6509},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9694,\"start\":9690},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10354,\"start\":10350},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10357,\"start\":10354},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10359,\"start\":10357},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10893,\"start\":10889},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10896,\"start\":10893},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10899,\"start\":10896},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10902,\"start\":10899},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10905,\"start\":10902},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10957,\"start\":10953},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10960,\"start\":10957},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10963,\"start\":10960},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10965,\"start\":10963},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10968,\"start\":10965},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10971,\"start\":10968},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11105,\"start\":11101},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11138,\"start\":11135},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11156,\"start\":11153},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11178,\"start\":11174},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11198,\"start\":11194},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11210,\"start\":11206},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11246,\"start\":11243},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11341,\"start\":11337},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11438,\"start\":11434},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11680,\"start\":11677},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11754,\"start\":11750},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12038,\"start\":12034},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12065,\"start\":12061},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12527,\"start\":12523},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12655,\"start\":12652},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12808,\"start\":12804},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12974,\"start\":12970},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13298,\"start\":13294},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14204,\"start\":14200},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14392,\"start\":14388},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14983,\"start\":14979},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16486,\"start\":16482},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":19386,\"start\":19382},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19460,\"start\":19456},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20788,\"start\":20784},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20791,\"start\":20788},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":22600,\"start\":22596},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30503,\"start\":30499},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":34010,\"start\":34006},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":39462,\"start\":39458}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38871,\"start\":38790},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39110,\"start\":38872},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40015,\"start\":39111},{\"attributes\":{\"id\":\"fig_4\"},\"end\":40396,\"start\":40016},{\"attributes\":{\"id\":\"fig_5\"},\"end\":41154,\"start\":40397},{\"end\":41159,\"start\":41155},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":41516,\"start\":41160},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":42020,\"start\":41517}]", "paragraph": "[{\"end\":1918,\"start\":1397},{\"end\":2889,\"start\":1967},{\"end\":3192,\"start\":2891},{\"end\":3601,\"start\":3203},{\"end\":4160,\"start\":3603},{\"end\":4978,\"start\":4162},{\"end\":6058,\"start\":4980},{\"end\":6316,\"start\":6060},{\"end\":6469,\"start\":6318},{\"end\":6551,\"start\":6471},{\"end\":7659,\"start\":6560},{\"end\":7903,\"start\":7661},{\"end\":8155,\"start\":7905},{\"end\":8211,\"start\":8157},{\"end\":10031,\"start\":8213},{\"end\":10126,\"start\":10033},{\"end\":10540,\"start\":10128},{\"end\":11501,\"start\":10542},{\"end\":12783,\"start\":11503},{\"end\":13277,\"start\":12785},{\"end\":14365,\"start\":13279},{\"end\":15107,\"start\":14367},{\"end\":15152,\"start\":15109},{\"end\":15430,\"start\":15168},{\"end\":15575,\"start\":15432},{\"end\":16029,\"start\":15577},{\"end\":16131,\"start\":16031},{\"end\":16926,\"start\":16133},{\"end\":17448,\"start\":16928},{\"end\":18066,\"start\":17450},{\"end\":18206,\"start\":18068},{\"end\":19073,\"start\":18208},{\"end\":19699,\"start\":19133},{\"end\":20077,\"start\":19701},{\"end\":20233,\"start\":20079},{\"end\":20707,\"start\":20235},{\"end\":21253,\"start\":20709},{\"end\":21990,\"start\":21255},{\"end\":22601,\"start\":21992},{\"end\":23167,\"start\":22610},{\"end\":23383,\"start\":23183},{\"end\":23458,\"start\":23385},{\"end\":24153,\"start\":23460},{\"end\":24703,\"start\":24155},{\"end\":25192,\"start\":24705},{\"end\":25701,\"start\":25194},{\"end\":26737,\"start\":25716},{\"end\":27041,\"start\":26751},{\"end\":27180,\"start\":27072},{\"end\":27268,\"start\":27182},{\"end\":27355,\"start\":27270},{\"end\":27456,\"start\":27357},{\"end\":27927,\"start\":27458},{\"end\":28483,\"start\":27929},{\"end\":28885,\"start\":28485},{\"end\":29117,\"start\":28920},{\"end\":29167,\"start\":29147},{\"end\":29625,\"start\":29182},{\"end\":29704,\"start\":29642},{\"end\":29724,\"start\":29706},{\"end\":29848,\"start\":29726},{\"end\":29960,\"start\":29850},{\"end\":29980,\"start\":29962},{\"end\":30130,\"start\":29982},{\"end\":30192,\"start\":30132},{\"end\":30467,\"start\":30209},{\"end\":31141,\"start\":30478},{\"end\":31470,\"start\":31154},{\"end\":32143,\"start\":31491},{\"end\":32239,\"start\":32155},{\"end\":32459,\"start\":32252},{\"end\":32752,\"start\":32461},{\"end\":33038,\"start\":32754},{\"end\":33233,\"start\":33040},{\"end\":33820,\"start\":33235},{\"end\":34346,\"start\":33839},{\"end\":34626,\"start\":34348},{\"end\":35115,\"start\":34690},{\"end\":35296,\"start\":35117},{\"end\":36187,\"start\":35311},{\"end\":36205,\"start\":36189},{\"end\":36324,\"start\":36207},{\"end\":36899,\"start\":36357},{\"end\":37438,\"start\":36901},{\"end\":37721,\"start\":37440},{\"end\":37830,\"start\":37723},{\"end\":38142,\"start\":37845},{\"end\":38789,\"start\":38144},{\"end\":38870,\"start\":38804},{\"end\":39109,\"start\":38886},{\"end\":40014,\"start\":39125},{\"end\":40395,\"start\":40030},{\"end\":41153,\"start\":40411},{\"end\":41294,\"start\":41173},{\"end\":41559,\"start\":41520}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":35310,\"start\":35297}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":34125,\"start\":34124}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1395,\"start\":1383},{\"attributes\":{\"n\":\"1.1\"},\"end\":1965,\"start\":1921},{\"attributes\":{\"n\":\"1.1.1\"},\"end\":3201,\"start\":3195},{\"attributes\":{\"n\":\"1.1.2\"},\"end\":6558,\"start\":6554},{\"attributes\":{\"n\":\"3.1\"},\"end\":15166,\"start\":15155},{\"attributes\":{\"n\":\"3.2\"},\"end\":19131,\"start\":19076},{\"attributes\":{\"n\":\"4\"},\"end\":22608,\"start\":22604},{\"attributes\":{\"n\":\"4.1\"},\"end\":23181,\"start\":23170},{\"attributes\":{\"n\":\"4.2\"},\"end\":25714,\"start\":25704},{\"attributes\":{\"n\":\"5\"},\"end\":26749,\"start\":26740},{\"attributes\":{\"n\":\"5.0.1\"},\"end\":27070,\"start\":27044},{\"attributes\":{\"n\":\"5.1\"},\"end\":28918,\"start\":28888},{\"end\":29145,\"start\":29120},{\"end\":29180,\"start\":29170},{\"attributes\":{\"n\":\"5.1.1\"},\"end\":29640,\"start\":29628},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":30207,\"start\":30195},{\"end\":30476,\"start\":30470},{\"end\":31152,\"start\":31144},{\"attributes\":{\"n\":\"5.1.3\"},\"end\":31489,\"start\":31473},{\"attributes\":{\"n\":\"6\"},\"end\":32153,\"start\":32146},{\"attributes\":{\"n\":\"6.1\"},\"end\":32250,\"start\":32242},{\"attributes\":{\"n\":\"6.2\"},\"end\":33837,\"start\":33823},{\"attributes\":{\"n\":\"6.3\"},\"end\":34688,\"start\":34629},{\"end\":36355,\"start\":36327},{\"attributes\":{\"n\":\"7\"},\"end\":37843,\"start\":37833},{\"end\":38801,\"start\":38791},{\"end\":38883,\"start\":38873},{\"end\":39122,\"start\":39112},{\"end\":40027,\"start\":40017},{\"end\":40408,\"start\":40398},{\"end\":41170,\"start\":41161}]", "table": "[{\"end\":41516,\"start\":41295},{\"end\":42020,\"start\":41560}]", "figure_caption": "[{\"end\":38871,\"start\":38803},{\"end\":39110,\"start\":38885},{\"end\":40015,\"start\":39124},{\"end\":40396,\"start\":40029},{\"end\":41154,\"start\":40410},{\"end\":41159,\"start\":41157},{\"end\":41295,\"start\":41172},{\"end\":41560,\"start\":41519}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3211,\"start\":3210},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7158,\"start\":7157},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7238,\"start\":7237},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":8221,\"start\":8220},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":8830,\"start\":8829},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":9721,\"start\":9720},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":11547,\"start\":11546},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19144,\"start\":19143},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19840,\"start\":19836},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28736,\"start\":28735},{\"end\":28838,\"start\":28837},{\"end\":29190,\"start\":29189}]", "bib_author_first_name": "[{\"end\":43464,\"start\":43458},{\"end\":43633,\"start\":43626},{\"end\":43663,\"start\":43657},{\"end\":43678,\"start\":43672},{\"end\":43903,\"start\":43902},{\"end\":43981,\"start\":43976},{\"end\":43996,\"start\":43990},{\"end\":44008,\"start\":44003},{\"end\":44023,\"start\":44016},{\"end\":44034,\"start\":44029},{\"end\":44131,\"start\":44120},{\"end\":44145,\"start\":44139},{\"end\":44164,\"start\":44156},{\"end\":44251,\"start\":44250},{\"end\":44253,\"start\":44252},{\"end\":44270,\"start\":44269},{\"end\":44445,\"start\":44437},{\"end\":44548,\"start\":44547},{\"end\":44555,\"start\":44554},{\"end\":44795,\"start\":44790},{\"end\":44809,\"start\":44803},{\"end\":44823,\"start\":44819},{\"end\":44839,\"start\":44833},{\"end\":44857,\"start\":44850},{\"end\":44868,\"start\":44863},{\"end\":44887,\"start\":44879},{\"end\":44961,\"start\":44960},{\"end\":44972,\"start\":44971},{\"end\":45153,\"start\":45147},{\"end\":45168,\"start\":45163},{\"end\":45269,\"start\":45268},{\"end\":45281,\"start\":45280},{\"end\":45409,\"start\":45402},{\"end\":45427,\"start\":45420},{\"end\":45548,\"start\":45543},{\"end\":45564,\"start\":45559},{\"end\":45566,\"start\":45565},{\"end\":45624,\"start\":45617},{\"end\":45746,\"start\":45745},{\"end\":45759,\"start\":45758},{\"end\":45867,\"start\":45861},{\"end\":45888,\"start\":45879},{\"end\":45902,\"start\":45896},{\"end\":46029,\"start\":46022},{\"end\":46044,\"start\":46036},{\"end\":46061,\"start\":46054},{\"end\":46371,\"start\":46370},{\"end\":46383,\"start\":46374},{\"end\":46385,\"start\":46384},{\"end\":46397,\"start\":46396},{\"end\":46409,\"start\":46408},{\"end\":46418,\"start\":46417},{\"end\":46663,\"start\":46662},{\"end\":46672,\"start\":46671},{\"end\":46674,\"start\":46673},{\"end\":46899,\"start\":46893},{\"end\":46916,\"start\":46909},{\"end\":46934,\"start\":46927},{\"end\":46989,\"start\":46988},{\"end\":47094,\"start\":47082},{\"end\":47158,\"start\":47146},{\"end\":47223,\"start\":47211},{\"end\":47234,\"start\":47233},{\"end\":47337,\"start\":47333},{\"end\":47349,\"start\":47345},{\"end\":47360,\"start\":47358},{\"end\":47393,\"start\":47389},{\"end\":47530,\"start\":47529},{\"end\":47543,\"start\":47542},{\"end\":47681,\"start\":47680},{\"end\":47777,\"start\":47770},{\"end\":47928,\"start\":47927},{\"end\":47939,\"start\":47938},{\"end\":47947,\"start\":47946},{\"end\":48016,\"start\":48010},{\"end\":48090,\"start\":48084},{\"end\":48190,\"start\":48183},{\"end\":48288,\"start\":48287},{\"end\":48299,\"start\":48298},{\"end\":48308,\"start\":48307},{\"end\":48321,\"start\":48320},{\"end\":48330,\"start\":48329},{\"end\":48339,\"start\":48338},{\"end\":48347,\"start\":48346},{\"end\":48357,\"start\":48356},{\"end\":48368,\"start\":48367},{\"end\":48470,\"start\":48467},{\"end\":48481,\"start\":48478},{\"end\":48638,\"start\":48635},{\"end\":48650,\"start\":48646},{\"end\":48657,\"start\":48651},{\"end\":48670,\"start\":48666},{\"end\":48685,\"start\":48680},{\"end\":48887,\"start\":48882},{\"end\":48899,\"start\":48895},{\"end\":48922,\"start\":48914},{\"end\":48946,\"start\":48941},{\"end\":48948,\"start\":48947},{\"end\":48961,\"start\":48955},{\"end\":49057,\"start\":49056},{\"end\":49073,\"start\":49072},{\"end\":49274,\"start\":49269},{\"end\":49295,\"start\":49289},{\"end\":49311,\"start\":49306},{\"end\":49322,\"start\":49317},{\"end\":49338,\"start\":49333},{\"end\":49352,\"start\":49348},{\"end\":49364,\"start\":49361},{\"end\":49385,\"start\":49379},{\"end\":49387,\"start\":49386},{\"end\":49401,\"start\":49396},{\"end\":49496,\"start\":49495},{\"end\":49514,\"start\":49513},{\"end\":49526,\"start\":49525},{\"end\":49528,\"start\":49527},{\"end\":49795,\"start\":49794},{\"end\":49805,\"start\":49804},{\"end\":49927,\"start\":49920}]", "bib_author_last_name": "[{\"end\":43472,\"start\":43465},{\"end\":43655,\"start\":43634},{\"end\":43670,\"start\":43664},{\"end\":43686,\"start\":43679},{\"end\":43910,\"start\":43904},{\"end\":43988,\"start\":43982},{\"end\":44001,\"start\":43997},{\"end\":44014,\"start\":44009},{\"end\":44027,\"start\":44024},{\"end\":44039,\"start\":44035},{\"end\":44137,\"start\":44132},{\"end\":44154,\"start\":44146},{\"end\":44171,\"start\":44165},{\"end\":44267,\"start\":44254},{\"end\":44279,\"start\":44271},{\"end\":44452,\"start\":44446},{\"end\":44552,\"start\":44549},{\"end\":44567,\"start\":44556},{\"end\":44801,\"start\":44796},{\"end\":44817,\"start\":44810},{\"end\":44831,\"start\":44824},{\"end\":44848,\"start\":44840},{\"end\":44861,\"start\":44858},{\"end\":44877,\"start\":44869},{\"end\":44895,\"start\":44888},{\"end\":44969,\"start\":44962},{\"end\":44979,\"start\":44973},{\"end\":44989,\"start\":44981},{\"end\":45161,\"start\":45154},{\"end\":45177,\"start\":45169},{\"end\":45278,\"start\":45270},{\"end\":45287,\"start\":45282},{\"end\":45418,\"start\":45410},{\"end\":45434,\"start\":45428},{\"end\":45557,\"start\":45549},{\"end\":45571,\"start\":45567},{\"end\":45631,\"start\":45625},{\"end\":45756,\"start\":45747},{\"end\":45764,\"start\":45760},{\"end\":45877,\"start\":45868},{\"end\":45894,\"start\":45889},{\"end\":45907,\"start\":45903},{\"end\":46034,\"start\":46030},{\"end\":46052,\"start\":46045},{\"end\":46068,\"start\":46062},{\"end\":46394,\"start\":46386},{\"end\":46406,\"start\":46398},{\"end\":46415,\"start\":46410},{\"end\":46424,\"start\":46419},{\"end\":46669,\"start\":46664},{\"end\":46682,\"start\":46675},{\"end\":46907,\"start\":46900},{\"end\":46925,\"start\":46917},{\"end\":46941,\"start\":46935},{\"end\":46998,\"start\":46990},{\"end\":47008,\"start\":47000},{\"end\":47102,\"start\":47095},{\"end\":47166,\"start\":47159},{\"end\":47231,\"start\":47224},{\"end\":47243,\"start\":47235},{\"end\":47343,\"start\":47338},{\"end\":47356,\"start\":47350},{\"end\":47402,\"start\":47394},{\"end\":47540,\"start\":47531},{\"end\":47550,\"start\":47544},{\"end\":47689,\"start\":47682},{\"end\":47696,\"start\":47691},{\"end\":47786,\"start\":47778},{\"end\":47936,\"start\":47929},{\"end\":47944,\"start\":47940},{\"end\":47956,\"start\":47948},{\"end\":48023,\"start\":48017},{\"end\":48095,\"start\":48091},{\"end\":48197,\"start\":48191},{\"end\":48296,\"start\":48289},{\"end\":48305,\"start\":48300},{\"end\":48318,\"start\":48309},{\"end\":48327,\"start\":48322},{\"end\":48336,\"start\":48331},{\"end\":48344,\"start\":48340},{\"end\":48354,\"start\":48348},{\"end\":48365,\"start\":48358},{\"end\":48372,\"start\":48369},{\"end\":48476,\"start\":48471},{\"end\":48488,\"start\":48482},{\"end\":48542,\"start\":48532},{\"end\":48644,\"start\":48639},{\"end\":48664,\"start\":48658},{\"end\":48678,\"start\":48671},{\"end\":48696,\"start\":48686},{\"end\":48893,\"start\":48888},{\"end\":48912,\"start\":48900},{\"end\":48929,\"start\":48923},{\"end\":48953,\"start\":48949},{\"end\":48965,\"start\":48962},{\"end\":49070,\"start\":49058},{\"end\":49080,\"start\":49074},{\"end\":49287,\"start\":49275},{\"end\":49304,\"start\":49296},{\"end\":49315,\"start\":49312},{\"end\":49331,\"start\":49323},{\"end\":49346,\"start\":49339},{\"end\":49359,\"start\":49353},{\"end\":49377,\"start\":49365},{\"end\":49394,\"start\":49388},{\"end\":49413,\"start\":49402},{\"end\":49511,\"start\":49497},{\"end\":49523,\"start\":49515},{\"end\":49533,\"start\":49529},{\"end\":49802,\"start\":49796},{\"end\":49811,\"start\":49806},{\"end\":49934,\"start\":49928}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":43500,\"start\":43409},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":175089},\"end\":43815,\"start\":43502},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":62574443},\"end\":43936,\"start\":43817},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":29167860},\"end\":44061,\"start\":43938},{\"attributes\":{\"doi\":\"CoRR, abs/1709.01620\",\"id\":\"b4\"},\"end\":44197,\"start\":44063},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":202704200},\"end\":44402,\"start\":44199},{\"attributes\":{\"id\":\"b6\"},\"end\":44458,\"start\":44404},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":579926},\"end\":44721,\"start\":44460},{\"attributes\":{\"id\":\"b8\"},\"end\":44901,\"start\":44723},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":26201609},\"end\":45033,\"start\":44903},{\"attributes\":{\"id\":\"b10\"},\"end\":45183,\"start\":45035},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6327584},\"end\":45329,\"start\":45185},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":18501680},\"end\":45474,\"start\":45331},{\"attributes\":{\"id\":\"b13\"},\"end\":45577,\"start\":45476},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":194089492},\"end\":45658,\"start\":45579},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":38285817},\"end\":45808,\"start\":45660},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3483927},\"end\":45946,\"start\":45810},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":12344932},\"end\":46307,\"start\":45948},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":59500609},\"end\":46594,\"start\":46309},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":198120366},\"end\":46762,\"start\":46596},{\"attributes\":{\"id\":\"b20\"},\"end\":46947,\"start\":46764},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":32157465},\"end\":47052,\"start\":46949},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":4159240},\"end\":47119,\"start\":47054},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":10879080},\"end\":47207,\"start\":47121},{\"attributes\":{\"id\":\"b24\"},\"end\":47294,\"start\":47209},{\"attributes\":{\"doi\":\"CoRR, abs/1708.03535\",\"id\":\"b25\"},\"end\":47387,\"start\":47296},{\"attributes\":{\"id\":\"b26\"},\"end\":47468,\"start\":47389},{\"attributes\":{\"id\":\"b27\"},\"end\":47556,\"start\":47470},{\"attributes\":{\"id\":\"b28\"},\"end\":47702,\"start\":47558},{\"attributes\":{\"id\":\"b29\"},\"end\":47806,\"start\":47704},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":35473181},\"end\":48006,\"start\":47808},{\"attributes\":{\"id\":\"b31\"},\"end\":48029,\"start\":48008},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":165421324},\"end\":48132,\"start\":48031},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":5709063},\"end\":48237,\"start\":48134},{\"attributes\":{\"id\":\"b34\"},\"end\":48447,\"start\":48239},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":64251645},\"end\":48566,\"start\":48449},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":18876242},\"end\":48833,\"start\":48568},{\"attributes\":{\"id\":\"b37\"},\"end\":48939,\"start\":48835},{\"attributes\":{\"id\":\"b38\"},\"end\":49005,\"start\":48941},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":54770261},\"end\":49224,\"start\":49007},{\"attributes\":{\"doi\":\"CoRR, abs/1609.03499\",\"id\":\"b40\"},\"end\":49439,\"start\":49226},{\"attributes\":{\"id\":\"b41\"},\"end\":49716,\"start\":49441},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":18695514},\"end\":49847,\"start\":49718},{\"attributes\":{\"doi\":\"CoRR, abs/1611.09733\",\"id\":\"b43\"},\"end\":49960,\"start\":49849}]", "bib_title": "[{\"end\":43624,\"start\":43502},{\"end\":43900,\"start\":43817},{\"end\":43974,\"start\":43938},{\"end\":44248,\"start\":44199},{\"end\":44545,\"start\":44460},{\"end\":44958,\"start\":44903},{\"end\":45266,\"start\":45185},{\"end\":45400,\"start\":45331},{\"end\":45615,\"start\":45579},{\"end\":45743,\"start\":45660},{\"end\":45859,\"start\":45810},{\"end\":46020,\"start\":45948},{\"end\":46368,\"start\":46309},{\"end\":46660,\"start\":46596},{\"end\":46986,\"start\":46949},{\"end\":47080,\"start\":47054},{\"end\":47144,\"start\":47121},{\"end\":47925,\"start\":47808},{\"end\":48082,\"start\":48031},{\"end\":48181,\"start\":48134},{\"end\":48285,\"start\":48239},{\"end\":48465,\"start\":48449},{\"end\":48633,\"start\":48568},{\"end\":49054,\"start\":49007},{\"end\":49493,\"start\":49441},{\"end\":49792,\"start\":49718}]", "bib_author": "[{\"end\":43474,\"start\":43458},{\"end\":43657,\"start\":43626},{\"end\":43672,\"start\":43657},{\"end\":43688,\"start\":43672},{\"end\":43912,\"start\":43902},{\"end\":43990,\"start\":43976},{\"end\":44003,\"start\":43990},{\"end\":44016,\"start\":44003},{\"end\":44029,\"start\":44016},{\"end\":44041,\"start\":44029},{\"end\":44139,\"start\":44120},{\"end\":44156,\"start\":44139},{\"end\":44173,\"start\":44156},{\"end\":44269,\"start\":44250},{\"end\":44281,\"start\":44269},{\"end\":44454,\"start\":44437},{\"end\":44554,\"start\":44547},{\"end\":44569,\"start\":44554},{\"end\":44803,\"start\":44790},{\"end\":44819,\"start\":44803},{\"end\":44833,\"start\":44819},{\"end\":44850,\"start\":44833},{\"end\":44863,\"start\":44850},{\"end\":44879,\"start\":44863},{\"end\":44897,\"start\":44879},{\"end\":44971,\"start\":44960},{\"end\":44981,\"start\":44971},{\"end\":44991,\"start\":44981},{\"end\":45163,\"start\":45147},{\"end\":45179,\"start\":45163},{\"end\":45280,\"start\":45268},{\"end\":45289,\"start\":45280},{\"end\":45420,\"start\":45402},{\"end\":45436,\"start\":45420},{\"end\":45633,\"start\":45617},{\"end\":45758,\"start\":45745},{\"end\":45766,\"start\":45758},{\"end\":45879,\"start\":45861},{\"end\":45896,\"start\":45879},{\"end\":45909,\"start\":45896},{\"end\":46036,\"start\":46022},{\"end\":46054,\"start\":46036},{\"end\":46070,\"start\":46054},{\"end\":46374,\"start\":46370},{\"end\":46396,\"start\":46374},{\"end\":46408,\"start\":46396},{\"end\":46417,\"start\":46408},{\"end\":46426,\"start\":46417},{\"end\":46671,\"start\":46662},{\"end\":46684,\"start\":46671},{\"end\":46909,\"start\":46893},{\"end\":46927,\"start\":46909},{\"end\":46943,\"start\":46927},{\"end\":47000,\"start\":46988},{\"end\":47010,\"start\":47000},{\"end\":47104,\"start\":47082},{\"end\":47168,\"start\":47146},{\"end\":47233,\"start\":47211},{\"end\":47245,\"start\":47233},{\"end\":47345,\"start\":47333},{\"end\":47358,\"start\":47345},{\"end\":47363,\"start\":47358},{\"end\":47404,\"start\":47389},{\"end\":47542,\"start\":47529},{\"end\":47552,\"start\":47542},{\"end\":47691,\"start\":47680},{\"end\":47698,\"start\":47691},{\"end\":47788,\"start\":47770},{\"end\":47938,\"start\":47927},{\"end\":47946,\"start\":47938},{\"end\":47958,\"start\":47946},{\"end\":48025,\"start\":48010},{\"end\":48097,\"start\":48084},{\"end\":48199,\"start\":48183},{\"end\":48298,\"start\":48287},{\"end\":48307,\"start\":48298},{\"end\":48320,\"start\":48307},{\"end\":48329,\"start\":48320},{\"end\":48338,\"start\":48329},{\"end\":48346,\"start\":48338},{\"end\":48356,\"start\":48346},{\"end\":48367,\"start\":48356},{\"end\":48374,\"start\":48367},{\"end\":48478,\"start\":48467},{\"end\":48490,\"start\":48478},{\"end\":48646,\"start\":48635},{\"end\":48666,\"start\":48646},{\"end\":48680,\"start\":48666},{\"end\":48698,\"start\":48680},{\"end\":48895,\"start\":48882},{\"end\":48914,\"start\":48895},{\"end\":48931,\"start\":48914},{\"end\":48955,\"start\":48941},{\"end\":48967,\"start\":48955},{\"end\":49072,\"start\":49056},{\"end\":49082,\"start\":49072},{\"end\":49289,\"start\":49269},{\"end\":49306,\"start\":49289},{\"end\":49317,\"start\":49306},{\"end\":49333,\"start\":49317},{\"end\":49348,\"start\":49333},{\"end\":49361,\"start\":49348},{\"end\":49379,\"start\":49361},{\"end\":49396,\"start\":49379},{\"end\":49415,\"start\":49396},{\"end\":49513,\"start\":49495},{\"end\":49525,\"start\":49513},{\"end\":49535,\"start\":49525},{\"end\":49804,\"start\":49794},{\"end\":49813,\"start\":49804},{\"end\":49936,\"start\":49920}]", "bib_venue": "[{\"end\":43456,\"start\":43409},{\"end\":43756,\"start\":43688},{\"end\":43928,\"start\":43912},{\"end\":44051,\"start\":44041},{\"end\":44118,\"start\":44063},{\"end\":44384,\"start\":44281},{\"end\":44435,\"start\":44404},{\"end\":44647,\"start\":44569},{\"end\":44788,\"start\":44723},{\"end\":45023,\"start\":44991},{\"end\":45145,\"start\":45035},{\"end\":45320,\"start\":45289},{\"end\":45465,\"start\":45436},{\"end\":45541,\"start\":45476},{\"end\":45650,\"start\":45633},{\"end\":45806,\"start\":45766},{\"end\":45925,\"start\":45909},{\"end\":46167,\"start\":46070},{\"end\":46508,\"start\":46426},{\"end\":46735,\"start\":46684},{\"end\":46891,\"start\":46764},{\"end\":47044,\"start\":47010},{\"end\":47110,\"start\":47104},{\"end\":47200,\"start\":47168},{\"end\":47287,\"start\":47245},{\"end\":47331,\"start\":47296},{\"end\":47444,\"start\":47404},{\"end\":47527,\"start\":47470},{\"end\":47678,\"start\":47558},{\"end\":47768,\"start\":47704},{\"end\":48004,\"start\":47958},{\"end\":48099,\"start\":48097},{\"end\":48228,\"start\":48199},{\"end\":48441,\"start\":48374},{\"end\":48530,\"start\":48490},{\"end\":48755,\"start\":48698},{\"end\":48880,\"start\":48835},{\"end\":48990,\"start\":48967},{\"end\":49151,\"start\":49082},{\"end\":49267,\"start\":49226},{\"end\":49630,\"start\":49535},{\"end\":49842,\"start\":49813},{\"end\":49918,\"start\":49849},{\"end\":43811,\"start\":43758},{\"end\":44057,\"start\":44053},{\"end\":44398,\"start\":44386},{\"end\":44712,\"start\":44649},{\"end\":46273,\"start\":46169},{\"end\":46577,\"start\":46510},{\"end\":46743,\"start\":46737},{\"end\":48552,\"start\":48544},{\"end\":48824,\"start\":48757},{\"end\":49220,\"start\":49153},{\"end\":49712,\"start\":49632}]"}}}, "year": 2023, "month": 12, "day": 17}