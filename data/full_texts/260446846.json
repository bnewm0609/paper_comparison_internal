{"id": 260446846, "updated": "2023-11-03 05:24:30.555", "metadata": {"title": "Session-based Recommendations with Recurrent Neural Networks", "authors": "[{\"first\":\"Bal'azs\",\"last\":\"Hidasi\",\"middle\":[]},{\"first\":\"Alexandros\",\"last\":\"Karatzoglou\",\"middle\":[]},{\"first\":\"Linas\",\"last\":\"Baltrunas\",\"middle\":[]},{\"first\":\"Domonkos\",\"last\":\"Tikk\",\"middle\":[]}]", "venue": null, "journal": "arXiv: Learning", "publication_date": {"year": 2015, "month": 11, "day": 21}, "abstract": "We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1511.06939", "mag": "2964316331", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/HidasiKBT15", "doi": null}}, "content": {"source": {"pdf_hash": "bda78ca4fbca0cc427741cb8992c2d6388ae7f67", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1511.06939v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d3c1bda5e25deac89949e00467a6b9bb67ccd33a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bda78ca4fbca0cc427741cb8992c2d6388ae7f67.txt", "contents": "\nSESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS\n\n\nBal\u00e1zs Hidasi balazs.hidasi@gravityrd.com \nAlexandros Karatzoglou \nLinas Baltrunas lbaltrunas@netflix.com \nDomonkos Tikk domonkos.tikk@gravityrd.com \n\nGravity R&D Inc. Budapest\nHungary\n\n\nTelefonica Research Barcelona\nSpain\n\n\nNetflix Los Gatos\nCAUSA\n\n\nGravity R&D Inc\nBudapestHungary\n\nSESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS\nPublished as a conference paper at ICLR 2016\nWe apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNNbased approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches. * The author spent 3 months at Telefonica Research during the research of this topic. \u2020 This work was done while the author was a member\n\nINTRODUCTION\n\nSession-based recommendation is a relatively unappreciated problem in the machine learning and recommender systems community. Many e-commerce recommender systems (particularly those of small retailers) and most of news and media sites do not typically track the user-id's of the users that visit their sites over a long period of time. While cookies and browser fingerprinting can provide some level of user recognizability, those technologies are often not reliable enough and moreover raise privacy concerns. Even if tracking is possible, lots of users have only one or two sessions on a smaller e-commerce site, and in certain domains (e.g. classified sites) the behavior of users often shows session-based traits. Thus subsequent sessions of the same user should be handled independently. Consequently, most session-based recommendation systems deployed for e-commerce are based on relatively simple methods that do not make use of a user profile e.g. itemto-item similarity, co-occurrence, or transition probabilities. While effective, those methods often take only the last click or selection of the user into account ignoring the information of past clicks.\n\nThe most common methods used in recommender systems are factor models (Koren et al., 2009;Weimer et al., 2007;Hidasi & Tikk, 2012) and neighborhood methods (Sarwar et al., 2001;Koren, 2008). Factor models work by decomposing the sparse user-item interactions matrix to a set of d dimensional vectors one for each item and user in the dataset. The recommendation problem is then treated as a matrix completion/reconstruction problem whereby the latent factor vectors are then used to fill the missing entries by e.g. taking the dot product of the corresponding user-item latent factors. Factor models are hard to apply in session-based recommendation due to the absence of a user profile. On the other hand, neighborhood methods, which rely on computing similarities between items (or users) are based on co-occurrences of items in sessions (or user profiles). Neighborhood methods have been used extensively in session-based recommendations.\n\nThe past few years have seen the tremendous success of deep neural networks in a number of tasks such as image and speech recognition (Russakovsky et al., 2014;Hinton et al., 2012) where unstructured data is processed through several convolutional and standard layers of (usually rectified linear) units. Sequential data modeling has recently also attracted a lot of attention with various flavors of RNNs being the model of choice for this type of data. Applications of sequence modeling range from test-translation to conversation modeling to image captioning.\n\nWhile RNNs have been applied to the aforementioned domains with remarkable success little attention, has been paid to the area of recommender systems. In this work we argue that RNNs can be applied to session-based recommendation with remarkable results, we deal with the issues that arise when modeling such sparse sequential data and also adapt the RNN models to the recommender setting by introducing a new ranking loss function suited to the task of training these models. The session-based recommendation problem shares some similarities with some NLP-related problems in terms of modeling as long as they both deals with sequences. In the session-based recommendation we can consider the first item a user clicks when entering a web-site as the initial input of the RNN, we then would like to query the model based on this initial input for a recommendation. Each consecutive click of the user will then produce an output (a recommendation) that depends on all the previous clicks. Typically the item-set to choose from in recommenders systems can be in the tens of thousands or even hundreds of thousands. Apart from the large size of the item set, another challenge is that click-stream datasets are typically quite large thus training time and scalability are really important. As in most information retrieval and recommendation settings, we are interested in focusing the modeling power on the top-items that the user might be interested in, to this end we use ranking loss function to train the RNNs.\n\n\nRELATED WORK\n\n\nSESSION-BASED RECOMMENDATION\n\nMuch of the work in the area of recommender systems has focused on models that work when a user identifier is available and a clear user profile can be built. In this setting, matrix factorization methods and neighborhood models have dominated the literature and are also employed on-line. One of the main approaches that is employed in session-based recommendation and a natural solution to the problem of a missing user profile is the item-to-item recommendation approach (Sarwar et al., 2001;Linden et al., 2003) in this setting an item to item similarity matrix is precomputed from the available session data, that is items that are often clicked together in sessions are deemed to be similar. This similarity matrix is then simply used during the session to recommend the most similar items to the one the user has currently clicked. While simple, this method has been proven to be effective and is widely employed. While effective, these methods are only taking into account the last click of the user, in effect ignoring the information of the past clicks.\n\nA somewhat different approach to session-based recommendation are Markov Decision Processes (MDPs) (Shani et al., 2002). MDPs are models of sequential stochastic decision problems. An MDP is defined as a four-tuple S, A, Rwd, tr where S is the set of states, A is a set of actions Rwd is a reward function and tr is the state-transition function. In recommender systems actions can be equated with recommendations and the simplest MPDs are essentially first order Markov chains where the next recommendation can be simply computed on the basis of the transition probability between items. The main issue with applying Markov chains in session-based recommendation is that the state space quickly becomes unmanageable when trying to include all possible sequences of user selections.\n\nThe extended version of the General Factorization Framework (GFF) (Hidasi & Tikk, 2015) is capable of using session data for recommendations. It models a session by the sum of its events. It uses two kinds of latent representations for items, one represents the item itself, the other is for representing the item as part of a session. The session is then represented as the average of the feature vectors of part-of-a-session item representation. However, this approach does not consider any ordering within the session.\n\n\nDEEP LEARNING IN RECOMMENDERS\n\nOne of the first related methods in the neural networks literature where the use of Restricted Boltzmann Machines (RBM) for Collaborative Filtering (Salakhutdinov et al., 2007). In this work an RBM is used to model user-item interaction and perform recommendations. This model has been shown to be one of the best performing Collaborative Filtering models. Deep Models have been used to extract features from unstructured content such as music or images that are then used together with more conventional collaborative filtering models. In Van den Oord et al. (2013) a convolutional deep network is used to extract feature from music files that are then used in a factor model. More recently Wang et al. (2015) introduced a more generic approach whereby a deep network is used to extract generic content-features from any types of items, these features are then incorporated in a standard collaborative filtering model to enhance the recommendation performance. This approach seems to be particularly useful in settings where there is not sufficient user-item interaction information.\n\n\nRECOMMENDATIONS WITH RNNS\n\nRecurrent Neural Networks have been devised to model variable-length sequence data. The main difference between RNNs and conventional feedforward deep models is the existence of an internal hidden state in the units that compose the network. Standard RNNs update their hidden state h using the following update function:\nh t = g(W x t + U h t\u22121 )\n(1) Where g is a smooth and bounded function such as a logistic sigmoid function x t is the input of the unit at time t. An RNN outputs a probability distribution over the next element of the sequence, given its current state h t .\n\nA Gated Recurrent Unit (GRU) (Cho et al., 2014) is a more elaborate model of an RNN unit that aims at dealing with the vanishing gradient problem. GRU gates essentially learn when and by how much to update the hidden state of the unit. The activation of the GRU is a linear interpolation between the previous activation and the candidate activation\u0125 t :\nh t = (1 \u2212 z t )h t\u22121 + z t\u0125t (2)\nwhere the update gate is given by:\nz t = \u03c3(W z x t + U z h t\u22121 )(3)\nwhile the candidate activation function\u0125 t is computed in a similar manner:\nh t = tanh (W x t + U (r t h t\u22121 ))(4)\nand finaly the reset gate r t is given by:\nr t = \u03c3(W r x t + U r h t\u22121 )(5)\n\nCUSTOMIZING THE GRU MODEL\n\nWe used the GRU-based RNN in our models for session-based recommendations. The input of the network is the actual state of the session while the output is the item of the next event in the session. The state of the session can either be the item of the actual event or the events in the session so far. In the former case 1-of-N encoding is used, i.e. the input vector's length equals to the number of items and only the coordinate corresponding to the active item is one, the others are zeros. The latter setting uses a weighted sum of these representations, in which events are discounted if they have occurred earlier. For the stake of stability, the input vector is then normalized. We expect this to help because it reinforces the memory effect: the reinforcement of very local ordering constraints which are not well captured by the longer memory of RNN. We also experimented with adding an additional embedding layer, but the 1-of-N encoding always performed better.\n\nThe core of the network is the GRU layer(s) and additional feedforward layers can be added between the last layer and the output. The output is the predicted preference of the items, i.e. the likelihood of being the next in the session for each item. When multiple GRU layers are used, the hidden state of the previous layer is the input of the next one. The input can also be optionally connected to GRU layers deeper in the network, as we found that this improves performance. See the whole architecture on Figure 1, which depicts the representation of a single event within a time series of events.\n\nSince recommender systems are not the primary application area of recurrent neural networks, we modified the base network to better suit the task. We also considered practical points so that our solution could be possibly applied in a live environment.\n\n\nSESSION-PARALLEL MINI-BATCHES\n\nRNNs for natural language processing tasks usually use in-sequence mini-batches. For example it is common to use a sliding window over the words of sentences and put these windowed fragments next to each other to form mini-batches. This does not fit our task, because (1) the length of sessions can be very different, even more so than that of sentences: some sessions consist of only 2 events, while others may range over a few hundreds;\n\n(2) our goal is to capture how a session evolves over time, so breaking down into fragments would make no sense. Therefore we use session-parallel mini-batches. First, we create an order for the sessions. Then, we use the first event of the first X sessions to form the input of the first mini-batch (the desired output is the second events of our active sessions). The second mini-batch is formed from the second events and so on. If any of the sessions end, the next available session is put in its place. Sessions are assumed to be independent, thus we reset the appropriate hidden state when this switch occurs. See Figure 2 for more details.\n\n, , Recommender systems are especially useful when the number of items is large. Even for a mediumsized webshop this is in the range of tens of thousands, but on larger sites it is not rare to have hundreds of thousands of items or even a few millions. Calculating a score for each item in each step would make the algorithm scale with the product of the number of items and the number of events. This would be unusable in practice. Therefore we have to sample the output and only compute the score for a small subset of the items. This also entails that only some of the weights will be updated. Besides the desired output, we need to compute scores for some negative examples and modify the weights so that the desired output is highly ranked.\nSession4 Session5 \u2026 ,, , , , , , , , , , , , , , , , Session1 Session2 Session3\nThe natural interpretation of an arbitrary missing event is that the user did not know about the existence of the item and thus there was no interaction. However there is a low probability that the user did know about the item and chose not to interact, because she disliked the item. The more popular the item, the more probable it is that the user knows about it, thus it is more likely that a missing event expresses dislike. Therefore we should sample items in proportion of their popularity. Instead of generating separate samples for each training example, we use the items from the other training examples of the mini-batch as negative examples. The benefit of this approach is that we can further reduce computational times by skipping the sampling. Additionally, there are also benefits on the implementation side from making the code less complex to faster matrix operations. Meanwhile, this approach is also a popularity-based sampling, because the likelihood of an item being in the other training examples of the mini-batch is proportional to its popularity.\n\n\nRANKING LOSS\n\nThe core of recommender systems is the relevance-based ranking of items. Although the task can also be interpreted as a classification task, learning-to-rank approaches (Rendle et al., 2009;Shi et al., 2012;Steck, 2015) generally outperform other approaches. Ranking can be pointwise, pairwise or listwise. Pointwise ranking estimates the score or the rank of items independently of each other and the loss is defined in a way so that the rank of relevant items should be low. Pairwise ranking compares the score or the rank of pairs of a positive and a negative item and the loss enforces that the rank of the positive item should be lower than that of the negative one. Listwise ranking uses the scores and ranks of all items and compares them to the perfect ordering. As it includes sorting, it is usually computationally more expensive and thus not used often. Also, if there is only one relevant item -as in our case -listwise ranking can be solved via pairwise ranking.\n\nWe included several pointwise and pairwise ranking losses into our solution. We found that pointwise ranking was unstable with this network (see Section 4 for more comments). Pairwise ranking losses on the other hand performed well. We use the following two.\n\n\u2022 BPR: Bayesian Personalized Ranking (Rendle et al., 2009) is a matrix factorization method that uses pairwise ranking loss. It compares the score of a positive and a sampled negative item. Here we compare the score of the positive item with several sampled items and use their average as the loss. The loss at a given point in one session is defined as: L s = \u2212 1 N S \u00b7 N S j=1 log (\u03c3 (r s,i \u2212r s,j )), where N S is the sample size,r s,k is the score on item k at the given point of the session, i is the desired item (next item in the session) and j are the negative samples.\n\n\u2022 TOP1: This ranking loss was devised by us for this task. It is the regularized approximation of the relative rank of the relevant item. The relative rank of the relevant item is given by\n1 N S \u00b7 N S\nj=1 I{r s,j >r s,i }. We approximate I{\u00b7} with a sigmoid. Optimizing for this would modify parameters so that the score for i would be high. However this is unstable as certain positive items also act as negative examples and thus scores tend to become increasingly higher. To avoid this, we want to force the scores of the negative examples to be around zero. This is a natural expectation towards the scores of negative items. Thus we added a regularization term to the loss. It is important that this term is in the same range as the relative rank and acts similarly to it. The final loss function is as follows:\nL s = 1 N S \u00b7 N Sj=1\n\u03c3 (r s,j \u2212r s,i ) + \u03c3 r 2 s,j\n\n\nEXPERIMENTS\n\nWe evaluate the proposed recursive neural network against popular baselines on two datasets.\n\nThe first dataset is that of RecSys Challenge 2015 1 . This dataset contains click-streams of an ecommerce site that sometimes end in purchase events. We work with the training set of the challenge and keep only the click events. We filter out sessions of length 1. The network is trained on \u223c 6 months of data, containing 7,966,257 sessions of 31,637,239 clicks on 37,483 items. We use the sessions of the subsequent day for testing. Each session is assigned to either the training or the test set, we do not split the data mid-session. Because of the nature of collaborative filtering methods, we filter out clicks from the test set where the item clicked is not in the train set. Sessions of length one are also removed from the test set. After the preprocessing we are left with 15,324 sessions of 71,222 events for the test set. This dataset will be referred to as RSC15.\n\nThe second dataset is collected from a Youtube-like OTT video service platform. Events of watching a video for at least a certain amount of time were collected. Only certain regions were subject to this collection that lasted for somewhat shorter than 2 months. During this time item-to-item recommendations were provided after each video at the left side of the screen. These were provided by a selection of different algorithms and influenced the behavior of the users. Preprocessing steps are similar to that of the other dataset with the addition of filtering out very long sessions as they were probably generated by bots. The training data consists of all but the last day of the aforementioned period and has \u223c 3 million sessions of \u223c 13 million watch events on 330 thousand videos. The test set contains the sessions of the last day of the collection period and has \u223c 37 thousand sessions with \u223c 180 thousand watch events. This dataset will be referred to as VIDEO.\n\nThe evaluation is done by providing the events of a session one-by-one and checking the rank of the item of the next event. The hidden state of the GRU is reset to zero after a session finishes. Items are ordered in descending order by their score and their position in this list is their rank. With RSC15, all of the 37,483 items of the train set were ranked. However, this would have been impractical with VIDEO, due to the large number of items. There we ranked the desired item against the most popular 30,000 items. This has negligible effect on the evaluations as rarely visited items often get low scores. Also, popularity based pre-filtering is common in practical recommender systems.\n\nAs recommender systems can only recommend a few items at once, the actual item a user might pick should be amongst the first few items of the list. Therefore, our primary evaluation metric is recall@20 that is the proportion of cases having the desired item amongst the top-20 items in all test cases. Recall does not consider the actual rank of the item as long as it is amongst the top-N. This models certain practical scenarios well where there is no highlighting of recommendations and the absolute order does not matter. Recall also usually correlates well with important online KPIs, such as click-through rate (CTR) (Liu et al., 2012;Hidasi & Tikk, 2012). The second metric used in the experiments is MRR@20 (Mean Reciprocal Rank). That is the average of reciprocal ranks of the desired items. The reciprocal rank is set to zero if the rank is above 20. MRR takes into account the rank of the item, which is important in cases where the order of recommendations matter (e.g. the lower ranked items are only visible after scrolling).\n\n\nBASELINES\n\nWe compare the proposed network to a set of commonly used baselines.\n\n\u2022 POP: Popularity predictor that always recommends the most popular items of the training set. Despite its simplicity it is often a strong baseline in certain domains. \u2022 S-POP: This baseline recommends the most popular items of the current session. The recommendation list changes during the session as items gain more events. Ties are broken up using global popularity values. This baseline is strong in domains with high repetitiveness. \u2022 Item-KNN: Items similar to the actual item are recommended by this baseline and similarity is defined as the cosine similarity between the vector of their sessions, i.e. it is the number of co-occurrences of two items in sessions divided by the square root of the product of the numbers of sessions in which the individual items are occurred. Regularization is also included to avoid coincidental high similarities of rarely visited items. This baseline is one of the most common item-to-item solutions in practical systems, that provides recommendations in the \"others who viewed this item also viewed these ones\" setting. Despite of its simplicity it is usually a strong baseline (Linden et al., 2003;Davidson et al., 2010).  \u2022 BPR-MF: BPR-MF (Rendle et al., 2009) is one of the commonly used matrix factorization methods. It optimizes for a pairwise ranking objective function (see Section 3) via SGD. Matrix factorization cannot be applied directly to session-based recommendations, because the new sessions do not have feature vectors precomputed. However we can overcome this by using the average of item feature vectors of the items that had occurred in the session so far as the user feature vector. In other words we average the similarities of the feature vectors between a recommendable item and the items of the session so far. Table 1 shows the results for the baselines. The item-KNN approach clearly dominates the other methods.\n\n\nPARAMETER & STRUCTURE OPTIMIZATION\n\nWe optimized the hyperparameters by running 100 experiments at randomly selected points of the parameter space for each dataset and loss function. The best parametrization was further tuned by individually optimizing each parameter. The number of hidden units was set to 100 in all cases. The best performing parameters were then used with hidden layers of different sizes. The optimization was done on a separate validation set. Then the networks were retrained on the training plus the validation set and evaluated on the final test set.\n\nThe best performing parametrizations are summarized in table 2. Weight matrices were initialized by random numbers drawn uniformly from [\u2212x, x] where x depends on the number of rows and columns of the matrix. We experimented with both rmsprop (Dauphin et al., 2015) and adagrad (Duchi et al., 2011). We found adagrad to give better results.\n\nWe briefly experimented with other units than GRU. We found both the classic RNN unit and LSTM to perform worse.\n\nWe tried out several loss functions. Pointwise ranking based losses, such as cross-entropy and MRR optimization (as in Steck (2015)) were usually unstable, even with regularization. For example cross-entropy yielded only 10 and 6 numerically stable networks of the 100 random runs for RSC15 and VIDEO respectively. We assume that this is due to independently trying to achieve high scores for the desired items and the negative push is small for the negative samples. On the other hand pairwise ranking-based losses performed well. We found the ones introduced in Section 3 (BPR and TOP1) to perform the best.\n\nSeveral architectures were examined and a single layer of GRU units was found to be the best performer. Adding addition layers always resulted in worst performance w.r.t. both training loss and recall and MRR measured on the test set. We assume that this is due to the generally short lifespan of the sessions not requiring multiple time scales of different resolutions to be properly represented. However the exact reason of this is unknown as of yet and requires further research. Using embedding of the items gave slightly worse results, therefore we kept the 1-of-N encoding. Also, putting all previous events of the session on the input instead of the preceding one did not result in additional accuracy gain; which is not surprising as GRU -like LSTM -has both long and short term memory. Adding additional feed-forward layers after the GRU layer did not help either. However increasing the size of the GRU layer improved the performance. We also found that it is beneficial to use tanh as the activation function of the output layer. Table 3 shows the results of the best performing networks. Cross-entropy for the VIDEO data with 1000 hidden units was numerically unstable and thus we present no results for that scenario.\n\n\nRESULTS\n\nThe results are compared to the best baseline (item-KNN). We show results with 100 and 1000 hidden units. The running time depends on the parameters and the dataset. Generally speaking the difference in runtime between the smaller and the larger variant is not too high on a GeForce GTX Titan X GPU and the training of the network can be done in a few hours 2 . On CPU, the smaller network can be trained in a practically acceptable timeframe. Frequent retraining is often desirable for recommender systems, because new users and items are introduced frequently.\n\nThe GRU-based approach has substantial gain over the item-KNN in both evaluation metrics on both datasets, even if the number of units is 100 3 . Increasing the number of units further improves the results for pairwise losses, but the accuracy decreases for cross-entropy. Even though cross-entropy gives better results with 100 hidden units, the pairwise loss variants surpass these results as the number of units increase. Although, increasing the number of units increases the training times, we found that it was not too expensive to move from 100 units to 1000 on GPU. Also, the cross-entropy based loss was found to be numerically unstable as the result of the network individually trying to increase the score for the target items, while the negative push is relatively small for the other items. Therefore we suggest using any of the two pairwise losses. The TOP1 loss performs slightly better on these two datasets, resulting in \u223c 20 \u2212 30% accuracy gain over the best performing baseline.\n\n\nCONCLUSION & FUTURE WORK\n\nIn this paper we applied a kind of modern recurrent neural network (GRU) to new application domain: recommender systems. We chose the task of session based recommendations, because it is a practically important area, but not well researched. We modified the basic GRU in order to fit the task better by introducing session-parallel mini-batches, mini-batch based output sampling and ranking loss function. We showed that our method can significantly outperform popular baselines that are used for this task. We think that our work can be the basis of both deep learning applications in recommender systems and session based recommendations in general.\n\nOur immediate future work will focus on the more thorough examination of the proposed network.\n\nWe also plan to train the network on automatically extracted item representation that is built on content of the item itself (e.g. thumbnail, video, text) instead of the current input.\n\nFigure 1 :\n1General architecture of the network. Processing of one event of the event stream at once.\n\nFigure 2 :\n2Session-parallel mini-batch creation 3.1.2 SAMPLING ON THE OUTPUT\n\nTable 1 :\n1Recall@20 and MRR@20 using the baseline methodsBaseline \nRSC15 \nVIDEO \nRecall@20 MRR@20 Recall@20 MRR@20 \n\nPOP \n0.0050 \n0.0012 \n0.0499 \n0.0117 \nS-POP \n0.2672 \n0.1775 \n0.1301 \n0.0863 \nItem-KNN \n0.5065 \n0.2048 \n0.5508 \n0.3381 \nBPR-MF \n0.2574 \n0.0618 \n0.0692 \n0.0374 \n\n\n\nTable 2 :\n2Best parametrizations for datasets/loss functionsDataset Loss \nMini-batch Dropout Learning rate Momentum \n\nRSC15 \nTOP1 \n50 \n0.5 \n0.01 \n0 \nRSC15 \nBPR \n50 \n0.2 \n0.05 \n0.2 \nRSC15 \nCross-entropy \n500 \n0 \n0.01 \n0 \nVIDEO TOP1 \n50 \n0.4 \n0.05 \n0 \nVIDEO BPR \n50 \n0.3 \n0.1 \n0 \nVIDEO Cross-entropy \n200 \n0.1 \n0.05 \n0.3 \n\n\n\nTable 3 :\n3Recall@20 and MRR@20 for different types of a single layer of GRU, compared to the best baseline (item-KNN). Best results per dataset are highlighted.Loss / #Units \nRSC15 \nVIDEO \nRecall@20 \nMRR@20 \nRecall@20 \nMRR@20 \n\nTOP1 100 \n0.5853 (+15.55%) \n0.2305 (+12.58%) \n0.6141 (+11.50%) \n0.3511 (+3.84%) \nBPR 100 \n0.6069 (+19.82%) \n0.2407 (+17.54%) \n0.5999 (+8.92%) \n0.3260 (-3.56%) \nCross-entropy 100 \n0.6074 (+19.91%) \n0.2430 (+18.65%) \n0.6372 (+15.69%) \n0.3720 (+10.04%) \nTOP1 1000 \n0.6206 (+22.53%) 0.2693 (+31.49%) 0.6624 (+20.27%) 0.3891 (+15.08%) \nBPR 1000 \n0.6322 (+24.82%) 0.2467 (+20.47%) \n0.6311 (+14.58%) \n0.3136 (-7.23%) \nCross-entropy 1000 0.5777 (+14.06%) \n0.2153 (+5.16%) \n-\n-\n\n\nhttp://2015.recsyschallenge.com/\nUsing Theano with fixes for the subtensor operators on GPU. 3 Except for using the BPR loss on the VIDEO data and evaluating for MRR.\nACKNOWLEDGMENTSThe work leading to these results has received funding from the European Union's Seventh Framework Programme (FP7/2007(FP7/ -2013 under CrowdRec Grant Agreement n \u2022 610594.\nOn the properties of neural machine translation. Kyunghyun Cho, Van Merri\u00ebnboer, Bart, Dzmitry Bahdanau, Yoshua Bengio, arXiv:1409.1259Encoder-decoder approaches. arXiv preprintCho, Kyunghyun, van Merri\u00ebnboer, Bart, Bahdanau, Dzmitry, and Bengio, Yoshua. On the proper- ties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.\n\nRmsprop and equilibrated adaptive learning rates for non-convex optimization. Yann N Dauphin, De Vries, Harm, Junyoung Chung, Yoshua Bengio, arXiv:1502.04390arXiv preprintDauphin, Yann N, de Vries, Harm, Chung, Junyoung, and Bengio, Yoshua. Rmsprop and equi- librated adaptive learning rates for non-convex optimization. arXiv preprint arXiv:1502.04390, 2015.\n\nThe YouTube video recommendation system. James Davidson, Liebald, Benjamin, Liu, Junning, 978-1- 60558-906-0Recsys'10: ACM Conf. on Recommender Systems. Davidson, James, Liebald, Benjamin, Liu, Junning, et al. The YouTube video recommendation system. In Recsys'10: ACM Conf. on Recommender Systems, pp. 293-296, 2010. ISBN 978-1- 60558-906-0.\n\nAdaptive subgradient methods for online learning and stochastic optimization. John Duchi, Elad Hazan, Yoram Singer, The Journal of Machine Learning Research. 12Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121-2159, 2011.\n\nFast ALS-based tensor factorization for context-aware recommendation from implicit feedback. B Hidasi, D Tikk, ECML-PKDD'12, Part II, number 7524 in LNCS. SpringerHidasi, B. and Tikk, D. Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback. In ECML-PKDD'12, Part II, number 7524 in LNCS, pp. 67-82. Springer, 2012.\n\nGeneral factorization framework for context-aware recommendations. Data Mining and Knowledge Discovery. Bal\u00e1zs Hidasi, Domonkos Tikk, 10.1007/s10618-015-0417-y1384-5810. doi: 10.1007/ s10618-015-0417-yHidasi, Bal\u00e1zs and Tikk, Domonkos. General factorization framework for context-aware recommen- dations. Data Mining and Knowledge Discovery, pp. 1-30, 2015. ISSN 1384-5810. doi: 10.1007/ s10618-015-0417-y. URL http://dx.doi.org/10.1007/s10618-015-0417-y.\n\nDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Geoffrey Hinton, Deng, Li, Yu, Dong, George E Dahl, Mohamed, Jaitly, Navdeep, Senior, Andrew, Vanhoucke, Vincent, Nguyen, Patrick, Tara N Sainath, Signal Processing Magazine. 296IEEEHinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E, Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior, Andrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara N, et al. Deep neural net- works for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 29(6):82-97, 2012.\n\nFactorization meets the neighborhood: a multifaceted collaborative filtering model. Y Koren, SIGKDD'08: ACM Int. Conf. on Knowledge Discovery and Data Mining. Koren, Y. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In SIGKDD'08: ACM Int. Conf. on Knowledge Discovery and Data Mining, pp. 426-434, 2008.\n\nMatrix factorization techniques for recommender systems. Yehuda Koren, Robert Bell, Chris Volinsky, Computer. 428Koren, Yehuda, Bell, Robert, and Volinsky, Chris. Matrix factorization techniques for recommender systems. Computer, 42(8):30-37, 2009.\n\ncom recommendations: Item-to-item collaborative filtering. G Linden, B Smith, J York, Amazon, Internet Computing. 71IEEELinden, G., Smith, B., and York, J. Amazon. com recommendations: Item-to-item collaborative filtering. Internet Computing, IEEE, 7(1):76-80, 2003.\n\nBaidu's recommender system for the biggest Chinese Q&A website. Qiwen Liu, Chen, Tianjian, Jing Cai, Dianhai Yu, Enlister, RecSys-12: Proc. of the 6th ACM Conf. on Recommender Systems. Liu, Qiwen, Chen, Tianjian, Cai, Jing, and Yu, Dianhai. Enlister: Baidu's recommender system for the biggest Chinese Q&A website. In RecSys-12: Proc. of the 6th ACM Conf. on Recommender Systems, pp. 285-288, 2012.\n\nBPR: Bayesian personalized ranking from implicit feedback. S Rendle, C Freudenthaler, Z Gantner, L Schmidt-Thieme, 978-0-9749039-5-8UAI'09: 25 th Conf. on Uncertainty in Artificial Intelligence. Rendle, S., Freudenthaler, C., Gantner, Z., and Schmidt-Thieme, L. BPR: Bayesian personalized ranking from implicit feedback. In UAI'09: 25 th Conf. on Uncertainty in Artificial Intelligence, pp. 452-461, 2009. ISBN 978-0-9749039-5-8.\n\n. Olga Russakovsky, Deng, Jia, Su, Hao, Jonathan Krause, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Andrej Karpathy, Aditya Khosla, Michael S Bernstein, Alexander C Berg, Li, Fei-Fei, Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael S., Berg, Alexander C., and Li, Fei-Fei. Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575, 2014. URL http://arxiv.org/abs/1409.0575.\n\nRestricted boltzmann machines for collaborative filtering. Ruslan Salakhutdinov, Andriy Mnih, Geoffrey Hinton, Proceedings of the 24th international conference on Machine learning. the 24th international conference on Machine learningACMSalakhutdinov, Ruslan, Mnih, Andriy, and Hinton, Geoffrey. Restricted boltzmann machines for collaborative filtering. In Proceedings of the 24th international conference on Machine learning, pp. 791-798. ACM, 2007.\n\nItem-based collaborative filtering recommendation algorithms. Sarwar, Badrul, Karypis, George, Joseph Konstan, John Riedl, Proceedings of the 10th international conference on World Wide Web. the 10th international conference on World Wide WebACMSarwar, Badrul, Karypis, George, Konstan, Joseph, and Riedl, John. Item-based collaborative filter- ing recommendation algorithms. In Proceedings of the 10th international conference on World Wide Web, pp. 285-295. ACM, 2001.\n\nAn mdp-based recommender system. Guy Shani, Ronen I Brafman, David Heckerman, Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence. the Eighteenth conference on Uncertainty in artificial intelligenceShani, Guy, Brafman, Ronen I, and Heckerman, David. An mdp-based recommender system. In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence, pp. 453-460.\n\nLearning to maximize reciprocal rank with collaborative less-is-more filtering. Yue Shi, Alexandros Karatzoglou, Baltrunas, Linas, Larson, Martha, Nuria Oliver, Alan Hanjalic, Climf, http:/doi.acm.org/10.1145/2365952.2365981Proceedings of the Sixth ACM Conference on Recommender Systems, RecSys '12. the Sixth ACM Conference on Recommender Systems, RecSys '12New York, NY, USAACMShi, Yue, Karatzoglou, Alexandros, Baltrunas, Linas, Larson, Martha, Oliver, Nuria, and Hanjalic, Alan. Climf: Learning to maximize reciprocal rank with collaborative less-is-more filtering. In Proceedings of the Sixth ACM Conference on Recommender Systems, RecSys '12, pp. 139-146, New York, NY, USA, 2012. ACM. ISBN 978-1-4503-1270-7. doi: 10.1145/2365952.2365981. URL http://doi.acm.org/10.1145/2365952.2365981.\n\nGaussian ranking by matrix factorization. Harald Steck, http:/doi.acm.org/10.1145/2792838.2800185Proceedings of the 9th ACM Conference on Recommender Systems, RecSys '15. the 9th ACM Conference on Recommender Systems, RecSys '15New York, NY, USAACMSteck, Harald. Gaussian ranking by matrix factorization. In Proceedings of the 9th ACM Confer- ence on Recommender Systems, RecSys '15, pp. 115-122, New York, NY, USA, 2015. ACM. ISBN 978-1-4503-3692-5. doi: 10.1145/2792838.2800185. URL http://doi.acm.org/ 10.1145/2792838.2800185.\n\nDeep content-based music recommendation. Aaron Van Den Oord, Sander Dieleman, Benjamin Schrauwen, Advances in Neural Information Processing Systems. Van den Oord, Aaron, Dieleman, Sander, and Schrauwen, Benjamin. Deep content-based music recommendation. In Advances in Neural Information Processing Systems, pp. 2643-2651, 2013.\n\nCollaborative deep learning for recommender systems. Wang, Hao, Naiyan Wang, Yeung, Dit-Yan, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '15. the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '15New York, NY, USAACMWang, Hao, Wang, Naiyan, and Yeung, Dit-Yan. Collaborative deep learning for recommender systems. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '15, pp. 1235-1244, New York, NY, USA, 2015. ACM.\n\nMaximum margin matrix factorization for collaborative ranking. Advances in neural information processing systems. Markus Weimer, Alexandros Karatzoglou, Le , Quoc Viet, Alex Smola, Weimer, Markus, Karatzoglou, Alexandros, Le, Quoc Viet, and Smola, Alex. Maximum margin ma- trix factorization for collaborative ranking. Advances in neural information processing systems, 2007.\n", "annotations": {"author": "[{\"end\":106,\"start\":64},{\"end\":130,\"start\":107},{\"end\":170,\"start\":131},{\"end\":213,\"start\":171},{\"end\":249,\"start\":214},{\"end\":287,\"start\":250},{\"end\":313,\"start\":288},{\"end\":347,\"start\":314},{\"end\":106,\"start\":64},{\"end\":130,\"start\":107},{\"end\":170,\"start\":131},{\"end\":213,\"start\":171},{\"end\":249,\"start\":214},{\"end\":287,\"start\":250},{\"end\":313,\"start\":288},{\"end\":347,\"start\":314}]", "publisher": null, "author_last_name": "[{\"end\":77,\"start\":71},{\"end\":129,\"start\":118},{\"end\":146,\"start\":137},{\"end\":184,\"start\":180},{\"end\":77,\"start\":71},{\"end\":129,\"start\":118},{\"end\":146,\"start\":137},{\"end\":184,\"start\":180}]", "author_first_name": "[{\"end\":70,\"start\":64},{\"end\":117,\"start\":107},{\"end\":136,\"start\":131},{\"end\":179,\"start\":171},{\"end\":70,\"start\":64},{\"end\":117,\"start\":107},{\"end\":136,\"start\":131},{\"end\":179,\"start\":171}]", "author_affiliation": "[{\"end\":248,\"start\":215},{\"end\":286,\"start\":251},{\"end\":312,\"start\":289},{\"end\":346,\"start\":315},{\"end\":248,\"start\":215},{\"end\":286,\"start\":251},{\"end\":312,\"start\":289},{\"end\":346,\"start\":315}]", "title": "[{\"end\":61,\"start\":1},{\"end\":408,\"start\":348},{\"end\":61,\"start\":1},{\"end\":408,\"start\":348}]", "venue": null, "abstract": "[{\"end\":1561,\"start\":454},{\"end\":1561,\"start\":454}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2833,\"start\":2813},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2853,\"start\":2833},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2873,\"start\":2853},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2920,\"start\":2899},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2932,\"start\":2920},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3846,\"start\":3820},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3866,\"start\":3846},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6305,\"start\":6284},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6325,\"start\":6305},{\"end\":6994,\"start\":6974},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7746,\"start\":7725},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8390,\"start\":8362},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8780,\"start\":8754},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8924,\"start\":8906},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9955,\"start\":9937},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15639,\"start\":15618},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15656,\"start\":15639},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15668,\"start\":15656},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16743,\"start\":16723},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21431,\"start\":21413},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21451,\"start\":21431},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23057,\"start\":23036},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23079,\"start\":23057},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23119,\"start\":23099},{\"end\":24520,\"start\":24513},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24642,\"start\":24620},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24675,\"start\":24655},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2833,\"start\":2813},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2853,\"start\":2833},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2873,\"start\":2853},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2920,\"start\":2899},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2932,\"start\":2920},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3846,\"start\":3820},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3866,\"start\":3846},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6305,\"start\":6284},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6325,\"start\":6305},{\"end\":6994,\"start\":6974},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7746,\"start\":7725},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8390,\"start\":8362},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8780,\"start\":8754},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8924,\"start\":8906},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9955,\"start\":9937},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15639,\"start\":15618},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15656,\"start\":15639},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15668,\"start\":15656},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16743,\"start\":16723},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21431,\"start\":21413},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21451,\"start\":21431},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23057,\"start\":23036},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23079,\"start\":23057},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23119,\"start\":23099},{\"end\":24520,\"start\":24513},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24642,\"start\":24620},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24675,\"start\":24655}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29312,\"start\":29210},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29391,\"start\":29313},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29670,\"start\":29392},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":29993,\"start\":29671},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30694,\"start\":29994},{\"attributes\":{\"id\":\"fig_0\"},\"end\":29312,\"start\":29210},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29391,\"start\":29313},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29670,\"start\":29392},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":29993,\"start\":29671},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30694,\"start\":29994}]", "paragraph": "[{\"end\":2741,\"start\":1577},{\"end\":3684,\"start\":2743},{\"end\":4248,\"start\":3686},{\"end\":5762,\"start\":4250},{\"end\":6873,\"start\":5810},{\"end\":7657,\"start\":6875},{\"end\":8180,\"start\":7659},{\"end\":9298,\"start\":8214},{\"end\":9648,\"start\":9328},{\"end\":9906,\"start\":9675},{\"end\":10261,\"start\":9908},{\"end\":10330,\"start\":10296},{\"end\":10439,\"start\":10364},{\"end\":10521,\"start\":10479},{\"end\":11556,\"start\":10583},{\"end\":12159,\"start\":11558},{\"end\":12413,\"start\":12161},{\"end\":12885,\"start\":12447},{\"end\":13533,\"start\":12887},{\"end\":14280,\"start\":13535},{\"end\":15432,\"start\":14361},{\"end\":16424,\"start\":15449},{\"end\":16684,\"start\":16426},{\"end\":17263,\"start\":16686},{\"end\":17453,\"start\":17265},{\"end\":18081,\"start\":17466},{\"end\":18132,\"start\":18103},{\"end\":18240,\"start\":18148},{\"end\":19118,\"start\":18242},{\"end\":20093,\"start\":19120},{\"end\":20788,\"start\":20095},{\"end\":21829,\"start\":20790},{\"end\":21911,\"start\":21843},{\"end\":23797,\"start\":21913},{\"end\":24375,\"start\":23836},{\"end\":24717,\"start\":24377},{\"end\":24831,\"start\":24719},{\"end\":25442,\"start\":24833},{\"end\":26674,\"start\":25444},{\"end\":27248,\"start\":26686},{\"end\":28247,\"start\":27250},{\"end\":28927,\"start\":28276},{\"end\":29023,\"start\":28929},{\"end\":29209,\"start\":29025},{\"end\":2741,\"start\":1577},{\"end\":3684,\"start\":2743},{\"end\":4248,\"start\":3686},{\"end\":5762,\"start\":4250},{\"end\":6873,\"start\":5810},{\"end\":7657,\"start\":6875},{\"end\":8180,\"start\":7659},{\"end\":9298,\"start\":8214},{\"end\":9648,\"start\":9328},{\"end\":9906,\"start\":9675},{\"end\":10261,\"start\":9908},{\"end\":10330,\"start\":10296},{\"end\":10439,\"start\":10364},{\"end\":10521,\"start\":10479},{\"end\":11556,\"start\":10583},{\"end\":12159,\"start\":11558},{\"end\":12413,\"start\":12161},{\"end\":12885,\"start\":12447},{\"end\":13533,\"start\":12887},{\"end\":14280,\"start\":13535},{\"end\":15432,\"start\":14361},{\"end\":16424,\"start\":15449},{\"end\":16684,\"start\":16426},{\"end\":17263,\"start\":16686},{\"end\":17453,\"start\":17265},{\"end\":18081,\"start\":17466},{\"end\":18132,\"start\":18103},{\"end\":18240,\"start\":18148},{\"end\":19118,\"start\":18242},{\"end\":20093,\"start\":19120},{\"end\":20788,\"start\":20095},{\"end\":21829,\"start\":20790},{\"end\":21911,\"start\":21843},{\"end\":23797,\"start\":21913},{\"end\":24375,\"start\":23836},{\"end\":24717,\"start\":24377},{\"end\":24831,\"start\":24719},{\"end\":25442,\"start\":24833},{\"end\":26674,\"start\":25444},{\"end\":27248,\"start\":26686},{\"end\":28247,\"start\":27250},{\"end\":28927,\"start\":28276},{\"end\":29023,\"start\":28929},{\"end\":29209,\"start\":29025}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9674,\"start\":9649},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10295,\"start\":10262},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10363,\"start\":10331},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10478,\"start\":10440},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10554,\"start\":10522},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14360,\"start\":14281},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17465,\"start\":17454},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18102,\"start\":18082},{\"attributes\":{\"id\":\"formula_0\"},\"end\":9674,\"start\":9649},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10295,\"start\":10262},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10363,\"start\":10331},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10478,\"start\":10440},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10554,\"start\":10522},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14360,\"start\":14281},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17465,\"start\":17454},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18102,\"start\":18082}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23701,\"start\":23694},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26492,\"start\":26485},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23701,\"start\":23694},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26492,\"start\":26485}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1575,\"start\":1563},{\"attributes\":{\"n\":\"2\"},\"end\":5777,\"start\":5765},{\"attributes\":{\"n\":\"2.1\"},\"end\":5808,\"start\":5780},{\"attributes\":{\"n\":\"2.2\"},\"end\":8212,\"start\":8183},{\"attributes\":{\"n\":\"3\"},\"end\":9326,\"start\":9301},{\"attributes\":{\"n\":\"3.1\"},\"end\":10581,\"start\":10556},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":12445,\"start\":12416},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":15447,\"start\":15435},{\"attributes\":{\"n\":\"4\"},\"end\":18146,\"start\":18135},{\"attributes\":{\"n\":\"4.1\"},\"end\":21841,\"start\":21832},{\"attributes\":{\"n\":\"4.2\"},\"end\":23834,\"start\":23800},{\"attributes\":{\"n\":\"4.3\"},\"end\":26684,\"start\":26677},{\"attributes\":{\"n\":\"5\"},\"end\":28274,\"start\":28250},{\"end\":29221,\"start\":29211},{\"end\":29324,\"start\":29314},{\"end\":29402,\"start\":29393},{\"end\":29681,\"start\":29672},{\"end\":30004,\"start\":29995},{\"attributes\":{\"n\":\"1\"},\"end\":1575,\"start\":1563},{\"attributes\":{\"n\":\"2\"},\"end\":5777,\"start\":5765},{\"attributes\":{\"n\":\"2.1\"},\"end\":5808,\"start\":5780},{\"attributes\":{\"n\":\"2.2\"},\"end\":8212,\"start\":8183},{\"attributes\":{\"n\":\"3\"},\"end\":9326,\"start\":9301},{\"attributes\":{\"n\":\"3.1\"},\"end\":10581,\"start\":10556},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":12445,\"start\":12416},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":15447,\"start\":15435},{\"attributes\":{\"n\":\"4\"},\"end\":18146,\"start\":18135},{\"attributes\":{\"n\":\"4.1\"},\"end\":21841,\"start\":21832},{\"attributes\":{\"n\":\"4.2\"},\"end\":23834,\"start\":23800},{\"attributes\":{\"n\":\"4.3\"},\"end\":26684,\"start\":26677},{\"attributes\":{\"n\":\"5\"},\"end\":28274,\"start\":28250},{\"end\":29221,\"start\":29211},{\"end\":29324,\"start\":29314},{\"end\":29402,\"start\":29393},{\"end\":29681,\"start\":29672},{\"end\":30004,\"start\":29995}]", "table": "[{\"end\":29670,\"start\":29451},{\"end\":29993,\"start\":29732},{\"end\":30694,\"start\":30156},{\"end\":29670,\"start\":29451},{\"end\":29993,\"start\":29732},{\"end\":30694,\"start\":30156}]", "figure_caption": "[{\"end\":29312,\"start\":29223},{\"end\":29391,\"start\":29326},{\"end\":29451,\"start\":29404},{\"end\":29732,\"start\":29683},{\"end\":30156,\"start\":30006},{\"end\":29312,\"start\":29223},{\"end\":29391,\"start\":29326},{\"end\":29451,\"start\":29404},{\"end\":29732,\"start\":29683},{\"end\":30156,\"start\":30006}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12075,\"start\":12067},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13515,\"start\":13507},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12075,\"start\":12067},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13515,\"start\":13507}]", "bib_author_first_name": "[{\"end\":31108,\"start\":31099},{\"end\":31144,\"start\":31137},{\"end\":31161,\"start\":31155},{\"end\":31505,\"start\":31501},{\"end\":31507,\"start\":31506},{\"end\":31541,\"start\":31533},{\"end\":31555,\"start\":31549},{\"end\":31830,\"start\":31825},{\"end\":32210,\"start\":32206},{\"end\":32222,\"start\":32218},{\"end\":32235,\"start\":32230},{\"end\":32568,\"start\":32567},{\"end\":32578,\"start\":32577},{\"end\":32939,\"start\":32933},{\"end\":32956,\"start\":32948},{\"end\":33402,\"start\":33394},{\"end\":33437,\"start\":33431},{\"end\":33439,\"start\":33438},{\"end\":33529,\"start\":33525},{\"end\":33531,\"start\":33530},{\"end\":33996,\"start\":33995},{\"end\":34316,\"start\":34310},{\"end\":34330,\"start\":34324},{\"end\":34342,\"start\":34337},{\"end\":34563,\"start\":34562},{\"end\":34573,\"start\":34572},{\"end\":34582,\"start\":34581},{\"end\":34840,\"start\":34835},{\"end\":34866,\"start\":34862},{\"end\":34879,\"start\":34872},{\"end\":35231,\"start\":35230},{\"end\":35241,\"start\":35240},{\"end\":35258,\"start\":35257},{\"end\":35269,\"start\":35268},{\"end\":35608,\"start\":35604},{\"end\":35650,\"start\":35642},{\"end\":35710,\"start\":35704},{\"end\":35727,\"start\":35721},{\"end\":35743,\"start\":35736},{\"end\":35745,\"start\":35744},{\"end\":35766,\"start\":35757},{\"end\":35768,\"start\":35767},{\"end\":36234,\"start\":36228},{\"end\":36256,\"start\":36250},{\"end\":36271,\"start\":36263},{\"end\":36723,\"start\":36717},{\"end\":36737,\"start\":36733},{\"end\":37130,\"start\":37127},{\"end\":37143,\"start\":37138},{\"end\":37145,\"start\":37144},{\"end\":37160,\"start\":37155},{\"end\":37592,\"start\":37589},{\"end\":37608,\"start\":37598},{\"end\":37661,\"start\":37656},{\"end\":37674,\"start\":37670},{\"end\":38352,\"start\":38346},{\"end\":38881,\"start\":38876},{\"end\":38902,\"start\":38896},{\"end\":38921,\"start\":38913},{\"end\":39235,\"start\":39229},{\"end\":39855,\"start\":39849},{\"end\":39874,\"start\":39864},{\"end\":39890,\"start\":39888},{\"end\":39897,\"start\":39893},{\"end\":39908,\"start\":39904},{\"end\":31108,\"start\":31099},{\"end\":31144,\"start\":31137},{\"end\":31161,\"start\":31155},{\"end\":31505,\"start\":31501},{\"end\":31507,\"start\":31506},{\"end\":31541,\"start\":31533},{\"end\":31555,\"start\":31549},{\"end\":31830,\"start\":31825},{\"end\":32210,\"start\":32206},{\"end\":32222,\"start\":32218},{\"end\":32235,\"start\":32230},{\"end\":32568,\"start\":32567},{\"end\":32578,\"start\":32577},{\"end\":32939,\"start\":32933},{\"end\":32956,\"start\":32948},{\"end\":33402,\"start\":33394},{\"end\":33437,\"start\":33431},{\"end\":33439,\"start\":33438},{\"end\":33529,\"start\":33525},{\"end\":33531,\"start\":33530},{\"end\":33996,\"start\":33995},{\"end\":34316,\"start\":34310},{\"end\":34330,\"start\":34324},{\"end\":34342,\"start\":34337},{\"end\":34563,\"start\":34562},{\"end\":34573,\"start\":34572},{\"end\":34582,\"start\":34581},{\"end\":34840,\"start\":34835},{\"end\":34866,\"start\":34862},{\"end\":34879,\"start\":34872},{\"end\":35231,\"start\":35230},{\"end\":35241,\"start\":35240},{\"end\":35258,\"start\":35257},{\"end\":35269,\"start\":35268},{\"end\":35608,\"start\":35604},{\"end\":35650,\"start\":35642},{\"end\":35710,\"start\":35704},{\"end\":35727,\"start\":35721},{\"end\":35743,\"start\":35736},{\"end\":35745,\"start\":35744},{\"end\":35766,\"start\":35757},{\"end\":35768,\"start\":35767},{\"end\":36234,\"start\":36228},{\"end\":36256,\"start\":36250},{\"end\":36271,\"start\":36263},{\"end\":36723,\"start\":36717},{\"end\":36737,\"start\":36733},{\"end\":37130,\"start\":37127},{\"end\":37143,\"start\":37138},{\"end\":37145,\"start\":37144},{\"end\":37160,\"start\":37155},{\"end\":37592,\"start\":37589},{\"end\":37608,\"start\":37598},{\"end\":37661,\"start\":37656},{\"end\":37674,\"start\":37670},{\"end\":38352,\"start\":38346},{\"end\":38881,\"start\":38876},{\"end\":38902,\"start\":38896},{\"end\":38921,\"start\":38913},{\"end\":39235,\"start\":39229},{\"end\":39855,\"start\":39849},{\"end\":39874,\"start\":39864},{\"end\":39890,\"start\":39888},{\"end\":39897,\"start\":39893},{\"end\":39908,\"start\":39904}]", "bib_author_last_name": "[{\"end\":31112,\"start\":31109},{\"end\":31129,\"start\":31114},{\"end\":31135,\"start\":31131},{\"end\":31153,\"start\":31145},{\"end\":31168,\"start\":31162},{\"end\":31515,\"start\":31508},{\"end\":31525,\"start\":31517},{\"end\":31531,\"start\":31527},{\"end\":31547,\"start\":31542},{\"end\":31562,\"start\":31556},{\"end\":31839,\"start\":31831},{\"end\":31848,\"start\":31841},{\"end\":31858,\"start\":31850},{\"end\":31863,\"start\":31860},{\"end\":31872,\"start\":31865},{\"end\":32216,\"start\":32211},{\"end\":32228,\"start\":32223},{\"end\":32242,\"start\":32236},{\"end\":32575,\"start\":32569},{\"end\":32583,\"start\":32579},{\"end\":32946,\"start\":32940},{\"end\":32961,\"start\":32957},{\"end\":33409,\"start\":33403},{\"end\":33415,\"start\":33411},{\"end\":33419,\"start\":33417},{\"end\":33423,\"start\":33421},{\"end\":33429,\"start\":33425},{\"end\":33444,\"start\":33440},{\"end\":33453,\"start\":33446},{\"end\":33461,\"start\":33455},{\"end\":33470,\"start\":33463},{\"end\":33478,\"start\":33472},{\"end\":33486,\"start\":33480},{\"end\":33497,\"start\":33488},{\"end\":33506,\"start\":33499},{\"end\":33514,\"start\":33508},{\"end\":33523,\"start\":33516},{\"end\":33539,\"start\":33532},{\"end\":34002,\"start\":33997},{\"end\":34322,\"start\":34317},{\"end\":34335,\"start\":34331},{\"end\":34351,\"start\":34343},{\"end\":34570,\"start\":34564},{\"end\":34579,\"start\":34574},{\"end\":34587,\"start\":34583},{\"end\":34595,\"start\":34589},{\"end\":34844,\"start\":34841},{\"end\":34850,\"start\":34846},{\"end\":34860,\"start\":34852},{\"end\":34870,\"start\":34867},{\"end\":34882,\"start\":34880},{\"end\":34892,\"start\":34884},{\"end\":35238,\"start\":35232},{\"end\":35255,\"start\":35242},{\"end\":35266,\"start\":35259},{\"end\":35284,\"start\":35270},{\"end\":35620,\"start\":35609},{\"end\":35626,\"start\":35622},{\"end\":35631,\"start\":35628},{\"end\":35635,\"start\":35633},{\"end\":35640,\"start\":35637},{\"end\":35657,\"start\":35651},{\"end\":35667,\"start\":35659},{\"end\":35676,\"start\":35669},{\"end\":35680,\"start\":35678},{\"end\":35686,\"start\":35682},{\"end\":35693,\"start\":35688},{\"end\":35702,\"start\":35695},{\"end\":35719,\"start\":35711},{\"end\":35734,\"start\":35728},{\"end\":35755,\"start\":35746},{\"end\":35773,\"start\":35769},{\"end\":35777,\"start\":35775},{\"end\":35786,\"start\":35779},{\"end\":36248,\"start\":36235},{\"end\":36261,\"start\":36257},{\"end\":36278,\"start\":36272},{\"end\":36690,\"start\":36684},{\"end\":36698,\"start\":36692},{\"end\":36707,\"start\":36700},{\"end\":36715,\"start\":36709},{\"end\":36731,\"start\":36724},{\"end\":36743,\"start\":36738},{\"end\":37136,\"start\":37131},{\"end\":37153,\"start\":37146},{\"end\":37170,\"start\":37161},{\"end\":37596,\"start\":37593},{\"end\":37620,\"start\":37609},{\"end\":37631,\"start\":37622},{\"end\":37638,\"start\":37633},{\"end\":37646,\"start\":37640},{\"end\":37654,\"start\":37648},{\"end\":37668,\"start\":37662},{\"end\":37683,\"start\":37675},{\"end\":37690,\"start\":37685},{\"end\":38358,\"start\":38353},{\"end\":38894,\"start\":38882},{\"end\":38911,\"start\":38903},{\"end\":38931,\"start\":38922},{\"end\":39222,\"start\":39218},{\"end\":39227,\"start\":39224},{\"end\":39240,\"start\":39236},{\"end\":39247,\"start\":39242},{\"end\":39256,\"start\":39249},{\"end\":39862,\"start\":39856},{\"end\":39886,\"start\":39875},{\"end\":39902,\"start\":39898},{\"end\":39914,\"start\":39909},{\"end\":31112,\"start\":31109},{\"end\":31129,\"start\":31114},{\"end\":31135,\"start\":31131},{\"end\":31153,\"start\":31145},{\"end\":31168,\"start\":31162},{\"end\":31515,\"start\":31508},{\"end\":31525,\"start\":31517},{\"end\":31531,\"start\":31527},{\"end\":31547,\"start\":31542},{\"end\":31562,\"start\":31556},{\"end\":31839,\"start\":31831},{\"end\":31848,\"start\":31841},{\"end\":31858,\"start\":31850},{\"end\":31863,\"start\":31860},{\"end\":31872,\"start\":31865},{\"end\":32216,\"start\":32211},{\"end\":32228,\"start\":32223},{\"end\":32242,\"start\":32236},{\"end\":32575,\"start\":32569},{\"end\":32583,\"start\":32579},{\"end\":32946,\"start\":32940},{\"end\":32961,\"start\":32957},{\"end\":33409,\"start\":33403},{\"end\":33415,\"start\":33411},{\"end\":33419,\"start\":33417},{\"end\":33423,\"start\":33421},{\"end\":33429,\"start\":33425},{\"end\":33444,\"start\":33440},{\"end\":33453,\"start\":33446},{\"end\":33461,\"start\":33455},{\"end\":33470,\"start\":33463},{\"end\":33478,\"start\":33472},{\"end\":33486,\"start\":33480},{\"end\":33497,\"start\":33488},{\"end\":33506,\"start\":33499},{\"end\":33514,\"start\":33508},{\"end\":33523,\"start\":33516},{\"end\":33539,\"start\":33532},{\"end\":34002,\"start\":33997},{\"end\":34322,\"start\":34317},{\"end\":34335,\"start\":34331},{\"end\":34351,\"start\":34343},{\"end\":34570,\"start\":34564},{\"end\":34579,\"start\":34574},{\"end\":34587,\"start\":34583},{\"end\":34595,\"start\":34589},{\"end\":34844,\"start\":34841},{\"end\":34850,\"start\":34846},{\"end\":34860,\"start\":34852},{\"end\":34870,\"start\":34867},{\"end\":34882,\"start\":34880},{\"end\":34892,\"start\":34884},{\"end\":35238,\"start\":35232},{\"end\":35255,\"start\":35242},{\"end\":35266,\"start\":35259},{\"end\":35284,\"start\":35270},{\"end\":35620,\"start\":35609},{\"end\":35626,\"start\":35622},{\"end\":35631,\"start\":35628},{\"end\":35635,\"start\":35633},{\"end\":35640,\"start\":35637},{\"end\":35657,\"start\":35651},{\"end\":35667,\"start\":35659},{\"end\":35676,\"start\":35669},{\"end\":35680,\"start\":35678},{\"end\":35686,\"start\":35682},{\"end\":35693,\"start\":35688},{\"end\":35702,\"start\":35695},{\"end\":35719,\"start\":35711},{\"end\":35734,\"start\":35728},{\"end\":35755,\"start\":35746},{\"end\":35773,\"start\":35769},{\"end\":35777,\"start\":35775},{\"end\":35786,\"start\":35779},{\"end\":36248,\"start\":36235},{\"end\":36261,\"start\":36257},{\"end\":36278,\"start\":36272},{\"end\":36690,\"start\":36684},{\"end\":36698,\"start\":36692},{\"end\":36707,\"start\":36700},{\"end\":36715,\"start\":36709},{\"end\":36731,\"start\":36724},{\"end\":36743,\"start\":36738},{\"end\":37136,\"start\":37131},{\"end\":37153,\"start\":37146},{\"end\":37170,\"start\":37161},{\"end\":37596,\"start\":37593},{\"end\":37620,\"start\":37609},{\"end\":37631,\"start\":37622},{\"end\":37638,\"start\":37633},{\"end\":37646,\"start\":37640},{\"end\":37654,\"start\":37648},{\"end\":37668,\"start\":37662},{\"end\":37683,\"start\":37675},{\"end\":37690,\"start\":37685},{\"end\":38358,\"start\":38353},{\"end\":38894,\"start\":38882},{\"end\":38911,\"start\":38903},{\"end\":38931,\"start\":38922},{\"end\":39222,\"start\":39218},{\"end\":39227,\"start\":39224},{\"end\":39240,\"start\":39236},{\"end\":39247,\"start\":39242},{\"end\":39256,\"start\":39249},{\"end\":39862,\"start\":39856},{\"end\":39886,\"start\":39875},{\"end\":39902,\"start\":39898},{\"end\":39914,\"start\":39909}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1409.1259\",\"id\":\"b0\"},\"end\":31421,\"start\":31050},{\"attributes\":{\"doi\":\"arXiv:1502.04390\",\"id\":\"b1\"},\"end\":31782,\"start\":31423},{\"attributes\":{\"doi\":\"978-1- 60558-906-0\",\"id\":\"b2\",\"matched_paper_id\":1145979},\"end\":32126,\"start\":31784},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":538820},\"end\":32472,\"start\":32128},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9480129},\"end\":32827,\"start\":32474},{\"attributes\":{\"doi\":\"10.1007/s10618-015-0417-y\",\"id\":\"b5\"},\"end\":33284,\"start\":32829},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206485943},\"end\":33909,\"start\":33286},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":207168823},\"end\":34251,\"start\":33911},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":58370896},\"end\":34501,\"start\":34253},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14604122},\"end\":34769,\"start\":34503},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13504603},\"end\":35169,\"start\":34771},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":10795036},\"end\":35600,\"start\":35171},{\"attributes\":{\"id\":\"b12\"},\"end\":36167,\"start\":35602},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7285098},\"end\":36620,\"start\":36169},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8047550},\"end\":37092,\"start\":36622},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":875571},\"end\":37507,\"start\":37094},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4108140},\"end\":38302,\"start\":37509},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14436871},\"end\":38833,\"start\":38304},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7118498},\"end\":39163,\"start\":38835},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4833213},\"end\":39733,\"start\":39165},{\"attributes\":{\"id\":\"b20\"},\"end\":40110,\"start\":39735},{\"attributes\":{\"doi\":\"arXiv:1409.1259\",\"id\":\"b0\"},\"end\":31421,\"start\":31050},{\"attributes\":{\"doi\":\"arXiv:1502.04390\",\"id\":\"b1\"},\"end\":31782,\"start\":31423},{\"attributes\":{\"doi\":\"978-1- 60558-906-0\",\"id\":\"b2\",\"matched_paper_id\":1145979},\"end\":32126,\"start\":31784},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":538820},\"end\":32472,\"start\":32128},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9480129},\"end\":32827,\"start\":32474},{\"attributes\":{\"doi\":\"10.1007/s10618-015-0417-y\",\"id\":\"b5\"},\"end\":33284,\"start\":32829},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206485943},\"end\":33909,\"start\":33286},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":207168823},\"end\":34251,\"start\":33911},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":58370896},\"end\":34501,\"start\":34253},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14604122},\"end\":34769,\"start\":34503},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13504603},\"end\":35169,\"start\":34771},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":10795036},\"end\":35600,\"start\":35171},{\"attributes\":{\"id\":\"b12\"},\"end\":36167,\"start\":35602},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7285098},\"end\":36620,\"start\":36169},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8047550},\"end\":37092,\"start\":36622},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":875571},\"end\":37507,\"start\":37094},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4108140},\"end\":38302,\"start\":37509},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14436871},\"end\":38833,\"start\":38304},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7118498},\"end\":39163,\"start\":38835},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4833213},\"end\":39733,\"start\":39165},{\"attributes\":{\"id\":\"b20\"},\"end\":40110,\"start\":39735}]", "bib_title": "[{\"end\":31823,\"start\":31784},{\"end\":32204,\"start\":32128},{\"end\":32565,\"start\":32474},{\"end\":33392,\"start\":33286},{\"end\":33993,\"start\":33911},{\"end\":34308,\"start\":34253},{\"end\":34560,\"start\":34503},{\"end\":34833,\"start\":34771},{\"end\":35228,\"start\":35171},{\"end\":36226,\"start\":36169},{\"end\":36682,\"start\":36622},{\"end\":37125,\"start\":37094},{\"end\":37587,\"start\":37509},{\"end\":38344,\"start\":38304},{\"end\":38874,\"start\":38835},{\"end\":39216,\"start\":39165},{\"end\":31823,\"start\":31784},{\"end\":32204,\"start\":32128},{\"end\":32565,\"start\":32474},{\"end\":33392,\"start\":33286},{\"end\":33993,\"start\":33911},{\"end\":34308,\"start\":34253},{\"end\":34560,\"start\":34503},{\"end\":34833,\"start\":34771},{\"end\":35228,\"start\":35171},{\"end\":36226,\"start\":36169},{\"end\":36682,\"start\":36622},{\"end\":37125,\"start\":37094},{\"end\":37587,\"start\":37509},{\"end\":38344,\"start\":38304},{\"end\":38874,\"start\":38835},{\"end\":39216,\"start\":39165}]", "bib_author": "[{\"end\":31114,\"start\":31099},{\"end\":31131,\"start\":31114},{\"end\":31137,\"start\":31131},{\"end\":31155,\"start\":31137},{\"end\":31170,\"start\":31155},{\"end\":31517,\"start\":31501},{\"end\":31527,\"start\":31517},{\"end\":31533,\"start\":31527},{\"end\":31549,\"start\":31533},{\"end\":31564,\"start\":31549},{\"end\":31841,\"start\":31825},{\"end\":31850,\"start\":31841},{\"end\":31860,\"start\":31850},{\"end\":31865,\"start\":31860},{\"end\":31874,\"start\":31865},{\"end\":32218,\"start\":32206},{\"end\":32230,\"start\":32218},{\"end\":32244,\"start\":32230},{\"end\":32577,\"start\":32567},{\"end\":32585,\"start\":32577},{\"end\":32948,\"start\":32933},{\"end\":32963,\"start\":32948},{\"end\":33411,\"start\":33394},{\"end\":33417,\"start\":33411},{\"end\":33421,\"start\":33417},{\"end\":33425,\"start\":33421},{\"end\":33431,\"start\":33425},{\"end\":33446,\"start\":33431},{\"end\":33455,\"start\":33446},{\"end\":33463,\"start\":33455},{\"end\":33472,\"start\":33463},{\"end\":33480,\"start\":33472},{\"end\":33488,\"start\":33480},{\"end\":33499,\"start\":33488},{\"end\":33508,\"start\":33499},{\"end\":33516,\"start\":33508},{\"end\":33525,\"start\":33516},{\"end\":33541,\"start\":33525},{\"end\":34004,\"start\":33995},{\"end\":34324,\"start\":34310},{\"end\":34337,\"start\":34324},{\"end\":34353,\"start\":34337},{\"end\":34572,\"start\":34562},{\"end\":34581,\"start\":34572},{\"end\":34589,\"start\":34581},{\"end\":34597,\"start\":34589},{\"end\":34846,\"start\":34835},{\"end\":34852,\"start\":34846},{\"end\":34862,\"start\":34852},{\"end\":34872,\"start\":34862},{\"end\":34884,\"start\":34872},{\"end\":34894,\"start\":34884},{\"end\":35240,\"start\":35230},{\"end\":35257,\"start\":35240},{\"end\":35268,\"start\":35257},{\"end\":35286,\"start\":35268},{\"end\":35622,\"start\":35604},{\"end\":35628,\"start\":35622},{\"end\":35633,\"start\":35628},{\"end\":35637,\"start\":35633},{\"end\":35642,\"start\":35637},{\"end\":35659,\"start\":35642},{\"end\":35669,\"start\":35659},{\"end\":35678,\"start\":35669},{\"end\":35682,\"start\":35678},{\"end\":35688,\"start\":35682},{\"end\":35695,\"start\":35688},{\"end\":35704,\"start\":35695},{\"end\":35721,\"start\":35704},{\"end\":35736,\"start\":35721},{\"end\":35757,\"start\":35736},{\"end\":35775,\"start\":35757},{\"end\":35779,\"start\":35775},{\"end\":35788,\"start\":35779},{\"end\":36250,\"start\":36228},{\"end\":36263,\"start\":36250},{\"end\":36280,\"start\":36263},{\"end\":36692,\"start\":36684},{\"end\":36700,\"start\":36692},{\"end\":36709,\"start\":36700},{\"end\":36717,\"start\":36709},{\"end\":36733,\"start\":36717},{\"end\":36745,\"start\":36733},{\"end\":37138,\"start\":37127},{\"end\":37155,\"start\":37138},{\"end\":37172,\"start\":37155},{\"end\":37598,\"start\":37589},{\"end\":37622,\"start\":37598},{\"end\":37633,\"start\":37622},{\"end\":37640,\"start\":37633},{\"end\":37648,\"start\":37640},{\"end\":37656,\"start\":37648},{\"end\":37670,\"start\":37656},{\"end\":37685,\"start\":37670},{\"end\":37692,\"start\":37685},{\"end\":38360,\"start\":38346},{\"end\":38896,\"start\":38876},{\"end\":38913,\"start\":38896},{\"end\":38933,\"start\":38913},{\"end\":39224,\"start\":39218},{\"end\":39229,\"start\":39224},{\"end\":39242,\"start\":39229},{\"end\":39249,\"start\":39242},{\"end\":39258,\"start\":39249},{\"end\":39864,\"start\":39849},{\"end\":39888,\"start\":39864},{\"end\":39893,\"start\":39888},{\"end\":39904,\"start\":39893},{\"end\":39916,\"start\":39904},{\"end\":31114,\"start\":31099},{\"end\":31131,\"start\":31114},{\"end\":31137,\"start\":31131},{\"end\":31155,\"start\":31137},{\"end\":31170,\"start\":31155},{\"end\":31517,\"start\":31501},{\"end\":31527,\"start\":31517},{\"end\":31533,\"start\":31527},{\"end\":31549,\"start\":31533},{\"end\":31564,\"start\":31549},{\"end\":31841,\"start\":31825},{\"end\":31850,\"start\":31841},{\"end\":31860,\"start\":31850},{\"end\":31865,\"start\":31860},{\"end\":31874,\"start\":31865},{\"end\":32218,\"start\":32206},{\"end\":32230,\"start\":32218},{\"end\":32244,\"start\":32230},{\"end\":32577,\"start\":32567},{\"end\":32585,\"start\":32577},{\"end\":32948,\"start\":32933},{\"end\":32963,\"start\":32948},{\"end\":33411,\"start\":33394},{\"end\":33417,\"start\":33411},{\"end\":33421,\"start\":33417},{\"end\":33425,\"start\":33421},{\"end\":33431,\"start\":33425},{\"end\":33446,\"start\":33431},{\"end\":33455,\"start\":33446},{\"end\":33463,\"start\":33455},{\"end\":33472,\"start\":33463},{\"end\":33480,\"start\":33472},{\"end\":33488,\"start\":33480},{\"end\":33499,\"start\":33488},{\"end\":33508,\"start\":33499},{\"end\":33516,\"start\":33508},{\"end\":33525,\"start\":33516},{\"end\":33541,\"start\":33525},{\"end\":34004,\"start\":33995},{\"end\":34324,\"start\":34310},{\"end\":34337,\"start\":34324},{\"end\":34353,\"start\":34337},{\"end\":34572,\"start\":34562},{\"end\":34581,\"start\":34572},{\"end\":34589,\"start\":34581},{\"end\":34597,\"start\":34589},{\"end\":34846,\"start\":34835},{\"end\":34852,\"start\":34846},{\"end\":34862,\"start\":34852},{\"end\":34872,\"start\":34862},{\"end\":34884,\"start\":34872},{\"end\":34894,\"start\":34884},{\"end\":35240,\"start\":35230},{\"end\":35257,\"start\":35240},{\"end\":35268,\"start\":35257},{\"end\":35286,\"start\":35268},{\"end\":35622,\"start\":35604},{\"end\":35628,\"start\":35622},{\"end\":35633,\"start\":35628},{\"end\":35637,\"start\":35633},{\"end\":35642,\"start\":35637},{\"end\":35659,\"start\":35642},{\"end\":35669,\"start\":35659},{\"end\":35678,\"start\":35669},{\"end\":35682,\"start\":35678},{\"end\":35688,\"start\":35682},{\"end\":35695,\"start\":35688},{\"end\":35704,\"start\":35695},{\"end\":35721,\"start\":35704},{\"end\":35736,\"start\":35721},{\"end\":35757,\"start\":35736},{\"end\":35775,\"start\":35757},{\"end\":35779,\"start\":35775},{\"end\":35788,\"start\":35779},{\"end\":36250,\"start\":36228},{\"end\":36263,\"start\":36250},{\"end\":36280,\"start\":36263},{\"end\":36692,\"start\":36684},{\"end\":36700,\"start\":36692},{\"end\":36709,\"start\":36700},{\"end\":36717,\"start\":36709},{\"end\":36733,\"start\":36717},{\"end\":36745,\"start\":36733},{\"end\":37138,\"start\":37127},{\"end\":37155,\"start\":37138},{\"end\":37172,\"start\":37155},{\"end\":37598,\"start\":37589},{\"end\":37622,\"start\":37598},{\"end\":37633,\"start\":37622},{\"end\":37640,\"start\":37633},{\"end\":37648,\"start\":37640},{\"end\":37656,\"start\":37648},{\"end\":37670,\"start\":37656},{\"end\":37685,\"start\":37670},{\"end\":37692,\"start\":37685},{\"end\":38360,\"start\":38346},{\"end\":38896,\"start\":38876},{\"end\":38913,\"start\":38896},{\"end\":38933,\"start\":38913},{\"end\":39224,\"start\":39218},{\"end\":39229,\"start\":39224},{\"end\":39242,\"start\":39229},{\"end\":39249,\"start\":39242},{\"end\":39258,\"start\":39249},{\"end\":39864,\"start\":39849},{\"end\":39888,\"start\":39864},{\"end\":39893,\"start\":39888},{\"end\":39904,\"start\":39893},{\"end\":39916,\"start\":39904}]", "bib_venue": "[{\"end\":36403,\"start\":36350},{\"end\":36864,\"start\":36813},{\"end\":37323,\"start\":37256},{\"end\":37885,\"start\":37809},{\"end\":38549,\"start\":38475},{\"end\":39476,\"start\":39367},{\"end\":36403,\"start\":36350},{\"end\":36864,\"start\":36813},{\"end\":37323,\"start\":37256},{\"end\":37885,\"start\":37809},{\"end\":38549,\"start\":38475},{\"end\":39476,\"start\":39367},{\"end\":31097,\"start\":31050},{\"end\":31499,\"start\":31423},{\"end\":31935,\"start\":31892},{\"end\":32284,\"start\":32244},{\"end\":32627,\"start\":32585},{\"end\":32931,\"start\":32829},{\"end\":33567,\"start\":33541},{\"end\":34068,\"start\":34004},{\"end\":34361,\"start\":34353},{\"end\":34615,\"start\":34597},{\"end\":34954,\"start\":34894},{\"end\":35364,\"start\":35303},{\"end\":36348,\"start\":36280},{\"end\":36811,\"start\":36745},{\"end\":37254,\"start\":37172},{\"end\":37807,\"start\":37733},{\"end\":38473,\"start\":38401},{\"end\":38982,\"start\":38933},{\"end\":39365,\"start\":39258},{\"end\":39847,\"start\":39735},{\"end\":31097,\"start\":31050},{\"end\":31499,\"start\":31423},{\"end\":31935,\"start\":31892},{\"end\":32284,\"start\":32244},{\"end\":32627,\"start\":32585},{\"end\":32931,\"start\":32829},{\"end\":33567,\"start\":33541},{\"end\":34068,\"start\":34004},{\"end\":34361,\"start\":34353},{\"end\":34615,\"start\":34597},{\"end\":34954,\"start\":34894},{\"end\":35364,\"start\":35303},{\"end\":36348,\"start\":36280},{\"end\":36811,\"start\":36745},{\"end\":37254,\"start\":37172},{\"end\":37807,\"start\":37733},{\"end\":38473,\"start\":38401},{\"end\":38982,\"start\":38933},{\"end\":39365,\"start\":39258},{\"end\":39847,\"start\":39735}]"}}}, "year": 2023, "month": 12, "day": 17}