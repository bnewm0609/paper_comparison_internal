{"id": 228096677, "updated": "2023-10-06 12:31:43.693", "metadata": {"title": "Learning Game-Theoretic Models of Multiagent Trajectories Using Implicit Layers", "authors": "[{\"first\":\"Philipp\",\"last\":\"Geiger\",\"middle\":[]},{\"first\":\"Christoph-Nikolas\",\"last\":\"Straehle\",\"middle\":[]}]", "venue": "ArXiv", "journal": "4950-4958", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "For prediction of interacting agents' trajectories, we propose an end-to-end trainable architecture that hybridizes neural nets with game-theoretic reasoning, has interpretable intermediate representations, and transfers to downstream decision making. It uses a net that reveals preferences from the agents' past joint trajectory, and a differentiable implicit layer that maps these preferences to local Nash equilibria, forming the modes of the predicted future trajectory. Additionally, it learns an equilibrium refinement concept. For tractability, we introduce a new class of continuous potential games and an equilibrium-separating partition of the action space. We provide theoretical results for explicit gradients and soundness. In experiments, we evaluate our approach on two real-world data sets, where we predict highway driver merging trajectories, and on a simple decision-making transfer task.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2008.07303", "mag": "3113168424", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/GeigerS21", "doi": "10.1609/aaai.v35i6.16628"}}, "content": {"source": {"pdf_hash": "9ee4712f6d56bfe53b5b0a39f888ff1a840f01db", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2008.07303v7.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4d39167cc222f003cf731bcb093d0bf1fc6b0a8b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9ee4712f6d56bfe53b5b0a39f888ff1a840f01db.txt", "contents": "\nLearning game-theoretic models of multiagent trajectories using implicit layers *\n\n\nPhilipp Geiger philipp.w.geiger@de.bosch.com \nBosch Center for Artificial Intelligence\nRenningenGermany\n\nChristoph-Nikolas Straehle christoph-nikolas.straehle@de.bosch.com \nBosch Center for Artificial Intelligence\nRenningenGermany\n\nLearning game-theoretic models of multiagent trajectories using implicit layers *\n\nFor prediction of interacting agents' trajectories, we propose an end-to-end trainable architecture that hybridizes neural nets with game-theoretic reasoning, has interpretable intermediate representations, and transfers to downstream decision making. It uses a net that reveals preferences from the agents' past joint trajectory, and a differentiable implicit layer that maps these preferences to local Nash equilibria, forming the modes of the predicted future trajectory. Additionally, it learns an equilibrium refinement concept. For tractability, we introduce a new class of continuous potential games and an equilibrium-separating partition of the action space. We provide theoretical results for explicit gradients and soundness. In experiments, we evaluate our approach on two real-world data sets, where we predict highway drivers' merging trajectories, and on a simple decision-making transfer task.\n\nIntroduction\n\nPrediction of interacting agents' trajectories has recently been advanced by flexible, tractable, multi-modal datadriven approaches. But it remains a challenge to use them for safety-critical domains with additional verification and decision-making transfer requirements, like automated driving or mobile robots in interaction with humans. Towards addressing this challenge, the following seem sensible intermediate goals: (1) incorporation of well-understood principles, prior knowledge and reasoning of the multiagent domain, allowing to generalize well and to transfer to robust downstream decision making; (2) interpretability of models' latent variables, allowing for verification beyond just testing the final output; (3) theoretical analysis of soundness.\n\nIn this paper, we take a step towards addressing multiagent trajectory prediction including these intermediate goals, while trying to keep as much as possible of the practical strength of data-driven approaches. For this, we hybridize neural learning with game-theoretic reasoning -because game theory provides well-established explanations of agents' behavior based on the principle of instrumental rationality, i.e., viewing agents as utility maximizers. Roughly speaking, we \"fit a game to the observed trajectory data\".\n\nAlong this hybrid direction one major obstacle -and a general reason why game theory often remains in abstract * Extended version of paper with: Copyright \u00a9 2021, Association for the Advancement of Artificial Intelligence. All rights reserved. settings -lies in classic game-theoretic solution concepts like the Nash equilibrium (NE) notoriously suffering from computational intractability. As one way to overcome this, we build on local NE Sastry 2013, 2016). We combine this with a specific class of games -(continuous) potential games (Monderer and Shapley 1996) -for which local NE usually coincide with local optima of a single objective function, simplifying search. Another challenge lies in combining game theory with neural nets in a way that makes the overall model still efficiently trainable by gradient-based methods. To address this, we build on implicit layers (Amos and Kolter 2017;Bai, Kolter, and Koltun 2019). Implicit layers specify the functional relation between a layer's input and its output not in closed form, but only implicitly, usually via an equation. Nonetheless they allow to get exact gradients by \"differentiating through\" the equation, based on the implicit function theorem.\n\nMain contributions and outline. We propose a modular architecture that outputs a multi-modal prediction of interacting agents' joint trajectory (where modes are interpretable as local Nash equilibria), from their past trajectory as input (Sec. 3). The architecture is depicted in Fig. 1, alongside the motivating example of highway drivers' merging trajectories. It builds on the following components:\n\n\u2022 a tractable, differentiable game solver implicit layer (Sec. 3.1) with explicit gradient formula, mapping game parameters to local Nash equilibria (Thm. 1). It is based on a new class of continuous-action trajectory games that allow to encode prior knowledge on agents' preferences (Def. 4). We prove that they are potential games (Lem. 1). And it builds on an equilibrium-separating concave partition of the action space that we introduce to ensure tractability (Def. 5).\n\n\u2022 Furthermore, the architecture contains a neural net that reveals the agents' preferences from their past, and a net that learns an equilibrium refinement concept (Sec. 3.2).\n\nThis architecture forms a model class where certain latent representations have clear game-theoretic interpretations and certain layers encode game-theoretic principles that help induction (also towards strategically-robust decisionmaking). At the same time, it has neural net-based capacity for learning, and is end-to-end trainable with analytic gradient formula. Furthermore: arXiv: 2008 Figure 1: Bottom: Our full architecture (Sec. 3.2). Top: Example (stylized): highway merging scenario, where reliable models of (human) driver interaction are key for safe automated driving. Top left: input x: initial trajectories of drivers. Top right: prediction of future trajectory y: depicted are two modes 1\u0177 , 2\u0177 corresponding to two local Nash eq. 1 a * , 2 a * : red going first vs. yellow first.\n\n\u2022 In Sec. 4, we give two concrete example scenarios that provably satisfy our approach's conditions (Prop. 1,etc.).\n\n\u2022 In the experiments reported in Sec. 5, we apply our architecture to prediction of real-world highway on-ramp merging driver interaction trajectories, on one established and one new data set we publish alongside this paper. We also apply it to a simple decision-making transfer task.\n\nKeep in mind that proofs are available in Sec. A and that the present paper is an extended version of (Geiger and Straehle 2021). In what follows, we first discuss related work and introduce setting and background (Sec. 2).\n\nClosest related work. Regarding general multiagent model learning from observational behavioral data with game-theoretic components: closest related is work by Kolter (2018, 2019), who use game solvers as differentiable implicit layers, learning these layers' input (i.e., agents' preferences) from covariates. They focus on discrete actions while we address continuous trajectory prediction. And they use different solution concepts, and do not consider equilibrium refinement. There is further work more broadly related in this direction (Kita 1999;Kita, Tanimoto, and Fukuyama 2002;Liu et al. 2007;Kang and Rakha 2017;Tian et al. 2018;Fox et al. 2018;Camara et al. 2018;Ma et al. 2017;Sun, Zhan, and Tomizuka 2018), sometimes also studying driver interaction, but they have no or little data-driven aspects (in particular no implicit layers) and/or use different approximations to rationality than our local NE, such as level-k reasoning, and often are less general than us, often focusing on discrete actions. More broadly related is multiagent inverse reinforcement learning (IRL) (Wang and Klabjan 2018;Reddy et al. 2012;Zhang et al. 2019;Etesami and Straehle 2020), often discrete-action.\n\nFor multiagent trajectory prediction, there generally is a growing number of papers on the machine learning side, of-ten building on deep learning principles and allowing multimodality -but without game-theoretic components. Without any claim to completeness, there is work using longshort term memories (LSTMs) Deo and Trivedi 2018;Salzmann et al. 2020), generative adversarial networks (GANs) (Gupta et al. 2018), and attention-based encoders (Tang and Salakhutdinov 2019). Kuderer et al. (2012) uses a partition (\"topological variants\") of the trajectory space related to ours. There is also work related to the principle of \"social force\" (Helbing and Molnar 1995;Robicquet et al. 2016;Blaiotta 2019), and related rule-based driver modeling approaches (Treiber, Hennecke, and Helbing 2000;Kesting, Treiber, and Helbing 2007).\n\nRegarding additional game-theoretic elements: W.r.t. the class of trajectory potential games we introduce (Def. 4), the closest related work we are aware of is (Zazo et al. 2016) who consider a related class, but they do not allow agents' utilities to have differing additive terms w.r.t. their own actions. Worth mentioning is further related work based on games (different ones than ours though), but towards pure control (not prediction) tasks (Peters et al. 2020;Zhang et al. 2018;Spica et al. 2018;Fisac et al. 2019). Peters et al. (2020) use a latent variable for the equilibrium selection, similar to our equilibrium weighting. For further related work see Sec. E in the appendix.\n\n\nGeneral Setting, Goals and Background\n\nWe consider scenes, each consisting of:\n\n\u2022 a set I := {1, . . . , n} of agents.\n\n\u2022 Each agent i \u2208 I at each time t \u2208 [0, T ] has an individual state y i t \u2208 R d Y . They yield an individual trajectory y i = (y i t ) t\u2208[0,T ] (think of 0 as the present time point and T as the horizon up to which we want to predict).\n\n\u2022 And y := ((y 1 t , . . . , y n t )) t\u2208[0,T ] \u2208 Y denotes the agents' joint (future) trajectory.\n\n\u2022 We assume that the past joint trajectory x \u2208 X of the agents until time point 0 is available as side information. Now, besides the other goals mentioned in Sec. 1, we formulate the main (passive) predictive problem as follows:\n\n\u2022 goal: in a new scene, predict the future joint trajectory y by a list of pairs (\u0177,q), corresponding to y's modes, where each\u0177 is a point prediction of y, andq the associated probability (for more details and metrics etc., see Sec. 3.2, 5);\n\n\u2022 given: (1) the past trajectory x of that new scene, as well as (2) a training set consisting of previously sampled scenes, i.e., pairs (x , y ), (x , y ), . . . of past and future trajectory (discrete-time subsampled of course). (We assume all scenes sampled from the same underlying distribution.)\n\nWe assume that agent i's (future) trajectory y i is parameterized by a finite-dimensional vector a i \u2208 A i \u2286 R d A , which we refer to as i's action, with A i the action space of i. So, in particular, there is a (joint) trajectory parameterization r : A \u2192 Y , with A := A 1 \u00d7 . . . \u00d7 A n the joint action space. Keep in mind that a = (a 1 , . . . , a n ), a \u2212i means (a 1 , . . . , a i\u22121 , a i+1 , a n ) and (a i , a \u2212i ) reads a.\n\nWe use games (Shoham and Leyton-Brown 2008;Osborne and Rubinstein 1994) to model our setting. A game specifies the set of agents (also called \"players\"), their possible actions and their utility functions. The following formal definition is slightly tailored to our setting: utilities are integrals over the trajectories parameterized by the actions.\n\nDefinition 1 (Game). A (trajectory) game consists of: the set I of agents, and for each agent i \u2208 I: the action space A i \u2286 R d A , and a utility function u i : A \u2192 R. We assume u i , i \u2208 I, to be of the form\nu i (a) = T 0 u i t (y t )d\u00b5(t),\nwhere a \u2208 A, y = r(a); u i t , t \u2208 [0, T ], are the stage-wise utility functions, and \u00b5 is a measure on [0, T ]. 1 This game formalizes the agents' \"decision-making problem\". Game theory also provides the \"Nash equilibrium\" as a concept of how rational agents will/should act to \"solve\" the game. Here we use a \"local\" version -for tractability:\n\nDefinition 2 (Local Nash equilibrium (NE) Sastry 2016, 2013)). Given a game, a joint action a \u2208 A is a (pure) local Nash equilibrium (local NE) if there are open sets S i \u2282 A i such that a i \u2208 S i and for each i,\nu i (a i , a \u2212i ) \u2265 u i (a i , a \u2212i ),\nfor any a i \u2208 S i . 2 If S i = A i for all i, then a is called a (pure, global) NE.\n\nThe following type of game can reduce finding local NE to finding local optima of a single objective (\"potential function\"), allowing for tractable gradient ascent-based search.\n\nDefinition 3 (Potential game (Monderer and Shapley 1996)). A game is called an (exact continuous) potential game, if there is a so-called potential function \u03c8 such that, for all agents i, all actions a i , a i and remaining actions a \u2212i ,\nu i (a i , a \u2212i ) \u2212 u i (a i , a \u2212i ) = \u03c8(a i , a \u2212i ) \u2212 \u03c8(a i , a \u2212i ).\nLet us also give some neural net-related background: Remark 1 (Implicit layers (Amos and Kolter 2017;Amos et al. 2018;Bai, Kolter, and Koltun 2019;El Ghaoui et al. 2019)). Classically, one specifies a neural net layer by specifying the functional relation between its input v and output w explicitly, in closed form, w = f (v), for some function f (e.g., a softmax). The idea of implicit layers is to specify the relation implicitly, usually via an equation h(v, w) = 0 (coming from, e.g., a stationarity condition of an optimization or dynamics modeling problem). To ensure that this specification is indeed useful in prediction and training, there are two important requirements: (1) the equation has to determine a unique, tractable function f that maps v to w, and (2) f has to be differentiable, ideally with explicitly given analytic gradients.\n\n\nGeneral Approach With Analysis\n\nWe now describe our general approach. It consists of (a) a game-theoretic model and differentiable reasoning about how the agents behave (Sec. 3.1), and (b) a neural net architecture that incorporates this game-theoretic model/reasoning as an implicit layer and combines it with learnable modules, with tractable training and decisionmaking transfer abilities (Sec. 3.2).\n\n\nCommon-Coupled Games, Equilibrium-Separation and Induced Implicit Layer\n\nFor the rest of the paper, let (\u0393 \u03b8 ) \u03b8\u2208\u0398 , \u0398 \u2286 R d\u0398 , be a parametric family of trajectory games (Def. 1). First let us introduce the following type of trajectory game to strike a balance between adequate modeling and tractability: Definition 4 (Common-coupled game). We call \u0393 \u03b8 a common-coupled(-term trajectory) game, if the stage-wise utility functions (Def. 1) have the following form, for all agents i \u2208 I, t \u2208 [0, T ]:\nu i,\u03b8 t (y t ) = u com,\u03b8 t (y t ) + u own,i,\u03b8 t (y i t ) + u oth,i,\u03b8 t (y \u2212i t ),\n(1) where y = r(a) (action parameterizes trajectory, Sec. 2), u com,\u03b8 t is a term that depends on all agents' trajectories and is common between agents, u own,i,\u03b8 t and u oth,i,\u03b8 t are terms that only depend on agent i's trajectory, or all other agents' trajectories, respectively, and may differ between agents.\n\nCommon-coupled games adequately approximate many multiagent trajectory settings where agents trade off (a) social norms and/or common interests (say, traffic rules or the common interest to avoid crashes), captured by the common utility term u com,\u03b8 t , against (b) individual inclinations related to their own state, captured by the terms u own,i,\u03b8 t . It is noncooperative, i.e., utilities differ, but more on the cooperative than adversarial end of games. For tractability we can state: Lemma 1. If \u0393 \u03b8 is a common-coupled game, then it is a potential game with the following potential function, where, as usual, y = r(a):\n\u03c8(\u03b8, a) = T 0 u com,\u03b8 t (y t ) + i\u2208I u own,i,\u03b8 t (y i t )d\u00b5(t).\nNote that this implies existence of NE, given continuity of the utilities and compactness (Monderer and Shapley 1996).\n\nWe now show how the mappings from parameters \u03b8 to local NE of the game \u0393 \u03b8 can be soundly defined, tractable and differentiable, so that we can use this game-theoretic reasoning 3 as one implicit layer (Rem. 1) in our architecture. For this, a helpful step is to (tractably) partition the action space into subspaces with exactly one equilibrium each -if the game permits this. For the rest of the paper, let (\u00c3 k ) k\u2208K be a finite collection of subspaces of A, i.e.,\u00c3 k \u2286 A. Definition 5 (Equilibrium-separating action subspaces). For a common-coupled game \u0393 \u03b8 , we call the action subspace collection (\u00c3 k ) k\u2208K equilibrium-separating (partition) if, for all k \u2208 K and \u03b8 \u2208 \u0398, the game's potential function \u03c8(\u03b8, \u00b7) is strictly concave on\u00c3 k . 4\n\nAs a simplified example, a first partition towards equilibrium-separation in the highway merging scenario of Fig. 1 would be into two subspaces: (1) those that result in joint trajectories where the red car goes first and (2) those where yellow goes first. More details follow in Scenario 1.\n\nKeep in mind that the equation \u2207 a \u03c8(\u03b8, a)=0 is a necessary condition for a to be a local NE of \u0393 \u03b8 (for interior points), since local optima of the potential function correspond to local NE. This equation induces an implicit layer: Assumption 1. Let \u0393 \u03b8 be a common-coupled game. Let (\u00c3 k ) k\u2208K be equilibrium-separating subspaces for it, and let all\u00c3 k , k \u2208 K be compact, given by the intersection of linear inequality constraints. On each subspace \u0398 \u00d7\u00c3 k , k \u2208 K, let \u0393 \u03b8 's potential function \u03c8 be continuous. Theorem 1 (Games-induced differentiable implicit layer). Let Assumption 1 hold true. 5 Then, for each k \u2208 K, there is a continuous mapping g k : \u0398 \u2192\u00c3 k , such that for any \u03b8 \u2208 \u0398, if g k (\u03b8) lies in the interior of\u00c3 k , then\n\u2022 g k (\u03b8) is a local NE of \u0393 \u03b8 , 3\nHere, we mean reasoning in the sense of drawing the \"local NE conclusions\" from the game \u0393 \u03b8 , due to the principle of rationality. More generally, game-theoretic reasoning also comprises equilibrium refinement/selection (Sec. 3.2). 4 We loosely speak of a partition of A, but we do not require to cover the full A, and we allow overlaps, so it is not a partition in the rigorous set-theoretic sense. NB: The subspaces also have the interpretation as macroscopic/high-level joint action of the agents: for instance, which car goes first in the merging scenario in Fig. 1. 5 Note: (1) (Parts of) this theorem translate to general potential games, not just common-coupled games.\n\n(2) For tractability/analysis reasons we consider the simple deterministic game form of Def. 1 instead of, say, a Markov game -which we leave to future work. (3) Our framework may still be applicable if assumptions like concavity, which is quite strong, are relaxed. However, deriving guarantees may become arbitrarily hard.\n\n\u2022 g k (\u03b8) is given by the unique argmax 6 of \u03c8(\u03b8, \u00b7) on\u00c3 k , with \u03c8 the game's potential function (Lem. 1),\n\n\u2022 g k is continuously differentiable in \u03b8 with gradient\nJ \u03b8 g k (\u03b8) = \u2212 (H a \u03c8(\u03b8, a)) \u22121 J \u03b8 \u2207 a \u03c8(\u03b8, a),\nwhenever \u03c8 is twice continuously differentiable on an open set containing (\u03b8, a), for a = g k (\u03b8), where \u2207, J and H denote gradient, Jacobian and Hessian, respectively.\n\nThe specifics of how the g k of Thm. 1 form an implicit layer will be discussed in Sec. 3.2.\n\nRemark on boundaries. There remain several questions: e.g., whether the action space partition introduces \"artificial\" local NE at the boundaries of the subspaces; and also regarding what happens to the gradient if g k (\u03b8) lies at the boundary of A or\u00c3 k . Here we state a preliminary answer 7 to the latter:\n\nLemma 2. Assume Assumption 1 and that \u03c8 is twice continuously differentiable on a neighborhood of \u0398\u00d7\u00c3 k , k\u2208K. If a=g k (\u03b8) lies on exactly one constraining affine hyperplane of\u00c3 k , defined by orthogonal vector v, with multiplier \u03bb and optimum \u03bb * >0 of \u03c8(\u03b8, a)'s Lagrangian (details see proof),\nthen J \u03b8 g k (\u03b8) is the upper left nd A \u00d7nd A -submatrix of \u2212 H a \u03c8(\u03b8, a) v \u03bb * v T 0 \u22121 J \u03b8 \u2207 a \u03c8(\u03b8, a) 0 .\nRemark on identifiability. Another natural question is whether the game's parameters are identifiable from observations, and, especially, whether the g k are invertible. While difficult to answer in general, we investigate this for one scenario in Sec. C in the appendix.\n\n\nFull Architecture With Further Modules, Tractable Training and Decision Making\n\nNow for the overall problem of mapping past joint trajectories x to predictions of their future continuations y, we propose the architecture depicted in Fig. 1 alongside a training procedure. We call it trajectory game learner (TGL). (Its forward pass is explicitly sketched in Alg. 1 in Sec. B in the appendix.) It contains the following modules (here we leave some of the modules fairly abstract because details depend on size of the data set etc.; for one concrete instances see the experimental setup in Sec. 5), which are well-defined under Assumption 1:\n\n\u2022 Preference revelation net: It maps the past joint trajectory x \u2208 X to the inferred game parameters \u03b8 \u2208 \u0398 (encoding agents preferences). 8 For example, this can be an LSTM.\n\n\u2022 Equilibrium refinement net: This net maps the past joint trajectory x \u2208 X to a subsetK \u2282 K(we encodeK e.g. via a multi-hot encoding), with |K| =k, fork arbitrary but fixed. This subsetK selects a subcollection (\u00c3 k ) k\u2208K of the full equilibrium-separating action space partition (\u00c3 k ) k\u2208K (introduced in Sec. 3.1, Def. 5). This directly determines a subcollection of local NE of the game \u0393 \u03b8 , denoted by ( k a * ) k\u2208K -those local NE that lie in one of the subspaces A k , k \u2208K. 9 The purpose is to narrow down the set of all local NE to a \"refined\" set of local NE that form the \"most likely\" candidates to be selected by the agents. 10 The reason why we not directly output the refined local NE (instead of the subspaces) is to simplify training (details follow). As a simple example, take a feed forward net with softmax as final layer to get a probability distribution over K, and then take thek most probable k \u2208 K to obtain the setK.\n\n\u2022 Game solver implicit layer 11 g := (g k ) k\u2208K : It maps the revealed game parameters \u03b8 \u2208 \u0398 together with the re-finedK to the refined subcollection ( k a * ) k\u2208K of local NE 12 (described in the equilibrium refinement net above). This is done by performing, for each k \u2208K, the concave optimization over the subspace\u00c3 k :\nk a * = g k (\u03b8) = arg max a\u2208\u00c3 k \u03c8(\u03b8, a),\nbased on Thm. 1. See also Line 3 to 4 in Alg. 1 in Sec. B in the appendix.\n\n\u2022 Equilibrium weighting net: It outputs probabilities ( kq ) k\u2208K over the refined equilibria, and thus probabilities of the modes of our prediction (introduced in Sec. 2). We think of them as the probabilities of the mixture components in a mixture model, but leave the precise metrics open. As input, in principle the variables \u03b8, ( k a * ) k\u2208K are allowed, plus possibly the agents' utilities attained in the respective equi-8 In a sense, this net is the inverse of the game solver implicit layer on x, but can be more flexible. 9 To be exact, in rare cases it can happen that some of these local NE are \"artificial\" as discussed in Sec. 3.1. 10 In game theory, \"equilibrium refinement concepts\" mean hand-crafted concepts that narrow down the set of equilibria of a game (for various reasons, such as achieving \"stable\" solutions) (Osborne and Rubinstein 1994). For us, the \"locality relaxation\" makes the problem of \"too many\" equilibria particularly severe, since the number of local NE can be even bigger than global NE; it can grow exponentially in the number of agents in our scenarios. 11 NB: Here, the implicit layer does not have parameters. Generally, implicit layers with parameters can be handled similarly.\n\n12 At first sight, local NE are a poorer approximation to rationality than global NE, and are mainly motivated by tractability. However, we found that in various scenarios, like the highway merging, local NE do seem to correspond to something meaningful, like the intuitive modes of the distribution of joint trajectories. NB: Generally, we do not consider humans as fully (instrumentally) rational, but we see (instrumental) rationality as a useful approximation. librium. And one can think of various function classes, for instance a feed forward net with softmax final layer. Its purpose is to (probabilistically) learn agents' \"equilibrium selection\" mechanism considered in game theory. 13\n\n\u2022 Trajectory parameterization r: This is the predetermined parameterization from Sec. 2: it maps each local NE's joint action k a * to the corresponding joint trajectory k\u0177 that results from it, corresponding to mode k of the prediction, where k \u2208K are the indices of the refined equilibria.\n\nTraining and tractability. Training of the architecture in principle happens as usual by fitting it to past scenes in the training set, sketched in Alg. 2 in Sec. B in the appendix. The implicit layer's gradient for backpropagation is given in Thm. 1. By default, we take the mean absolute error (MAE) averaged over the prediction horizon [0, T ] (see also Sec. 5). Note that the full architecture -all modules plugged together -is not differentiable, because the equilibrium refinement net's output is discrete. However, it is easy to see that (1) the equilibrium refinement net and (2) the rest of the architecture can be trained separately and are both differentiable themselves: in training, for each sample (x, y), we directly know which subspace y lies in, so we first only train the equilibrium refinement net with this subspace's index k as target, and then train the full architecture with the equilibrium refinement net's weights fixed. 14 15\n\nObserve that in training there is an outer (weight fitting) and an inner (game solver, i.e., potential function maximizer, during forward pass) optimization loop, so their speed is crucial. For the game solver, we recommend quasi-Newton methods like L-BFGS, because this is possible due to the subspace-wise concavity of the potential function (Assumption 1). For the outer loop, we recommend recent stochastic quasi-Newton methods (Wang et al. 2017;Li and Liu 2018).\n\nTransferability to decision making. Once the game \u0393 \u03b8 's parameters \u03b8 are learned (for arbitrary numbers of agents) as described above, it does not just help for prediction -i.e., a model of how an observed set of strategic agents will behave -but also for prescription. This means (among other things) that it tells how a newly introduced agent should decide to maximize its utility, while aware of how the other agents respond to it based on their utilities in \u0393 \u03b8 (think of a self-driving car entering a scene with other -humandrivers). 16 Note: the knowledge of \u0393 \u03b8 cannot resolve the remaining equilibrium selection problem (but the equilibrium weighting net may help). For an example see Sec. 5.2. \n\n\nConcrete Example Scenarios With Analysis\n\nWe give two examples of settings alongside games and action space partitions that provably fulfill the conditions for our general approach (Sec. 3) to apply. First we consider a scenario that captures various non-trivial driver interactions like overtaking or merging at on-ramps. Essentially, it consists of a straight road section with multiple (samedirectional) lanes, where some lanes can end within the section. \ny i t := ((v i t , w i t ), (v i t\u22121 , w i t\u22121 ), (v i t\u22122 , w i t\u22122 )\n). 17 And let y i be the linear interpolation. Game: Let, for t = 0, . . . , T , the stage utilities of agent i in the game \u0393 \u03b8 be the following sum of terms for distance between agents, distance to center of lane, desired velocities, acceleration penalty, and end of lane overshooting penalty, respectively: 18\nu i,\u03b8 t (y i t ) = \u2212\u03b8 dist 1 |v j t \u2212 v j t |+\u03b6 \u2212 \u03b8 cen,i t (w i t \u2212c i t ) 2 (2a) \u2212 \u03b8 vel,i t (\u03b4v i t \u2212 \u03b8 v,i ) 2 \u2212 \u03b8 velw,i (\u03b4w i t ) 2 \u2212 \u03b8 acc,i (\u03b4 2 v i t ) 2 (2b) \u2212 \u03b8 end,i max(0, v t \u2212 e i t ),(2c)\nwhere the sum ranges over all (j, j ) such that driver j is right before j on the same lane; \u03b6 > 0 is a constant, c i t is the respective center of the lane, \u03b4v i t means velocity along lane, \u03b4w i t means lateral velocity, \u03b4 2 v i t means acceleration (vector), e i t is the end of i's lane, if it ends, otherwise \u2212\u221e; furthermore, \u00b5 is the counting measure on {0, . . . , T } (i.e., discrete). and \u03b8=(\u03b8 dist , \u03b8 cen,i\n\n[0:T ] , \u03b8 vel,i [0:T ] , \u03b8 v,i , \u03b8 velw,i , \u03b8 acc,i , \u03b8 end,i ) i\u2208I . 19 Action 17 We do this state augmentation so that utilities can also depend on velocity/acceleration (not just position) while still rigorously fitting into Def. 1. When calculating prediction errors for\u0177, only the position component is considered.\n\n18 Note that the invariance over time of the utility terms, as we assume it here, is a key element of how rationality principles can give informative priors. 19 We allow some of the weights to vary with t to add some flex-subspaces: Consider the following equivalence relation on the trajectory space Y : two joint trajectories y, y \u2208 Y are equivalent if at each time point t, (1) each agent i is on the same lane in y as in y , and (2) within each lane, the order of the agents (along the driving direction) is the same in y as in y . Now let the subspace collection (\u00c3 k ) k\u2208K be obtained by taking the (closures of the) resulting equivalence classes. 20\n\nProposition 1 (Scenario 1's suitability). Scenario 1 satisfies Assumption 1. So, in particular, Thm. 1's implications on the induced implicit layer hold true.\n\nOur general approach (Sec. 3) in principle is also applicable to various other multiagent trajectory settings, such as pedestrian interaction, relevant for mobile robots. We analyze a simplistic such scenario in Sec. C in the appendix, see Fig. 2 (right) for a foretaste.\n\n\nExperiments\n\nWe evaluate our approach on (1) an observational prediction task 21 on two real-world data sets (Sec. 5.1), as well as (2) a simple decision-making transfer task (Sec. 5.2).\n\n\nPrediction Task on Highway Merging Scenarios in Two Real-World Data Set\n\nWe consider a highway merging interaction scenario with two cars similar as sketched in Fig. 1. This is considered a challenging scenario for autonomous driving.\n\nImplementation details for our method for these merging scenario. We use the following generic implementation of our general approach (Sec. 3), with concrete setting, game and action subspaces from Scenario 1 (with n = 2), referring to it as TGL (trajectory game learner): We use validation-based early stopping. We combine equilibrium refinement and weighting net into one module, consisting of two nets that predict the weights ( kq ) k\u2208K on the combination of (1) merging order (before/after) probabilities via a cross-entropy loss (2 hidden layers: 1 \u00d7 16, 1 \u00d7 4 neurons; dropout 0.6), and (2) Gaussian distribution over merging time point (discretized and truncated, thus the support inducing a refinement; 2 hidden layers: 1\u00d764, 1\u00d732 neurons; dropout 0.6), given x. For the preference revelation net we use a feed forward net (two hidden layers: 1\u00d716, 1\u00d724 neurons). 22 As training loss we use mean absolute error (MAE; see also evaluation details below).\n\nibility. In the experiments (Sec. 5), we use \"terminal\" costs only; more specifically \u03b8 vel,i t = 0 for 0 \u2264 t \u2264 T \u2212 6 and \u03b8 cen,i t = 0 for 0 \u2264 t \u2264 T \u2212 1, which we found works best. 20 In the two-driver on-ramp scenario of Fig. 1 and experiments (Sec. 5.1), these subspaces roughly amount to splitting the action space A w.r.t. (1) time point of merge and (2) which driver goes first. Note that (1) are additional splits beyond the intuitive ones in (2) (see Fig. 1), but they help for concavity and for the analysis. 21 This directly evaluates the method's abilities for the observational/passive prediction task, but it is also a proxy metric/task for decision making. 22 For varying initial trajectory lengths, an LSTM might be more suitable.  (Tang and Salakhutdinov 2019)) for a prediction task on merging scenarios in two real-world highway data sets, averaged over a 7s prediction horizon. Figure 3: Decision-making transfer task: Solution trajectorie(s) that the (partially learned) game implies for the self-driving car's decision-making task (each circle/square corresponds to one time step). Left: First local NE: the self-driving car (red) does a full emergency break and the other (blue) merges before it. Right: Second local NE: the other merges after it, both slow down.\n\nBesides this generic instantiation, we also consider a version of it, termed TGL-D: Instead of predicting the desired velocity \u03b8 v,i itself, the preference revelation net predicts the difference to the past velocity, and then squashes this into a sensible range using a sigmoid. (This can be seen as encoding a bit of additional prior knowledge which may not always be easy to specify and depend on the situation.) For further details, see Sec. D in the appendix. Code is available at: https://github.com/boschresearch/ trajectory_games_learning.\n\nBaselines. As baselines we use the state-of-the-art datadriven methods \"convolutional social pooling\" -specifically: CS-LSTM (Deo and Trivedi 2018) -and \"Multiple Futures Prediction\" (MFP) (Tang and Salakhutdinov 2019). Evaluation. We use four-fold cross validation (splitting the data into 4 \u00d7 75% train and 25% validation). As metrics, we use rooted mean squared error (RMSE) and MAE (in meters) between predicted future trajectory\u0177 and truth y, averaged over a 7s horizon, with prediction step size of 0.2s, applying this to the most likely mode given by our method.\n\nData sets (one new one) and filtering: 1st data set: We use the \"highD\" data set , which consists of car trajectories recorded by drones over several highway sections. It is increasingly used for benchmarking (Rudenko et al. 2019;Zhang et al. 2020). From this data set, we use the recordings done over a section with an on-ramp.\n\n2nd data set: We publish a new data set with this paper, termed HEE (Highway Eagle Eye). It consists of \u223c12000 individual car trajectories (\u223c4h), recorded by drones over a highway section (length \u223c600m) with an entry lane. The link to the data set and further details are in Sec. D.2 in the appendix. Keep in mind that this data set can be useful for studies like ours, but some aspects of it may be noisy, so it is only meant for such experimental purposes.\n\nSelection of merging scenes in both data sets: We filter for all joint trajectories of two cars where one is merging from the on-ramp, one is on the rightmost highway lane, and all other cars are far enough to not interact with these two. This leaves 25 trajectories of highD and 23 of our new data set.\n\nResults. The results are in Table 1 (with more details in Sec. D in the appendix). Our generic method TGL outperforms CS-LSTM and MFP on highD. And our slightly more hand-crafted method TGL-D outperforms them on both data set. Keep in mind that the data sets are small. So, while the results do indicate the practicality of our method in this small-sample regime, their significance is comparably limited.\n\n\nSimple Decision-Making Transfer Task in Simulation\n\nAs discussed in Sec. 3.2 the game \u0393 \u03b8 -once \u03b8 is given, e.g., by our learned preference revelation net -naturally transfers to decision-making tasks in situations with multiple strategic agents (something which predictive methods like the above CS-LSTM usually cannot do). To test and illustrate its ability for this, we consider a simple scenario: Take the above two-car highway on-ramp situation (Sec. 5.1, Scenario 1), but assume that the car on the highway lane is a self-driving car. Assume it has a technical failure roughly at the height of the on-ramp's end, and it should do an emergency break (i.e., desired velocity \u03b8 v,i in (2) is set to 0) while at the same time ensuring that the other car coming from the on-ramp will not crash into it. Which trajectory should it choose? Result. Fed with this situation, our game solver suggests two possible solutions -two local NE, see Fig. 3: (1) the selfdriving car completely stops and the on-ramp car will merge in front of it, accepting to touch the on-ramp's end;\n\n(2) the self-driving car moves slowly, but at a non-zero speed, with the other car right behind it (keeping a rational distance). While a toy scenario, we feel that these are sensible solutions. Note that, Additionally, this shows that we (similar to IRL) have a strong ability to reasonably generalize out of sample, since a fully braking vehicle is actually not in the data.\n\nFor modeling of realistic continuous multiagent trajectories, in this work we proposed an end-to-end trainable model class that hybridizes neural nets with game-theoretic reasoning. We accompanied it with theoretical guarantees as well as an empirical demonstration of its practicality, on realworld highway data. We consider this as one step towards machine learning methods for this task that are more interpretable, verifiable and transferable to decision making. This is particularly relevant for safety-critical domains that involve interaction with humans. A major challenge is to make game-theoretic concepts tractable for such settings, and we were only partially able to address this. Specifically, potential for future work lies in relaxing subspace-wise concavity, common-coupled games and related assumptions we made. A Proofs and remarks A.1 Lemma 1\n\nLet us first restate the result.\n\nLemma 1. If \u0393 \u03b8 is a common-coupled game, then it is a potential game with the following potential function, where, as usual, y = r(a):\n\u03c8(a, \u03b8) = T 0 u com,\u03b8 t (y t ) + i\u2208I u own,i,\u03b8 t (y i t )d\u00b5(t).\nProof of Lemma 1. Recall that based on the definition of a common-coupled game we have have for the stage-wise utility for all i and t,\nu i,\u03b8 t (y t ) = u com,\u03b8 t (y t ) + u own,i,\u03b8 t (y i t ) + u oth,i,\u03b8 t (y \u2212i t ).(3)\nIntuitively, the statement directly follows because taking the \"derivative\"of i's utility above w.r.t. y i will just leave the derivative of the common term and i's own term, because the other is constant in y i . And the same happens for \u03c8.\n\nFormally, observe that for all i and t,\nu i,\u03b8 t ((y i t , y \u2212i t )) \u2212 u i,\u03b8 t ((y i t , y \u2212i t )) = u com,\u03b8 t ((y i t , y \u2212i t )) \u2212 u com,\u03b8 t ((y i t , y \u2212i t )) + u own,i,\u03b8 t (y i t ) \u2212 u own,i,\u03b8 t (y i t ) + 0 = u com,\u03b8 t ((y i t , y \u2212i t )) \u2212 u com,\u03b8 t ((y i t , y \u2212i t )) + u own,i,\u03b8 t (y i t ) \u2212 u own,i,\u03b8 t (y i t ) + j\u2208I\\i u own,j,\u03b8 t (y j t ) \u2212 j\u2208I\\i u own,j,\u03b8 t (y j t ).\nIntegrating w.r.t. t and using the linearity of the integral completes the proof.\n\n\nA.2 Theorem 1\n\nLet us first restate the result.\n\nTheorem 1 (Game-induced differentiable implicit layer). Let Assumption 1 hold true. 23 Then, for each k \u2208 K, there is a continuous mapping g k : \u0398 \u2192\u00c3 k , such that for any \u03b8 \u2208 \u0398, if g k (\u03b8) lies in the interior of\u00c3 k , then\n\u2022 g k (\u03b8) is a local NE of \u0393 \u03b8 ,\n\u2022 g k (\u03b8) is given by the unique argmax 24 of \u03c8(\u03b8, \u00b7) on\u00c3 k , with \u03c8 the game's potential function (Lem. 1),\n\n\u2022 g k is continuously differentiable in \u03b8 with gradient\nJ \u03b8 g k (\u03b8) = \u2212 (H a \u03c8(\u03b8, a)) \u22121 J \u03b8 \u2207 a \u03c8(\u03b8, a),\nwhenever \u03c8 is twice continuously differentiable on an open set containing (\u03b8, a), for a = g k (\u03b8), where \u2207, J and H denote gradient, Jacobian and Hessian, respectively.\n\nProof of Theorem 1.\n\nGradient etc. Based on Lemma 1, the potential function \u03c8 exists. Let k \u2208 K be arbitrary but fixed. Let g k be the function that maps each \u03b8 to the corresponding unique maximum of \u03c8(\u03b8, \u00b7) on\u00c3 k (exists and is unique by the assumption of strict concavity and convexity and compactness of the\u00c3 k ). From the definition of the potential function and the local Nash equilibrium (NE), it follows directly that a maximum of the potential function is a local NE of the game.\n\nTo apply the implicit function theorem, let us consider the point (\u03b8, g k (\u03b8)). If the minimum g k (\u03b8) lies in the interior of A k , and \u03c8 is continuously differentiable on an open set containing (\u03b8, g k (\u03b8)), then we have \u2207 a \u03c8(\u03b8, a)| (\u03b8,a)=(\u03b8,g k (\u03b8)) = 0 and furthermore, by assumption of strict concavity, J a \u2207 a \u03c8(\u03b8, a)| (\u03b8,a)=(\u03b8,g k (\u03b8)) non-singular. Then the implicit function theorem implies that there is a an open set O containing (\u03b8, g k (\u03b8)), and a unique continuously differentiable function f : O \u2192\u00c3 k , such that \u2207 a \u03c8(\u03b8 , f (\u03b8 )) = 0 for \u03b8 \u2208 O, with gradient\nJ \u03b8 f (\u03b8) = \u2212 (J a \u2207 a \u03c8(\u03b8, a)) \u22121 J \u03b8 \u2207 a \u03c8(\u03b8, a) = \u2212 (H a \u03c8(\u03b8, a)) \u22121 J \u03b8 \u2207 a \u03c8(\u03b8, a).(4)\nNow on O, f and g k coincide since f is uniquely determined (specifically, based on the implicit function theorem, locally, the graph of f coincides with the solution set of \u2207 a \u03c8(\u00b7, \u00b7) = 0, and if f, g k would differ in at least one point \u03b8 , then there would be a solution (\u03b8 , g k (\u03b8 )) outside the solution set -a contradiction). Therefore g k is also continuously differentiable on O with gradient\nJ \u03b8 g k (\u03b8) = J \u03b8 f (\u03b8).(5)\nWe can do this for every \u03b8, which completes the proof.\n\nContinuity. Since \u03c8 is continuous,\u00c3 k is compact, and the maxima are unique, the maximum theorem implies that the mapping g k is in fact continuous (hemicontinuity reduces to continuity when the correspondence is in fact a function).\n\n\nA.3 Lemma 2 with remark on zero gradient\n\nLet us first restate the result. Lemma 2. Let Assumption 1 hold true and additionally assume \u03c8 to be continuously differentiable on (a neighborhood of) \u0398 \u00d7\u00c3 k , k \u2208 K. If a = g k (\u03b8) lies on (exactly) one constraining affine hyperplane of\u00c3 k , defined by orthogonal vector v, with multiplier \u03bb and optimum \u03bb * > 0 of \u03c8(\u03b8, a)'s Lagrangian (details in the proof), then\nJ \u03b8 g k (\u03b8) = \u2212 H a \u03c8(\u03b8, a) v \u03bb * v T 0 \u22121 J \u03b8 \u2207 a \u03c8(\u03b8, a) 0 1:(n\u00b7d A )\u00d71:(n\u00b7d A )\n.\n\nRemark on zero gradient. Note that under the conditions of this lemma, i.e., when g k (\u03b8) lies at the boundary, then the above gradient J \u03b8 g k (\u03b8) in fact often becomes zero, which can be a problem for paramter fitting. So the above result is only meant as a first step. Then the Karush-Kuhn-Tucker optimality conditions (Boyd and Vandenberghe 2004) (note that we assumed differentiability of \u03c8 on a a neighborhood of \u0398 \u00d7\u00c3 k , k \u2208 K) include the following equations:\n\u2207 a \u039b(\u03b8, a, \u03bb 1 , . . . , \u03bb M ) = 0, (6a) \u03bb m (v T m a \u2212 b) = 0, m = 1, . . . , M. (6b)\nNow let a * = g k (\u03b8) and let \u03bb * 1 , . . . , \u03bb * M be the (optimal) duals for a * . And assume that a * lies on exactly one bounding (affine) hyperplane. W.l.o.g. let this hyperplane correspond to v 1 , \u03bb * 1 . Also recall that g k is continuous (as in Theorem 1). Therefore, in a neighborhood of \u03b8, the corresponding optimum will not lie within any of the other boundaries. So in this neighborhood of \u03b8, all corresponding optimal duals will be zero (inactive) except for the assumed one. Therefore, given a \u03b8 from the mentioned neighborhood of \u03b8, we have that a, v 1 satisfy the optimality conditions in (6) (for some remaining duals) iff they satisfy the reduced conditions \u2207 a \u039b(\u03b8 , a, \u03bb 1 , \u03bb, 0, . . . , 0) = 0,\n\u03bb 1 (v T 1 a \u2212 b) = 0.(7a)\nFor succinctness, in what follows we write \u03bb instead of \u03bb 1 , i.e., drop the subscript. Let\nh(\u03b8 , a, \u03bb) := (\u2207 a \u039b(\u03b8 , a, \u03bb, 0, . . . , 0), \u03bb(v T a \u2212 b)) = (\u2207 a \u03c8(\u03b8 , a) + \u2207 a \u03bb(v T a \u2212 b), \u03bb(v T a \u2212 b)) = (\u2207 a \u03c8(\u03b8 , a) + \u03bbv T , \u03bb(v T a \u2212 b))\nSo the conditions in (7) are h(\u03b8 , a, \u03bb) = 0.\n\nSimilar as in Theorem 1, around the point \u03b8, a * , \u03bb * , g k satisfies (8) (for some \u03bb's). So we can apply the implicit function theorem to get its gradient.\n\nWe have\nJ (\u03b8,a,\u03bb) h(\u03b8, a, \u03bb) = J \u03b8 \u03c8(\u03b8, a) H a \u03c8(\u03b8, a) v T 0 \u03bbv T v T a \u2212 b .\nNote that\nH a \u03c8(\u03b8, a * ) v T \u03bb * v T v T a * \u2212 b = H a \u03c8(\u03b8, a * ) v T \u03bb * v T 0 is invertible, since det H a \u03c8(\u03b8, a * ) v T \u03bb * v T 0 = det(H a \u03c8(\u03b8, a * )) det(\u2212\u03bb * v T (H a \u03c8(\u03b8, a * )) \u22121 v)\nand both factors are non-zero since H a \u03c8(\u03b8, a * ) is positive definite and \u03bb * , v are non-zero. Therefore, the implicit function theorem is applicable to the equation, and we get as gradient\nJ \u03b8 g k (\u03b8) = \u2212 H a \u03c8(\u03b8, a * ) v T \u03bb * v T 0 \u22121 J \u03b8 \u03c8(\u03b8, a * ) 0\n\nA.4 Proposition 1\n\nLet us first restate the result.\n\nProposition 1 (Scenario 1's suitability). Scenario 1 satisfies Assumption 1. So, in particular, Thm. 1's implications on the induced implicit layer hold true.\n\nProof of Proposition 1.\n\nCommon-coupled game. It is directly clear from the form of the utilities in Scenario 1, that this forms a common-coupled game.\n\nStrict concavity of the potential function. Observe that, for each i, within any one subspace\u00c3 k , for each t,\n\n\u2022 i does not changes lane, so c t , e t are simple constants.\n\n\u2022 For each lane, there is a fixed set of agents. Consider the set S of pairs (j, j ) of agents that are on this lane and j is right before j . This ordering (and thus S) is invariant within\u00c3 k . Therefore the agent distance term can be rewritten like\n\u03b8 dist j right before j on same lane 1 |v j t \u2212 v j t | + \u03b6 (9) = \u03b8 dist (j,j )\u2208S 1 v j t \u2212 v j t + \u03b6(10)\nSo all terms are concave (the other terms are obviously concave), therefore the overall potential function, which is just a sum of them, is concave.\n\nFuthermore, note that the sum of all velocity and distance to lane center terms is a sum of functions such that for each component of the vector a there is exactly one function of it, and only of it; and each function is strictly concave. This implies that the overall sum is strictly concave in the whole a. So the potential function is a sum of concave and a strictly concave term, meaning it is strictly concave.\n\nNB: On the subspaces, the potential function is also differentiable.\n\nAlgorithm 1: TGL -forward pass (sketch) Input: past joint trajectory x Output: modes k\u0177 and their probabilities kq , k \u2208K, as prediction for future joint trajectory y 1 \u03b8 = preference revelation net (x) 25 ; 2K = equilibrium refinement net (x); // Implicit layer g: 3 foreach k \u2208K do // Solve for local NE k a * in\u00c3 k : 4 k a * = g k (\u03b8) = arg max a \u03c8(\u03b8, a) s.t. a \u2208\u00c3 k ; 5 ( k\u0177 ) k\u2208K = trajectory parametrization r (( k a * ) k\u2208K ) ; 6 ( kq ) k\u2208K = eq. weighting net (\u03b8, ( k a * ) k\u2208K ), . . .);\n\nAlgorithm 2: TGL -training (sketch, in particular how to separate the equilibrium refinement net) Input: set S of training samples (x , y ), (x , y ), . . . // Train equilibrium refinement net: 1 Train the weights w er of the equilibrium refinement net on pairs (x, k), where k \u2208 K is the index of the subspace the ground truth y lies in, for (x, y) \u2208 S ; // Train full architecture but with the equilibrium refinement net's weights fixed: 2 Perform gradient-based training of the full architecture by training all its weights except for the equilibrium refinement net's weights w er which are fixed to the one obtained by the training above. The forward pass is given by Alg. 1. To get gradients via back-propagation, for the implicit layer's gradient use the formula given by Thm. 1 ;\n\nCompactness and linearity of constraints. Besides the constraints that define the compact complete action space A, which are obviously linear, the constraints that define the action subspaces\u00c3 k are given by the intersection of constraints for each time point t that are all of the form\n\u2022 w i t \u2265 const. or w i t \u2264 const., or \u2022 v i t \u2264 v j t ,\nso they are linear.\n\n\nB Further details on general architecture\n\nElaborating on Sec. 3.2: Alg. 1 describes TGL's forward pass, in particular the parallel local NE search over the refined subspaces. Alg. 2 describes the training more formally, in particular the splitting into first training the equilibrium refinement net alone.\n\nC Further example scenario: simple pedestrian encounter As indicated in the main text, let us now elaborate the second -pedestrian scenario -our general method applies to. When considering settings with characteristics such as continuous time, even if they still satisfy the conditions of our framework, to prove so can become arbitrarily complex. Here let us give a simplistic but verified second example with properties somewhat different from the first one: Consider two pedestrians who walk with constant velocity (this velocity is their respective action) along straight paths which are orthogonal and intersect such that they could bump into each other (Fig. 4). Formally:\n\nScenario 2 (Simple pedestrian encounter). Setting: There are n = 2 agents, the actions parameterize the trajectories via y 1 t = (0, ta 1 + z 1 ), y 2 t = (ta 2 + z 2 , 0) (for ease of notation, we put the intersection to the origin but translations are possible of course), the joint action space is A = [ z 1 T , c] \u00d7 [ z 2 T , c], for constants z 1 , z 2 < 0 and c > 0, (the lower bound on the action is to make sure they reach the intersection). Game: Let the final stage utility be given by the following sum of a distance penalty term and a desired velocity term\nu i,\u03b8 T (y i T ) = \u2212h(\u03b8 dist , a) max t\u2208[0,T ] 1 y 1 t \u2212 y 2 t 1 (11a) \u2212 \u03b8 vel,i (a i \u2212 \u03b8 v,i ) 2 (= \u2212\u221e if division by 0) ,(11b)\nfor some function h, \u03b8 vel,i > 0, and let \u00b5 be the Dirac measure on T . Action subspaces: Let the subspaces (\u00c3 k ) k\u2208K be given by (1) taking the two subspaces that satisfy a k \u2265 a 2\u2212k + \u03b5, for k = 1, 2 respectively, and some small \u03b5 > 0, i.e., split by which agent is faster, and (2) additionally split by which agent first reaches their paths' intersection (altogether this yields two or three subspaces).\n\nProposition 2 (Scenario 2's suitability and partial identifiability). Assume Scenario 2 with h(\u03b8 dist , a) = 1 a k , with k the faster agent in a. Then Assumption 1 is satisfied. Furthermore, while the complete game parameter vector \u03b8 = (\u03b8 vel,1 , \u03b8 v,1 , \u03b8 vel,2 , \u03b8 v,2 , ) is not identifiable in general, if \u03b8 vel,i , i = 1, 2 is constant, then (\u03b8 v,1 , \u03b8 v,2 ) is identifiable from y on the pre-image of the interior of\u00c3 k , for any k.\n\n\nC.1 Proof of Proposition 2\n\nProof of Proposition 2.\n\nRemark on utility form. Note that in Scenario 2 the stage-utility at time T in its \"distance term\" uses a form of maximum operator that takes the whole trajectories as input. But according to our Definition 1, actually we allow the stage utility to only depend on respective current state. But this can be resolved by observing that the \"distance term\" can in fact be rewritten as a function of (a 1 , a 2 ), as we will see in Equation (17) below. And (a 1 , a 2 ) in turn is given by (y T 1 \u2212 z 1 , y T 2 \u2212 z 2 )/T , i.e., determined by y T .\n\nCommon-coupled game. It is directly clear from the form of the utilities in Scenario 2, that this forms a common-coupled game.\n\nStrict concavity of the potential function. Consider all subspaces\u00c3 k where agent 1 is faster, i.e., a 1 > a 2 . Note that this can be one or two subspaces: if y 0 1 > y 0 2 , then it is one subspace, but otherwise it is two (namely, where 1 or 2 reaches the intersection first, respectively).\n\nOn any one of these subspaces, the following holds true: Regarding the distance-based term, observe that arg min t\u2208[0,T ] |t \u00b7 a 1 + z 1 | + |t \u00b7 a 2 + z 2 | = \u2212 z 1 a 1 .\n\nTo see this, observe that the argmin is given by t where agent 1 reaches the intersection, i.e, t where t \u00b7 a 1 + z 1 = 0, i.e., t = \u2212 z 1 a 1 . (And this holds regardless of whether 1 or 2 first reaches the intersection.) To see this in turn, first consider the case that agent 1 first reaches the intersection. Then, before this t, both terms are bigger (both agents are further away from the origin), while at this t, the first term is 0, and after it the left term grows faster (because the agent is faster) than the right term decreases (until the right term hits 0 as well and then also increases again).\n\nSecond, consider the case where agent 2 first reaches the intersection (in case this happens at all -i.e., if 2 starts so much closer to the origin to make this possible). Then, before 2 reaches the intersection: obviously both terms are bigger than when Therefore,\nu com,\u03b8 T (y [0,T ] ) = \u2212 1 a 1 max t\u2208[0,T ] 1 (0, t \u00b7 a 1 + z 1 ) \u2212 (t \u00b7 a 2 + z 2 , 0) 1 (13) = \u2212 1 a 1 max t\u2208[0,T ] 1 |t \u00b7 a 1 + z 1 | + |t \u00b7 a 2 + z 2 | (14) = \u2212 1 a 1 1 | \u2212 z 1 a 1 \u00b7 a 1 + z 1 | + | \u2212 z 1 a 1 \u00b7 a 2 + z 2 | (15) = \u2212 1 a 1 1 | \u2212 z 1 a 1 \u00b7 a 2 + z 2 | (16) = \u2212 1 |z 2 a 1 \u2212 z 1 a 2 |(17)\nKeep in mind that for agent i, the time when it reaches the intersection is given by t i = \u2212 z i a i . Now, if, for the subspace under consideration, agent 1 first reaches the intersection, i.e., \u2212 z 1 a 1 > \u2212 z 2 a 2 , i.e., z 1 a 1 < z 2 a 2 , i.e., z 1 a 2 < z 2 a 1 . Then (17) becomes \u2212 1 z 2 a 1 \u2212z 1 a 2 , which is obviously concave. Similarly for the case that agent 2 first reaches the intersection. Regarding the velocity terms, obviously their sum is strictly concave. So the sum of all terms is strictly concave.\n\nLinearity and compactness of constraints. The constraints are all of the form a 1 \u2265 a 2 + \u03b5 or z 1 a 2 < z 2 a 1 , i.e., linear.\n\nIdentifiability. Obviously the full \u03b8 cannot be identifiable because there are no (local) diffeomorphisms between spaces of differing dimension.\n\nKeep in mind that the parametrization from a to y is injective, so we just need to show identifiability from a. That is, we have to show that g k is invertible on g \u22121 k (int( k\u00c3 k )), where int(\u00b7) denotes the interior. Since we fixed \u03b8 vel,i , i = 1, 2, consider them as constants, and for what follows, for simplicity let \u03b8 stand for (\u03b8 v,1 , \u03b8 v,2 ). W.l.o.g. (the other cases work similarly) assume\u00c3 k is the subspace where agent 1 is faster and first reaches the intersection, so the potential function becomes \u03c8(\u03b8, a) = 1 z 2 a 1 \u2212 z 1 a 2 \u2212 \u03b8 vel,1 (a 1 \u2212 \u03b8 v,1 ) 2 \u2212 \u03b8 vel,2 (a 2 \u2212 \u03b8 v,2 ) 2 .\n\nNow let a be such that a = g k (\u03b8) for some \u03b8. We have to show that there can only be one such \u03b8. To see this, note that a is a local NE, and thus\n0 = \u2207 a \u03c8(\u03b8, a) = (\u22121) i\u22121 z 3\u2212i (z 2 a 1 \u2212 z 1 a 2 ) 2 + 2\u03b8 vel,i a i \u2212 2\u03b8 vel,i \u03b8 v,i i=1,2 .(19)\nBut this implies\n\u03b8 v,i = (\u22121) i\u22121 z 3\u2212i (z 2 a 1 \u2212z 1 a 2 ) 2 \u2212 2\u03b8 vel,i a i 2\u03b8 vel,i .(20)\n\nD Further details on experiments and implementation\n\nKeep in mind that in the main text (Sec. 5), we already described two implementations/versions we propose as instantiations of our general method (Sec. 3.2) for the highway merging setting: \u2022 TGL, \u2022 TGL-D.\n\nHere we present one further method: \u2022 TGL-DP (also \"TGL-P\" for short) -building on TGL-D, we use the interpretable representation of the desired velocity parameter predicted by the preference revelation net, which we first validate, and then encode additional prior-knowledge based constraints (e.g., we clip maximum and minimum desired speed) -see Appendix D.1 for details. Additionally, in this section we provide further details on the new data set (Appendix D.2) as well as more fine-grained empirical evaluations metrics (individual prediction time points) for all methods. Note that the recorded section is in fact slightly more to the right than the picture indicates. Figure 6: This is a zoom-in on roughly the sub-part of the highway section in Fig. 5 that is relevant for the on-ramp merging trajectories used in the experiment.\n\nAlgorithm 3: Part of preference revelation net of TGL-P that outputs the desired velocity game parameters \u03b8 v,2 , \u03b8 v,1 and differs compared to TGL Input: old vx other,old vx merger // velocities along x-axis at the last step of the past trajectory x, where ''merger'' is the on-ramp car, and ''other'' is the highway car 1 desired vx other,desired vx merger // output \u03b8 v,2 , \u03b8 v,1 of TGL's original preference revelation net 2 merger in front // most probable subspace given by equilibrium refinement/weighting net, whether merger merges before or after other 3 big change // parameter --factor for allowed big change, for the experiment we used \n\n\nD.1 More detailed description of TGL-DP\n\nNB: on what follows, we refer to the method TGL-DP also simply by TGL-P:\n\nLet us give further details on our method TGL-P, that was only briefly introduced in Appendix D. This method is the same as TGL-D described in Sec. 5.1 (building on Scenario 1), except that we modified the preference revelation net based on plausible reasoning and using parts of the equilibrium refinement/weighting net. Note that this is still a special case of our general architecture (Sec. 3.2) in the rigorous sense, just with a preference revelation net that incorporates a fair amount of additional reasoning and shares some structure with the equilibrium refinement net.\n\nThere are two reasons why we introduce TGL-P: First, the preference revelation net has to be trained together with the local game solver implicit layer. This means that training takes comparably long (we have an outer and an inner optimization loop, as described in Sec. 3.2). And one particular problem we experienced and have not fully solved yet, is that often the preference revelation net already starts overfitting while the game parameters that are learned \"globally\", i.e., not inferred from the past trajectory by the preference revelation net, are not properly learned yet. Now TGL-P allows to demonstrate what we believe TGL itself can also achieve once the mentioned problems are overcome; in particular, it shows that the game model class (\u0393 \u03b8 ) has a substantial capacity to resemble the future trajecories. Second, TGL-P shows how the intermediate representation \u03b8 can be inspected and high level knowledge/reasoning about agents' preferences/utilities and behavior can be incoporated.\n\nSpecifically, TGL-P can be described as being the same as TGL, except that the part of preference revelation net of TGL that outputs \u03b8 v,2 , \u03b8 v,1 is replaced by Alg. 3 (on page 16). The function clamp(\u00b7) is, as usual, defined by clamp(z, m, M ) = max(m, min(z, M )),\n\ni.e., clipping values to m / M if they are below / above. While the details of the algorithm may look complex, the main idea is simple: we clip the outputs of the preference revelation net if they are too far off compared to what one would expect given initial velocities and output of the equilibrium refinement/weighting net. The algorithm has two parameters big change and small change, for which we used the values 1.2 and 1.04, respectively, in the experiment. Note that, while in general, the equilibrium refinement/weighting net and the preference revelation net serve separate purposes, the reason why in TGL-P we share structure between them is mainly a pragmatic one: the equilibrium refinement/weighting net can be trained separately from the implicit layer and thus much faster -and we made the experience that it learns a reliable signal to predict the future (subspaces / selected equilibria). : Example joint trajectories on highD data set (top) and our new highway data set (bottom). The ground truth future y is green and yellow for on-ramp and highway car, respectively, with the past trajectory segment x (at the very beginning) depicted by 'x' markers. The prediction\u0177 (most likely mode) of our method TGL-P is in blue and red, respectively. Note that xand y-axis are in meters; in particular, the x-axis is significantly squeezed.\n\n\nData set\n\nHorizon TGL (ours) TGL-D (ours) TGL-DP (ours) CS-LSTM MFP highD  1s 0.5 0.5 0.5   other phenomena may play an important role. Nonetheless, we believe that instrumental rationality is reasonably good approximation in many settings. And of course, game-theoretic rationality has the advantage that there is an elegant theory for it. \u2022 In fact we use the local Nash equilibrium, which can also be seen as a bounded form of rationality. However, we feel that it can often be a better, more advanced approximation (to reality and/or rationality) than other concepts like level-k game theory (Stahl and Wilson 1995). In particular, we found it interesting that the local Nash equilibria can in fact correspond to intuitive modes of the joint trajectories, like which car goes first in Fig. 1. A reason for this may be that, while there may be one perfect solution (say a global Nash), due to errors and stochasticity of the environment the agents may be perturbed towards ending up at a state where a previously local Nash equilibrium may in fact now be a global Nash equilibrium. \u2022 The problem of equilibrium selection mentioned in the main text may be seen as a form of incompleteness of game theoryit cannot always make a unique prediction (or prescription). Our equilibrium refinement and weighting nets can be seen as a data-driven approach to fill this incompleteness. An interesting question in this context is whether the missing information is actually contained in the preferences of the agents, or if there is additional (hidden) information required to find the unique solution/prediction. \u2022 In some situations, it may be that knowledge of an (local-)equilibrium-separating partition may in fact tractably tell us the global Nash equilibrium (because if we know all local optima then we can also know the global optimum of a function). But this does not always seem to be the case for general common-coupled/potential games and, in particular, also because we do not require the equilibrium-separation to cover all possible equilibria.\n\nFigure 2 :\n2Left: Simple illustration of Scenario 1's variables. Right: Illustration of simplistic pedestrian encounter scenario (Sec. C in the appendix).\n\n\nFig. 1 and 2 (left) are examples. This setting will be used in the experiments (Sec. 5). Scenario 1 (Multi-lane driver interaction). Setting: The set of possible individual states, denote it by Y 0 , is of the form [b, c] \u00d7 [d, e] -positions on a road section. There are m parallel lanes (some of which may end), parallel to the x-axis. Agent i's action a i \u2208 A i is given by the sequence of planar (i.e., 2-D) positions denoted (v i t , w i t ) \u2208 Y 0 , t = 0, . . . , T , but not allowing backward moves (and possibly other constraints). Define the states\n\nProof of Lemma 2 .\n2Let\u00c3 k be defined by the inequality constraints v T m a \u2264 b, m = 1, . . . , M . Consider the Lagrangian \u039b(\u03b8, a, \u03bb 1 , . . . , \u03bb M ) = \u03c8(\u03b8, a) + m \u03bb m (v T m a \u2212 b).\n\nFigure 4 :\n4Simplistic pedestrian encounter.\n\nFigure 5 :\n5For the new highway data set , this is roughly the recorded highway section (only the lower lane, incl. exit/entry).\n\nFigure 7\n7Figure 7: Example joint trajectories on highD data set (top) and our new highway data set (bottom). The ground truth future y is green and yellow for on-ramp and highway car, respectively, with the past trajectory segment x (at the very beginning) depicted by 'x' markers. The prediction\u0177 (most likely mode) of our method TGL-P is in blue and red, respectively. Note that xand y-axis are in meters; in particular, the x-axis is significantly squeezed.\n\n\n.07303v7 [cs.GT] 18 Feb 2022past joint \ntrajectory \n\nx \n\ngame \nparameters \n\n\u03b8 \n\nrefined \nlocal NEs \n\n( k a  *  ) k\u2208K \n\npredicted joint \ntrajectories' \nmodes k\u0177 and \nprobabilites \n\nkq , k \u2208K \n\nrefined \nsubspace \nindex set \n\nK \n\ngame solver \nimplicit \n\nlayer g \n\npreference \nrevelation \nnet \n\nequilibrium \nrefinement \nnet \n\ntrajectory \nparametriza-\n\ntion r \n\nequilibrium \nweighting \nnet \n\n\n\n\nbig change = 1.2 4 small change // parameter --factor for allowed small change, for the experiment we used small change = 1.04 Output: new vx other,new vx merger // new desired velocity game parameters \u03b8 v,2 , \u03b8 v,1 5 if merger in front == 0 then // if the highway vehicle is in front // clamp change of highway vehicle desired speed new vx other = clamp(desired vx other, old vx other / small change, old vx other*small change) // only allow accelerating merger vehicle, but limit maximum speed change new vx merger = clamp(desired vx merger, old vx merger, old vx merger*big change) 8 else // if the merger vehicle is in front // clamp desired merger velocity to make it coherent with driving in front of other new vx merger = clamp(desired vx merger, min( old vx merger*big change, old vx other/big change), old vx merger*big change) // also clamp the desired speed of the other, distinguishing between two cases: 10 if new vx merger > old vx other then 11 new vx other = clamp(desired vx other, old vx other / small change, old vx other*small change) new vx other = clamp(desired vx other, old vx other / big change, old vx other)6 \n\n7 \n\n9 \n\n12 \n\nelse \n\n13 \n\n\n\nTable 2 :\n2Mean absolute error (MAE) for all prediction horizons for all methods. Data set Horizon TGL (ours) TGL-D (ours) TGL-DP (ours) CS-LSTM MFPhighD (Krajewski et al. 2018) \n1s \n0.6 \n0.5 \n0.5 \n1.8 \n1.6 \n2s \n1.4 \n1.2 \n1.2 \n3.2 \n3.0 \n3s \n2.7 \n2.1 \n2.0 \n5.0 \n4.8 \n4s \n4.3 \n3.2 \n3.0 \n6.3 \n6.8 \n5s \n6.1 \n4.5 \n4.2 \n7.7 \n9.0 \n6s \n8.4 \n6.1 \n5.7 \n9.1 \n11.2 \n7s \n11.0 \n8.0 \n7.6 \n14.5 \n13.6 \n\nNew data set (Sec. 5.1) \n1s \n0.8 \n0.7 \n0.8 \n0.9 \n1.0 \n2s \n1.8 \n1.7 \n1.7 \n2.0 \n1.9 \n3s \n3.1 \n2.8 \n2.8 \n2.8 \n3.1 \n4s \n4.4 \n3.9 \n3.9 \n4.0 \n4.5 \n5s \n5.9 \n5.2 \n5.1 \n5.1 \n6.0 \n6s \n7.6 \n6.6 \n6.4 \n6.5 \n7.6 \n7s \n9.4 \n8.1 \n7.7 \n8.7 \n9.3 \n\n\n\nTable 3 :\n3Root mean square error (RMSE) for all prediction horizons for all methods.\nThis general integral-based formulation contains discrete-time Scenario 1 as special case with \u00b5's mass on discrete time points. 2 I.e., no agent can improve its utility by unilaterally and locally deviating from its action in a local NE -a \"consistency\" condition.\nDue to the concavity assumption, we can use established tractable, guaranteeably sound algorithms to calculate this argmax. 7 Note that similar results have already been established, in the sense of constrained optimizers as implicit layers (Amos and Kolter 2017), but we give the precise preconditions for our setting. See also Sec. E in the appendix. Moreover, note that under the conditions of this lemma, i.e., when g k (\u03b8) lies at the boundary, then the above gradient J \u03b8 g k (\u03b8) in fact often becomes zero, which can be a problem for parameter fitting.\n\"Equilibrium selection\"(Harsanyi, Selten et al. 1988) refers to the problem of which single equilibrium agents will end up choosing if there are multiple -possibly even after a refinement.14 Therefore we loosely refer to the full architecture as \"end-toend trainable\", not \"end-to-end differentiable\".15 On a related note, we learn the common term's parameter \u03b8 (see (1)) as shared between all scenes, while the other parameters are predicted from the individual's sample past trajectory.16  This is the general double nature of game theory -predictive and prescriptive(Shoham and Leyton-Brown 2008).\nNote that this theorem in fact holds for potential games in general, not just common-coupled games. Note also that for tractability/analysis reasons we consider the simple deterministic game form of Def. 1 instead of, say, a Markov game.24  Due to the concavity assumption, we can use established tractable, guaranteeably sound algorithms to calculate this argmax.\n\"A net (B)\" reads \"output of the A net applied to input B\".\nreaches the intersection. Between 2 reaching the intersection and 1 reaching the intersection: in this time span, the right term grows slower than the left term decreases, therefore the minimum (for this time span) happens when 1 reaches the intersection. Now after 1 has reached the intersection obviously both terms just grow.\nOne thing we found was that there is a slight mismatch between trajectory data compared to a preliminary map of the highway section (available in the data repository as well). This rather seems an error in the map, but could also be a distortion in the trajectory recordings.\nAcknowledgmentsWe thank Jalal Etesami, Markus Spies and Mathias Buerger for insightful discussions and anonymous reviewers for their hints.Appendix\nOne of the data sets we used in the experiment is a new one which we publish along this paper (introduced in Sec. 5.1). We refer to it as HEE (Highway Eagle Eye) data set. Here are the links relevant for the data set: \u2022 Data set. One of the data sets we used in the experiment is a new one which we publish along this paper (introduced in Sec. 5.1). We refer to it as HEE (Highway Eagle Eye) data set. Here are the links relevant for the data set: \u2022 Data set itself: https://github.com/boschresearch/hee_dataset.\n\nNote that this repository also contains the precise filtered data set of two-player highway merge scenes that we used in the experiments. \u2022 For example code of how to load/preprocess trajectories from the raw data set, please look at the code repository for this paper. Further remarks on HEE\u2022 For example code of how to load/preprocess trajectories from the raw data set, please look at the code repository for this paper: https://github.com/boschresearch/trajectory_games_learning. Note that this repository also contains the precise filtered data set of two-player highway merge scenes that we used in the experiments. Further remarks on HEE:\n\n\u2022 The full recorded highway section in HEE is roughly as the lower lane (incl. exit/entry) in Fig. 5 (on page 15) (the recorded section is in fact slightly more to the right than the picture indicates). For the experiments we focused on merging scene trajectories, which roughly take place within the smaller highway section depicted in Fig. on page 15). Note that the images in Fig. 1 are a stylized (significantly squashed in the x-dimension) version of Fig. 6\u2022 The full recorded highway section in HEE is roughly as the lower lane (incl. exit/entry) in Fig. 5 (on page 15) (the recorded section is in fact slightly more to the right than the picture indicates). For the experiments we focused on merging scene trajectories, which roughly take place within the smaller highway section depicted in Fig. 6 (on page 15). Note that the images in Fig. 1 are a stylized (significantly squashed in the x-dimension) version of Fig. 6.\n\n\u2022 Note that the recorded highway section in our data set -length \u223c 600m -is longer compared to the one section with an on-ramp in the highD data set. Krajewski, But highD also contains other highway sections\u2022 Note that the recorded highway section in our data set -length \u223c 600m -is longer compared to the one section with an on-ramp in the highD data set (Krajewski et al. 2018). But highD also contains other highway sections.\n\nAs stated earlier, keep in mind that this HEE data set can be useful for studies like ours, but some aspects of it may be noisy, so it is only meant for experimental purposes. 26\u2022 As stated earlier, keep in mind that this HEE data set can be useful for studies like ours, but some aspects of it may be noisy, so it is only meant for experimental purposes. 26\n\nIn the main text we only gave results averaged over all prediction horizons. Here, Tables 2, 3 (on page 19) give the results -MAE and RMSE for all methods (including CS-LSTM (Deo and Trivedi 2018), MFP (Tang and Salakhutdinov 2019)) -for each prediction horizon individually. from 1s to 7sIn the main text we only gave results averaged over all prediction horizons. Here, Tables 2, 3 (on page 19) give the results - MAE and RMSE for all methods (including CS-LSTM (Deo and Trivedi 2018), MFP (Tang and Salakhutdinov 2019)) -for each prediction horizon individually, from 1s to 7s.\n\n7 (on page 18) we show example joint trajectories on highD data set and our new highway data set: the past trajectory x, the prediction\u0177 (most likely mode) of our method TGL-DP, and the ground truth future trajectory. In Additionally, Fig, Note that the x-axis is significantly squeezed in these figuresAdditionally, in Fig. 7 (on page 18) we show example joint trajectories on highD data set and our new highway data set: the past trajectory x, the prediction\u0177 (most likely mode) of our method TGL-DP, and the ground truth future trajectory y. Note that the x-axis is significantly squeezed in these figures.\n\nE Further related work Regarding work that combines machine learning and game theory, noteworthy is also (Hartford, Wright, and Leyton-Brown 2016) who learn solution concepts, but not equilibrium refinement concepts. Generally, learning agent behavior. but in an active/experimental settings, is also extensively studied, e.g., in multiagent reinforcement learning (Shoham and Leyton-BrownE Further related work Regarding work that combines machine learning and game theory, noteworthy is also (Hartford, Wright, and Leyton-Brown 2016) who learn solution concepts, but not equilibrium refinement concepts. Generally, learning agent behavior, but in an active/experimental settings, is also extensively studied, e.g., in multiagent reinforcement learning (Shoham and Leyton-Brown\n\nwhere a computational assistant interacts with agents to learn their preferences and based on this support their coordination in the game-theoretic sense of equilibrium selection. Geiger, other related setting have been studied. Recently, also other related setting have been studied, e.g., where a computational assistant interacts with agents to learn their preferences and based on this support their coordination in the game-theoretic sense of equilibrium selection (Geiger et al.\n\nRegarding imitation learning of agents' interaction from observational data when there may be relevant unobserved confounders, there is also work based on causal models with a focus on identifiability analysis. Etesami and GeigerRegarding imitation learning of agents' interaction from observational data when there may be relevant unobserved con- founders, there is also work based on causal models with a focus on identifiability analysis (Etesami and Geiger 2020;\n\n. P. Haan. P. Haan 2018;\n\n. Hofmann Geiger, Sch\u00f6lkopf , Geiger, Hofmann, and Sch\u00f6lkopf 2016).\n\nBeyond inverse reinforcement learning mentioned already in the introduction, our work is closely related to the general area of imitation learning (Osa et al. Beyond inverse reinforcement learning mentioned already in the introduction, our work is closely related to the general area of imitation learning (Osa et al. 2018).\n\nRegarding implicit layers, the following work is also worth mentioning explicitly: Amos and Kolter (2017) take a (constrained) optimizer as implicit layer and derive the implicit layer's gradient formula from the (Lagrangian/Karush-Kuhn-Tucker) optimality condition. Note that this is similar to one part of our Thm. 1, which, after the reduction to potential games, also derives the implicit layer's gradient formula from an optimality condition. But our Thm. 1 contains additional elements in the sense of the precise preconditions to fit with the rest of our setting, the parallelization of solving for several local optima/equilibria, and the continuity implication for the g k , which follows easily but which, so far, we have not seen in other work. Bhattacharyya, ) who differentiate through (single-agent) model predictive control (MPC) conditions. Worth mentioning are also. who also use implicit differentiation to learn about games. F Remarks on rationality, local Nash equilibria, equilibrium selection, etcRegarding probabilistic (multi-)agent trajectory prediction, on the machine learning side there are also recent methods using normalizing flows (Bhattacharyya et al. 2019, 2020). Regarding implicit layers, the following work is also worth mentioning explicitly: Amos and Kolter (2017) take a (con- strained) optimizer as implicit layer and derive the implicit layer's gradient formula from the (Lagrangian/Karush-Kuhn-Tucker) optimality condition. Note that this is similar to one part of our Thm. 1, which, after the reduction to potential games, also de- rives the implicit layer's gradient formula from an optimality condition. But our Thm. 1 contains additional elements in the sense of the precise preconditions to fit with the rest of our setting, the parallelization of solving for several local optima/equilibria, and the continuity implication for the g k , which follows easily but which, so far, we have not seen in other work. Also note Amos et al. (2018) who differentiate through (single-agent) model predictive control (MPC) conditions. Worth mentioning are also Li et al. (2020), who also use implicit differentiation to learn about games. F Remarks on rationality, local Nash equilibria, equilibrium selection, etc.\n\n\u2022 As a remark on the (local) rationality principle in our model: We do not think that humans are perfectly instrumentally rational (i.e., strategic utility maximizers) in general, and in many multiagent trajectory settings bounded rationality and References. \u2022 As a remark on the (local) rationality principle in our model: We do not think that humans are perfectly instrumentally rational (i.e., strategic utility maximizers) in general, and in many multiagent trajectory settings bounded rationality and References\n\nSocial lstm: Human trajectory prediction in crowded spaces. A Alahi, K Goel, V Ramanathan, A Robicquet, L Fei-Fei, S Savarese, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionAlahi, A.; Goel, K.; Ramanathan, V.; Robicquet, A.; Fei-Fei, L.; and Savarese, S. 2016. Social lstm: Human trajectory prediction in crowded spaces. In Proceedings of the IEEE conference on computer vision and pattern recognition, 961-971.\n\nDifferentiable MPC for end-to-end planning and control. B Amos, I Jimenez, J Sacks, B Boots, J Kolter, Advances in Neural Information Processing Systems. Amos, B.; Jimenez, I.; Sacks, J.; Boots, B.; and Kolter, J. Z. 2018. Differentiable MPC for end-to-end planning and control. In Advances in Neural Information Processing Systems, 8289-8300.\n\nOptnet: Differentiable optimization as a layer in neural networks. B Amos, J Z Kolter, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningJMLR. org70Amos, B.; and Kolter, J. Z. 2017. Optnet: Differentiable optimization as a layer in neural networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, 136-145. JMLR. org.\n\nDeep equilibrium models. S Bai, J Z Kolter, V Koltun, Advances in Neural Information Processing Systems. Bai, S.; Kolter, J. Z.; and Koltun, V. 2019. Deep equilibrium models. In Advances in Neural Information Processing Systems, 688-699.\n\nConditional flow variational autoencoders for structured sequence prediction. A Bhattacharyya, M Hanselmann, M Fritz, B Schiele, C.-N Straehle, NeurIPS Workshop on Machine Learning for Autonomous Driving. Bhattacharyya, A.; Hanselmann, M.; Fritz, M.; Schiele, B.; and Straehle, C.-N. 2019. Conditional flow variational autoencoders for structured sequence prediction. NeurIPS Workshop on Machine Learning for Autonomous Driving .\n\nA Bhattacharyya, C.-N Straehle, M Fritz, B Schiele, Haar Wavelet based Block Autoregressive Flows for Trajectories. German Conference on Pattern Recognition. Bhattacharyya, A.; Straehle, C.-N.; Fritz, M.; and Schiele, B. 2020. Haar Wavelet based Block Autoregressive Flows for Trajectories. German Conference on Pattern Recognition .\n\nLearning generative socially aware models of pedestrian motion. C Blaiotta, IEEE Robotics and Automation Letters. 44Blaiotta, C. 2019. Learning generative socially aware models of pedestrian motion. IEEE Robotics and Automation Letters 4(4): 3433-3440.\n\nS Boyd, L Vandenberghe, Convex optimization. Cambridge university pressBoyd, S.; and Vandenberghe, L. 2004. Convex optimization. Cambridge university press.\n\nEmpirical game theory of pedestrian interaction for autonomous vehicles. F Camara, R Romano, G Markkula, R Madigan, N Merat, C Fox, Proceedings of Measuring Behavior. Measuring BehaviorManchester Metropolitan UniversityCamara, F.; Romano, R.; Markkula, G.; Madigan, R.; Merat, N.; and Fox, C. 2018. Empirical game theory of pedestrian interaction for autonomous vehicles. In Proceedings of Measuring Behavior 2018. Manchester Metropolitan University.\n\nConvolutional social pooling for vehicle trajectory prediction. N Deo, M M Trivedi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsDeo, N.; and Trivedi, M. M. 2018. Convolutional social pooling for vehicle trajectory prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 1468-1476.\n\n. L El Ghaoui, F Gu, B Travacca, A Askari, arXiv:1908.06315Implicit deep learning. arXiv preprintEl Ghaoui, L.; Gu, F.; Travacca, B.; and Askari, A. 2019. Implicit deep learning. arXiv preprint arXiv:1908.06315 .\n\nCausal Transfer for Imitation Learning and Decision Making under Sensor-shift. J Etesami, P Geiger, Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI). the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)Etesami, J.; and Geiger, P. 2020. Causal Transfer for Imitation Learning and Decision Making under Sensor-shift. In Proceed- ings of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI).\n\nJ Etesami, C.-N Straehle, arXiv:2005.12360Non-cooperative Multi-agent Systems with Exploring Agents. arXiv preprintEtesami, J.; and Straehle, C.-N. 2020. Non-cooperative Multi-agent Systems with Exploring Agents. arXiv preprint arXiv:2005.12360 .\n\nHierarchical game-theoretic planning for autonomous vehicles. J F Fisac, E Bronstein, E Stefansson, D Sadigh, S S Sastry, A D Dragan, 2019 International Conference on Robotics and Automation (ICRA). IEEEFisac, J. F.; Bronstein, E.; Stefansson, E.; Sadigh, D.; Sastry, S. S.; and Dragan, A. D. 2019. Hierarchical game-theoretic planning for autonomous vehicles. In 2019 International Conference on Robotics and Automation (ICRA), 9590-9596. IEEE.\n\n. C Fox, F Camara, G Markkula, R Romano, R Madigan, N Merat, et al. 2018. When should the chicken cross the road?: Game theory for autonomous vehicle-human interactionsFox, C.; Camara, F.; Markkula, G.; Romano, R.; Madigan, R.; Merat, N.; et al. 2018. When should the chicken cross the road?: Game theory for autonomous vehicle-human interactions .\n\nCoordinating users of shared facilities based on data-driven assistants and game-theoretic analysis. P Geiger, M Besserve, J Winkelmann, C Proissl, B Sch\u00f6lkopf, Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence (UAI). the 35th Conference on Uncertainty in Artificial Intelligence (UAI)Geiger, P.; Besserve, M.; Winkelmann, J.; Proissl, C.; and Sch\u00f6lkopf, B. 2019. Coordinating users of shared facilities based on data-driven assistants and game-theoretic analysis. In Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence (UAI).\n\nExperimental and causal view on information integration in autonomous agents. P Geiger, K Hofmann, B Sch\u00f6lkopf, Proceedings of the 6th International Workshop on Combinations of Intelligent Methods and Applications. the 6th International Workshop on Combinations of Intelligent Methods and ApplicationsCIMAGeiger, P.; Hofmann, K.; and Sch\u00f6lkopf, B. 2016. Experimental and causal view on information integration in autonomous agents. In Proceedings of the 6th International Workshop on Combinations of Intelligent Methods and Applications (CIMA 2016).\n\nLearning game-theoretic models of multiagent trajectories using implicit layers. P Geiger, C.-N Straehle, Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI). the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)Geiger, P.; and Straehle, C.-N. 2021. Learning game-theoretic models of multiagent trajectories using implicit layers. In Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI).\n\nSocial GAN: Socially acceptable trajectories with generative adversarial networks. A Gupta, J Johnson, L Fei-Fei, S Savarese, A Alahi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionGupta, A.; Johnson, J.; Fei-Fei, L.; Savarese, S.; and Alahi, A. 2018. Social GAN: Socially acceptable trajectories with gener- ative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2255-2264.\n\nA general theory of equilibrium selection in games. J C Harsanyi, R Selten, The MIT Press1Harsanyi, J. C.; Selten, R.; et al. 1988. A general theory of equilibrium selection in games, volume 1. The MIT Press.\n\nDeep learning for predicting human strategic behavior. J S Hartford, J R Wright, K Leyton-Brown, Advances in Neural Information Processing Systems. Hartford, J. S.; Wright, J. R.; and Leyton-Brown, K. 2016. Deep learning for predicting human strategic behavior. In Advances in Neural Information Processing Systems, 2424-2432.\n\nSocial force model for pedestrian dynamics. D Helbing, P Molnar, Physical review E. 5154282Helbing, D.; and Molnar, P. 1995. Social force model for pedestrian dynamics. Physical review E 51(5): 4282.\n\nGame theoretical approach to model decision making for merging maneuvers at freeway on-ramps. K Kang, H A Rakha, Transportation Research Record. 26231Kang, K.; and Rakha, H. A. 2017. Game theoretical approach to model decision making for merging maneuvers at freeway on-ramps. Transportation Research Record 2623(1): 19-28.\n\nGeneral lane-changing model MOBIL for car-following models. A Kesting, M Treiber, D Helbing, Transportation Research Record. 19991Kesting, A.; Treiber, M.; and Helbing, D. 2007. General lane-changing model MOBIL for car-following models. Transportation Research Record 1999(1): 86-94.\n\nA merging-giveway interaction model of cars in a merging section: a game theoretic analysis. H Kita, Transportation Research Part A: Policy and Practice. 333-4Kita, H. 1999. A merging-giveway interaction model of cars in a merging section: a game theoretic analysis. Transportation Research Part A: Policy and Practice 33(3-4): 305-312.\n\nA game theoretic analysis of merging-giveway interaction: a joint estimation model. Transportation and Traffic Theory in the 21st Century. H Kita, K Tanimoto, K Fukuyama, Kita, H.; Tanimoto, K.; and Fukuyama, K. 2002. A game theoretic analysis of merging-giveway interaction: a joint estimation model. Transportation and Traffic Theory in the 21st Century 503-518.\n\nThe highD Dataset: A Drone Dataset of Naturalistic Vehicle Trajectories on German Highways for Validation of Highly Automated Driving Systems. R Krajewski, J Bock, L Kloeker, L Eckstein, IEEE 21st International Conference on Intelligent Transportation Systems (ITSC). Krajewski, R.; Bock, J.; Kloeker, L.; and Eckstein, L. 2018. The highD Dataset: A Drone Dataset of Naturalistic Vehicle Trajectories on German Highways for Validation of Highly Automated Driving Systems. In 2018 IEEE 21st International Conference on Intelligent Transportation Systems (ITSC).\n\nFeature-based prediction of trajectories for socially compliant navigation. M Kuderer, H Kretzschmar, C Sprunk, W Burgard, Robotics: science and systems. Kuderer, M.; Kretzschmar, H.; Sprunk, C.; and Burgard, W. 2012. Feature-based prediction of trajectories for socially compliant navigation. In Robotics: science and systems.\n\nEnd-to-end learning and intervention in games. J Li, J Yu, Y Nie, Z Wang, Advances in Neural Information Processing Systems. 33Li, J.; Yu, J.; Nie, Y.; and Wang, Z. 2020. End-to-end learning and intervention in games. Advances in Neural Information Processing Systems 33.\n\nGame theoretic modeling of vehicle interactions at unsignalized intersections and application to autonomous vehicle control. N Li, I Kolmanovsky, A Girard, Y Yildiz, 2018 Annual American Control Conference (ACC). IEEELi, N.; Kolmanovsky, I.; Girard, A.; and Yildiz, Y. 2018. Game theoretic modeling of vehicle interactions at unsignalized intersections and application to autonomous vehicle control. In 2018 Annual American Control Conference (ACC), 3215-3220. IEEE.\n\nImplementation of Stochastic Quasi-Newton's Method in PyTorch. Y Li, H Liu, arXiv:1805.02338arXiv preprintLi, Y.; and Liu, H. 2018. Implementation of Stochastic Quasi-Newton's Method in PyTorch. arXiv preprint arXiv:1805.02338 .\n\nWhat game are we playing? End-to-end learning in normal and extensive form games. C K Ling, F Fang, J Kolter, IJCAI-ECAI-18: The 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence. Ling, C. K.; Fang, F.; and Kolter, J. Z. 2018. What game are we playing? End-to-end learning in normal and extensive form games. In IJCAI-ECAI-18: The 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence.\n\nLarge scale learning of agent rationality in two-player zero-sum games. C K Ling, F Fang, J Z Kolter, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Ling, C. K.; Fang, F.; and Kolter, J. Z. 2019. Large scale learning of agent rationality in two-player zero-sum games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, 6104-6111.\n\nA game theoretical approach for modelling merging and yielding behaviour at freeway on-ramp sections. H X Liu, W Xin, Z Adam, J Ban, Transportation and traffic theory. 3Liu, H. X.; Xin, W.; Adam, Z.; and Ban, J. 2007. A game theoretical approach for modelling merging and yielding behaviour at freeway on-ramp sections. Transportation and traffic theory 3: 197-211.\n\nForecasting interactive dynamics of pedestrians with fictitious play. W.-C Ma, D.-A Huang, N Lee, K M Kitani, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMa, W.-C.; Huang, D.-A.; Lee, N.; and Kitani, K. M. 2017. Forecasting interactive dynamics of pedestrians with fictitious play. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 774-782.\n\nPotential games. D Monderer, L S Shapley, Games and economic behavior. 141Monderer, D.; and Shapley, L. S. 1996. Potential games. Games and economic behavior 14(1): 124-143.\n\nT Osa, J Pajarinen, G Neumann, J A Bagnell, P Abbeel, J Peters, arXiv:1811.06711An algorithmic perspective on imitation learning. arXiv preprintOsa, T.; Pajarinen, J.; Neumann, G.; Bagnell, J. A.; Abbeel, P.; and Peters, J. 2018. An algorithmic perspective on imitation learning. arXiv preprint arXiv:1811.06711 .\n\nA course in game theory. M J Osborne, A Rubinstein, MIT pressOsborne, M. J.; and Rubinstein, A. 1994. A course in game theory. MIT press.\n\nCausal Confusion in Imitation Learning. D Haan, S L Jayaraman, NIPS Workshop. Haan, D. Jayaraman, S. L. 2018. Causal Confusion in Imitation Learning. In NIPS Workshop.\n\nL Peters, D Fridovich-Keil, C J Tomlin, Z N Sunberg, arXiv:2002.04354Inference-Based Strategy Alignment for General-Sum Differential Games. arXiv preprintPeters, L.; Fridovich-Keil, D.; Tomlin, C. J.; and Sunberg, Z. N. 2020. Inference-Based Strategy Alignment for General-Sum Differential Games. arXiv preprint arXiv:2002.04354 .\n\nCharacterization and computation of local Nash equilibria in continuous games. L J Ratliff, S A Burden, S S Sastry, 51st Annual Allerton Conference on Communication, Control, and Computing. AllertonIEEERatliff, L. J.; Burden, S. A.; and Sastry, S. S. 2013. Characterization and computation of local Nash equilibria in continuous games. In 2013 51st Annual Allerton Conference on Communication, Control, and Computing (Allerton), 917-924. IEEE.\n\nOn the characterization of local Nash equilibria in continuous games. L J Ratliff, S A Burden, S S Sastry, IEEE Transactions on Automatic Control. 618Ratliff, L. J.; Burden, S. A.; and Sastry, S. S. 2016. On the characterization of local Nash equilibria in continuous games. IEEE Transactions on Automatic Control 61(8): 2301-2307.\n\nInverse reinforcement learning for decentralized noncooperative multiagent systems. T S Reddy, SMCV Gopikrishna, SMCG Zaruba, SMCM Huber, SMC2012 IEEE International Conference on Systems, Man, and Cybernetics. IEEEReddy, T. S.; Gopikrishna, V.; Zaruba, G.; and Huber, M. 2012. Inverse reinforcement learning for decentralized non- cooperative multiagent systems. In 2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1930-1935. IEEE.\n\nLearning social etiquette: Human trajectory understanding in crowded scenes. A Robicquet, A Sadeghian, A Alahi, S Savarese, European conference on computer vision. SpringerRobicquet, A.; Sadeghian, A.; Alahi, A.; and Savarese, S. 2016. Learning social etiquette: Human trajectory understanding in crowded scenes. In European conference on computer vision, 549-565. Springer.\n\nHuman motion trajectory prediction: A survey. A Rudenko, L Palmieri, M Herman, K M Kitani, D M Gavrila, K O Arras, arXiv:1905.06113arXiv preprintRudenko, A.; Palmieri, L.; Herman, M.; Kitani, K. M.; Gavrila, D. M.; and Arras, K. O. 2019. Human motion trajectory prediction: A survey. arXiv preprint arXiv:1905.06113 .\n\nT Salzmann, B Ivanovic, P Chakravarty, M Pavone, arXiv:2001.03093Trajectron++: Multi-Agent Generative Trajectory Forecasting With Heterogeneous Data for Control. arXiv preprintSalzmann, T.; Ivanovic, B.; Chakravarty, P.; and Pavone, M. 2020. Trajectron++: Multi-Agent Generative Trajectory Forecasting With Heterogeneous Data for Control. arXiv preprint arXiv:2001.03093 .\n\nMultiagent systems: Algorithmic, game-theoretic, and logical foundations. Y Shoham, K Leyton-Brown, Cambridge University PressShoham, Y.; and Leyton-Brown, K. 2008. Multiagent systems: Algorithmic, game-theoretic, and logical foundations. Cam- bridge University Press.\n\nA real-time game theoretic planner for autonomous two-player drone racing. R Spica, D Falanga, E Cristofalo, E Montijano, D Scaramuzza, M Schwager, arXiv:1801.02302arXiv preprintSpica, R.; Falanga, D.; Cristofalo, E.; Montijano, E.; Scaramuzza, D.; and Schwager, M. 2018. A real-time game theoretic planner for autonomous two-player drone racing. arXiv preprint arXiv:1801.02302 .\n\nOn players' models of other players: Theory and experimental evidence. D O Stahl, P W Wilson, Games and Economic Behavior. 101Stahl, D. O.; and Wilson, P. W. 1995. On players' models of other players: Theory and experimental evidence. Games and Economic Behavior 10(1): 218-254.\n\nProbabilistic prediction of interactive driving behavior via hierarchical inverse reinforcement learning. L Sun, W Zhan, M Tomizuka, 21st International Conference on Intelligent Transportation Systems (ITSC). IEEESun, L.; Zhan, W.; and Tomizuka, M. 2018. Probabilistic prediction of interactive driving behavior via hierarchical inverse reinforcement learning. In 2018 21st International Conference on Intelligent Transportation Systems (ITSC), 2111-2117. IEEE.\n\nMultiple futures prediction. C Tang, R R Salakhutdinov, Advances in Neural Information Processing Systems. Tang, C.; and Salakhutdinov, R. R. 2019. Multiple futures prediction. In Advances in Neural Information Processing Systems, 15398-15408.\n\nAdaptive game-theoretic decision making for autonomous vehicle control at roundabouts. R Tian, S Li, N Li, I Kolmanovsky, A Girard, Y Yildiz, 2018 IEEE Conference on Decision and Control (CDC). IEEETian, R.; Li, S.; Li, N.; Kolmanovsky, I.; Girard, A.; and Yildiz, Y. 2018. Adaptive game-theoretic decision making for au- tonomous vehicle control at roundabouts. In 2018 IEEE Conference on Decision and Control (CDC), 321-326. IEEE.\n\nCongested traffic states in empirical observations and microscopic simulations. M Treiber, A Hennecke, D Helbing, Physical review E. 6221805Treiber, M.; Hennecke, A.; and Helbing, D. 2000. Congested traffic states in empirical observations and microscopic simula- tions. Physical review E 62(2): 1805.\n\nCompetitive multi-agent inverse reinforcement learning with sub-optimal demonstrations. X Wang, D Klabjan, arXiv:1801.02124arXiv preprintWang, X.; and Klabjan, D. 2018. Competitive multi-agent inverse reinforcement learning with sub-optimal demonstrations. arXiv preprint arXiv:1801.02124 .\n\nStochastic quasi-Newton methods for nonconvex stochastic optimization. X Wang, S Ma, D Goldfarb, W Liu, SIAM Journal on Optimization. 272Wang, X.; Ma, S.; Goldfarb, D.; and Liu, W. 2017. Stochastic quasi-Newton methods for nonconvex stochastic optimization. SIAM Journal on Optimization 27(2): 927-956.\n\nDynamic potential games with constraints: Fundamentals and applications in communications. S Zazo, S V Macua, M S\u00e1nchez-Fern\u00e1ndez, J Zazo, IEEE Transactions on Signal Processing. 6414Zazo, S.; Macua, S. V.; S\u00e1nchez-Fern\u00e1ndez, M.; and Zazo, J. 2016. Dynamic potential games with constraints: Fundamentals and applications in communications. IEEE Transactions on Signal Processing 64(14): 3806-3821.\n\nC Zhang, J Zhu, W Wang, J Xi, arXiv:2003.00759Spatiotemporal Learning of Multivehicle Interaction Patterns in Lane-Change Scenarios. arXiv preprintZhang, C.; Zhu, J.; Wang, W.; and Xi, J. 2020. Spatiotemporal Learning of Multivehicle Interaction Patterns in Lane-Change Scenarios. arXiv preprint arXiv:2003.00759 .\n\nAddressing Mandatory Lane Change Problem with Game Theoretic Model Predictive Control and Fuzzy Markov Chain. Q Zhang, D Filev, H Tseng, S Szwabowski, R Langari, Annual American Control Conference. IEEEZhang, Q.; Filev, D.; Tseng, H.; Szwabowski, S.; and Langari, R. 2018. Addressing Mandatory Lane Change Problem with Game Theoretic Model Predictive Control and Fuzzy Markov Chain. In 2018 Annual American Control Conference (ACC), 4764-4771. IEEE.\n\nNon-Cooperative Inverse Reinforcement Learning. X Zhang, K Zhang, E Miehling, T Basar, Advances in Neural Information Processing Systems. Zhang, X.; Zhang, K.; Miehling, E.; and Basar, T. 2019. Non-Cooperative Inverse Reinforcement Learning. In Advances in Neural Information Processing Systems, 9482-9493.\n", "annotations": {"author": "[{\"end\":189,\"start\":85},{\"end\":316,\"start\":190}]", "publisher": null, "author_last_name": "[{\"end\":99,\"start\":93},{\"end\":216,\"start\":208}]", "author_first_name": "[{\"end\":92,\"start\":85},{\"end\":207,\"start\":190}]", "author_affiliation": "[{\"end\":188,\"start\":131},{\"end\":315,\"start\":258}]", "title": "[{\"end\":82,\"start\":1},{\"end\":398,\"start\":317}]", "venue": null, "abstract": "[{\"end\":1309,\"start\":400}]", "bib_ref": "[{\"end\":3073,\"start\":3055},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3179,\"start\":3152},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3512,\"start\":3490},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3541,\"start\":3512},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5272,\"start\":5268},{\"end\":5789,\"start\":5780},{\"end\":5794,\"start\":5789},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6211,\"start\":6185},{\"end\":6487,\"start\":6468},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6859,\"start\":6848},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6893,\"start\":6859},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6909,\"start\":6893},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6929,\"start\":6909},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":6946,\"start\":6929},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6962,\"start\":6946},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6981,\"start\":6962},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6996,\"start\":6981},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":7025,\"start\":6996},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":7417,\"start\":7394},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":7435,\"start\":7417},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":7453,\"start\":7435},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7479,\"start\":7453},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7838,\"start\":7817},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7859,\"start\":7838},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7919,\"start\":7900},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":7979,\"start\":7950},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8002,\"start\":7981},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8173,\"start\":8148},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8195,\"start\":8173},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8209,\"start\":8195},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":8298,\"start\":8261},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8333,\"start\":8298},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":8514,\"start\":8496},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8803,\"start\":8783},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":8821,\"start\":8803},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":8839,\"start\":8821},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8857,\"start\":8839},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8879,\"start\":8859},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":10732,\"start\":10702},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":10760,\"start\":10732},{\"end\":11690,\"start\":11672},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":12202,\"start\":12175},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12559,\"start\":12537},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12576,\"start\":12559},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12605,\"start\":12576},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12627,\"start\":12605},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":15420,\"start\":15393},{\"end\":17471,\"start\":17470},{\"end\":22443,\"start\":22442},{\"end\":22558,\"start\":22556},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":22774,\"start\":22745},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":25527,\"start\":25509},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":25543,\"start\":25527},{\"end\":28201,\"start\":28199},{\"end\":30433,\"start\":30431},{\"end\":30705,\"start\":30703},{\"end\":31041,\"start\":31039},{\"end\":31194,\"start\":31192},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":31297,\"start\":31268},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":32574,\"start\":32545},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":33157,\"start\":33136},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":33175,\"start\":33157},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":41326,\"start\":41299},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":58794,\"start\":58771},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":64907,\"start\":64877},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":65453,\"start\":65423}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":60382,\"start\":60227},{\"attributes\":{\"id\":\"fig_1\"},\"end\":60941,\"start\":60383},{\"attributes\":{\"id\":\"fig_2\"},\"end\":61127,\"start\":60942},{\"attributes\":{\"id\":\"fig_3\"},\"end\":61173,\"start\":61128},{\"attributes\":{\"id\":\"fig_4\"},\"end\":61303,\"start\":61174},{\"attributes\":{\"id\":\"fig_5\"},\"end\":61766,\"start\":61304},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":62156,\"start\":61767},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":63322,\"start\":62157},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":63940,\"start\":63323},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":64027,\"start\":63941}]", "paragraph": "[{\"end\":2087,\"start\":1325},{\"end\":2612,\"start\":2089},{\"end\":3824,\"start\":2614},{\"end\":4227,\"start\":3826},{\"end\":4703,\"start\":4229},{\"end\":4880,\"start\":4705},{\"end\":5678,\"start\":4882},{\"end\":5795,\"start\":5680},{\"end\":6081,\"start\":5797},{\"end\":6306,\"start\":6083},{\"end\":7503,\"start\":6308},{\"end\":8334,\"start\":7505},{\"end\":9023,\"start\":8336},{\"end\":9104,\"start\":9065},{\"end\":9144,\"start\":9106},{\"end\":9381,\"start\":9146},{\"end\":9480,\"start\":9383},{\"end\":9710,\"start\":9482},{\"end\":9953,\"start\":9712},{\"end\":10255,\"start\":9955},{\"end\":10687,\"start\":10257},{\"end\":11039,\"start\":10689},{\"end\":11249,\"start\":11041},{\"end\":11628,\"start\":11283},{\"end\":11842,\"start\":11630},{\"end\":11965,\"start\":11882},{\"end\":12144,\"start\":11967},{\"end\":12384,\"start\":12146},{\"end\":13308,\"start\":12458},{\"end\":13714,\"start\":13343},{\"end\":14216,\"start\":13790},{\"end\":14611,\"start\":14299},{\"end\":15238,\"start\":14613},{\"end\":15421,\"start\":15303},{\"end\":16168,\"start\":15423},{\"end\":16461,\"start\":16170},{\"end\":17201,\"start\":16463},{\"end\":17913,\"start\":17237},{\"end\":18239,\"start\":17915},{\"end\":18348,\"start\":18241},{\"end\":18405,\"start\":18350},{\"end\":18624,\"start\":18456},{\"end\":18718,\"start\":18626},{\"end\":19028,\"start\":18720},{\"end\":19326,\"start\":19030},{\"end\":19707,\"start\":19436},{\"end\":20349,\"start\":19790},{\"end\":20524,\"start\":20351},{\"end\":21469,\"start\":20526},{\"end\":21793,\"start\":21471},{\"end\":21909,\"start\":21835},{\"end\":23132,\"start\":21911},{\"end\":23828,\"start\":23134},{\"end\":24121,\"start\":23830},{\"end\":25075,\"start\":24123},{\"end\":25544,\"start\":25077},{\"end\":26250,\"start\":25546},{\"end\":26712,\"start\":26295},{\"end\":27095,\"start\":26784},{\"end\":27717,\"start\":27300},{\"end\":28039,\"start\":27719},{\"end\":28697,\"start\":28041},{\"end\":28857,\"start\":28699},{\"end\":29130,\"start\":28859},{\"end\":29319,\"start\":29146},{\"end\":29556,\"start\":29395},{\"end\":30519,\"start\":29558},{\"end\":31806,\"start\":30521},{\"end\":32354,\"start\":31808},{\"end\":32925,\"start\":32356},{\"end\":33255,\"start\":32927},{\"end\":33715,\"start\":33257},{\"end\":34020,\"start\":33717},{\"end\":34427,\"start\":34022},{\"end\":35502,\"start\":34482},{\"end\":35880,\"start\":35504},{\"end\":36744,\"start\":35882},{\"end\":36778,\"start\":36746},{\"end\":36915,\"start\":36780},{\"end\":37115,\"start\":36980},{\"end\":37442,\"start\":37201},{\"end\":37483,\"start\":37444},{\"end\":37906,\"start\":37825},{\"end\":37956,\"start\":37924},{\"end\":38181,\"start\":37958},{\"end\":38323,\"start\":38215},{\"end\":38380,\"start\":38325},{\"end\":38599,\"start\":38431},{\"end\":38620,\"start\":38601},{\"end\":39088,\"start\":38622},{\"end\":39666,\"start\":39090},{\"end\":40161,\"start\":39759},{\"end\":40244,\"start\":40190},{\"end\":40479,\"start\":40246},{\"end\":40890,\"start\":40524},{\"end\":40975,\"start\":40974},{\"end\":41444,\"start\":40977},{\"end\":42250,\"start\":41533},{\"end\":42369,\"start\":42278},{\"end\":42565,\"start\":42520},{\"end\":42724,\"start\":42567},{\"end\":42733,\"start\":42726},{\"end\":42813,\"start\":42804},{\"end\":43188,\"start\":42996},{\"end\":43306,\"start\":43274},{\"end\":43466,\"start\":43308},{\"end\":43491,\"start\":43468},{\"end\":43619,\"start\":43493},{\"end\":43731,\"start\":43621},{\"end\":43794,\"start\":43733},{\"end\":44046,\"start\":43796},{\"end\":44301,\"start\":44153},{\"end\":44718,\"start\":44303},{\"end\":44788,\"start\":44720},{\"end\":45286,\"start\":44790},{\"end\":46074,\"start\":45288},{\"end\":46362,\"start\":46076},{\"end\":46439,\"start\":46420},{\"end\":46748,\"start\":46485},{\"end\":47428,\"start\":46750},{\"end\":47998,\"start\":47430},{\"end\":48535,\"start\":48128},{\"end\":48976,\"start\":48537},{\"end\":49030,\"start\":49007},{\"end\":49575,\"start\":49032},{\"end\":49703,\"start\":49577},{\"end\":49998,\"start\":49705},{\"end\":50171,\"start\":50000},{\"end\":50783,\"start\":50173},{\"end\":51050,\"start\":50785},{\"end\":51882,\"start\":51358},{\"end\":52012,\"start\":51884},{\"end\":52158,\"start\":52014},{\"end\":52761,\"start\":52160},{\"end\":52909,\"start\":52763},{\"end\":53026,\"start\":53010},{\"end\":53361,\"start\":53156},{\"end\":54201,\"start\":53363},{\"end\":54851,\"start\":54203},{\"end\":54967,\"start\":54895},{\"end\":55548,\"start\":54969},{\"end\":56550,\"start\":55550},{\"end\":56819,\"start\":56552},{\"end\":58172,\"start\":56821},{\"end\":60226,\"start\":58185}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11282,\"start\":11250},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11881,\"start\":11843},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12457,\"start\":12385},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14298,\"start\":14217},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15302,\"start\":15239},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17236,\"start\":17202},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18455,\"start\":18406},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19435,\"start\":19327},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21834,\"start\":21794},{\"attributes\":{\"id\":\"formula_9\"},\"end\":26783,\"start\":26713},{\"attributes\":{\"id\":\"formula_10\"},\"end\":27299,\"start\":27096},{\"attributes\":{\"id\":\"formula_11\"},\"end\":36979,\"start\":36916},{\"attributes\":{\"id\":\"formula_12\"},\"end\":37200,\"start\":37116},{\"attributes\":{\"id\":\"formula_13\"},\"end\":37824,\"start\":37484},{\"attributes\":{\"id\":\"formula_14\"},\"end\":38214,\"start\":38182},{\"attributes\":{\"id\":\"formula_15\"},\"end\":38430,\"start\":38381},{\"attributes\":{\"id\":\"formula_16\"},\"end\":39758,\"start\":39667},{\"attributes\":{\"id\":\"formula_17\"},\"end\":40189,\"start\":40162},{\"attributes\":{\"id\":\"formula_18\"},\"end\":40973,\"start\":40891},{\"attributes\":{\"id\":\"formula_19\"},\"end\":41532,\"start\":41445},{\"attributes\":{\"id\":\"formula_20\"},\"end\":42277,\"start\":42251},{\"attributes\":{\"id\":\"formula_22\"},\"end\":42519,\"start\":42370},{\"attributes\":{\"id\":\"formula_24\"},\"end\":42803,\"start\":42734},{\"attributes\":{\"id\":\"formula_25\"},\"end\":42995,\"start\":42814},{\"attributes\":{\"id\":\"formula_26\"},\"end\":43253,\"start\":43189},{\"attributes\":{\"id\":\"formula_27\"},\"end\":44152,\"start\":44047},{\"attributes\":{\"id\":\"formula_28\"},\"end\":46419,\"start\":46363},{\"attributes\":{\"id\":\"formula_29\"},\"end\":48127,\"start\":47999},{\"attributes\":{\"id\":\"formula_31\"},\"end\":51357,\"start\":51051},{\"attributes\":{\"id\":\"formula_33\"},\"end\":53009,\"start\":52910},{\"attributes\":{\"id\":\"formula_34\"},\"end\":53101,\"start\":53027}]", "table_ref": "[{\"end\":34057,\"start\":34050}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1323,\"start\":1311},{\"attributes\":{\"n\":\"2\"},\"end\":9063,\"start\":9026},{\"attributes\":{\"n\":\"3\"},\"end\":13341,\"start\":13311},{\"attributes\":{\"n\":\"3.1\"},\"end\":13788,\"start\":13717},{\"attributes\":{\"n\":\"3.2\"},\"end\":19788,\"start\":19710},{\"attributes\":{\"n\":\"4\"},\"end\":26293,\"start\":26253},{\"attributes\":{\"n\":\"5\"},\"end\":29144,\"start\":29133},{\"attributes\":{\"n\":\"5.1\"},\"end\":29393,\"start\":29322},{\"attributes\":{\"n\":\"5.2\"},\"end\":34480,\"start\":34430},{\"end\":37922,\"start\":37909},{\"end\":40522,\"start\":40482},{\"end\":43272,\"start\":43255},{\"end\":46483,\"start\":46442},{\"end\":49005,\"start\":48979},{\"end\":53154,\"start\":53103},{\"end\":54893,\"start\":54854},{\"end\":58183,\"start\":58175},{\"end\":60238,\"start\":60228},{\"end\":60961,\"start\":60943},{\"end\":61139,\"start\":61129},{\"end\":61185,\"start\":61175},{\"end\":61313,\"start\":61305},{\"end\":63333,\"start\":63324},{\"end\":63951,\"start\":63942}]", "table": "[{\"end\":62156,\"start\":61797},{\"end\":63322,\"start\":63293},{\"end\":63940,\"start\":63472}]", "figure_caption": "[{\"end\":60382,\"start\":60240},{\"end\":60941,\"start\":60385},{\"end\":61127,\"start\":60963},{\"end\":61173,\"start\":61141},{\"end\":61303,\"start\":61187},{\"end\":61766,\"start\":61315},{\"end\":61797,\"start\":61769},{\"end\":63293,\"start\":62159},{\"end\":63472,\"start\":63335},{\"end\":64027,\"start\":63953}]", "figure_ref": "[{\"end\":4112,\"start\":4106},{\"end\":5281,\"start\":5273},{\"end\":16285,\"start\":16279},{\"end\":17807,\"start\":17801},{\"end\":19949,\"start\":19943},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29105,\"start\":29099},{\"end\":29489,\"start\":29483},{\"end\":30750,\"start\":30744},{\"end\":30986,\"start\":30980},{\"end\":31426,\"start\":31418},{\"end\":35376,\"start\":35369},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":47417,\"start\":47409},{\"end\":54047,\"start\":54039},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":54123,\"start\":54117},{\"end\":58970,\"start\":58964}]", "bib_author_first_name": "[{\"end\":70316,\"start\":70314},{\"end\":72476,\"start\":72469},{\"end\":72494,\"start\":72485},{\"end\":75694,\"start\":75693},{\"end\":75703,\"start\":75702},{\"end\":75711,\"start\":75710},{\"end\":75725,\"start\":75724},{\"end\":75738,\"start\":75737},{\"end\":75749,\"start\":75748},{\"end\":76198,\"start\":76197},{\"end\":76206,\"start\":76205},{\"end\":76217,\"start\":76216},{\"end\":76226,\"start\":76225},{\"end\":76235,\"start\":76234},{\"end\":76554,\"start\":76553},{\"end\":76562,\"start\":76561},{\"end\":76564,\"start\":76563},{\"end\":76938,\"start\":76937},{\"end\":76945,\"start\":76944},{\"end\":76947,\"start\":76946},{\"end\":76957,\"start\":76956},{\"end\":77230,\"start\":77229},{\"end\":77247,\"start\":77246},{\"end\":77261,\"start\":77260},{\"end\":77270,\"start\":77269},{\"end\":77284,\"start\":77280},{\"end\":77583,\"start\":77582},{\"end\":77603,\"start\":77599},{\"end\":77615,\"start\":77614},{\"end\":77624,\"start\":77623},{\"end\":77982,\"start\":77981},{\"end\":78172,\"start\":78171},{\"end\":78180,\"start\":78179},{\"end\":78403,\"start\":78402},{\"end\":78413,\"start\":78412},{\"end\":78423,\"start\":78422},{\"end\":78435,\"start\":78434},{\"end\":78446,\"start\":78445},{\"end\":78455,\"start\":78454},{\"end\":78846,\"start\":78845},{\"end\":78853,\"start\":78852},{\"end\":78855,\"start\":78854},{\"end\":79231,\"start\":79230},{\"end\":79244,\"start\":79243},{\"end\":79250,\"start\":79249},{\"end\":79262,\"start\":79261},{\"end\":79522,\"start\":79521},{\"end\":79533,\"start\":79532},{\"end\":79894,\"start\":79893},{\"end\":79908,\"start\":79904},{\"end\":80204,\"start\":80203},{\"end\":80206,\"start\":80205},{\"end\":80215,\"start\":80214},{\"end\":80228,\"start\":80227},{\"end\":80242,\"start\":80241},{\"end\":80252,\"start\":80251},{\"end\":80254,\"start\":80253},{\"end\":80264,\"start\":80263},{\"end\":80266,\"start\":80265},{\"end\":80591,\"start\":80590},{\"end\":80598,\"start\":80597},{\"end\":80608,\"start\":80607},{\"end\":80620,\"start\":80619},{\"end\":80630,\"start\":80629},{\"end\":80641,\"start\":80640},{\"end\":81040,\"start\":81039},{\"end\":81050,\"start\":81049},{\"end\":81062,\"start\":81061},{\"end\":81076,\"start\":81075},{\"end\":81087,\"start\":81086},{\"end\":81597,\"start\":81596},{\"end\":81607,\"start\":81606},{\"end\":81618,\"start\":81617},{\"end\":82151,\"start\":82150},{\"end\":82164,\"start\":82160},{\"end\":82617,\"start\":82616},{\"end\":82626,\"start\":82625},{\"end\":82637,\"start\":82636},{\"end\":82648,\"start\":82647},{\"end\":82660,\"start\":82659},{\"end\":83112,\"start\":83111},{\"end\":83114,\"start\":83113},{\"end\":83126,\"start\":83125},{\"end\":83325,\"start\":83324},{\"end\":83327,\"start\":83326},{\"end\":83339,\"start\":83338},{\"end\":83341,\"start\":83340},{\"end\":83351,\"start\":83350},{\"end\":83642,\"start\":83641},{\"end\":83653,\"start\":83652},{\"end\":83893,\"start\":83892},{\"end\":83901,\"start\":83900},{\"end\":83903,\"start\":83902},{\"end\":84184,\"start\":84183},{\"end\":84195,\"start\":84194},{\"end\":84206,\"start\":84205},{\"end\":84503,\"start\":84502},{\"end\":84887,\"start\":84886},{\"end\":84895,\"start\":84894},{\"end\":84907,\"start\":84906},{\"end\":85257,\"start\":85256},{\"end\":85270,\"start\":85269},{\"end\":85278,\"start\":85277},{\"end\":85289,\"start\":85288},{\"end\":85752,\"start\":85751},{\"end\":85763,\"start\":85762},{\"end\":85778,\"start\":85777},{\"end\":85788,\"start\":85787},{\"end\":86052,\"start\":86051},{\"end\":86058,\"start\":86057},{\"end\":86064,\"start\":86063},{\"end\":86071,\"start\":86070},{\"end\":86403,\"start\":86402},{\"end\":86409,\"start\":86408},{\"end\":86424,\"start\":86423},{\"end\":86434,\"start\":86433},{\"end\":86809,\"start\":86808},{\"end\":86815,\"start\":86814},{\"end\":87058,\"start\":87057},{\"end\":87060,\"start\":87059},{\"end\":87068,\"start\":87067},{\"end\":87076,\"start\":87075},{\"end\":87577,\"start\":87576},{\"end\":87579,\"start\":87578},{\"end\":87587,\"start\":87586},{\"end\":87595,\"start\":87594},{\"end\":87597,\"start\":87596},{\"end\":88028,\"start\":88027},{\"end\":88030,\"start\":88029},{\"end\":88037,\"start\":88036},{\"end\":88044,\"start\":88043},{\"end\":88052,\"start\":88051},{\"end\":88366,\"start\":88362},{\"end\":88375,\"start\":88371},{\"end\":88384,\"start\":88383},{\"end\":88391,\"start\":88390},{\"end\":88393,\"start\":88392},{\"end\":88781,\"start\":88780},{\"end\":88793,\"start\":88792},{\"end\":88795,\"start\":88794},{\"end\":88939,\"start\":88938},{\"end\":88946,\"start\":88945},{\"end\":88959,\"start\":88958},{\"end\":88970,\"start\":88969},{\"end\":88972,\"start\":88971},{\"end\":88983,\"start\":88982},{\"end\":88993,\"start\":88992},{\"end\":89279,\"start\":89278},{\"end\":89281,\"start\":89280},{\"end\":89292,\"start\":89291},{\"end\":89433,\"start\":89432},{\"end\":89441,\"start\":89440},{\"end\":89443,\"start\":89442},{\"end\":89562,\"start\":89561},{\"end\":89572,\"start\":89571},{\"end\":89590,\"start\":89589},{\"end\":89592,\"start\":89591},{\"end\":89602,\"start\":89601},{\"end\":89604,\"start\":89603},{\"end\":89973,\"start\":89972},{\"end\":89975,\"start\":89974},{\"end\":89986,\"start\":89985},{\"end\":89988,\"start\":89987},{\"end\":89998,\"start\":89997},{\"end\":90000,\"start\":89999},{\"end\":90409,\"start\":90408},{\"end\":90411,\"start\":90410},{\"end\":90422,\"start\":90421},{\"end\":90424,\"start\":90423},{\"end\":90434,\"start\":90433},{\"end\":90436,\"start\":90435},{\"end\":90756,\"start\":90755},{\"end\":90758,\"start\":90757},{\"end\":90770,\"start\":90769},{\"end\":90788,\"start\":90787},{\"end\":90801,\"start\":90800},{\"end\":91208,\"start\":91207},{\"end\":91221,\"start\":91220},{\"end\":91234,\"start\":91233},{\"end\":91243,\"start\":91242},{\"end\":91553,\"start\":91552},{\"end\":91564,\"start\":91563},{\"end\":91576,\"start\":91575},{\"end\":91586,\"start\":91585},{\"end\":91588,\"start\":91587},{\"end\":91598,\"start\":91597},{\"end\":91600,\"start\":91599},{\"end\":91611,\"start\":91610},{\"end\":91613,\"start\":91612},{\"end\":91826,\"start\":91825},{\"end\":91838,\"start\":91837},{\"end\":91850,\"start\":91849},{\"end\":91865,\"start\":91864},{\"end\":92274,\"start\":92273},{\"end\":92284,\"start\":92283},{\"end\":92545,\"start\":92544},{\"end\":92554,\"start\":92553},{\"end\":92565,\"start\":92564},{\"end\":92579,\"start\":92578},{\"end\":92592,\"start\":92591},{\"end\":92606,\"start\":92605},{\"end\":92923,\"start\":92922},{\"end\":92925,\"start\":92924},{\"end\":92934,\"start\":92933},{\"end\":92936,\"start\":92935},{\"end\":93238,\"start\":93237},{\"end\":93245,\"start\":93244},{\"end\":93253,\"start\":93252},{\"end\":93624,\"start\":93623},{\"end\":93632,\"start\":93631},{\"end\":93634,\"start\":93633},{\"end\":93927,\"start\":93926},{\"end\":93935,\"start\":93934},{\"end\":93941,\"start\":93940},{\"end\":93947,\"start\":93946},{\"end\":93962,\"start\":93961},{\"end\":93972,\"start\":93971},{\"end\":94354,\"start\":94353},{\"end\":94365,\"start\":94364},{\"end\":94377,\"start\":94376},{\"end\":94665,\"start\":94664},{\"end\":94673,\"start\":94672},{\"end\":94940,\"start\":94939},{\"end\":94948,\"start\":94947},{\"end\":94954,\"start\":94953},{\"end\":94966,\"start\":94965},{\"end\":95264,\"start\":95263},{\"end\":95272,\"start\":95271},{\"end\":95274,\"start\":95273},{\"end\":95283,\"start\":95282},{\"end\":95304,\"start\":95303},{\"end\":95572,\"start\":95571},{\"end\":95581,\"start\":95580},{\"end\":95588,\"start\":95587},{\"end\":95596,\"start\":95595},{\"end\":95998,\"start\":95997},{\"end\":96007,\"start\":96006},{\"end\":96016,\"start\":96015},{\"end\":96025,\"start\":96024},{\"end\":96039,\"start\":96038},{\"end\":96387,\"start\":96386},{\"end\":96396,\"start\":96395},{\"end\":96405,\"start\":96404},{\"end\":96417,\"start\":96416}]", "bib_author_last_name": "[{\"end\":68883,\"start\":68874},{\"end\":70329,\"start\":70317},{\"end\":70334,\"start\":70331},{\"end\":71673,\"start\":71667},{\"end\":72483,\"start\":72477},{\"end\":73631,\"start\":73618},{\"end\":75700,\"start\":75695},{\"end\":75708,\"start\":75704},{\"end\":75722,\"start\":75712},{\"end\":75735,\"start\":75726},{\"end\":75746,\"start\":75739},{\"end\":75758,\"start\":75750},{\"end\":76203,\"start\":76199},{\"end\":76214,\"start\":76207},{\"end\":76223,\"start\":76218},{\"end\":76232,\"start\":76227},{\"end\":76242,\"start\":76236},{\"end\":76559,\"start\":76555},{\"end\":76571,\"start\":76565},{\"end\":76942,\"start\":76939},{\"end\":76954,\"start\":76948},{\"end\":76964,\"start\":76958},{\"end\":77244,\"start\":77231},{\"end\":77258,\"start\":77248},{\"end\":77267,\"start\":77262},{\"end\":77278,\"start\":77271},{\"end\":77293,\"start\":77285},{\"end\":77597,\"start\":77584},{\"end\":77612,\"start\":77604},{\"end\":77621,\"start\":77616},{\"end\":77632,\"start\":77625},{\"end\":77991,\"start\":77983},{\"end\":78177,\"start\":78173},{\"end\":78193,\"start\":78181},{\"end\":78410,\"start\":78404},{\"end\":78420,\"start\":78414},{\"end\":78432,\"start\":78424},{\"end\":78443,\"start\":78436},{\"end\":78452,\"start\":78447},{\"end\":78459,\"start\":78456},{\"end\":78850,\"start\":78847},{\"end\":78863,\"start\":78856},{\"end\":79241,\"start\":79232},{\"end\":79247,\"start\":79245},{\"end\":79259,\"start\":79251},{\"end\":79269,\"start\":79263},{\"end\":79530,\"start\":79523},{\"end\":79540,\"start\":79534},{\"end\":79902,\"start\":79895},{\"end\":79917,\"start\":79909},{\"end\":80212,\"start\":80207},{\"end\":80225,\"start\":80216},{\"end\":80239,\"start\":80229},{\"end\":80249,\"start\":80243},{\"end\":80261,\"start\":80255},{\"end\":80273,\"start\":80267},{\"end\":80595,\"start\":80592},{\"end\":80605,\"start\":80599},{\"end\":80617,\"start\":80609},{\"end\":80627,\"start\":80621},{\"end\":80638,\"start\":80631},{\"end\":80647,\"start\":80642},{\"end\":81047,\"start\":81041},{\"end\":81059,\"start\":81051},{\"end\":81073,\"start\":81063},{\"end\":81084,\"start\":81077},{\"end\":81097,\"start\":81088},{\"end\":81604,\"start\":81598},{\"end\":81615,\"start\":81608},{\"end\":81628,\"start\":81619},{\"end\":82158,\"start\":82152},{\"end\":82173,\"start\":82165},{\"end\":82623,\"start\":82618},{\"end\":82634,\"start\":82627},{\"end\":82645,\"start\":82638},{\"end\":82657,\"start\":82649},{\"end\":82666,\"start\":82661},{\"end\":83123,\"start\":83115},{\"end\":83133,\"start\":83127},{\"end\":83336,\"start\":83328},{\"end\":83348,\"start\":83342},{\"end\":83364,\"start\":83352},{\"end\":83650,\"start\":83643},{\"end\":83660,\"start\":83654},{\"end\":83898,\"start\":83894},{\"end\":83909,\"start\":83904},{\"end\":84192,\"start\":84185},{\"end\":84203,\"start\":84196},{\"end\":84214,\"start\":84207},{\"end\":84508,\"start\":84504},{\"end\":84892,\"start\":84888},{\"end\":84904,\"start\":84896},{\"end\":84916,\"start\":84908},{\"end\":85267,\"start\":85258},{\"end\":85275,\"start\":85271},{\"end\":85286,\"start\":85279},{\"end\":85298,\"start\":85290},{\"end\":85760,\"start\":85753},{\"end\":85775,\"start\":85764},{\"end\":85785,\"start\":85779},{\"end\":85796,\"start\":85789},{\"end\":86055,\"start\":86053},{\"end\":86061,\"start\":86059},{\"end\":86068,\"start\":86065},{\"end\":86076,\"start\":86072},{\"end\":86406,\"start\":86404},{\"end\":86421,\"start\":86410},{\"end\":86431,\"start\":86425},{\"end\":86441,\"start\":86435},{\"end\":86812,\"start\":86810},{\"end\":86819,\"start\":86816},{\"end\":87065,\"start\":87061},{\"end\":87073,\"start\":87069},{\"end\":87083,\"start\":87077},{\"end\":87584,\"start\":87580},{\"end\":87592,\"start\":87588},{\"end\":87604,\"start\":87598},{\"end\":88034,\"start\":88031},{\"end\":88041,\"start\":88038},{\"end\":88049,\"start\":88045},{\"end\":88056,\"start\":88053},{\"end\":88369,\"start\":88367},{\"end\":88381,\"start\":88376},{\"end\":88388,\"start\":88385},{\"end\":88400,\"start\":88394},{\"end\":88790,\"start\":88782},{\"end\":88803,\"start\":88796},{\"end\":88943,\"start\":88940},{\"end\":88956,\"start\":88947},{\"end\":88967,\"start\":88960},{\"end\":88980,\"start\":88973},{\"end\":88990,\"start\":88984},{\"end\":89000,\"start\":88994},{\"end\":89289,\"start\":89282},{\"end\":89303,\"start\":89293},{\"end\":89438,\"start\":89434},{\"end\":89453,\"start\":89444},{\"end\":89569,\"start\":89563},{\"end\":89587,\"start\":89573},{\"end\":89599,\"start\":89593},{\"end\":89612,\"start\":89605},{\"end\":89983,\"start\":89976},{\"end\":89995,\"start\":89989},{\"end\":90007,\"start\":90001},{\"end\":90419,\"start\":90412},{\"end\":90431,\"start\":90425},{\"end\":90443,\"start\":90437},{\"end\":90764,\"start\":90759},{\"end\":90782,\"start\":90771},{\"end\":90795,\"start\":90789},{\"end\":90807,\"start\":90802},{\"end\":91218,\"start\":91209},{\"end\":91231,\"start\":91222},{\"end\":91240,\"start\":91235},{\"end\":91252,\"start\":91244},{\"end\":91561,\"start\":91554},{\"end\":91573,\"start\":91565},{\"end\":91583,\"start\":91577},{\"end\":91595,\"start\":91589},{\"end\":91608,\"start\":91601},{\"end\":91619,\"start\":91614},{\"end\":91835,\"start\":91827},{\"end\":91847,\"start\":91839},{\"end\":91862,\"start\":91851},{\"end\":91872,\"start\":91866},{\"end\":92281,\"start\":92275},{\"end\":92297,\"start\":92285},{\"end\":92551,\"start\":92546},{\"end\":92562,\"start\":92555},{\"end\":92576,\"start\":92566},{\"end\":92589,\"start\":92580},{\"end\":92603,\"start\":92593},{\"end\":92615,\"start\":92607},{\"end\":92931,\"start\":92926},{\"end\":92943,\"start\":92937},{\"end\":93242,\"start\":93239},{\"end\":93250,\"start\":93246},{\"end\":93262,\"start\":93254},{\"end\":93629,\"start\":93625},{\"end\":93648,\"start\":93635},{\"end\":93932,\"start\":93928},{\"end\":93938,\"start\":93936},{\"end\":93944,\"start\":93942},{\"end\":93959,\"start\":93948},{\"end\":93969,\"start\":93963},{\"end\":93979,\"start\":93973},{\"end\":94362,\"start\":94355},{\"end\":94374,\"start\":94366},{\"end\":94385,\"start\":94378},{\"end\":94670,\"start\":94666},{\"end\":94681,\"start\":94674},{\"end\":94945,\"start\":94941},{\"end\":94951,\"start\":94949},{\"end\":94963,\"start\":94955},{\"end\":94970,\"start\":94967},{\"end\":95269,\"start\":95265},{\"end\":95280,\"start\":95275},{\"end\":95301,\"start\":95284},{\"end\":95309,\"start\":95305},{\"end\":95578,\"start\":95573},{\"end\":95585,\"start\":95582},{\"end\":95593,\"start\":95589},{\"end\":95599,\"start\":95597},{\"end\":96004,\"start\":95999},{\"end\":96013,\"start\":96008},{\"end\":96022,\"start\":96017},{\"end\":96036,\"start\":96026},{\"end\":96047,\"start\":96040},{\"end\":96393,\"start\":96388},{\"end\":96402,\"start\":96397},{\"end\":96414,\"start\":96406},{\"end\":96423,\"start\":96418}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":67145,\"start\":66633},{\"attributes\":{\"id\":\"b1\"},\"end\":67792,\"start\":67147},{\"attributes\":{\"id\":\"b2\"},\"end\":68722,\"start\":67794},{\"attributes\":{\"id\":\"b3\"},\"end\":69152,\"start\":68724},{\"attributes\":{\"id\":\"b4\"},\"end\":69512,\"start\":69154},{\"attributes\":{\"id\":\"b5\"},\"end\":70094,\"start\":69514},{\"attributes\":{\"id\":\"b6\"},\"end\":70705,\"start\":70096},{\"attributes\":{\"id\":\"b7\"},\"end\":71485,\"start\":70707},{\"attributes\":{\"id\":\"b8\"},\"end\":71971,\"start\":71487},{\"attributes\":{\"id\":\"b9\"},\"end\":72439,\"start\":71973},{\"attributes\":{\"id\":\"b10\"},\"end\":72465,\"start\":72441},{\"attributes\":{\"id\":\"b11\"},\"end\":72534,\"start\":72467},{\"attributes\":{\"id\":\"b12\"},\"end\":72860,\"start\":72536},{\"attributes\":{\"id\":\"b13\"},\"end\":75113,\"start\":72862},{\"attributes\":{\"id\":\"b14\"},\"end\":75631,\"start\":75115},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":9854676},\"end\":76139,\"start\":75633},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":53108183},\"end\":76484,\"start\":76141},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1791473},\"end\":76910,\"start\":76486},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":202539738},\"end\":77149,\"start\":76912},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":201668865},\"end\":77580,\"start\":77151},{\"attributes\":{\"id\":\"b20\"},\"end\":77915,\"start\":77582},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":198931828},\"end\":78169,\"start\":77917},{\"attributes\":{\"id\":\"b22\"},\"end\":78327,\"start\":78171},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":53963613},\"end\":78779,\"start\":78329},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":21712887},\"end\":79226,\"start\":78781},{\"attributes\":{\"doi\":\"arXiv:1908.06315\",\"id\":\"b25\"},\"end\":79440,\"start\":79228},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":211677639},\"end\":79891,\"start\":79442},{\"attributes\":{\"doi\":\"arXiv:2005.12360\",\"id\":\"b27\"},\"end\":80139,\"start\":79893},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":53114690},\"end\":80586,\"start\":80141},{\"attributes\":{\"id\":\"b29\"},\"end\":80936,\"start\":80588},{\"attributes\":{\"id\":\"b30\"},\"end\":81516,\"start\":80938},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2724539},\"end\":82067,\"start\":81518},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":228096677},\"end\":82531,\"start\":82069},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":4461350},\"end\":83057,\"start\":82533},{\"attributes\":{\"id\":\"b34\"},\"end\":83267,\"start\":83059},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":7853085},\"end\":83595,\"start\":83269},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":29333691},\"end\":83796,\"start\":83597},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":116468473},\"end\":84121,\"start\":83798},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":39345746},\"end\":84407,\"start\":84123},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":154983017},\"end\":84745,\"start\":84409},{\"attributes\":{\"id\":\"b40\"},\"end\":85111,\"start\":84747},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":53115284},\"end\":85673,\"start\":85113},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":12022063},\"end\":86002,\"start\":85675},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":225076244},\"end\":86275,\"start\":86004},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":52014667},\"end\":86743,\"start\":86277},{\"attributes\":{\"doi\":\"arXiv:1805.02338\",\"id\":\"b45\"},\"end\":86973,\"start\":86745},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":13687242},\"end\":87502,\"start\":86975},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":70026426},\"end\":87923,\"start\":87504},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":108672224},\"end\":88290,\"start\":87925},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":13972155},\"end\":88761,\"start\":88292},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":17381677},\"end\":88936,\"start\":88763},{\"attributes\":{\"doi\":\"arXiv:1811.06711\",\"id\":\"b51\"},\"end\":89251,\"start\":88938},{\"attributes\":{\"id\":\"b52\"},\"end\":89390,\"start\":89253},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":145816735},\"end\":89559,\"start\":89392},{\"attributes\":{\"doi\":\"arXiv:2002.04354\",\"id\":\"b54\"},\"end\":89891,\"start\":89561},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":857142},\"end\":90336,\"start\":89893},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":14233739},\"end\":90669,\"start\":90338},{\"attributes\":{\"id\":\"b57\"},\"end\":91128,\"start\":90671},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":3150075},\"end\":91504,\"start\":91130},{\"attributes\":{\"doi\":\"arXiv:1905.06113\",\"id\":\"b59\"},\"end\":91823,\"start\":91506},{\"attributes\":{\"doi\":\"arXiv:2001.03093\",\"id\":\"b60\"},\"end\":92197,\"start\":91825},{\"attributes\":{\"id\":\"b61\"},\"end\":92467,\"start\":92199},{\"attributes\":{\"doi\":\"arXiv:1801.02302\",\"id\":\"b62\"},\"end\":92849,\"start\":92469},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":153979956},\"end\":93129,\"start\":92851},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":52181043},\"end\":93592,\"start\":93131},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":202778848},\"end\":93837,\"start\":93594},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":52901941},\"end\":94271,\"start\":93839},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":1100293},\"end\":94574,\"start\":94273},{\"attributes\":{\"doi\":\"arXiv:1801.02124\",\"id\":\"b68\"},\"end\":94866,\"start\":94576},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":1625130},\"end\":95170,\"start\":94868},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":15960041},\"end\":95569,\"start\":95172},{\"attributes\":{\"doi\":\"arXiv:2003.00759\",\"id\":\"b71\"},\"end\":95885,\"start\":95571},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":52017786},\"end\":96336,\"start\":95887},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":202778790},\"end\":96644,\"start\":96338}]", "bib_title": "[{\"end\":71665,\"start\":71487},{\"end\":75691,\"start\":75633},{\"end\":76195,\"start\":76141},{\"end\":76551,\"start\":76486},{\"end\":76935,\"start\":76912},{\"end\":77227,\"start\":77151},{\"end\":77979,\"start\":77917},{\"end\":78400,\"start\":78329},{\"end\":78843,\"start\":78781},{\"end\":79519,\"start\":79442},{\"end\":80201,\"start\":80141},{\"end\":81037,\"start\":80938},{\"end\":81594,\"start\":81518},{\"end\":82148,\"start\":82069},{\"end\":82614,\"start\":82533},{\"end\":83322,\"start\":83269},{\"end\":83639,\"start\":83597},{\"end\":83890,\"start\":83798},{\"end\":84181,\"start\":84123},{\"end\":84500,\"start\":84409},{\"end\":85254,\"start\":85113},{\"end\":85749,\"start\":85675},{\"end\":86049,\"start\":86004},{\"end\":86400,\"start\":86277},{\"end\":87055,\"start\":86975},{\"end\":87574,\"start\":87504},{\"end\":88025,\"start\":87925},{\"end\":88360,\"start\":88292},{\"end\":88778,\"start\":88763},{\"end\":89430,\"start\":89392},{\"end\":89970,\"start\":89893},{\"end\":90406,\"start\":90338},{\"end\":90753,\"start\":90671},{\"end\":91205,\"start\":91130},{\"end\":92920,\"start\":92851},{\"end\":93235,\"start\":93131},{\"end\":93621,\"start\":93594},{\"end\":93924,\"start\":93839},{\"end\":94351,\"start\":94273},{\"end\":94937,\"start\":94868},{\"end\":95261,\"start\":95172},{\"end\":95995,\"start\":95887},{\"end\":96384,\"start\":96338}]", "bib_author": "[{\"end\":68885,\"start\":68874},{\"end\":70331,\"start\":70314},{\"end\":70336,\"start\":70331},{\"end\":71675,\"start\":71667},{\"end\":72485,\"start\":72469},{\"end\":72497,\"start\":72485},{\"end\":73633,\"start\":73618},{\"end\":75702,\"start\":75693},{\"end\":75710,\"start\":75702},{\"end\":75724,\"start\":75710},{\"end\":75737,\"start\":75724},{\"end\":75748,\"start\":75737},{\"end\":75760,\"start\":75748},{\"end\":76205,\"start\":76197},{\"end\":76216,\"start\":76205},{\"end\":76225,\"start\":76216},{\"end\":76234,\"start\":76225},{\"end\":76244,\"start\":76234},{\"end\":76561,\"start\":76553},{\"end\":76573,\"start\":76561},{\"end\":76944,\"start\":76937},{\"end\":76956,\"start\":76944},{\"end\":76966,\"start\":76956},{\"end\":77246,\"start\":77229},{\"end\":77260,\"start\":77246},{\"end\":77269,\"start\":77260},{\"end\":77280,\"start\":77269},{\"end\":77295,\"start\":77280},{\"end\":77599,\"start\":77582},{\"end\":77614,\"start\":77599},{\"end\":77623,\"start\":77614},{\"end\":77634,\"start\":77623},{\"end\":77993,\"start\":77981},{\"end\":78179,\"start\":78171},{\"end\":78195,\"start\":78179},{\"end\":78412,\"start\":78402},{\"end\":78422,\"start\":78412},{\"end\":78434,\"start\":78422},{\"end\":78445,\"start\":78434},{\"end\":78454,\"start\":78445},{\"end\":78461,\"start\":78454},{\"end\":78852,\"start\":78845},{\"end\":78865,\"start\":78852},{\"end\":79243,\"start\":79230},{\"end\":79249,\"start\":79243},{\"end\":79261,\"start\":79249},{\"end\":79271,\"start\":79261},{\"end\":79532,\"start\":79521},{\"end\":79542,\"start\":79532},{\"end\":79904,\"start\":79893},{\"end\":79919,\"start\":79904},{\"end\":80214,\"start\":80203},{\"end\":80227,\"start\":80214},{\"end\":80241,\"start\":80227},{\"end\":80251,\"start\":80241},{\"end\":80263,\"start\":80251},{\"end\":80275,\"start\":80263},{\"end\":80597,\"start\":80590},{\"end\":80607,\"start\":80597},{\"end\":80619,\"start\":80607},{\"end\":80629,\"start\":80619},{\"end\":80640,\"start\":80629},{\"end\":80649,\"start\":80640},{\"end\":81049,\"start\":81039},{\"end\":81061,\"start\":81049},{\"end\":81075,\"start\":81061},{\"end\":81086,\"start\":81075},{\"end\":81099,\"start\":81086},{\"end\":81606,\"start\":81596},{\"end\":81617,\"start\":81606},{\"end\":81630,\"start\":81617},{\"end\":82160,\"start\":82150},{\"end\":82175,\"start\":82160},{\"end\":82625,\"start\":82616},{\"end\":82636,\"start\":82625},{\"end\":82647,\"start\":82636},{\"end\":82659,\"start\":82647},{\"end\":82668,\"start\":82659},{\"end\":83125,\"start\":83111},{\"end\":83135,\"start\":83125},{\"end\":83338,\"start\":83324},{\"end\":83350,\"start\":83338},{\"end\":83366,\"start\":83350},{\"end\":83652,\"start\":83641},{\"end\":83662,\"start\":83652},{\"end\":83900,\"start\":83892},{\"end\":83911,\"start\":83900},{\"end\":84194,\"start\":84183},{\"end\":84205,\"start\":84194},{\"end\":84216,\"start\":84205},{\"end\":84510,\"start\":84502},{\"end\":84894,\"start\":84886},{\"end\":84906,\"start\":84894},{\"end\":84918,\"start\":84906},{\"end\":85269,\"start\":85256},{\"end\":85277,\"start\":85269},{\"end\":85288,\"start\":85277},{\"end\":85300,\"start\":85288},{\"end\":85762,\"start\":85751},{\"end\":85777,\"start\":85762},{\"end\":85787,\"start\":85777},{\"end\":85798,\"start\":85787},{\"end\":86057,\"start\":86051},{\"end\":86063,\"start\":86057},{\"end\":86070,\"start\":86063},{\"end\":86078,\"start\":86070},{\"end\":86408,\"start\":86402},{\"end\":86423,\"start\":86408},{\"end\":86433,\"start\":86423},{\"end\":86443,\"start\":86433},{\"end\":86814,\"start\":86808},{\"end\":86821,\"start\":86814},{\"end\":87067,\"start\":87057},{\"end\":87075,\"start\":87067},{\"end\":87085,\"start\":87075},{\"end\":87586,\"start\":87576},{\"end\":87594,\"start\":87586},{\"end\":87606,\"start\":87594},{\"end\":88036,\"start\":88027},{\"end\":88043,\"start\":88036},{\"end\":88051,\"start\":88043},{\"end\":88058,\"start\":88051},{\"end\":88371,\"start\":88362},{\"end\":88383,\"start\":88371},{\"end\":88390,\"start\":88383},{\"end\":88402,\"start\":88390},{\"end\":88792,\"start\":88780},{\"end\":88805,\"start\":88792},{\"end\":88945,\"start\":88938},{\"end\":88958,\"start\":88945},{\"end\":88969,\"start\":88958},{\"end\":88982,\"start\":88969},{\"end\":88992,\"start\":88982},{\"end\":89002,\"start\":88992},{\"end\":89291,\"start\":89278},{\"end\":89305,\"start\":89291},{\"end\":89440,\"start\":89432},{\"end\":89455,\"start\":89440},{\"end\":89571,\"start\":89561},{\"end\":89589,\"start\":89571},{\"end\":89601,\"start\":89589},{\"end\":89614,\"start\":89601},{\"end\":89985,\"start\":89972},{\"end\":89997,\"start\":89985},{\"end\":90009,\"start\":89997},{\"end\":90421,\"start\":90408},{\"end\":90433,\"start\":90421},{\"end\":90445,\"start\":90433},{\"end\":90769,\"start\":90755},{\"end\":90787,\"start\":90769},{\"end\":90800,\"start\":90787},{\"end\":90812,\"start\":90800},{\"end\":91220,\"start\":91207},{\"end\":91233,\"start\":91220},{\"end\":91242,\"start\":91233},{\"end\":91254,\"start\":91242},{\"end\":91563,\"start\":91552},{\"end\":91575,\"start\":91563},{\"end\":91585,\"start\":91575},{\"end\":91597,\"start\":91585},{\"end\":91610,\"start\":91597},{\"end\":91621,\"start\":91610},{\"end\":91837,\"start\":91825},{\"end\":91849,\"start\":91837},{\"end\":91864,\"start\":91849},{\"end\":91874,\"start\":91864},{\"end\":92283,\"start\":92273},{\"end\":92299,\"start\":92283},{\"end\":92553,\"start\":92544},{\"end\":92564,\"start\":92553},{\"end\":92578,\"start\":92564},{\"end\":92591,\"start\":92578},{\"end\":92605,\"start\":92591},{\"end\":92617,\"start\":92605},{\"end\":92933,\"start\":92922},{\"end\":92945,\"start\":92933},{\"end\":93244,\"start\":93237},{\"end\":93252,\"start\":93244},{\"end\":93264,\"start\":93252},{\"end\":93631,\"start\":93623},{\"end\":93650,\"start\":93631},{\"end\":93934,\"start\":93926},{\"end\":93940,\"start\":93934},{\"end\":93946,\"start\":93940},{\"end\":93961,\"start\":93946},{\"end\":93971,\"start\":93961},{\"end\":93981,\"start\":93971},{\"end\":94364,\"start\":94353},{\"end\":94376,\"start\":94364},{\"end\":94387,\"start\":94376},{\"end\":94672,\"start\":94664},{\"end\":94683,\"start\":94672},{\"end\":94947,\"start\":94939},{\"end\":94953,\"start\":94947},{\"end\":94965,\"start\":94953},{\"end\":94972,\"start\":94965},{\"end\":95271,\"start\":95263},{\"end\":95282,\"start\":95271},{\"end\":95303,\"start\":95282},{\"end\":95311,\"start\":95303},{\"end\":95580,\"start\":95571},{\"end\":95587,\"start\":95580},{\"end\":95595,\"start\":95587},{\"end\":95601,\"start\":95595},{\"end\":96006,\"start\":95997},{\"end\":96015,\"start\":96006},{\"end\":96024,\"start\":96015},{\"end\":96038,\"start\":96024},{\"end\":96049,\"start\":96038},{\"end\":96395,\"start\":96386},{\"end\":96404,\"start\":96395},{\"end\":96416,\"start\":96404},{\"end\":96425,\"start\":96416}]", "bib_venue": "[{\"end\":75901,\"start\":75839},{\"end\":76696,\"start\":76643},{\"end\":78514,\"start\":78496},{\"end\":79026,\"start\":78954},{\"end\":79691,\"start\":79625},{\"end\":81250,\"start\":81183},{\"end\":81823,\"start\":81733},{\"end\":82326,\"start\":82259},{\"end\":82809,\"start\":82747},{\"end\":87715,\"start\":87669},{\"end\":88543,\"start\":88481},{\"end\":90091,\"start\":90083},{\"end\":66861,\"start\":66633},{\"end\":67283,\"start\":67147},{\"end\":68134,\"start\":67794},{\"end\":68872,\"start\":68724},{\"end\":69328,\"start\":69154},{\"end\":69788,\"start\":69514},{\"end\":70312,\"start\":70096},{\"end\":70958,\"start\":70707},{\"end\":71714,\"start\":71675},{\"end\":72182,\"start\":71973},{\"end\":72450,\"start\":72443},{\"end\":72693,\"start\":72536},{\"end\":73616,\"start\":72862},{\"end\":75372,\"start\":75115},{\"end\":75837,\"start\":75760},{\"end\":76293,\"start\":76244},{\"end\":76641,\"start\":76573},{\"end\":77015,\"start\":76966},{\"end\":77354,\"start\":77295},{\"end\":77738,\"start\":77634},{\"end\":78029,\"start\":77993},{\"end\":78214,\"start\":78195},{\"end\":78494,\"start\":78461},{\"end\":78952,\"start\":78865},{\"end\":79623,\"start\":79542},{\"end\":79992,\"start\":79935},{\"end\":80338,\"start\":80275},{\"end\":81181,\"start\":81099},{\"end\":81731,\"start\":81630},{\"end\":82257,\"start\":82175},{\"end\":82745,\"start\":82668},{\"end\":83109,\"start\":83059},{\"end\":83415,\"start\":83366},{\"end\":83679,\"start\":83662},{\"end\":83941,\"start\":83911},{\"end\":84246,\"start\":84216},{\"end\":84561,\"start\":84510},{\"end\":84884,\"start\":84747},{\"end\":85379,\"start\":85300},{\"end\":85827,\"start\":85798},{\"end\":86127,\"start\":86078},{\"end\":86488,\"start\":86443},{\"end\":86806,\"start\":86745},{\"end\":87226,\"start\":87085},{\"end\":87667,\"start\":87606},{\"end\":88091,\"start\":88058},{\"end\":88479,\"start\":88402},{\"end\":88832,\"start\":88805},{\"end\":89066,\"start\":89018},{\"end\":89276,\"start\":89253},{\"end\":89468,\"start\":89455},{\"end\":89699,\"start\":89630},{\"end\":90081,\"start\":90009},{\"end\":90483,\"start\":90445},{\"end\":90879,\"start\":90812},{\"end\":91292,\"start\":91254},{\"end\":91550,\"start\":91506},{\"end\":91985,\"start\":91890},{\"end\":92271,\"start\":92199},{\"end\":92542,\"start\":92469},{\"end\":92972,\"start\":92945},{\"end\":93338,\"start\":93264},{\"end\":93699,\"start\":93650},{\"end\":94031,\"start\":93981},{\"end\":94404,\"start\":94387},{\"end\":94662,\"start\":94576},{\"end\":95000,\"start\":94972},{\"end\":95349,\"start\":95311},{\"end\":95702,\"start\":95617},{\"end\":96083,\"start\":96049},{\"end\":96474,\"start\":96425}]"}}}, "year": 2023, "month": 12, "day": 17}