{"id": 213183269, "updated": "2023-10-06 17:34:55.073", "metadata": {"title": "RAB: Provable Robustness Against Backdoor Attacks", "authors": "[{\"first\":\"Maurice\",\"last\":\"Weber\",\"middle\":[]},{\"first\":\"Xiaojun\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Bojan\",\"last\":\"Karlavs\",\"middle\":[]},{\"first\":\"Ce\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Bo\",\"last\":\"Li\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks, including evasion and backdoor (poisoning) attacks. On the defense side, there have been intensive efforts on improving both empirical and provable robustness against evasion attacks; however, the provable robustness against backdoor attacks still remains largely unexplored. In this paper, we focus on certifying the machine learning model robustness against general threat models, especially backdoor attacks. We first provide a unified framework via randomized smoothing techniques and show how it can be instantiated to certify the robustness against both evasion and backdoor attacks. We then propose the first robust training process, RAB, to smooth the trained model and certify its robustness against backdoor attacks. We prove the robustness bound for machine learning models trained with RAB and prove that our robustness bound is tight. In addition, we theoretically show that it is possible to train the robust smoothed models efficiently for simple models such as K-nearest neighbor classifiers, and we propose an exact smooth-training algorithm that eliminates the need to sample from a noise distribution for such models. Empirically, we conduct comprehensive experiments for different machine learning (ML) models such as DNNs, support vector machines, and K-NN models on MNIST, CIFAR-10, and ImageNette datasets and provide the first benchmark for certified robustness against backdoor attacks. In addition, we evaluate K-NN models on a spambase tabular dataset to demonstrate the advantages of the proposed exact algorithm. Both the theoretic analysis and the comprehensive evaluation on diverse ML models and datasets shed light on further robust learning strategies against general training time attacks.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2003.08904", "mag": "3011700838", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/sp/WeberXKZL23", "doi": "10.1109/sp46215.2023.10179451"}}, "content": {"source": {"pdf_hash": "e9ddb7d8e7f78f5840741db77abb83eb23ad47ee", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2003.08904v8.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2b2505919f80d489bd429e69de3fc40cd5ad24ce", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e9ddb7d8e7f78f5840741db77abb83eb23ad47ee.txt", "contents": "\nRAB: Provable Robustness Against Backdoor Attacks\n\n\nMaurice Weber maurice.weber@inf.ethz.ch \nUniversity of Illinois at Urbana-Champaign\nUSA\n\nXiaojun Xu xiaojun3@illinois.edu \nUniversity of Illinois at Urbana-Champaign\nUSA\n\nBojan Karla\u0161 karlasb@inf.ethz.ch \nUniversity of Illinois at Urbana-Champaign\nUSA\n\nCe Zhang ce.zhang@inf.ethz.ch \nUniversity of Illinois at Urbana-Champaign\nUSA\n\nBo Li \nUniversity of Illinois at Urbana-Champaign\nUSA\n\n\u2021 \nUniversity of Illinois at Urbana-Champaign\nUSA\n\nEth Zurich \nUniversity of Illinois at Urbana-Champaign\nUSA\n\nSwitzerland \nUniversity of Illinois at Urbana-Champaign\nUSA\n\nRAB: Provable Robustness Against Backdoor Attacks\n\nRecent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks, including evasion and backdoor (poisoning) attacks. On the defense side, there have been intensive efforts on improving both empirical and provable robustness against evasion attacks; however, the provable robustness against backdoor attacks still remains largely unexplored. In this paper, we focus on certifying the machine learning model robustness against general threat models, especially backdoor attacks. We first provide a unified framework via randomized smoothing techniques and show how it can be instantiated to certify the robustness against both evasion and backdoor attacks. We then propose the first robust training process, RAB, to smooth the trained model and certify its robustness against backdoor attacks. We prove the robustness bound for machine learning models trained with RAB and prove that our robustness bound is tight. In addition, we theoretically show that it is possible to train the robust smoothed models efficiently for simple models such as Knearest neighbor classifiers, and we propose an exact smoothtraining algorithm that eliminates the need to sample from a noise distribution for such models. Empirically, we conduct comprehensive experiments for different machine learning (ML) models such as DNNs, support vector machines, and K-NN models on MNIST, CIFAR-10, and ImageNette datasets and provide the first benchmark for certified robustness against backdoor attacks. In addition, we evaluate K-NN models on a spambase tabular dataset to demonstrate the advantages of the proposed exact algorithm. Both the theoretic analysis and the comprehensive evaluation on diverse ML models and datasets shed light on further robust learning strategies against general training time attacks. Clean Feature Training Set Backdoor'ed Training Set \u2032 Attack RAB Training RAB Training Clean Model Poinsoned Model \"\n\nIntroduction\n\nBuilding machine learning algorithms that are robust to adversarial attacks has been an emerging topic over the last decade. There are mainly two different types of adversarial attacks: (1) evasion attacks, in which the attackers manipulate the test examples against a trained machine learning (ML) model, and (2) data poisoning attacks, in which the attackers are allowed to perturb the training set. Both types of attacks have attracted intensive interests from academia as well as industry [14], [53], [57], [61].\n\nIn response, several empirical solutions have been proposed as defenses against evasion attacks [5], [31], [55], [59]. For instance, adversarial training has been proposed to retrain the ML models with generated adversarial examples [33]; quantization has been applied to either inputs or Figure 1: In this paper, we define a robust training process RAB against backdoor attacks. Given a poisoned dataset D \u2032 -produced by adding backdoor patterns \u2206 to some instances in the dataset D with clean features -this robust training process guarantees that, for all test examples x, A D \u2032 (x) = A D (x), with high probability when the magnitude of the backdoor pattern \u2206 is within the certification radius.\n\nneural network weights to defend against potential adversarial instances [55]. However, recent studies have shown that these defenses are not resilient against intelligent adversaries responding dynamically to the deployed defenses [1], [5].\n\nAs a result, one recent, exciting line of research aims to develop certifiably robust algorithms against evasion attacks, including both deterministic and probabilistic certification approaches [28]. In particular, among these certified robustness approaches, only randomized smoothing and its variations are able to provide certified robustness against evasion attacks on large-scale datasets such as ImageNet [10], [25], [58]. Intuitively, the randomized smoothing-based approaches are able to certify the robustness of a smoothed classifier, by outputting a consistent prediction for an adversarial input as long as the perturbation is within a certain radius. The smoothed classifier is obtained by taking the expectation over the possible outputs given a set of randomized inputs which are generated by adding noise drawn from a certain distribution.\n\nDespite these recent developments on certified robustness against evasion attacks, only empirical studies have been conducted to defend against backdoor attacks [13], [16], [27], [50], and the question of how to improve and certify the robustness of given machine learning models against backdoor attacks remains largely unanswered. To the best of our knowledge, there is no certifiably robust strategy to deal with backdoor attacks yet. Naturally, we ask: Can we develop certifiably robust ML models against backdoor attacks?\n\nIt is clear that extending existing certification meth-ods against evasion attacks to certifying training-time attacks is challenging given these two significantly different threat models. For instance, even certifying a label flipping training-time attack is non-trivial as illustrated in a concurrent work [40], which proposes to certify against a label flipping attack by setting a limit to how many labels in the training set may be flipped such that it does not affect the final prediction leveraging randomized smoothing. As backdoor attacks involve both label flipping and instance pattern manipulations, providing certifications can be even more challenging.\n\nIn particular, to carry out a backdoor attack, an attacker adds small backdoor patterns to a subset of training instances such that the trained model is biased toward test images with the same patterns [8], [15]. Such attacks can be applied to various real-world scenarios such as online face recognition systems [8], [27]. In this paper, we present the first certification process, referred to as RAB, which offers provable robustness for ML models against backdoor attacks. As shown in Figure 1, our certification goal is to guarantee that a test instance, which may contain backdoor patterns, will be classified the same, independent of whether the models were trained on data with or without backdoors, as long as the embedded backdoor patterns are within an L p -ball of radius R. We formally define the corresponding threat model and our certification goal in Section 3.\n\nOur approach to achieving this is mainly inspired by randomized smoothing, a technique to certify robustness against evasion attacks [10], but goes significantly beyond it due to the different settings (e.g. evasion and backdoor attacks). Our first step/contribution is to develop a general theoretical framework to generalize randomized smoothing to a much larger family of functions and smoothing distributions. This allows us to support cases in which a classifier is a function that takes as input a test instance and a training set. With our framework, we can (1) provide robustness certificates against both evasion and dataset poisoning attacks;\n\n(2) certify any classifier which takes as input a tuple of test instance and training dataset and (3) prove that the derived robustness bound is tight. Given this general framework, we can enable a basic version of the proposed RAB framework. At a high level, as shown in Figure 2, given training set D, RAB generates N additional \"smoothed\" training sets D+\u03f5 i by adding noise \u03f5 i (i \u2208 {1, . . . , N }) drawn from a certain smoothing distribution and, for each of these N training sets, a corresponding classifier is trained resulting in an ensemble of N different classifiers. These models are then aggregated to generate a \"smoothed classifier\" for which we prove that its output will be consistent regardless of whether there are backdoors added during training, as long as the backdoor patterns satisfy certain conditions. However, this basic version is not enough to provide satisfactory certified robustness against backdoor attacks. When we instantiate our theoretical framework with a practical training pipeline to provide certified robustness against backdoor attacks, we need to further develop nontrivial techniques to improve two aspects: (1) Certification Radius and (2) Certification Efficiency. Our second step/contribution are two non-trivial technical optimizations. (1) To improve the certification radius, we certify DNN classifiers with a data augmentation the scheme enabled by hash functions and, in the meantime, explore different design decisions such as the smoothness of the training process. This provides additional guidance for improving the certified robustness against backdoor attacks and we hope that it can inspire other researches in the future. (2) To improve the certification efficiency, we observed that for certain families of classifiers, namely K-nearest neighbor classifiers, we can develop an efficient algorithm to compute the smoothing result exactly, eliminating the need to resort to Monte Carlo algorithms as for generic classifiers.\n\nOur third contribution is an extensive benchmark, evaluating our framework RAB on multiple machine learning models and provide the first collection of certified robustness bounds on a diverse range of datasets, namely MNIST, CIFAR-10, ImageNette, as well as spambase tabular data. We hope that these experiments and benchmarks can provide future directions for improving the robustness of ML models against backdoors.\n\nBeing the first result on certified robustness against backdoor attacks, we believe that these results can be further improved by future research endeavours inspired by this work. We make the code and evaluation protocol publicly available with the hope to facilitate future research by the community.\n\nSummary of Technical Contributions. Our technical contributions are as follows: \u2022 We propose a unified framework to certify the model robustness against both evasion and backdoor attacks and prove that our robustness bound is tight. \u2022 We provide the first certifiable robustness bound for general machine learning models against backdoor attacks considering different smoothing noise distributions. \u2022 We propose an exact efficient smoothing algorithm for K-NN models without needing to sample random noise during training. \u2022 We conduct extensive reproducible large-scale experiments and provide a benchmark for certified robustness against three representative backdoor attacks for multiple types of models (e.g., DNNs, support vector machines, and K-NN) on diverse datasets. We also provide a series of ablation studies to further analyze the factors that affect model robustness against backdoors.\n\nOutline. The remainder of this paper is organized as follows. Section 2 provides background on backdoor attacks and related verifiable robustness techniques, followed by the threat model and method overview in Section 3. Section 4 presents the proposed general theoretical framework for certifying robustness against evasion and poisoning attacks, the tightness of the derived robustness bound, and sheds light on a connection between statistical hypothesis testing and certifiable robustness. Section 5 explains in detail the proposed approach RAB for certifying robustness against backdoor attacks under the general framework with Gaussian and uniform noise distributions. Section 6 analyzes the robustness properties of DNNs and K-NN classifiers and presents algorithms to certify robustness for such models (mainly with binary classifiers). Experimental results are presented in section 7. Finally, Section 8 puts our results in context with existing work, Section 9 discusses the limitations of our work, and Section 10 concludes.\n\n\nBackground\n\nIn this section, we provide an overview of different backdoor attacks and briefly review the randomized smoothing technique for certifying robustness against evasion attacks.\n\n\nBackdoor attacks\n\nA backdoor attack aims to inject certain \"backdoor\" patterns into the training set and associate such patterns with a specific adversarial target (label). As a result, during testing time, any test instance with such a pattern will be misclassified as the preselected adversarial target [8], [16]. ML models with injected backdoors are called backdoored models and they are typically able to achieve performance similar to clean models on benign data, making it challenging to detect whether the model has been backdoored.\n\nThere are several ways to categorize backdoor attacks. First, based on the adversarial target design, the attacks can be characterized either as single target attacks or all-to-all attacks. In a single target attack, the backdoor pattern will cause the poisoned classifier to always return a designed target label. An all-to-all attack leverages the backdoor pattern to permute the classifier results.\n\nThe second categorization is based on different types of backdoor patterns. There are region based and blending backdoor attacks. In the region based attack, a specific region of the training instance is manipulated in a subtle way that will not cause human notification [16], [61]. In particular, it has been shown that such backdoor patterns can be as small as only one or four pixels [48]. On the other hand, Chen et al. [8] shows that by blending the whole instance with a certain pattern such as a fixed random noise pattern, it is possible to generate effective backdoors to poison the ML models.\n\nIn this work, we focus on certifying the robustness against general backdoor attacks, where the attacker is able to add any specific or uncontrollable random backdoor patterns for arbitrary adversarial targets.\n\n\nRandomized smoothing\n\nTo defend against evasion attacks, different approaches have been studied: some provide empirical approaches such as adversarial training [4], [30], and some provide theoretical guarantees against L p bounded adversarial perturbations. In particular, Cohen et al. [10] have proposed randomized smoothing to certify the robustness of ML models against the L 2 norm bounded evasion attacks.\n\nOn a high level, the randomized smoothing technique [10] provides a way to certify the robustness of Then, the classification gap between a lower bound of the confidence on the top-1 class p A and an upper bound of the confidence on the top-2 class p B are obtained. The smoothed classifier will be guaranteed to provide consistent predictions within the perturbation radius, which is a function of the standard deviation \u03c3 of the smoothing noise, and the gap between the class probabilities p A and p B , for each test instance.\n\nHowever, all these approaches focus on the robustness against evasion attacks only. In contrast, in this work, we aim to provide a function smoothing framework to certify the robustness against both evasion and poisoning attacks.\n\nIn particular, the current randomized smoothing strategy focuses on adding noise to induce smoothness on the level of test instance, while our unified framework generalizes this to smoothing on the level of classifiers. Putting this generalization into practice in the context of certifying robustness against backdoor attacks naturally bears additional challenges which we describe and address in detail. In addition, we provide theoretical robustness guarantees for different machine learning models, smoothing noise distributions, as well as the tightness of the robustness bounds.\n\n\nThreat Model and Method Overview\n\nHere we first define the threat model including concise definitions of a backdoor attack, and then introduce the method overview, where we define our robustness guarantee.\n\n\nNotation\n\nWe write random variables as uppercase letters X and use the notation P X to denote the probability measure induced by X and write f X to denote the probability density function. Realizations of random variables are written in lowercase letters. For discrete random variables, we use lowercase letters to denote their probability mass function, e.g. p(y) for distribution over labels. Feature vectors are taken to be d-dimensional real vectors x \u2208 R d and the set of labels y for a C-multiclass classification problem is given by C = {1, . . . , C}. A training set D consists of n (feature, label)-pairs D = {(x 1 , y 1 ), . . . , (x n , y n )}. For a dataset D and a collection of n feature vectors d = {d 1 , . . . , d n }, we write D + d to denote the set {(x 1 + d 1 , y 1 ), . . . , (x n + d n , y n )}. We view a classifier as a deterministic function that takes as input a tuple with a test instance x and training set D and returns a class label y \u2208 C. Formally, given a dataset D and a test instance x, a classifier h learns a conditional probability distribution p(y| x, D) over class labels and outputs the label which is deemed most likely under the learned distribution p:\nh(x, D) = arg max y p(y| x, D).(1)\nWe omit the dependence on model parameters throughout this paper and tacitly assume that the model is optimized based on training dataset D via some optimization schemes such as stochastic gradient descent. \n\n\nThreat\nD = {(x i , y i )}.\nThe attacker has in mind a target backdoor pattern \u2126 x and a target class\u1ef9 and the adversarial goal is to alter the dataset such that, given a clean test example x, adding the backdoor pattern to x (i.e., x + \u2126 x ) will alter the classifier output\u1ef9 with high probability. In general, the attack can replace r training instances (x i , y i ) by backdoored instances (x i + \u2126 x ,\u1ef9 i ). We remark that the attacker could embed distinct patterns to each instance and our result naturally extends to this case. Thus, summarizing the backdoor patterns as the collection \u2206(\u2126 x ) := {\u03b4 1 , . . . , \u03b4 r , 0, . . . , 0}, we formalize a backdoor attack as the transformation (D, \u2126 x ,\u1ef9) \u2192 D BD (\u2126 x ,\u1ef9) with\nD BD (\u2126 x ,\u1ef9) = {(x i + \u03b4 i ,\u1ef9 i )} r i=1 \u222a {(x i , y i )} n i=r+1(2)\nWe often write D BD (\u2126 x ) instead of D BD (\u2126 x ,\u1ef9) when our focus is on the backdoor pattern \u2126 x instead of the target class\u1ef9. The backdoor attack succeeds on test example\nx whenever h(x + \u2126 x , D BD (\u2126 x )) =\u1ef9(3)\n3.2.2. Goal of Defense. One natural goal to defend against the above backdoor attack is to ensure that the prediction of h(x + \u2126 x , D BD (\u2126 x )) is independent of the backdoor patterns \u2206(\u2126 x ) which are present in the dataset, i.e.,\nh(x + \u2126 x , D BD (\u2126 x )) = h(x + \u2126 x , D BD (\u2205))(4)\nwhere D BD (\u2205) is the dataset without any embedded backdoor patterns (\u03b4 i = 0). When this is true, the attacker obtained no additional information by knowing the pattern \u2126 x embedded in the training set. That is to say, given a test instance which may contain a backdoor pattern, its prediction stays the same, independent of whether the models were trained with or without backdoors. We assume that the defender has full control of the training process. See Section 9 for more discussions on the assumptions and limitations of RAB.\n\n\nMethod Overview\n\n3.3.1. Certified Robustness against Backdoor Attacks. We aim to obtain robustness bound R such that, whenever the sum of the magnitude of backdoors is below R, the prediction of the backdoored classifier is the same as when the classifier is trained on benign data. Formally, if D BD (\u2126 x ) denotes the backdoored training set, and D the training set containing clean features, we say that a classifier is provably robust whenever\nr i=1 \u2225\u03b4 i \u2225 2 2 < R implies that h(x + \u2126 x , D BD (\u2126 x )) = h(x + \u2126 x , D BD (\u2205)).\nOur approach to obtaining the aforementioned robustness guarantee is based on randomized smoothing, which leads to the robust RAB training pipeline, as is illustrated in Figure 2. Given a clean dataset D and a backdoored dataset D BD (\u2126 x ), the goal of the defender is to make sure that the prediction on test instances embedded with the pattern \u2126 x is the same as for models trained with D BD (\u2205).\n\nDifferent from randomized smoothing-based certification against evasion attacks, here it is not enough to only smooth the test instances. Instead, in RAB, we will first add noise vectors, sampled from a smoothing distribution, to the given training instances, to obtain a collection of \"smoothed\" training sets. We subsequently train a model on each training set and aggregate their final outputs together as the final \"smoothed\" prediction. After this process, we show that it is possible to leverage the Neyman Pearson lemma to derive a robustness condition for this smoothed RAB training process. Additionally, the connection with the Neyman Pearson lemma also allows us to prove that the robustness bound is tight. Note that the RAB framework requires the training instances to be \"smoothed\" by a set of independent noises drawn from a certain distribution.\n\nAdditional Challenges. We remark that, within this RAB training and certification process, there are several additional challenges. First, after adding noise to the training data, the clean accuracy of the trained classifier typically drops due to the distribution shift in the training data. To mitigate this problem, we add a deterministic value, based on the hash of the trained model, to test instances (Section 6), which minimizes the distribution shift and leads to improved accuracy scores. Second, considering different smoothing distributions for the training data, we provide rigorous analysis and a robustness bound for both Gaussian and uniform smoothing distributions (Section 5). Third, we note that the proposed training process requires sampling a large number of randomly perturbed training sets. As this is computationally expensive, we propose an efficient PTIME algorithm for K-NN classifiers (Section 6).\n\nOutline. In the following, we illustrate the RAB pipeline in three steps. In Section 4, we introduce the theoretical foundations for a unified framework for certifying robustness against both evasion and backdoor attacks. In Section 5, we introduce how to apply our unified framework to defend against backdoor attacks. In Section 6, we present RAB pipeline for two types of models -DNNs and K-NN.\n\n\nUnified Framework for Certified Robustness\n\nIn this section, we propose a unified theoretical framework for certified robustness against evasion and poisoning attacks for classification models. Our framework is based on the intuition that randomizing the prediction or training process will \"smoothen\" the final prediction and therefore reduce the vulnerability to adversarial attacks. This principle has been successfully applied to certifying robustness against evasion attacks for classification models [10]. We first formally define the notion of a smoothed classifier where we extend upon previous work by randomizing both the test instance and the training set. We then introduce basic terminology of hypothesis testing, from where we leverage the Neyman Pearson lemma to derive a generic robustness condition in Theorem 1. Finally, we show that this robustness condition is tight.\n\n\nPreliminaries\n4.1.1. Smoothed Classifiers.\nOn a high level, a smoothed classifier g is derived from a base classifier h by introducing additive noise to the input consisting of test and training instances. In a nutshell, the intuition behind randomized smoothing classifiers is that noise reduces the occurrence of regions with high curvature in the decision boundaries, resulting in reduced vulnerability to adversarial attacks. Recall that a classifier h, here serving as a base classifier, is defined as h(x, D) = arg max y p(y| x, D) where p is learned from a dataset D and defines a conditional probability distribution over labels y. The final prediction is given by the most likely class under this learned distribution. A smoothed classifier is defined by\nq(y| x, D) = P X,D (h(x + X, D + D) = y)(5)\nwhere we have introduced random variables X \u223c P X and D \u223c P D which act as smoothing distributions and are assumed to be independent. We emphasize that D is a collection of n independent and identically distributed random variables D (i) , each of which is added to a training instance in D. The final, smoothed classifier then assigns the most likely class to an instance x under this new, \"smoothed\" model q, so that g(x, D) = arg max y q(y| x, D).\n\nWithin this formulation of a smoothed classifier, we can also model randomized smoothing for defending against evasion attacks by setting the training set noise to be zero, i.e. D \u2261 0. We emphasize at this point that the smoothed classifier g implicitly depends on the choice of noise distributions P X and P D . In section 5 we instantiate this classifier with Gaussian noise and with uniform noise and show how this leads to different robustness bounds.\n\n\nStatistical Hypothesis Testing.\n\nHypothesis testing is a statistical problem that is concerned with the question of whether or not some hypothesis that has been formulated is correct. A decision procedure for such a problem is called a statistical hypothesis test. Formally, the decision is based on the value of a realization x for a random variable X whose distribution is known to be either P 0 (the null hypothesis) or P 1 (the alternative hypothesis). Given a sample x \u2208 X , a randomized test \u03d5 can be modeled as a function \u03d5 : X \u2192 [0, 1] which rejects the null hypothesis with probability \u03d5(x) and accepts it with probability 1 \u2212 \u03d5(x). The two central quantities of interest are the probabilities of making a type I error, denoted by \u03b1(\u03d5; P 0 ) and the probability of making a type II error, denoted by \u03b2(\u03d5; P 1 ).\n\nThe former corresponds to the situation where the test \u03d5 decides for the alternative when the null is true, while the latter occurs when the alternative is true but the test decides for the null. Formally, \u03b1 and \u03b2 are defined as\n\u03b1(\u03d5; P 0 ) = E 0 (\u03d5(X)), \u03b2(\u03d5; P 1 ) = E 1 (1 \u2212 \u03d5(X)) (7)\nwhere E 0 (\u00b7) (E 1 (\u00b7)) denotes the expected value with respect to P 0 (P 1 ). The problem is to select the test \u03d5 which minimizes the probability of making a type II error, subject to the constraint that the probability of making a type-I error is below a given threshold \u03b1 0 . The Neyman Pearson lemma [35] states that a likelihood ratio test \u03d5 N P is optimal, i.e. that \u03b1(\u03d5 N P ; P 0 ) = \u03b1 0 and \u03b2(\u03d5 N P ; P 1 ) = \u03b2 * (\u03b1 0 ; P 0 , P 1 ) where \u03b2 * (\u03b1 0 ; P 0 , P 1 ) = inf \u03d5 : \u03b1(\u03d5; P0)\u2264\u03b10 \u03b2(\u03d5; P 1 ).\n\nIn Theorem 1, we will see that we can leverage this formalism to get a robustness guarantee for smoothed classifiers. Additionally, stemming from the optimality of the likelihood ratio test, we show in Theorem 2 that this condition is tight.\n\n\nA General Condition for Provable Robustness\n\nIn this section, we derive a tight robustness condition by drawing a connection between statistical hypothesis testing and the robustness of classification models subject to adversarial attacks. We allow adversaries to conduct an attack on either (i) the test instance x, (ii) the training set D or (iii) a combined attack on test and training set. The resulting robustness condition is of a general nature and is expressed in terms of the optimal type II errors for likelihood ratio tests. We remark that this theorem is a more general version of the result presented in [10], by extending it to general smoothing distributions and smoothing on the training set. In Section 5 we will show how this result can be used to obtain robustness bound in terms of L p -norm bounded backdoor attacks. We show that smoothing on the training set makes it possible for certifying the robustness against backdoors, and the general smoothing distribution allows us to explore the robustness bound certified by different smoothing distributions. Theorem 1. Let q be the smoothed classifier as in (5) with smoothing distribution Z := (X, D) with X taking values in R d and D being a collection of n independent R d -valued random variables, D = (D (1) , . . . , D (n) ). Let \u2126 x \u2208 R d and let \u2206 := (\u03b4 1 , . . . , \u03b4 n ) for backdoor patterns \u03b4 i \u2208 R d . Let y A \u2208 C and let p A , p B \u2208 [0, 1] such that y A = g(x, D) and\nq(y A | x, D) \u2265 p A > p B \u2265 max y\u0338 =y A q(y| x, D).(9)\nIf the optimal type II errors, for testing the null Z \u223c P 0 against the alternative Z + (\u2126 x , \u2206) \u223c P 1 , satisfy\n\u03b2 * (1 \u2212 p A ; P 0 , P 1 ) + \u03b2 * (p B ; P 0 , P 1 ) > 1,(10)\nthen it is guaranteed that y A = arg max y q(y| x + \u2126 x , D + \u2206).\n\nThe following is a short sketch of the proof for this theorem. We refer the reader to Appendix A.1 for details.\n\nProof (Sketch). We first explicitly construct the likelihood ratio tests \u03d5 A and \u03d5 B for testing the null hypothesis Z against the alternative Z + (\u2126 x , \u2206) with type I errors \u03b1(\u03d5 A ; P 0 ) = 1 \u2212 p A and \u03b1(\u03d5 B ; P 0 ) = p B respectively. An argument similar to the Neyman-Pearson Lemma [35] shows that the class probability for y A given by q on the perturbed input is lower bounded by \u03b2(\u03d5 A ; P 1 ) = \u03b2 * (1\u2212p A ; P 0 , P 1 ). A similar reasoning leads to the fact that an upper bound on the prediction score for y \u0338 = y A on the perturbed input is given by 1 \u2212 \u03b2(\u03d5 B ; P 1 ) = 1 \u2212 \u03b2 * (p B ; P 0 , P 1 ). Combining this leads to condition (10).\n\nWe now make some observations about Theorem 1 to get intuition on the robustness condition (10):\n\n\u2022 Different smoothing distributions lead to robustness bounds in terms of different norms. For example, Gaussian noise yields robustness bound in L 2 norm while Uniform noise leads to other L p norms. \u2022 The robustness condition (10) does not make any assumption on the underlying classifier other than on the class probabilities predicted by its smoothed version. \u2022 The random variable Z + (\u2126 x , \u2206) models a general adversarial attack including evasion and backdoor attacks. \u2022 If no attack is present, i.e., if (\u2126 x , \u2206) = (0, 0), then we get the trivial condition p A > p B . \u2022 As p A increases, the optimal type II error increases for given backdoor (\u2126 x , \u2206). Thus, in the simplified setup where p A + p B = 1 and the robustness condition reads \u03b2 * (1 \u2212 p A ; P 0 , P 1 ) > 1/2, the distribution shift caused by (\u2126 x , \u2206) can increase. Thus, as the smoothed classifier becomes more confident, the robust region becomes larger. While the generality of Theorem 1 allows us to model a multitude of threat models, it bears the challenge of how one should instantiate this theorem such that it is applicable to defend against a specific adversarial attack. In addition to the flexibility with regard to the underlying threat model, we are also provided with flexibility regarding the smoothing distributions, resulting in different robustness guarantees. This again begs the question, of which smoothing distribution results in useful robustness bounds. In Section 5, we will show how this theorem can be applied to obtain the robustness guarantee against backdoor attacks described in Section 3.\n\nNext, we show that our robustness condition is tight in the following sense: If (9) is all that is known about the smoothed classifier g, then there is no perturbation (\u2126 x , \u2206) that violates (10). On the other hand, if (10) is violated, then we can always construct a smoothed classifier g * such that it satisfies the class probabilities (9) but is not robust against this perturbation. (10), then there exists a base classifier h * such that the smoothed classifer g * is consistent with the class probabilities (9) and for which g * (x + \u2126 x , D + \u2206) \u0338 = y A .\nTheorem 2. Suppose that 1 \u2265 p A + p B \u2265 1 \u2212 (C \u2212 2) \u00b7 p B . If the adversarial perturbations (\u2126 x , \u2206) violate\n\nProvable Robustness Against Backdoors\n\nIt is not straightforward to use the result from Theorem 1 to get a robustness certificate against backdoor attacks in terms of L p -norm bounded backdoor patterns. In this section, we aim to answer the question: how can we instantiate this result to obtain robustness guarantees against backdoor attacks? In particular, we show that by leveraging Theorem 1, we obtain the robustness guarantee defined in Section 3. To that end, we derive robustness bounds for smoothing with isotropic Gaussian noise and we also illustrate how to derive certification bounds using other smoothing distributions. Since isotropic Gaussian noise leads to a better radius, we will use this distribution in our experiments as a demonstration.\n\n\nMethod Outline\n\n5.1.1. Intuition. Suppose that we are given a base classifier that has been trained on a backdoored dataset that contains r training samples which are infected with backdoor patterns \u2206(\u2126 x ). Our goal is to derive a condition on the backdoor patterns \u2206(\u2126 x ) such that the prediction for x + \u2126 x with a classifier trained on the backdoored dataset D BD (\u2206(\u2126 x )) is the same as the prediction (on the same input) that a smoothed classifier would have made, had it been trained on a dataset without the backdoor triggers, D BD (\u2205). In other words, we obtain the guarantee that an attacker can not achieve their goal of systematically leading the test instance with the backdoor pattern to the adversarial target, meaning they will always obtain the same prediction as long as the added pattern \u03b4 satisfies certain conditions (bounded magnitude).\n\n\nGaussian\n\nSmoothing. We obtain this certificate by instantiating Theorem 1 in the following way. Suppose an attacker injects backdoor patterns \u2206(\u2126 x ) = {\u03b4 1 , . . . , \u03b4 r } \u2282 R d to r \u2264 n training instances of the training set D, yielding the backdoored training set D BD (\u2206(\u2126 x )). We then train the base classifier on this poisoned dataset, augmented with additional noise on the feature vectors D BD (\u2206(\u2126 x )) + D, where D is the smoothing noise added to the training features. We obtain a prediction of the smoothed classifier g by taking the expectation with respect to the distribution of the smoothing noise D. Suppose that the smoothed classifier obtained in this way predicts a malicious instance x+\u2126 x to be of a certain class with probability at least p A and the runnerup class with probability at most p B . Our result tells us that, as long as the introduced patterns satisfy condition (10), we get the guarantee that the malicious test input would have been classified equally as when the classifier had been trained on the dataset with clean features D BD (\u2205). In the case where the noise variables are isotropic Gaussians with standard deviation \u03c3, the condition (10) yields a robustness bound in terms of the sum of L 2 -norms of the backdoors.\nCorollary 1 (Gaussian Smoothing). Let \u2206 = (\u03b4 1 , . . . , \u03b4 n )\nand \u2126 x be R d -valued backdoor patterns and let D be a training set. Suppose that for each i, the smoothing noise on the training features is\nD (i) iid \u223c N (0, \u03c3 2 1 d ). Let y A \u2208 C such that y A = g(x + \u2126 x , D + \u2206) with class probabilities satisfying q(y A | x + \u2126 x , D + \u2206) \u2265 p A > p B \u2265 max y\u0338 =y A q(y| x + \u2126 x , D + \u2206). (11)\nThen, if the backdoor patterns are bounded by\nn i=1 \u2225\u03b4 i \u2225 2 2 < \u03c3 2 \u03a6 \u22121 (p A ) \u2212 \u03a6 \u22121 (p B ) ,(12)\nit is guaranteed\ny A = g(x + \u2126 x , D) = g(x + \u2126 x , D + \u2206).\nThis result shows that, whenever the norms of the backdoor patterns are below a certain value, we obtain the guarantee that the classifier makes the same prediction on the test data with backdoors as it does when trained without embedded patterns in the training set. We can further simplify the robustness bound in (12) if we can assume that an attacker poisons at most r \u2264 n training instances with one single pattern \u03b4. In this case, the bound (12) is given by\n\u2225\u03b4\u2225 2 < \u03c3 2 \u221a r \u03a6 \u22121 (p A ) \u2212 \u03a6 \u22121 (p B ) .(13)\nWe see that, as we know more about the capabilities of an attacker and the nature of the backdoor patterns, we are able to certify a larger robustness radius, proportional to 1/ \u221a r.\n\n\nOther Smoothing Distributions\n\nGiven the generality of our framework, it is possible to derive certification bounds using other smoothing distributions. However, different smoothing distributions have vastly different performance and a comparative study among different smoothing distributions is interesting future work. In this paper, we will just illustrate one example of smoothing using a uniform distribution.\nCorollary 2 (Uniform Smoothing). Let \u2206 = (\u03b4 1 , . . . , \u03b4 n )\nand \u2126 x be R d valued backdoor patterns and let D be a training set. Suppose that for each i, the smoothing noise on the training features is\nD (i) iid \u223c U([a, b]). Let y A \u2208 C such that y A = g(x + \u2126 x , D + \u2206) with class probabilities satisfying q(y A | x + \u2126 x , D + \u2206) \u2265 p A > p B \u2265 max y\u0338 =y A q(y| x + \u2126 x , D + \u2206). (14)\nThen, if the backdoor patterns satisfy\n1 \u2212 p A \u2212 p B 2 < n i=1 d j=1 1 \u2212 |\u03b4 i,j | b \u2212 a +(15)\nwhere\n(x) + = max{x, 0}, it is guaranteed that y A = g(x + \u2126 x , D) = g(x + \u2126 x , D + \u2206).\nAs in the Gaussian case, the robustness bound in (15) can again be simplified in a similar fashion, if we assume that an attacker poisons at most r \u2264 n training instances with one single pattern \u03b4. In this case, the bound (15) is given by\n1 \u2212 p A \u2212 p B 2 < d j=1 1 \u2212 |\u03b4 j | b \u2212 a + r .(16)\nWe see again that, as the number of infected training samples r gets smaller, this corresponds to a larger bound since the RHS of (16) gets larger. In other words, if we know that the attacker injects fewer backdoors, then we can certify a backdoor pattern with a larger magnitude.\n\nDiscussions. We emphasize that in this paper, we focus on protecting the system against attackers who aim to trigger a targeted error with a specific backdoor pattern. The system can still be vulnerable to other types of poisoning attacks. One such example is the label flipping attack, in which one flips the labels of a subset of examples while keeping the features untouched. Interestingly, one concurrent work explored the possibility of using randomized smoothing to defend against label flipping attack [40]. Developing a single framework to be robust against both backdoor and label flipping attacks is an exciting future direction, and we expect it to require nontrivial extensions of both approaches to achieve non-trivial certified accuracy. Furthermore, while we focus the experiments on Gaussian smoothing and L 2norm guarantees, it is in principle possible to certify other L p -norms with different smoothing distributions. For evasion attacks, [29] use exponential smoothing noise with certificates in L 1 -norm. Such analysis of different smoothing distributions for different experimental settings goes beyond the scope of this work and is interesting for future research.\n\n\nInstantiating the General Framework with Specific ML Models\n\nIn the preceding sections, we presented our approach to certifying robustness against backdoor attacks. Here, we will analyze and provide detailed algorithms for the RAB training pipeline for two types of machine learning models:\nAlgorithm 1 DNN-RAB for training certifiably robust DNNs. Require: Poisoned training dataset D = {(x i + \u03b4 i ,\u1ef9 i ) n i=1 }, noise scale \u03c3, model number N 1: for k = 1, . . . , N do 2: Sample \u03f5 k,1 , . . . , \u03f5 k,n iid \u223c N (0, \u03c3 2 1 d ). 3: D k = {(x i + \u03b4 i + \u03f5 k,i ,\u1ef9 i ) n i=1 }. 4: h k = train_model(D k ). 5: Sample u k from N (0, \u03c3 2 1 d ) deterministically with random seed based on hash(h k ). 6: end for 7: return Model collection {(h 1 , u 1 ), . . . , (h N , u N )} Algorithm 2 Certified inference with RAB-trained models. Require: Test sample x, noise scale \u03c3, models {(h k , u k )} N k=1 , backdoor magnitude \u2225\u03b4\u2225 2 , number of poisoned training samples r 1: counts = |{k : h k (x + u k , D + \u03f5 k ) = y}| for y = 1, . . . , C 2: y A , y B = top two indices in counts 3: n A , n B = counts[y A ], counts[y B ] 4: p A , p B = calculate_bound(n A , n B , N, \u03b1). 5: if p A > p B then 6: R = \u03c3 2 \u221a r \u03a6 \u22121 (p A ) \u2212 \u03a6 \u22121 (p B ) 7: if R \u2265 \u2225\u03b4\u2225 2 then 8:\nreturn prediction y A , robust radius R.\n\n\n9:\n\nend if 10: end if 11: return ABSTAIN deep neural networks and K-nearest neighbor classifiers. The success of backdoor poisoning attacks against DNNs has caused a lot of attention recently. Thus, we first aim to evaluate and certify the robustness of DNNs against backdoor attacks. Secondly, given the fact that K-NN models have been widely applied in different applications, either based on raw data or on trained embeddings, it is of great interest to know about the robustness of this type of ML models. Specifically, we are inspired by a recent result [22] and develop an exact efficient smoothing algorithm for K-NN models, such that we do not need to draw a large number of random samples from the smoothing distribution for these models. This makes our approach considerably more practical for this type of classifier as it avoids the expensive training of a large number of models, as is required with generic classification algorithms including DNNs.\n\n\nDeep Neural Networks\n\nIn this section, we consider smoothed models which use DNNs as base classifiers. For a given test input x test , the goal is to calculate the prediction of g on (x test , D + \u2206) according to Corollary 1 and the corresponding certified bound given in the right hand side of Eq. (12). In the following, we first describe the training process and then the inference algorithm.\n6.1.1. RAB Training for DNNs. First, we draw N samples d 1 , . . . , d N from the distribution of D \u223c n i=1 N (0, \u03c3 2 1 d ).\nGiven the N samples of training noise (each consisting of |D| = n noise vectors), we train N DNN models on the datasets D + d k for k = 1, . . . , N and obtain classifiers h 1 , . . . , h N . Along with each model h k , we draw a random noise u k from N (0, \u03c3 2 1 d ) with a random seed based on the hash of the trained model file. This noise vector is stored along with the model parameters and added to each test input during inference. The reason for this is that, empirically, we observed that inputting test samples without this additional augmentation leads to poor prediction performance since the ensemble of models {h 1 , . . . , h N } has to classify an input that has not been perturbed by Gaussian noise, while it has only \"seen\" noisy samples, leading to a mismatch between training and test distributions. Algorithm 1 shows the pseudocode describing RAB-training for DNN models.\n\n\nInference.\n\nTo get the prediction of the smoothed classifier on a test sample x test we first compute the empirical majority vote as an unbiased estimat\u00ea\nq(y| x, D) = |{k : h k (x test + u k , D + d k ) = y}| N(17)\nof the class probabilities and where u k is the (model-) deterministic noise vector sampled during training in Algorithm 1. Second, for a given error tolerance \u03b1, we compute p A and p B using one-sided (1 \u2212 \u03b1) lower confidence intervals for the binomial distribution with parameters n A and n B and N samples. Finally, we invoke Corollary 1 and first compute the robust radius according to Eq. (13), based on p A , p B the smoothing noise parameter \u03c3 and the number of poisoned training samples r. If the resulting radius R is larger than the magnitude of the backdoor samples \u03b4, the prediction is certified, i.e. the backdoor attack has failed on this particular sample. Algorithm 2 shows the pseudocode for the DNN inference with RAB.\n\n6.1.3. Model-deterministic Test-time Augmentation. One caveat in directly applying Equation (17) is the mismatch of the training and test distribution -during training, all examples are perturbed with sampled noise, whereas the test example is without noise. In practice, we see that this mismatch significantly decreases the test accuracy. One natural idea is to also add noise to the test examples, however, this requires careful design (e.g., simply drawing k independent noise vectors and applying them to Equation (17) will lead to a less powerful bound). We thus modify the inference function given a learned model h k in the following way. Instead of directly classifying an unperturbed input x test , we use the hash value of the trained h k model parameters as the random seed and sample u k \u223c N hash(h k ) (0, \u03c3 2 1 d ).\n\nIn practice, we use SHA256 hashing [52] of the trained model file. In this way, the noise we add is a deterministic function of the trained model, which is equivalent to altering the inference function in a deterministic way, h k (x test ) = h k (x test + u k ). We show in the experiments that this leads to significantly better prediction performance in practice. Note that the reason for using a hash function instead of random sampling every time is to ensure that the noise generation process is deterministic, so the choice of different hash functions is flexible.\n\n\nK-Nearest Neighbors\n\nIf the base classifier h is a K-nearest neighbor classifier, we can evaluate the corresponding smoothed classifier exactly and efficiently, in polynomial time, if the smoothing noise is drawn from a Gaussian distribution. In other words, for this type of model, we can eliminate the need to approximate the expectation value via Monte Carlo sampling and evaluate the classifier exactly. Finally, it is worth remarking that bypassing the need to do Monte Carlo sampling ultimately results in a considerable speed-up as it avoids the expensive training of independent models as is required for generic models including DNNs.\n\nA K-NN classifier works in the following way: Given a training set D = {(x i , y i ) n i=1 } and a test example x, we first calculate the similarity between x and each x i , s i := \u03ba(x i , x) where \u03ba is a similarity function. Given all these similarity scores {s i } i , we choose the K most similar training examples with the largest similarity score\n{x \u03c3i } K i=1 along with corresponding labels {y \u03c3i } K i=1 .\nThe final prediction is made according to a majority vote among the top-K labels.\n\nSimilar to DNNs, we obtain a smoothed K-NN classifier by adding Gaussian noise to training points and evaluate the expectation with respect to this noise distribution \n\nwhere D = (D (1) , . . . , D (n) ) \u223c n i=1 N (0, \u03c3 2 1 d ). The next theorem shows that (18) can be computed exactly and efficiently when we measure the similarity with euclidean distance quantized into finite number similarity of levels.\n\nTheorem 3. Given n training instances, a C-multiclass K-NN classifier based on quantized euclidean distance with L similarity levels, smoothed with isotropic Gaussian noise can be evaluated exactly with complexity O(K 2+C \u00b7n 2 \u00b7L\u00b7C).\n\nProof (sketch). The first step to computing (18) is to notice that we can summarize all possible arrangements {x \u03c3i + D (\u03c3i) } K i=1 of top-K instances leading to a prediction by using tally vectors \u03b3 \u2208 [K] C . A tally vector has as its k-th element the number of instances in the top-K with label k, \u03b3 k = #{y \u03c3i = k}. In the second step, we partition the event that a tally vector \u03b3 occurs into events where an instance i with similarity \u03b2 is in the top-K but would not be in the top-(K \u22121). These first two steps result in a summation over O(K C \u00b7 n \u00b7 L \u00b7 C) terms. In the last step, we compute the probabilities of the events {tally \u03b3 \u2227 \u03ba(x i + D (i) , x) = \u03b2} with dynamic programming in O(n \u00b7 K 2 ) steps, resulting in a final time complexity of O(K 2+C \u00b7 n 2 \u00b7 L \u00b7 C).\n\nIf K = 1, an efficient algorithm can even achieve time complexity linear in the number of training samples n. We refer the reader to Appendix B for details and the algorithm. \n\n\nExperimental Results\n\nIn this section, we present an extensive experimental evaluation of our approach and provide a benchmark for certified robustness for DNN and KNN classifiers on different datasets. In addition, we consider three different types of backdoor attack patterns, namely one-pixel, four-pixel, and blending-based attacks. The attack patterns are illustrated in Figure 3 which shows that these patterns can be hard to spot by a human, in particular for the one-pixel pattern on highresolution images. At a high level, our experiments reveal the following set of observations: 1\n\n\u2022 RAB is able to achieve comparable robustness on benign instances compared with vanilla trained models, and achieves non-trivial certified accuracy under a range of realistic backdoor attack settings. \u2022 There is a gap between the certified accuracy provided by RAB and empirical robust accuracy achieved by the state-of-the-art empirical defenses against backdoor attacks without formal guarantees, which serves as the upper bound of the certified accuracy; however, such a gap is reasonably small and we are optimistic that future research can further close this gap. \u2022 RAB's efficient KNN algorithm provides a very effective solution for tabular data. \u2022 Simply applying randomized smoothing to RAB is not effective and careful optimizations (e.g., deterministic test-time augmentation) are necessary.\n\n\nExperiment Setup\n\nIn this paper, we follow the popular transfer learning setting for poisoning attacks [16], [41], [42], [44], [62] in our experiments, specifically [43]. We first use models initialized with pretrained weights obtained from a clean dataset, and then finetune the model with a subset of training data containing backdoored instances. Preliminary experiments and existing work [51] showed that it is difficult to successfully inject backdoors if only a subset of parameters is finetuned. As a result, we always finetune the entire set of model parameters. 1 [24] consisting of 60,000 images of handwritten digits from 0-9, the CIFAR-10 dataset [23] which includes 50,000 images of 10 different classes of natural objects such as horse, airplane, automobile, etc. Furthermore, we perform evaluations on the highresolution ImageNette dataset [19] which is a 10-class subset of the original large-scale ImageNet dataset [11]. Finally, we evaluate the K-NN model on a tabular dataset, namely the UCI Spambase dataset [12], which consists of bag-ofwords feature vectors on E-mails and determines whether the message is spam or not. The dataset contains 4,601 data cases, each of which is a 57-dimensional input. We use 0.1% of the MNIST and CIFAR-10 training data to finetune our models; on ImageNette and Spambase, we use 1% for finetuning. For evaluations on DNNs, we choose the CNN architecture from [15] on MNIST and the ResNet used in [10] on CIFAR-10, whereas for ImageNette, we use the standard ResNet-18 [18] architecture.\n\n\nTraining Protocol.\n\nWe set the number of sampled noise vectors (i.e. augmented datasets) to N = 1, 000 on MNIST and CIFAR, and N = 200 on ImageNette, leading to an ensemble of 1, 000 and 200 models, respectively. The added smoothing noise is sampled from the Gaussian distribution with location parameter \u00b5 = 0 and scale \u03c3 = 0.5 for MNIST and Spambase. For CIFAR-10 and ImageNette we use \u00b5 = 0 and set the scale to \u03c3 = 0.2. The impact of different \u03c3 is shown in Section 7.2.4. The confidence intervals for the binomial distribution are calculated with an error rate of \u03b1 = 0.001. For the KNN models, we use K = 3 neighbors and set the number of similarity levels to L = 200, meaning that the similarity scores according to euclidean distance are quantized into 200 distinct levels.\n\n\nBaselines of Empirical Backdoor Removal Based\n\nDefenses. Since this is the first paper providing rigorous certified robustness against backdoor attacks, there is no baseline that allows a comparison of the certified accuracy. We remark that a technical report [49] directly applies the randomized smoothing technique to certify robustness against backdoors without evaluation or analysis. However, as we will show in Section 7.2.5, directly applying randomized smoothing without deterministic test-time augmentation does not provide high certified robustness. We will, on the other hand, compare our empirical robust accuracy with the state-of-the-art empirical defenses. We briefly review these defenses in the following.\n\nActivation clustering (AC) [6] extracts the activation of the last hidden layer of a trained model and uses clustering analysis to remove training instances with anomalies. We use the default parameter setting provided in the Adversarial Robustness Toolbox (ART) [36]. Spectral Signature (Spectral) [48] uses matrix decomposition on the feature representations to detect and remove training instances with anomalies. We again use the default parameter setting provided in ART. Sphere [45] performs dimensionality reduction and removes instances with anomalies in the lower dimensions. The top-15% anomaly instances are removed. Neural Cleanse (NC) [50] first reverse-engineers a \"pseudotrigger\" for each class. Then, to detect and remove anomaly instances, the distances between each instance with and without the pseudo-trigger are compared, and the most similar ones are recognized as anomaly instances. We use pixel-level distance as the distance metric, 100 epochs for trigger generation, and an initial \u03bb = 0.01 for MNIST and \u03bb = 0.0001 for CIFAR and ImageNette. Statistical Contamination Analyzer (SCAn) [47] first performs an EM algorithm to decompose two subgroups over a small clean dataset. Then, for each class in the train set, the parameters of a mixture model for all the data are estimated, before we calculate the likelihood for anomaly detection. To identify the backdoored instances, we recognize the smaller set in the most anomalous class as the backdoored instances. Mixup [3], following the data augmentation technique in the paper, we use a 4-way mixup training algorithm to train the model over the train set. The convex coefficients are drawn from a Dirichlet distribution with \u03b1 = 1.0.\n\nThe initial goal of all these approaches, with the exception of Mixup, is to detect backdoored instances, i.e., to determine whether there exists a trigger. To apply them as a defense (i.e., to train a clean model despite the existence of backdoored data), we make adaptations either following the original paper (AC, Spectral, Sphere and NC) or by our design (SCAn) so that we remove training data with anomalies detected by these approaches and retrain a clean model. Some detection cannot be adapted to the defense task, such as [56], and are not included in the comparison.\n\n\nEvaluation Metrics.\n\nWe evaluate the model accuracy trained on the backdoored dataset with vanilla training and RAB training strategies. In particular, we evaluate both the model performance on benign instances (benign accuracy) and backdoored instances for which the attack was successful against the vanilla model (empirical robust accuracy). With RAB, we are also able to calculate the certified accuracy, which means that the RAB model not only certifies that the prediction is the same as if it were trained on the clean dataset, but also that the prediction is equal to the ground truth. The certified accuracy is defined below.\nCertified Acc. = 1 n |{x i : R i > \u2225\u03b4\u2225 2 \u2227\u0177 i = y i }|(19)\nwhere R i is the robus radius according to Eq. (12),\u0177 i is the predicted label, and y i is the ground truth for input x i . We emphasize that we only evaluate the backdoored test instances for which the attack is successful against the vanilla trained models, which is why the vanilla models always have 0% empirical robust accuracy on these backdoored instances in Table 1. This is to evaluate against the effective backdoor attacks and better illustrate the comparison between RAB-trained models with vanilla and baseline backdoor defense models (empirical robust accuracy). Such empirical robust accuracy of different methods serves as an upper bound for the certified accuracy. 7.1.5. Backdoor Patterns. We evaluate RAB against three representative backdoor attacks, namely a one-pixel pattern in the middle of the image, a four-pixel pattern, and blending a random, but fixed, noise pattern to the entire image [8]. We visualize all backdoor patterns on different datasets in Fig. 3. We control the perturbation magnitude of the attack via the L 2 -norm of the backdoor patterns, setting \u2225\u03b4\u2225 2 = 0.1 for all attacks where \u03b4 is the backdoor pattern. On MNIST, we inject 10% backdoored instances and 5% for CIFAR and ImageNette respectively. If not described differently, the attack goal is to fool the model into predicting \"0\" on MNIST, \"airplane\" on CIFAR and \"tench\" on ImageNette.\n\nIn Appendix C.1, we also consider an all-to-all attack goal [15] so that the fooled model will change its prediction conditioned on the original label.\n\nIt is possible to use different backdoor patterns via optimization and other approaches. However, since our goal is to provide certified robustness against backdoor attacks, a task that is by definition agnostic to the specific backdoor pattern but only depends on the magnitude of the pattern and the number of backdoored training instances, we mainly focus on these representative backdoor patterns. In addition, we only evaluate the backdoor attack to poison the dataset, while other attacks that interfere with the training process are not evaluated [38], as RAB is a robust training pipeline against training data manipulation based poisoning attacks.\n\n\nCertified Robustness of DNNs against Backdoor Attacks\n\nIn this section we evaluate RAB against backdoor attacks on different models and datasets. We present both the certified robust accuracy of RAB, as well we the empirical robust accuracy comparison between RAB and baseline defenses. Furthermore, we also present several ablation studies to further explore the properties of RAB.\n\n\nCertified Robustness with RAB.\n\nWe first evaluate the certified robustness of RAB on DNNs against different backdoor patterns on different datasets. We also present the performance of RAB on benign instances and backdoored instances empirically. Table 1 lists the benchmark results on MNIST, CIFAR-10, and ImageNette, respectively. From the results, we can see that RAB achieves significantly nontrivial certified robust accuracy against backdoor attacks at a negligible cost of benign accuracy; while there are no certified results for any other method. The slight drop in benign accuracy results from training on noisy instances. However, this loss in benign accuracy is less than 3% in most cases and is clearly outweighed by the achieved certified robust accuracy. In particular, RAB achieves over 23% certified accuracy on the backdoored instances for MNIST and CIFAR-10, and around 12% for ImageNette. In other words, we can successfully certify for these instances that our model predicts the same result as if it were trained on the clean training set. We run the experiment multiple times and show in Appendix C.7 that the standard deviation is less than 1% in most cases. We also show the abstain rate of certification in Appendix C.6 and observe that it is generally low. If the abstain rate is high, we can perform the similar way as in Cohen et al. [10] to obtain a variation of our theorem to certify the radius by some margin.\n\n\nEmpirical\n\nRobustness: without RAB vs. with RAB. In addition to the certificates that RAB can provide, RAB's training process also provides good robustness accuracy empirically, without theoretical guarantees.\n\nIn Table 1, the \"RAB\" column reports the empirical robust accuracy -how often can a malicious input that successfully attacks a vanilla model trick RAB? We can see that, RAB achieves high empirical robust accuracy, and such empirical robust accuracy achieved by either RAB or other methods serves as an upper bound for the certified robust accuracy provided by RAB under the \"RAB-certified\" column. It is shown that RAB achieves around 40% empirical robust accuracy on the backdoored instances for MNIST and CIFAR-10, and over 30% for ImageNette. In Appendix C.8, we also try an empirical adversarial attack on the RAB model and observe a similar behavior as on vanilla models.\n\n\nComparison with\n\nState-of-the-art Empirical Backdoor Defenses. Another line of research is to develop empirical methods to automatically detect and remove backdoored training instances. How does RAB compare with these methods? We empirically compare the robustness of RAB with other state-of-the-art baseline methods introduced in Section 7.1.3, as shown in Table 1. we observe that although RAB is not specifically designed for empirical defense, it achieves comparable empirical robust accuracy compared with these baseline methods. RAB outperforms about half of the baselines methods on MNIST and ImageNette and all the baselines on CIFAR-10. Interestingly, our approach performs better on CIFAR-10 than on other tasks while other baselines usually perform badly on CIFAR-10. We attribute this observation to the fact that the benign accuracy on CIFAR-10 is comparably low, so that the baselines based on analyzing feature representations or on model reverse engineering are largely affected and the performance is thus worse. By comparison, RAB only needs to add noise to smooth the training process without analyzing model properties, and is hence less affected by the model viability (similarly, the performance of Mixup is less affected too).\n\nIn addition, in Appendix C.1, we additionally evaluate the defenses against a more challenging all-to-all attack where many baseline approaches fail, and RAB still achieves good performance. We also show that our approach can be applied to an SVM model for three tabular datasets in Appendix C.4, while existing approaches cannot work well since there is no distinct \"activation layer\" in a simple SVM model. Furthermore, for very large attack perturbations, the certification will fail as shown in Appendix C.2; however, RAB still achieves non-trivial empirical robustness.\n\n\nCertified Accuracy Under different Radii.\n\nWe further discuss how different certified radii affect the certified accuracy. In Fig. 4, we present the certified accuracy as a function of the robust radius given different values for the smoothing parameter \u03c3 against blending attack. The conclusions on other backdoor patterns are similar.\n\nIn the figures, we plot the certified accuracy of all test cases (instead of only on successfully attacked cases) so that the overall trend can be seen. We can see that the certified accuracy decreases with increased radii and, at a certain point, it suddenly goes to zero, which aligns with existing observations on certified robustness against evasion attacks [10]. Furthermore, stronger noise harms the certified accuracy at a small radius, while improving it at a larger robust radius. It is thus essential to choose an appropriate smoothing noise magnitude according to the task. The certified accuracy of KNN is comparatively low due to its simple structure, but it achieves non-trivial certified accuracy at a larger radius as we do not need Monte Carlo sampling which would result in a finite sampling error that decreases the certified robustness.\n\n\nAblation Study: Impact of Deterministic Test-time\n\nAugmentation. We compare the certification accuracy of RAB with and without deterministic test-time augmentation in Figure 5. We plot the certified accuracy of all test cases instead of only on successfully attacked cases to show the comparison on the entire dataset. We observe that the certified accuracy significantly improves with the proposed hash function based deterministic test-time augmentation, especially at small certification radii and with a particularly large gap on ImageNette dataset -without the augmentation, the certified accuracy is only around 20%, while it increases to around 80% with the augmentation. This shows that it is important to include the test-time augmentation during inference, and directly adopting randomized smoothing may not provide satisfactory certified accuracy. The detailed empirical and certified robust accuracies are shown in Appendix C.5.\n\n\nCertified Robustness of KNN Models\n\nHere we present the benchmarks based on our proposed efficient algorithm for KNN models. We perform experiments on the UCI spambase tabular datasets and show the results for K=3 in Table 2. The NC baseline relies   on gradient-based reverse engineering, while Mixup relies on mixing label information during training, so these two methods are not included here. The other baselines use intermediate feature vectors in DNN models, which do not exist in KNN models. Therefore, we use the output prediction vector as the feature vector. From the results, we see that for KNN models, RAB achieves good performance for both empirical and certified robustness and outperforms all the baselines, indicating its advantages for specific domains.\n\nThis comparison might seem unfair at first glance, since the considered baselines are based on deep feature representations, which are absent in the KNN case. However, firstly, we emphasize that none of the approaches, including RAB, use deep features for this comparison and have hence access to the same amount of information. Secondly, this comparison reveals an important property of our approach: while the baselines struggle to handle ML models beyond DNN, RAB is applicable to a wider range of models and still yields non-trivial empirical and certified robust accuracy. To enable a comparison for KNN models which is more favorable to the baselines, we consider kernel KNN with a CNN as the kernel function. From the table in Appendix C.3, we see that for this scenario, some baselines indeed outperform RAB. Figure 6 illustrates the runtime of the exact algorithm for KNN vs. the sampling-based method of DNN. We observe that for certifying one input on KNN with K = 3 neighbors, using the proposed exact certification algorithm takes only 2.5 seconds, which is around 2-3 times faster than the vanilla RAB on MNIST and 6-7 times faster on CIFAR-10. In addition, the runtime is agnostic to the input size but related to the size of the training set. It would be interesting future work to design similar efficient certification algorithms for DNNs. Nevertheless, the KNN algorithm remains slower than the algorithm without certification (which is 1000 times faster than the RAB DNN pipeline), and the improvement of running time is an important future direction.\n\n\nRelated Work\n\nIn this section, we discuss current backdoor (poisoning) attacks on machine learning models and existing defenses. Backdoor attacks There have been several works developing optimal poisoning attacks against machine learning models such as SVM and logistic regression [2], [27]. Furthermore, [34] proposes a similar optimization-based poisoning attack against neural networks that can only be applied to shallow MLP models. In addition to these optimizationbased poisoning attacks, the backdoor attacks are shown to be very effective against deep neural networks [8], [15]. The backdoor patterns can be either static or generated dynamically [57]. Static backdoor patterns can be as small as one pixel, or as large as an entire image [8]. Empirical defenses against backdoor attacks Given the potentially severe consequences caused by backdoor attacks, multiple defense approaches have been proposed. Neural-Cleanse [50] proposes to detect the backdoored models based on the observation that there exists a \"short path\" to make an instance to be predicted as a malicious one. [7] improves upon the approach by using model inversion to obtain training data, and then applying GANs to generate the \"short path\" and apply anomaly detection algorithm as in Neural Cleanse. Activation Clustering [6] leverages the activation vectors from the backdoored model as features to detect backdoor instances. Spectral Signature [48] identifies the \"spectral signature\" in the activation vector for backdoored instances. STRIP [13] proposes to identify the backdoor instances by checking whether the model will still provide a confident answer when it sees the backdoor pattern. SentiNet [9] leverages computer vision techniques to search for the parts in the image that contribute the most to the model output, which are very likely to be the backdoor pattern. In [32], differential privacy has been leveraged as a defense against poisoning attacks. Note that RAB can not guarantee that the trained models are differentially private, although both aim to decrease the model sensitivity intuitively. A further empirical defense against backdoor attacks is proposed in [17] using covariance estimation with the aim of amplifying the spectral signature of backdoored instances.\n\nCertified Defenses against poisoning attacks Another interesting application of randomized smoothing is presented in [40] to certify the robustness against labelflipping attacks and randomize the entire training procedure of the classifier by randomly flipping labels in the training set. This work is orthogonal to ours in that we investigate the robustness with respect to perturbations on the training inputs rather than labels. In a further line of work on provable defenses against poisoning attacks, [26] proposes an ensemble method, deep partition aggregation (DPA). Similar to our work, DPA is related to randomized smoothing, however, in contrast to our work, the goal is to certify the number of poisoned instances for which the prediction remains unaffected. Similarly, [20] use an ensemble technique to certify robustness against poisoning attacks. This is also orthogonal to ours as it certifies the number of poisoned instances, rather than the trigger size. The same certification goal is considered in [21], but is restricted to nearest neighbor algorithms and derives an intrinsic certificate by viewing them as ensemble methods. In addition to these works aiming to certify the robustness of a single model, [60] provides a new way to certify the robustness of an end-to-end sensing-reasoning pipeline. Finally, [54] propose a technique to certify robustness against backdoor attacks within the federated learning framework by controlling the global model smoothness. Furthermore, a technical report also proposes to directly apply the randomized smoothing technique to certify robustness against backdoor attacks without any evaluation or analysis [49]. In addition, as we have shown, directly applying randomized smoothing will not provide high certified robustness bounds. Contrary to that, in this paper, we first provide a unified framework based on randomized smoothing, and then propose the RAB robust training process to provide certified robustness against backdoor attacks based on the framework. We provide the tightness analysis for the robustness bound, analyze different smoothing distributions, and propose the hash function-based model deterministic test-time augmentation approach to achieve good certified robustness. In addition, we analyze different machine learning models with corresponding properties such as model smoothness to provide guidance to further improve the certified robustness.\n\n\nLimitations\n\nOne major limitation of RAB is that it introduces nonnegligible runtime overhead. To certify the robustness, we need to train and evaluate multiple models (here, 1000 for MNIST/CIFAR-10 and 200 for ImageNette), which is expensive despite the fact that it is parallelizable and can be speeded up with multiple GPUs. Nevertheless, with our polynomial-time KNN algorithm, we have shown a first step towards mitigating the computational cost and leave further endeavors in this direction as future work.\n\nAnother limitation is the defender's knowledge of the attack. Indeed, to certify the robustness, the defender needs to know 1) an upper bound on the backdoor trigger magnitude (in terms of an L p norm), 2) an upper bound on the number of poisoned training instances, and, 3) control over the training process. However, to use RAB only as a defense (i.e. without any certificate), the defender only needs to control the training process while 1) and 2) are not needed. The assumption 3) restricts RAB to be a robust training algorithm given an untrusted dataset. In other words, RAB cannot be used to defend against backdoor attacks that interfere with the training process (e.g., [38]).\n\n\nDiscussion and Conclusion\n\nIn this paper, we aim to propose a unified smoothing framework to certify the model robustness against different attacks. In particular, towards the popular backdoor poisoning attacks, we propose the first robust smoothing pipeline RAB as well as a model deterministic test-time augmentation mechanism to certify the prediction robustness against diverse backdoor attacks. In particular, we evaluate our certified robustness against backdoors on DNNs and KNN models. In addition, we propose an exact algorithm for KNN models without requiring to sample from the smoothing distributions. We provide comprehensive benchmarks of certified model robustness against backdoors on diverse datasets, which we believe will provide the first set of certified robustness against backdoor attacks for future work to compare with, and hopefully our results and analysis will inspire a new line of research on tighter certified accuracy against backdoor attacks. n \u2192 \u221e. Hence, we have that \u2229 n A n = {x : \u039b(x) \u2264 t} and thus lim n\u2192\u221e P (\u039b(X 0 ) \u2264 t n ) = P (\u039b(X 0 ) \u2264 t) since lim n\u2192\u221e P (X 0 \u2208 A n ) = P(X 0 \u2208 \u2229 n A n ) for A n+1 \u2286 A n . We conclude that t \u2192 P (\u039b(X 0 ) \u2264 t) is right-continuous and in particular P (\u039b(X 0 ) \u2264 t p ) \u2265 p. We now show the LHS of inequality (21). For that purpose, consider the set B n := {x : \u039b(x) < t p \u2212 1 /n} and let B := {x : \u039b(x) < t p }. Clearly, if x \u2208 \u222a n B n , then \u2203n such that \u039b(x) < t p \u2212 1 /n < t p and hence x \u2208 B. If on the other hand x \u2208 B, then we can choose n large enough such that \u039b(x) < t p \u2212 1 /n and thus x \u2208 \u222a n B n . It follows that B = \u222a n B n . Furthermore, by the definition of t p and since for any n \u2208 N we have that P (X 0 \u2208 B n ) = P (\u039b(X 0 ) < t p \u2212 1 /n) < p it follows that P (\u039b(X 0 ) < t p ) = lim n\u2192\u221e P (X 0 \u2208 B n ) \u2264 p since B n \u2286 B n+1 . This concludes the proof.\n\nLemma A.2. Let X 0 and X 1 be random variables taking values in Z and with probability density functions f 0 and f 1 with respect to a measure \u00b5. Let \u03d5 * be a likelihood ratio test for testing the null X 0 against the alternative X 1 . Then for any deterministic function \u03d5 : Z \u2192 [0, 1] the following implications hold:\ni) \u03b1(\u03d5) \u2265 1 \u2212 \u03b1(\u03d5 * ) \u21d2 1 \u2212 \u03b2(\u03d5) \u2265 \u03b2(\u03d5 * ) ii) \u03b1(\u03d5) \u2264 \u03b1(\u03d5 * ) \u21d2 \u03b2(\u03d5) \u2265 \u03b2(\u03d5 * )\nProof. We first show (i). Let \u03d5 * be a likelihood ratio test as defined in (20). Then, for any other test \u03d5 we have\n1 \u2212 \u03b2(\u03d5 * ) \u2212 \u03b2(\u03d5) = = \u039b>t \u03d5f1d\u00b5 + \u039b\u2264t (\u03d5 \u2212 1) f1d\u00b5 + q \u039b=t f1d\u00b5 = \u039b>t \u03d5\u039bf0d\u00b5 + \u039b\u2264t (\u03d5 \u2212 1) \u22640 \u039bf0d\u00b5 + q \u039b=t \u039bf0d\u00b5 \u2265 t \u00b7 \uf8ee \uf8f0 \u039b>t \u03d5f0d\u00b5 + \u039b\u2264t (\u03d5 \u2212 1) f0d\u00b5 + q \u039b=t f0d\u00b5 \uf8f9 \uf8fb = t \u00b7 [\u03b1(\u03d5) \u2212 (1 \u2212 \u03b1(\u03d5 * ))] \u2265 0(22)\nwith the last inequality following from the assumption and t \u2265 0. Thus, (i) follows; (ii) can be proved analogously.\n\nProof of Theorem 1. We first show the existence of a likelihood ratio test \u03d5 A with significance level 1 \u2212 p A . Let Z \u2032 := (\u2126 x , \u2206) + Z and recall that the likelihood ratio \u039b between the densities of Z and Z \u2032 is given by\n\u039b(z) = f Z \u2032 (z) f Z (z)\nand let X \u2032 := \u2126 x + X and D \u2032 = \u2206 + D. Furthermore, for any p \u2208 [0, 1], let t p := inf{t \u2265 0 : P(\u039b(Z) \u2264 t) \u2265 p} and\nqp = 0 if P (\u039b(Z) = tp) = 0, P(\u039b(Z)\u2264tp)\u2212p P(\u039b(Z)=tp)\notherwise. (23) Note that by Lemma A.1 we have that P(\u039b(Z) \u2264 t p ) \u2265 p and P(\u039b(Z) \u2264 tp) = P(\u039b(Z) < tp) + P(\u039b(Z) = tp) \u2264 p + P(\u039b(Z) = tp) (24) and hence q p \u2208 [0, 1]. For p \u2208 [0, 1], let \u03d5 p be the likelihood ratio test defined in (20) with q \u2261 q p and t \u2261 t p . Note that \u03d5 p has type-I error probability \u03b1(\u03d5 p ) = 1 \u2212 p. Thus, the test \u03d5 A \u2261 \u03d5 p A satisfies \u03b1(\u03d5 A ) = 1 \u2212 p A . It follows from assumption (9) that P X, D (h(x + X, D + D) = y A ) = q(y A | x, D) \u2265 1\u2212\u03b1(\u03d5 A ) and thus, by applying the first part of Lemma A.2 to the functions \u03d5(z) \u2261 1 {h((x, D)+z)=y A } (z) and \u03d5 * \u2261 \u03d5 A , it follows that\nq(y A | x + \u2126 x , D + \u2206) = 1 \u2212 \u03b2(\u03d5) \u2265 \u03b2(\u03d5 A ).(25)\nSimilarly, the likelihood ratio test \u03d5 B \u2261 \u03d5 1\u2212p B satisfies \u03b1(\u03d5 B ) = p B and, for y \u0338 = y A , it follows from the assumption (9) that P X, D (h(x + X, D + D) = y) = q(y| x, D) \u2264 p B = \u03b1(\u03d5 B ). Thus, applying the second part of Lemma A.2 to the functions \u03d5(z) = 1 {h((x, D)+z)=y} (z) and \u03d5 * \u2261 \u03d5 B yields\nq(y| x + \u2126 x , D + \u2206) = 1 \u2212 \u03b2(\u03d5) \u2264 1 \u2212 \u03b2(\u03d5 B ).(26)\nCombining (25) and (26) we see that, if \u03b2(\u03d5 A ) + \u03b2(\u03d5 B ) > 1, then it is guaranteed that q(y A | x + \u2126 x , D + \u2206) > max y\u0338 =y A q(y| x+\u2126 x , D+\u2206) what completes the proof.\n\n\nA.2. Proof of Theorem 2\n\nProof. We show tightness by constructing a base classifier h * , such that the smoothed classifier is consistent with the class probabilities (9) for a given (fixed) input (x 0 , D 0 ) but whose smoothed version is not robust for adversarial perturbations (\u2126 x , \u2206) that violate (10). Let \u03d5 A and \u03d5 B be two likelihood ratio tests for testing the null Z \u223c P 0 against the alternative Z + (\u2126 x , \u2206) \u223c P 1 and let \u03d5 A be such that \u03b1(\u03d5 A ) = 1 \u2212 p A and \u03d5 B such that \u03b1(\u03d5 B ) = p B . Since\n\n(\u2126 x , \u2206) violates (10), we have that \u03b2(\u03d5 A ) + \u03b2(\u03d5 B ) \u2264 1. Let p * be given by\np * (y| x, D) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 \u2212 \u03d5 A (x \u2212 x 0 , D \u2212 D 0 ) y = y A \u03d5 B (x \u2212 x 0 , D \u2212 D 0 ) y = y B 1\u2212p * (y A | x, D)\u2212p(y B | x, D) C\u22122 o.w.(27)\nwhere the notation D \u2212 D 0 denotes subtraction on the features but not on the labels. Note that for binary classification, C = 2 we have that \u03d5 A = \u03d5 B and hence p * is well defined since in this case, by assumption p A + p B = 1. If C > 2, note that it follows immediately from the definition of p * that k p * (y| x, D) = 1. Note that, from the construction of \u03d5 A and \u03d5 B in the proof of Theorem 1 (Appendix A.1) that\n(pointwise) \u03d5 A \u2265 \u03d5 B provided p A + p B \u2264 1. It follows that for y \u0338 = y A , y B we have p * (y| x, D) \u221d \u03d5 A \u2212 \u03d5 B \u2265 0.\nThus, p * is a well defined (conditional) probability distribution over labels and h * (x, D) := arg max y p * (y| x, D) is a base classifier. Furthermore, to see that the corresponding smoothed classifier q * is consistent with the class probabilities (9), consider\nq * (y A | x 0 , D 0 ) = E(1 \u2212 \u03d5 A (X, D)) = p A(28)\nand\nq * (y B | x 0 , D 0 ) = E(\u03d5 B (X, D)) = \u03b1(\u03d5 B ) = p B .(29)\nIn addition, for any y \u0338 = y A , y B , we have q * (y| x 0 ,\nD 0 ) = (1 \u2212 p A \u2212 p B )/(C \u2212 2) \u2264 p B since by assumption p A + p B \u2265 1 \u2212 (C \u2212 2) \u00b7 p B .\nThus, q * is consistent with the class probabilities (9) In addition, note that q * (y A | x 0 + \u2126 x , D 0 + \u2206) = 1 \u2212 \u03b2(\u03d5 A ) and \u03b2(\u03d5 B ) = q * (y B | x 0 + \u2126 x , D 0 + \u2206).\n\nSince by assumption 1\u2212\u03b2(\u03d5 A ) < \u03b2(\u03d5 B ) we see that indeed y A \u0338 = g * (x 0 + \u2126 x , D 0 + \u2206).\n\n\nA.3. Proof of Corollary 1\n\nProof. We prove this statement by direct application of Theorem 1. Let Z = (X, D) be the smoothing distribution for q and letZ := (\u2126 x , \u2206) + Z andZ \u2032 := (0, \u2212\u2206) +Z. Correspondingly, letq(y| x, D) = q(y| x + \u2126 x , D + \u2206).\n\nBy assumption, we have thatq(y A | x, D) \u2265 p A and max y\u0338 =y Aq (y| x, D) \u2264 p B . We will now apply Theorem 1 to the smoothed classifierq. By Theorem 1, there exist likelihood ratio tests \u03d5 A and \u03d5 B for testingZ againstZ \u2032 such that, if\n\u03b2(\u03d5 A ) + \u03b2(\u03d5 B ) > 1(30)\nthen it follows that y A = arg max yq (y| x, D\u2212\u2206) The statement then follows, sinceq(y| x, D\u2212\u2206) = arg max yq (y| x+ \u2126 x , D+\u2206). We will now construct the corresponding likelihood ratio tests and show that (30) has the form (12). Note that the likelihood ratio betweenZ andZ \u2032 at z = (x, d) is given by\n\u039b(z) = exp n i=1 \u27e8d i , \u2212\u03b4 i \u27e9 \u03a3 + 1 2 \u27e8\u03b4 i , \u03b4 i \u27e9 \u03a3 where \u03a3 = \u03c3 2 1 d and \u27e8a, b\u27e9 \u03a3 := n i=1 a i b i /\u03c3 2 .\nThus, since singletons have probability 0 under the Gaussian distribution, any likelihood ratio test for testingZ againstZ \u2032 has the form\n\u03d5 t (z) = 1, \u039b(z) \u2265 t. 0, \u039b(z) < t.(31)For p \u2208 [0, 1], let t p := exp(\u03a6 \u22121 (p) n i=1 \u27e8\u03b4 i , \u03b4 i \u27e9 \u03a3 \u2212 1 2 n i=1 \u27e8\u03b4 i , \u03b4 i \u27e9 \u03a3 ) and note that \u03b1(\u03d5 tp ) = 1 \u2212 p since \u03b1(\u03d5 tp ) = 1 \u2212 \u03a6( log(tp)+ 1 2 n i=1 \u27e8\u03b4i, \u03b4i\u27e9\u03a3 \u221a n i=1 \u27e8\u03b4i, \u03b4i\u27e9\u03a3 ) where \u03a6 is the CDF of the standard normal distribution. Thus, the test \u03d5 A \u2261 \u03d5 t A with t A \u2261 t p A satisfies \u03b1(\u03d5 A ) = 1 \u2212 p A and the test \u03d5 B \u2261 \u03d5 t B with t B \u2261 t 1\u2212p B satisfies \u03b1(\u03d5 B ) = p B . Computing the type II error probability of \u03d5 A yields \u03b2(\u03d5 A ) = \u03a6(\u03a6 \u22121 (p A ) \u2212 n i=1 \u27e8\u03b4 i , \u03b4 i \u27e9 \u03a3 )\n. and, similarly, the type II error probability of \u03d5 B is given by\n\u03b2(\u03d5 B ) = \u03a6(\u03a6 \u22121 (1 \u2212 p B ) \u2212 n i=1 \u27e8\u03b4 i , \u03b4 i \u27e9 \u03a3 ). Finally, we see that \u03b2(\u03d5 A ) + \u03b2(\u03d5 B ) > 1 is satisfied if and only if n i=1 \u2225\u03b4 i \u2225 2 2 < \u03c3 2 (\u03a6 \u22121 (p A ) \u2212 \u03a6 \u22121 (p B )).\n\nA.4. Proof of Corollary 2\n\nProof. We proceed analogously to the proof of Note that the likelihood ratio betweenZ againstZ \u2032 at z = (x, w, v) for any w \u2208 S \u222a S \u2032 is given by (32) and that any likelihood ratio test for testingZ againstZ \u2032 has the form (20). We now construct such likelihood ratio tests \u03d5 A , \u03d5 B with \u03b1(\u03d5 A ) = 1\u2212p A and \u03b1(\u03d5 B ) = p B by following the construction in the proof of Theorem 1. Specifically, we compute q A , t A such that these type I error probabilities are satisfied. Notice that p 0 :\n\u039b(z) = fZ \u2032 (z) fZ(z) = fW \u2032 (w) fW (w) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 0 w \u2208 S \\ S \u2032 , 1 w \u2208 S \u2229 S \u2032 , \u221e w \u2208 S \u2032 \\ S.= P(W \u2208 S \\ S \u2032 ) = 1 \u2212 n i=1 ( d j=1 (1 \u2212 |\u03b4ij | b\u2212a ) + ) where (x) + = max{x, 0}. For t \u2265 0 we have P(\u039b(Z) \u2264 t) = p 0 if t < 1 and 1 otherwise. Thus t p := inf{t \u2265 0 : P(\u039b(Z) \u2264 t) \u2265 p} is given by t p = 0 if p \u2264 p 0 and t p = 1 if p > p 0 . We notice that, if p A \u2264 p 0 , then t A \u2261 t p A = 0.\nThis implies that the type II error probability of the corresponding test \u03d5 A is 0 since in this case\n\u03b2(\u03d5 A ) = 1 \u2212 P \u039b(Z \u2032 ) > 0 \u2212 q A \u00b7 P \u039b(Z \u2032 ) = 0 (33) = 1 \u2212 P D \u2032 \u2208 S \u2032 \u2212 q A \u00b7 P D \u2032 \u2208 S \\ S \u2032 = 0. (34)\nSimilarly, if 1 \u2212 p B \u2264 p 0 then t B \u2261 t p B = 0 and we obtain that the corresponding test \u03d5 B satisfies \u03b2(\u03d5 B ) = 0. In both cases \u03b2(\u03d5 A ) + \u03b2(\u03d5 B ) > 1 can never be satisfied and we find that p A > p 0 and 1 \u2212 p B > p 0 is a necessary condition. In this case, we have that t A = t B = 1. Let q A and q B be defined as in the proof of Theorem 1\nq A := P(\u039b(Z) \u2264 1) \u2212 p A P(\u039b(Z) = 1) = 1 \u2212 p A 1 \u2212 p 0 ,(35)q B := P(\u039b(Z) \u2264 1) \u2212 (1 \u2212 p B ) P(\u039b(Z) = 1) = 1 \u2212 (1 \u2212 p B ) 1 \u2212 p 0 .(36)\nClearly, the corresponding likelihood ratio tests \u03d5 A and \u03d5 B have significance 1 \u2212 p A and p B respectively. Furthermore, notice that\nP D \u2032 \u2208 S \u2032 \\ S = P D \u2208 S \\ S \u2032 = p 0 (37) P D \u2032 \u2208 S \u2032 \u2229 S = P D \u2208 S \u2032 \u2229 S = 1 \u2212 p 0(38)\nand hence \u03b2(\u03d5 A ) is given by\n\u03b2(\u03d5 A ) = 1 \u2212 P \u039b(Z \u2032 ) > 1 \u2212 q A \u00b7 P \u039b(Z \u2032 ) = 1 (39) = 1 \u2212 p 0 \u2212 q A \u00b7 (1 \u2212 p 0 ) = p A \u2212 p 0 .(40)\nand similarly\n\u03b2(\u03d5 B ) = 1 \u2212 P \u039b(Z \u2032 ) > 1 \u2212 q B \u00b7 P \u039b(Z \u2032 ) = 1 (41) = 1 \u2212 p 0 \u2212 q B \u00b7 (1 \u2212 p 0 ) = 1 \u2212 p B \u2212 p 0 .(42)\nFinally, the statement follows, since \u03b2(\u03d5 A ) + \u03b2(\u03d5 B ) > 1 if\nand only if 1 \u2212 p A \u2212p B 2 < n i=1 d j=1 1 \u2212 |\u03b4ij | b\u2212a + .\n\nAppendix B. Smoothed K-NN Classifiers\n\nWe first formalize K-NN classifiers which use quantizeed Euclidean distance as a notion of similarity. Specifically, let B 1 = [0, b 1 ), . . . , B L := [b L\u22121 , \u221e) be similarity buckets with increasing b 1 < b 2 < . . . , b L\u22121 and associated similarity levels \u03b2 1 > \u03b2 2 > . . . > \u03b2 L . Then for x, x \u2032 \u2208 R d we define their similarity as \u03ba(x,\nx \u2032 ) := L l=1 \u03b2 l 1 B l (\u2225x \u2212 x \u2032 \u2225 2 2 ) where 1 B l is the indicator function of B l . Given a dataset D = (x i , y i ) n i=1\nand a test instance x, we define the relation\nx i \u2ab0 x j \u21d0\u21d2 \u03ba(x i , x) > \u03ba(x j , x) if i > j \u03ba(x i , x) \u2265 \u03ba(x j , x) if i \u2264 j(43)\nwhich says that the instance x i is more similar to x, if either it has strictly larger similarity or, if it has the same similarity as x j , then x i is more similar if i < j. With this binary relation, we define the set of K nearest neighbours of x as\nI K (x, D) := {i : |{j \u0338 = i : x j \u2ab0 x i }| \u2264 K \u2212 1} \u2286 [n]\nand summarize the per class votes in I K as a label tally vector \u03b3 k (x, D) := #{i \u2208 I K (x, D) : y i = k}. The K-NN prediction is given by K-NN(x, D) = arg max k \u03b3 k (x, D).\n\n\nB.1. Proof of Theorem 3\n\nProof. Our goal is to show that we can compute the smoothed classifier q with Z = (0, D), D \u223c \n\nin time O(K 2+C \u00b7 n 2 \u00b7 L \u00b7 C). For ease of notation, let X i := x i + D (i) and S i := \u03ba(X i , x) and note that\np l i := P (S i = \u03b2 l ) = F d,\u03bbi b l \u03c3 2 \u2212 F d,\u03bbi b l\u22121 \u03c3 2\nwhere F d,\u03bbi is the CDF of the non-central \u03c7 2 -distribution with d degrees of freedom and non-locality parameter \u03bb i = \u2225x i + \u03b4 i \u2212 x\u2225 2 2 /\u03c3 2 . Note that we can write (44) equivalently as P D (arg max k \u2032 \u03b3 k \u2032 (x, D + D)) = y) and thus q(y| x, D) = \u03b3\u2208\u0393 k P D (\u03b3(x, D + D) = \u03b3) with \u0393 k := {\u03b3 \u2208 [K] C : arg max k \u2032 \u03b3 k \u2032 = k}. Next, we notice that the event that a tally vector \u03b3 occurs, can be partitioned into the events that lead to the given \u03b3 and for which instance i has similarity \u03b2 l and is in the top-K but not in the top-(K \u2212 1). We define these to be the boundary events\nB l i (\u03b3) := {\u2200c : #{j \u2208 I c : X j \u2ab0 X i } = \u03b3 c , S i = \u03b2 l } where I c = {i : y i = c}.\nThe probability that a given tally vector \u03b3 occurs is thus given by P D (\u03b3(x, D + D) = \u03b3) = n i=1 L l=1 P B l i (\u03b3) . For fixed i we notice that the for different classes, the events {#{j \u2208 I c : X j \u2ab0 X i } = \u03b3 c } are pairwise independent, conditioned on {S i = \u03b2 l }. Writing P l c (i, \u03b3) for the conditional probability P (#{i \u2208 I c : y i = c} = \u03b3 c | S i = \u03b2 l ) yields P B l i (\u03b3) = p l i \u00b7 C c=1 P l c (i, \u03b3) and hence q(y| x, D) = \u03b3\u2208\u0393 k n i=1 L l=1 p l i \u00b7 C c=1 P l c (i, \u03b3) which requires O(K C \u00b7n\u00b7L\u00b7C) evaluations of P l c . The next step is to compute the probabilities P l c . For that purpose, we need to open up the binary relation \u2ab0. Suppose first that y i \u0338 = c. Then the event that exactly \u03b3 c instances of class c satisfy X j \u2ab0 X i is the same as the event that for some r \u2264 \u03b3 c exactly r instances with index larger than i have similarity strictly larger than X i and exactly \u03b3 c \u2212 r instances with an index smaller than i have similarity larger or equal than X i . Now suppose that y i = c. Then, the event that exactly \u03b3 c instances of the same class c satisfy X j \u2ab0 X i is the same as the event that for some r \u2264 \u03b3 c exactly r instances with an index larger than i have similarity strictly larger than X i and exactly \u03b3 c \u2212 r \u2212 1 instances with an index smaller than i have similarity larger or equal than X i . This reasoning allows us to write P l c in terms of functions R l c (i, r) := P (|{j \u2208 I c : S j > \u03b2 l , j > i}| = r)\n(45) Q l c (i, r) := P (|{j \u2208 I c : S j \u2265 \u03b2 l , j < i}| = r)(46)as P l c (i, \u03b3) = \u03b3c r=0 R l c (i, r) \u00b7 Q l c (i, \u03b3 c \u2212 r) y i \u0338 = c \u03b3c\u22121 r=0 R l c (i, r) \u00b7 Q l c (i, \u03b3 c \u2212 r \u2212 1) y i = c.\nThe functions R c l and Q c l exhibit a recursive structure that we wish to exploit to get an efficient algorithm. Specifically, we write \u03b1 l i := P(S i \u2264 \u03b2 l ) = L s=l p l i , and\u1fb1 l i := 1 \u2212 \u03b1 l i and use the following recursion\nR c l (i \u2212 1, r) = R l c (i, r) y i \u0338 = c \u03b1 l i \u00b7 R l c (i, r \u2212 1) + \u03b1 l i \u00b7 R l c (i, r) y i = c\nstarting at i = n and r = 0 and with initial values R l c (i, \u22121) = 0, R c l (n, 0) = 1 and R c l (n, r) = 0 for r \u2265 1. Similarly,\nQ l c (i + 1, r) = \uf8f1 \uf8f2 \uf8f3 Q l c (i, r) y i \u0338 = c \u03b1 l+1 i \u00b7 Q l c (i, r \u2212 1) + \u03b1 l+1 i \u00b7 Q l c (i, r) y i = c(47)\nstarting the recursion at i = 1 and r = 0 and with initial values Q l c (i, \u22121) = 0, Q l c (1, 0) = 1 and Q l c (1, r) = 0 for r \u2265 1. Evaluating P l c requires O(K) calls to R l c and Q l c each. The computation of R l c and Q l c can be achieved in O(n \u00b7 K) if the values \u03b1 l i are computed beforehand and stored separately (requiring O(n \u00b7 L) computations). The entire computation has complexity O(K C+2 \u00b7n 2 \u00b7L\u00b7C).\n\n\nAppendix C. Additional Experimental Results\n\n\nC.1. All-to-all Attacks\n\nIn previous evaluations, the attack goal is to fool the model into a specific class. Here, we consider another attack goal that, on seeing the trigger pattern, the model will change its prediction from the i-th class to the ((i + 1)%C)th class, where C is the number of classes. Different with the previous goal, the model here will need to recognize both the image and the trigger to make the malicious prediction. Thus, the defenses which assume that the backdoored model makes behavior only based on the backdoor trigger (e.g. NC) will intuitively not have a good performance.\n\nThe result of the all-to-all attack is shown in Table C.3. We observe that our approach achieves a similar performance for empirical and certified robustness. The performance on MNIST and ImageNette is slightly better compared with the standard attack, while on CIFAR-10 it decreases a little. As for the baselines, we can observe that the performance of Mixup is also consistent with that on the standard attack. This is understandable as Mixup also performs defense by processing the input and does not rely on model analysis. By comparison, the other baseline approaches based on model analysis does not achieve a good performance here. We owe it to the reason that in all-to-all attacks, the trained model needs to focus on both original image and the trigger pattern, so it is more difficult to detect the backdoors by model analysis than in standard attack where the model only focuses on the trigger pattern.\n\n\nC.2. Larger Perturbation\n\nWe consider a larger perturbation consisting of a 4 \u00d7 4 trigger pattern with poison rate at 20% and perturbation scale ||\u03b4 i || = 4.0 on MNIST and ||\u03b4 i || = 4 \u221a 3 on CIFAR-10 and ImageNette (the \u221a 3 here comes from the fact that we add perturbation on all 3 channels). The results are shown in Table C.4. We can see that such strong perturbation is too large to be within our certification radius, which is a limit of our work. Therefore, the certified robust accuracy is 0. Nevertheless, we can still achieve some non-trivial empirical robustness and is comparable with baselines. This shows that our approach can be applied empirically to defend against strong backdoors with larger perturbation.\n\n\nC.3. Kernel-KNN\n\nWe evaluate the defense on KNNs with a kernel function. The kernel function is learned with the convolution neural network trained on the supervised task and uses the hidden representation of the last layer before output as the kernel output. Note that in this case, our exact KNN certification algorithm cannot be applied since the output with Gaussian variable cannot be analyzed with the kernel function. Therefore, we use the evaluation algorithm as in DNN to evaluate the certification performance. As shown in Table C.5, our approach achieves worse performance than on DNNs, which is understandable since KNN models are known to usually underperform DNN models. On the other hand, we observe that many baselines actually have a better performance than DNN. We view the reason to be that the baselines are based on the detection-and-removal algorithm. We find that the detection will only remove a subset of backdoored instances, so a trained DNN model will still be backdoored; however, any removal of backdoored training data will help the performance of KNN since fewer backdoored instances will be viewed as neighborhood, so the performance may improve. By comparison, RAB will not detect and remove instances and thus will not have a better performance on KNN.\n\n\nC.4. SVM-based model on tabular data\n\nAs our certification for DNN can be applied to any machine learning model, we now evaluate RAB on three tabular data -UCI Spambase dataset (Spambase) [12], and \"Adult\" and \"Agaricus lepiota\" (Mushroom) in the Penn Machine Learning Benchmarks (PMLB) datasets [39]. These datasets are all binary classification tasks. Spambase contains 4,601 data points, with 57-dimensional input; Adult contains 48,842 data points with 14-dimensional input; Mushroom contains 8,145 data points with 22-dimensional input. We train a support vector machine (SVM) with RBF kernel using the default setting in scikit-learn toolkit [37]. As for the baselines where activation vectors are required, we use the output prediction vector as its representation, since there are no hidden activation layers in an SVM model.\n\nThe result of the SVM dataset is shown in Table C.6. NC is not evaluated because it relies on anomaly detection among different classes, and therefore cannot be applied on these binary classification tasks; Mixup is not evaluated because it cannot be applied in the SVM training algorithm. We can see that our approach still achieves good robustness both empirically and certifiably. Meanwhile, the baseline approaches cannot perform well as they are designed specifically for deep neural networks. In the SVM case where they use the output as the representation vector, the detection performance cannot be good.\n\n\nC.5. With & Without Test-time Augmentation\n\nTable C.7 shows the comparison of empirical and certified robustness with and without test-time augmentation. We see that the test-time augmentation indeed helps with the model robustness both empirically and certifiably.\n\n\nOne-Pixel\n\nFour-Pixel Blending Figure 7: Adversarial examples against backdoored RAB model on the ImageNette dataset.\n\nC.6. Abstain Rate Table C.8 shows the abstain rate of RAB against attacks. We see that in general, the abstain rate is relatively low and will not be a serious concern in the pipeline. Note that if the denial-of-service attack is indeed a concern, we can perform a similar way as in [10] to prove a certified radius in which we can certify our defense rather than abstaining the input.\n\n\nC.7. Multiple Runs\n\nTo see the stability of RAB, we run our algorithm 5 times and report the mean and standard deviation in Table C.9. We can see that the standard deviation is relatively small, indicating that our algorithm is stable.\n\n\nC.8. Adversarial Atacks on RAB Models\n\nIn [46], the authors show that if they smooth a backdoored model, the robustified version will still be broken (i.e. with obvious adversarial pattern). We replicate the experiments on the RAB model by performing adversarial attacks against RAB model. In order to do attack, we use the PGD attack where the gradient is calculated by aggregating the gradient on all the trained models. In Figure 7, We show the results on ImageNette with \u03b5 = 60 so that the pattern is the most clear. We observe that the adversarial examples look similar with those of unsmoothed model in [46]. Thus, the RAB pipeline is different with the smoothing process; rather, it is similar with an unsmoothed vanilla model.  \n\n\nq(y| x, D) = P (K-NN(x, D + D) = y)\n\nFigure 3 :\n3Examples of the applied backdoor patterns.\n\nFigure 4 :\n4Certified accuracy of DNN and KNN at different radii with different smoothing parameters \u03c3 against blending attack.\n\nFigure 5 :\n5Comparison of the certified accuracy at different radii with and without the proposed deterministic test-time augmentation. The accuracy is evaluated against blending attack with smoothing parameter \u03c3 = 0.2.\n\nFigure 6 :\n6Runtime comparison for certifying one input.\n\n\nCorollary 1 but with a uniform distribution on the feature vectors D \u223c U([a, b]) and construct the likelihood ratio tests in the uniform case. Denote by S := n i=1 S i , S i := d j=1 [a + \u03b4 ij , b + \u03b4 ij ] the support ofD := \u2206 + D and by S \u2032 := n i=1 [a, b] d the support ofD \u2032 := D.\n\nn i=1 N\ni=1(0, \u03c3 2 1 d ) and defined by the probability q(y| x, D) = P D (K-NN(x, D + D) = y)\n\n\nPoisoned Training Set ! Figure 2: An illustration of the RAB robust training process. Given a poisoned training set D+\u2206 and a training process A vulnerable to backdoor attacks, RAB generates N smoothed training sets {D i } i\u2208[N ] and trains N different classifiers A i . a smoothed classifier against adversarial examples during test time. First, a given classifier is smoothed by adding Gaussian noise \u03f5 \u223c N (0, \u03c3 2 1 d ) around each test instance.\u2026 \n\nSmoothed \nTraining \nSet ! + # ! \n\nSmoothed \nTraining \nSet ! + # \" \n\nRAB Process \n\nModel 1 \n\nModel \" \n\n\u2026 \n\u2026 \n\nClean/ \nPoisoned Test \nExample \n\nAggregation \n\nRobust \nPrediction \n(e.g., Lion) \n\n\n\n\nModel and the Goal of Defense 3.2.1. Threat Model. An adversary carries out a backdoor attack against a classifier h and a clean dataset\n\nTABLE 1 :\n1Evaluation on DNNs with different datasets. We use \u03c3 = 0.5 for MNIST and \u03c3 = 0.2 for CIFAR-10 and ImageNette. \"Vanilla\" denotes DNNs without RAB training and \"RAB-cert\" is the certified accuracy of RAB. The highest empirical robust accuracies are bolded. The robust accuracy scores are evaluated only on successfully backdoored instances.Backdoor \nPattern \n\nAcc. on Benign Instances \nEmpirical Robust Acc. \nCertified Robust Acc. \n\nVanilla \nRAB \nVanilla \nRAB \nAC [6] Spectral [48] Sphere [45] NC [50] SCAn [47] Mixup [3] \nRAB-cert \n\nMNIST \n\nOne-pixel \n92.7% \n92.6% \n0% \n41.2% \n64.3% \n3.4% \n3.1% \n76.2% \n45.6% \n34.5% \n23.5% \nFour-pixel \n92.7% \n92.6% \n0% \n40.7% \n56.9% \n2.8% \n2.1% \n79.9% \n45.4% \n33.2% \n24.1% \nBlending \n92.9% \n92.6% \n0% \n39.6% \n63.6% \n3.0% \n1.8% \n63.0% \n44.7% \n28.3% \n23.1% \n\nCIFAR-10 \n\nOne-pixel \n59.9% \n56.7% \n0% \n42.9% \n31.4% \n31.2% \n16.5% \n15.7% \n12.9% \n26.5% \n24.5% \nFour-pixel \n59.4% \n56.8% \n0% \n42.8% \n28.9% \n31.4% \n15.0% \n16.8% \n16.5% \n31.8% \n24.1% \nBlending \n60.5% \n56.8% \n0% \n42.8% \n27.4% \n28.0% \n16.5% \n16.6% \n15.8% \n30.0% \n24.1% \n\nImageNette \n\nOne-pixel \n93.0% \n91.6% \n0% \n38.6% \n44.7% \n47.8% \n29.6% \n69.9% \n35.2% \n55.1% \n15.9% \nFour-pixel \n93.7% \n91.5% \n0% \n38.4% \n54.2% \n52.8% \n42.1% \n67.9% \n49.7% \n51.6% \n12.6% \nBlending \n94.8% \n91.8% \n0% \n29.9% \n46.3% \n18.4% \n31.0% \n66.7% \n33.3% \n56.3% \n9.2% \n\n\n\nTABLE 2 :\n2Evaluation on KNNs with K = 3 on the UCI Spambase tabular dataset. We use \u03c3 = 0.5 for Spam. \"Vanilla\" denotes DNNs without RAB training and \"RAB-cert\" is the certified accuracy of RAB. The highest empirical robust accuracies are bolded. The robust accuracy scores are evaluated only on successfully backdoored instances.Backdoor \nPattern \n\nAccuracy on Benign Instances \nEmpirical Robust Acc. \nCertified Robust Acc. \n\nVanilla \nRAB \nVanilla \nRAB \nAC [6] Spectral [48] Sphere [45] SCAn [47] \nRAB-cert \n\nUCI Spambase \n\nOne-pixel \n98.7% \n98.4% \n0% \n54.6% \n9.0% \n9.6% \n2.4% \n10.5% \n36.4% \nFour-pixel \n98.7% \n98.4% \n0% \n50.0% \n9.6% \n9.6% \n3.0% \n11.2% \n33.3% \nBlending \n98.7% \n98.4% \n0% \n58.3% \n8.1% \n8.1% \n1.7% \n9.9% \n41.7% \n\n7.1.1. Datasets and Model. We consider four different \ndatasets, namely the MNIST dataset \n\nTABLE C .\nC3: Evaluation on DNNs with different datasets with an all-to-all attack goal. We use \u03c3 = 0.5 for MNIST and \u03c3 = 0.2 for CIFAR-10 and ImageNette. \"Vanilla\" denotes DNNs without RAB training and \"RAB-cert\" presents certified accuracy of RAB. The highest empirical robust accuracies are bolded.TABLE C.4: Evaluation onDNNs with different datasets with a large attack perturbation. We use \u03c3 = 0.5 for MNIST and \u03c3 = 0.2 for CIFAR-10 and ImageNette. \"Vanilla\" denotes DNNs without RAB training and \"RAB-cert\" presents certified accuracy of RAB. The highest empirical robust accuracies are bolded.TABLE C.5: Evaluation onKernel KNN with different datasets. We use \u03c3 = 0.5 for MNIST and \u03c3 = 0.2 for CIFAR-10 and ImageNette. \"Vanilla\" denotes DNNs without RAB training and \"RAB-cert\" presents certified accuracy of RAB. The highest empirical robust accuracies are bolded.Acc. on Benign InstancesEmpirical Robust Acc. Certified Robust Acc.Backdoor \nPattern \n\nAcc. on Benign Instances \nEmpirical Robust Acc. \nCertified Robust Acc. \n\nVanilla \nRAB \nVanilla \nRAB \nAC [6] Spectral [48] Sphere [45] NC [50] SCAn [47] Mixup [3] \nRAB-cert \n\nMNIST \n\nOne-pixel \n91.5% \n90.2% \n0% \n51.2% \n17.3% \n3.0% \n2.8% \n28.4% \n4.9% \n37.1% \n24.4% \nFour-pixel \n91.6% \n91.3% \n0% \n60.3% \n16.1% \n2.7% \n1.8% \n30.0% \n1.8% \n38.7% \n39.9% \nBlending \n91.5% \n91.2% \n0% \n59.7% \n15.4% \n3.0% \n1.8% \n30.1% \n4.7% \n34.6% \n39.1% \n\nCIFAR-10 \n\nOne-pixel \n58.4% \n52.2% \n0% \n24.9% \n26.7% \n5.7% \n18.2% \n13.2% \n10.1% \n19.7% \n10.5% \nFour-pixel \n57.5% \n52.1% \n0% \n25.1% \n11.2% \n17.8% \n18.3% \n17.0% \n13.3% \n18.7% \n11.6% \nBlending \n58.3% \n52.1% \n0% \n24.8% \n10.0% \n17.7% \n15.9% \n12.5% \n10.7% \n17.0% \n10.9% \n\nImageNette \n\nOne-pixel \n92.5% \n93.0% \n0% \n43.1% \n32.8% \n19.6% \n41.2% \n23.5% \n23.5% \n49.2% \n7.8% \nFour-pixel \n93.6% \n93.0% \n0% \n37.5% \n18.8% \n18.8% \n43.8% \n26.3% \n21.7% \n58.3% \n18.7% \nBlending \n95.0% \n92.9% \n0% \n44.9% \n46.9% \n22.9% \n34.7% \n21.0% \n14.3% \n49.0% \n16.3% \n\nBackdoor \nPattern \n\nAcc. on Benign Instances \nEmpirical Robust Acc. \nCertified Robust Acc. \n\nVanilla \nRAB \nVanilla \nRAB \nAC [6] \nSpectral [48] Sphere [45] NC [50] SCAn [47] Mixup [3] \nRAB-cert \n\nMNIST \nLarge \n86.8% \n86.5% \n0% \n42.3% \n65.5% \n8.1% \n0.6% \n70.9% \n11.9% \n20.4% \n0% \n\nCIFAR-10 \nLarge \n52.1% \n44.8% \n0% \n27.2% 20.88% \n16.34% \n11.96% \n25.5% \n8.6% \n2.4% \n0% \n\nImageNette \nLarge \n84.7% \n81.6% \n0% \n46.4% \n62.6% \n36.3% \n1.5% \n74.9% \n55.5% \n59.5% \n0% \n\nBackdoor \nPattern \n\nVanilla \nRAB \nVanilla \nRAB \nAC [6] Spectral [48] Sphere [45] NC [50] SCAn [47] Mixup [3] \nRAB-cert \n\nMNIST \n\nOne-pixel \n88.5% \n78.2% \n0% \n35.7% \n45.4% \n53.0% \n48.2% \n53.0% \n55.8% \n59.5% \n18.0% \nFour-pixel \n88.5% \n78.1% \n0% \n36.6% \n50.6% \n53.6% \n48.3% \n69.9% \n55.6% \n52.2% \n18.8% \nBlending \n88.4% \n78.4% \n0% \n36.6% \n44.8% \n52.4% \n47.4% \n51.5% \n55.8% \n52.9% \n18.8% \n\nCIFAR-10 \n\nOne-pixel \n49.7% \n46.5% \n0% \n21.6% \n9.0% \n24.9% \n15.6% \n16.5% \n12.9% \n25.1% \n11.3% \nFour-pixel \n49.5% \n46.6% \n0% \n21.9% \n15.9% \n21.7% \n22.7% \n13.4% \n15.0% \n19.2% \n11.7% \nBlending \n49.8% \n46.6% \n0% \n20.6% \n17.0% \n19.6% \n15.1% \n14.7% \n16.8% \n21.8% \n10.5% \n\nImageNette \n\nOne-pixel \n90.1% \n88.6% \n0% \n35.3% \n56.8% \n22.2% \n28.4% \n40.9% \n19.3% \n31.3% \n8.8% \nFour-pixel \n90.7% \n88.5% \n0% \n32.0% \n52.2% \n29.6% \n41.5% \n34.0% \n30.8% \n27.7% \n7.6% \nBlending \n91.5% \n88.5% \n0% \n32.1% \n33.3% \n17.2% \n2.5% \n23.0% \n13.8% \n21.8% \n7.6% \n\n\n\nTABLE C .\nC6: Evaluation on SVM with different tabular datasets. We use \u03c3 = 0.5 for Spam and \u03c3 = 0.2 for Adult and Mushroom. \"Vanilla\" denotes DNNs without RAB training and \"RAB-cert\" presents certified accuracy of RAB. The highest empirical robust accuracies are bolded.Acc. on Benign InstancesEmpirical Robust Acc. Certified Robust Acc.TABLE C.7: Robustness of RAB on DNNs with and without test-time augmentation.TABLE C.8: The abstain rate of the certification on DNNs.Backdoor \nPattern \n\nVanilla \nRAB \nVanilla \nRAB \nAC [6] Spectral [48] Sphere [45] SCAn [47] \nRAB-cert \n\nSpam \n\nOne-pixel \n91.8% \n88.4% \n0% \n49.1% \n0% \n18.3% \n4.8% \n12.9% \n33.3% \nFour-pixel \n91.2% \n88.6% \n0% \n48.2% \n0% \n6.6% \n7.4% \n11.5% \n32.1% \nBlending \n92.0% \n89.2% \n0% \n44.7% \n0% \n5.8% \n5.8% \n11.5% \n29.8% \n\nAdult \n\nOne-pixel \n79.0% \n77.2% \n0% \n50.7% \n6.3% \n15.3% \n32.2% \n8.4% \n17.1% \nFour-pixel \n77.4% \n73.1% \n0% \n53.0% \n5.4% \n12.8% \n14.4% \n7.1% \n21.5% \nBlending \n78.8% \n76.4% \n0% \n55.9% \n8.0% \n5.0% \n11.6% \n4.7% \n26.1% \n\nMushroom \n\nOne-pixel \n87.5% \n82.0% \n0% \n42.5% \n16.9% \n0% \n6.4% \n17.3% \n23.5% \nFour-pixel \n86.6% \n80.1% \n0% \n42.2% \n14.2% \n0% \n2.8% \n13.9% \n22.5% \nBlending \n87.4% \n81.4% \n0% \n43.5% \n13.1% \n0% \n11.1% \n14.2% \n24.0% \n\nBackdoor \nPattern \n\nWith Aug \nWithout Aug \n\nRAB \nRAB-cert \nRAB \nRAB-cert \n\nMNIST \n\nOne-pixel \n41.2% \n23.5% \n27.0% \n12.7% \nFour-pixel 40.7% \n24.1% \n27.4% \n12.8% \nBlending \n39.6% \n23.1% \n26.2% \n12.1% \n\nCIFAR-10 \n\nOne-pixel \n42.9% \n24.5% \n26.9% \n15.2% \nFour-pixel 44.4% \n25.7% \n28.4% \n16.4% \nBlending \n42.8% \n24.1% \n27.8% \n15.8% \n\nImageNette \n\nOne-pixel \n38.6% \n15.9% \n22.7% \n5.1% \nFour-pixel 38.4% \n12.6% \n22.6% \n8.2% \nBlending \n29.9% \n9.2% \n18.7% \n4.1% \n\nBackdoor Pattern Abstain Rate \n\nMNIST \n\nOne-pixel \n3.32% \nFour-pixel \n3.21% \nBlending \n3.02% \n\nCIFAR-10 \n\nOne-pixel \n5.59% \nFour-pixel \n6.00% \nBlending \n5.29% \n\nImageNette \n\nOne-pixel \n3.89% \nFour-pixel \n4.08% \nBlending \n1.90% \n\n\n\nTABLE C .\nC9: The mean and standard deviation of the RAB robustness on DNNs with 5 runs.Backdoor Pattern \nRAB \nRAB-cert \n\nMNIST \n\nOne-pixel \n40.79\u00b10.72% 23.36\u00b10.52% \nFour-pixel \n40.27\u00b10.87% 24.37\u00b10.49% \nBlending \n40.72\u00b10.65% 23.58\u00b10.88% \n\nCIFAR-10 \n\nOne-pixel \n42.66\u00b10.29% 24.35\u00b10.31% \nFour-pixel \n42.56\u00b10.32% 25.25\u00b10.37% \nBlending \n42.89\u00b10.21% 23.95\u00b10.17% \n\nImageNette \n\nOne-pixel \n38.64\u00b10.80% 15.45\u00b10.94% \nFour-pixel \n37.23\u00b10.69% 12.45\u00b10.82% \nBlending \n28.74\u00b11.15% \n9.20\u00b11.40% \n\nAcknowledgementThis work is partially supported by NSFAppendix A. ProofsHere we provide the proofs for the results stated in the main part of the paper. We write \u03b1(\u03d5) = \u03b1(\u03d5; P 0 ) and \u03b2(\u03d5) = \u03b2(\u03d5; P 0 , P 1 ) for type-I and -II error probabilities.A.1. Proof of Theorem 1Preliminaries and Auxiliary Lemmas: Central to our theoretical results are likelihood ratio tests which are statistical hypothesis tests for testing whether a sample x originates from a distribution X 0 or X 1 . These tests are defined aswhere q and t are chosen such that \u03d5 has significance \u03b1 0 , i.e. \u03b1(\u03d5) = P 0 (\u039b(X) > t) + q \u00b7 P 0 (\u039b(X) = t) = \u03b1 0 .Lemma A.1. Let X 0 and X 1 be two random variables with densities f 0 and f 1 with respect to a measure \u00b5 and denote by \u039b the likelihood ratio \u039b(Proof. We first show the RHS of inequality(21). This follows directly from the definition of t p if we show that the function t \u2192 P(\u039b(X 0 ) \u2264 t) is right-continuous. Let t \u2265 0 and let {t n } n be a sequence in R \u22650 such that t n \u2193 t. Define the sets A n := {x : \u039b(x) \u2264 t n } and note that P(\u039b(X 0 ) \u2264 t n ) = P(X 0 \u2208 A n ). Clearly, if x \u2208 {x : \u039b(x) \u2264 t} then \u2200n : \u039b(x) \u2264 t \u2264 t n and thus x \u2208 \u2229 n A n . If on the other hand x \u2208 \u2229 n A n then \u2200n : \u039b(x) \u2264 t n \u2192 t as\nObfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. A Athalye, N Carlini, D Wagner, International conference on machine learning. PMLRA. Athalye, N. Carlini, and D. Wagner, \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples,\" in International conference on machine learning. PMLR, 2018, pp. 274-283.\n\nPoisoning attacks against support vector machines. B Biggio, B Nelson, P Laskov, Proceedings of the 29th International Coference on Machine Learning. the 29th International Coference on Machine LearningB. Biggio, B. Nelson, and P. Laskov, \"Poisoning attacks against support vector machines,\" in Proceedings of the 29th International Coference on Machine Learning, 2012, p. 1467-1474.\n\nStrong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff. E Borgnia, V Cherepanova, L Fowl, A Ghiasi, J Geiping, M Goldblum, T Goldstein, A Gupta, ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEICASSPE. Borgnia, V. Cherepanova, L. Fowl, A. Ghiasi, J. Geiping, M. Gold- blum, T. Goldstein, and A. Gupta, \"Strong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff,\" in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 3855- 3859.\n\nMitigating evasion attacks to deep neural networks via region-based classification. X Cao, N Z Gong, Proceedings of the 33rd Annual Computer Security Applications Conference. the 33rd Annual Computer Security Applications ConferenceX. Cao and N. Z. Gong, \"Mitigating evasion attacks to deep neural networks via region-based classification,\" in Proceedings of the 33rd Annual Computer Security Applications Conference, 2017, pp. 278- 287.\n\nAdversarial examples are not easily detected: Bypassing ten detection methods. N Carlini, D Wagner, Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. the 10th ACM Workshop on Artificial Intelligence and SecurityACMN. Carlini and D. Wagner, \"Adversarial examples are not easily detected: Bypassing ten detection methods,\" in Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. ACM, 2017, pp. 3-14.\n\nDetecting backdoor attacks on deep neural networks by activation clustering. B Chen, W Carvalho, N Baracaldo, H Ludwig, B Edwards, T Lee, I Molloy, B Srivastava, SafeAI@ AAAI. B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee, I. Molloy, and B. Srivastava, \"Detecting backdoor attacks on deep neural networks by activation clustering,\" in SafeAI@ AAAI, 2019.\n\nDeepinspect: a blackbox trojan detection and mitigation framework for deep neural networks. H Chen, C Fu, J Zhao, F Koushanfar, Proceedings of the 28th International Joint Conference on Artificial Intelligence. the 28th International Joint Conference on Artificial IntelligenceAAAI PressH. Chen, C. Fu, J. Zhao, and F. Koushanfar, \"Deepinspect: a black- box trojan detection and mitigation framework for deep neural net- works,\" in Proceedings of the 28th International Joint Conference on Artificial Intelligence. AAAI Press, 2019, pp. 4658-4664.\n\nTargeted backdoor attacks on deep learning systems using data poisoning. X Chen, C Liu, B Li, K Lu, D Song, arXiv:1712.05526X. Chen, C. Liu, B. Li, K. Lu, and D. Song, \"Targeted back- door attacks on deep learning systems using data poisoning,\" arXiv:1712.05526, 2017.\n\nSentinet: Detecting localized universal attacks against deep learning systems. E Chou, F Tramer, G Pellegrino, 2020 IEEE Security and Privacy Workshops. IEEEE. Chou, F. Tramer, and G. Pellegrino, \"Sentinet: Detecting local- ized universal attacks against deep learning systems,\" in 2020 IEEE Security and Privacy Workshops (SPW). IEEE, 2020, pp. 48-54.\n\nCertified adversarial robustness via randomized smoothing. J Cohen, E Rosenfeld, Z Kolter, Proceedings of the 36th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research. the 36th International Conference on Machine Learning, ser. Machine Learning Research97J. Cohen, E. Rosenfeld, and Z. Kolter, \"Certified adversarial ro- bustness via randomized smoothing,\" in Proceedings of the 36th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, vol. 97, 09-15 Jun 2019, pp. 1310-1320.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Imagenet: A large-scale hierarchical image database,\" in 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009, pp. 248-255.\n\nUCI machine learning repository. D Dua, C Graff, D. Dua and C. Graff, \"UCI machine learning repository,\" 2017. [Online]. Available: http://archive.ics.uci.edu/ml\n\nStrip: A defence against trojan attacks on deep neural networks. Y Gao, C Xu, D Wang, S Chen, D C Ranasinghe, S Nepal, Proceedings of the 35th Annual Computer Security Applications Conference. the 35th Annual Computer Security Applications ConferenceNew York, NY, USAAssociation for Computing MachineryY. Gao, C. Xu, D. Wang, S. Chen, D. C. Ranasinghe, and S. Nepal, \"Strip: A defence against trojan attacks on deep neural networks,\" in Proceedings of the 35th Annual Computer Security Applications Conference. New York, NY, USA: Association for Computing Machinery, 2019, p. 113-125.\n\nExplaining and harnessing adversarial examples. I J Goodfellow, J Shlens, C Szegedy, International Conference on Learning Representations. I. J. Goodfellow, J. Shlens, and C. Szegedy, \"Explaining and har- nessing adversarial examples,\" International Conference on Learning Representations, 2015.\n\nBadnets: Evaluating backdooring attacks on deep neural networks. T Gu, K Liu, B Dolan-Gavitt, S Garg, IEEE Access. 7T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, \"Badnets: Evaluating backdooring attacks on deep neural networks,\" IEEE Access, vol. 7, pp. 47 230-47 244, 2019.\n\nBadnets: Identifying vulnerabilities in the machine learning model supply chain. T Gu, B Dolan-Gavitt, S Garg, arXiv:1708.06733T. Gu, B. Dolan-Gavitt, and S. Garg, \"Badnets: Identifying vulnerabil- ities in the machine learning model supply chain,\" arXiv:1708.06733, 2017.\n\nSpectre: defending against backdoor attacks using robust statistics. J Hayase, W Kong, R Somani, S Oh, Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning. Research, M. Meila and T. Zhangthe 38th International Conference on Machine Learning, ser. Machine LearningPMLR139J. Hayase, W. Kong, R. Somani, and S. Oh, \"Spectre: defending against backdoor attacks using robust statistics,\" in Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, M. Meila and T. Zhang, Eds., vol. 139. PMLR, 18-24 Jul 2021, pp. 4129-4139. [Online].\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770-778.\n\nImagenette. J Howard, J. Howard, \"Imagenette.\" [Online]. Available: https://github.com/ fastai/imagenette/\n\nIntrinsic certified robustness of bagging against data poisoning attacks. J Jia, X Cao, N Z Gong, AAAIJ. Jia, X. Cao, and N. Z. Gong, \"Intrinsic certified robustness of bagging against data poisoning attacks,\" in AAAI, 2021.\n\nCertified robustness of nearest neighbors against data poisoning and backdoor attacks. J Jia, Y Liu, X Cao, N Z Gong, AAAIJ. Jia, Y. Liu, X. Cao, and N. Z. Gong, \"Certified robustness of nearest neighbors against data poisoning and backdoor attacks,\" in AAAI, 2022.\n\nNearest neighbor classifiers over incomplete information: From certain answers to certain predictions. B Karla\u0161, P Li, R Wu, N M G\u00fcrel, X Chu, W Wu, C Zhang, Proc. VLDB Endow. 143B. Karla\u0161, P. Li, R. Wu, N. M. G\u00fcrel, X. Chu, W. Wu, and C. Zhang, \"Nearest neighbor classifiers over incomplete information: From cer- tain answers to certain predictions,\" Proc. VLDB Endow., vol. 14, no. 3, p. 255-267, nov 2020.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, Tech. Rep. A. Krizhevsky, \"Learning multiple layers of features from tiny im- ages,\" Tech. Rep., 2009.\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. 8611Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \"Gradient-based learning applied to document recognition,\" Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, 1998.\n\nCertified robustness to adversarial examples with differential privacy. M Lecuyer, V Atlidakis, R Geambasu, D Hsu, S Jana, 2019 IEEE Symposium on Security and Privacy (SP). M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana, \"Cer- tified robustness to adversarial examples with differential privacy,\" in 2019 IEEE Symposium on Security and Privacy (SP), 2019, pp. 656-672.\n\nDeep partition aggregation: Provable defenses against general poisoning attacks. A Levine, S Feizi, 9th International Conference on Learning Representations. A. Levine and S. Feizi, \"Deep partition aggregation: Provable de- fenses against general poisoning attacks,\" in 9th International Con- ference on Learning Representations, 2021.\n\nData poisoning attacks on factorization-based collaborative filtering. B Li, Y Wang, A Singh, Y Vorobeychik, Advances in neural information processing systems. B. Li, Y. Wang, A. Singh, and Y. Vorobeychik, \"Data poisoning attacks on factorization-based collaborative filtering,\" in Advances in neural information processing systems, 2016, pp. 1885-1893.\n\nSok: Certified robustness for deep neural networks. L Li, X Qi, T Xie, B Li, arXiv:2009.04131L. Li, X. Qi, T. Xie, and B. Li, \"Sok: Certified robustness for deep neural networks,\" arXiv:2009.04131, 2020.\n\nTss: Transformation-specific smoothing for robustness certification. L Li, M Weber, X Xu, L Rimanic, B Kailkhura, T Xie, C Zhang, B Li, Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security. the 2021 ACM SIGSAC Conference on Computer and Communications SecurityL. Li, M. Weber, X. Xu, L. Rimanic, B. Kailkhura, T. Xie, C. Zhang, and B. Li, \"Tss: Transformation-specific smoothing for robustness certification,\" in Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security, 2021, p. 535-557.\n\nTowards robust neural networks via random self-ensemble. X Liu, M Cheng, H Zhang, C.-J Hsieh, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)X. Liu, M. Cheng, H. Zhang, and C.-J. Hsieh, \"Towards robust neural networks via random self-ensemble,\" in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 369-385.\n\nCharacterizing adversarial subspaces using local intrinsic dimensionality. X Ma, B Li, Y Wang, S M Erfani, S Wijewickrema, G Schoenebeck, M E Houle, D Song, J Bailey, International Conference on Learning Representations. X. Ma, B. Li, Y. Wang, S. M. Erfani, S. Wijewickrema, G. Schoenebeck, M. E. Houle, D. Song, and J. Bailey, \"Character- izing adversarial subspaces using local intrinsic dimensionality,\" in International Conference on Learning Representations, 2018.\n\nData poisoning against differentiallyprivate learners: Attacks and defenses. Y Ma, X Zhu, J Hsu, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. International Joint Conferences on Artificial Intelligence Organization. the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. International Joint Conferences on Artificial Intelligence OrganizationY. Ma, X. Zhu, and J. Hsu, \"Data poisoning against differentially- private learners: Attacks and defenses,\" in Proceedings of the Twenty- Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. International Joint Conferences on Artificial Intelligence Organization, 7 2019, pp. 4732-4738.\n\nTowards deep learning models resistant to adversarial attacks. A Madry, A Makelov, L Schmidt, D Tsipras, A Vladu, International Conference on Learning Representations. A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, \"Towards deep learning models resistant to adversarial attacks,\" in International Conference on Learning Representations, 2018.\n\nTowards poisoning of deep learning algorithms with back-gradient optimization. L Mu\u00f1oz-Gonz\u00e1lez, B Biggio, A Demontis, A Paudice, V Wongrassamee, E C Lupu, F Roli, Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. the 10th ACM Workshop on Artificial Intelligence and SecurityACML. Mu\u00f1oz-Gonz\u00e1lez, B. Biggio, A. Demontis, A. Paudice, V. Won- grassamee, E. C. Lupu, and F. Roli, \"Towards poisoning of deep learning algorithms with back-gradient optimization,\" in Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. ACM, 2017, pp. 27-38.\n\nOn the problem of the most efficient tests of statistical hypotheses. 231. J Neyman, E Pearson, Phil. Trans. Roy. Statistical Soc. A. 289J. Neyman and E. Pearson, \"On the problem of the most efficient tests of statistical hypotheses. 231,\" Phil. Trans. Roy. Statistical Soc. A, vol. 289, 1933.\n\n. M.-I Nicolae, M Sinn, M N Tran, B Buesser, A Rawat, M Wistuba, V Zantedeschi, N Baracaldo, B Chen, H Ludwig, arXiv:1807.01069Adversarial robustness toolbox v1. 0.0M.-I. Nicolae, M. Sinn, M. N. Tran, B. Buesser, A. Rawat, M. Wis- tuba, V. Zantedeschi, N. Baracaldo, B. Chen, H. Ludwig et al., \"Adversarial robustness toolbox v1. 0.0,\" arXiv:1807.01069, 2018.\n\nScikit-learn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 12F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Van- derplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, \"Scikit-learn: Machine learning in Python,\" Journal of Machine Learning Research, vol. 12, pp. 2825-2830, 2011.\n\nSimtrojan: Stealthy backdoor attack. Y Ren, L Li, J Zhou, 2021 IEEE International Conference on Image Processing (ICIP). Y. Ren, L. Li, and J. Zhou, \"Simtrojan: Stealthy backdoor attack,\" in 2021 IEEE International Conference on Image Processing (ICIP).\n\n. IEEE. IEEE, 2021, pp. 819-823.\n\nPMLB v1.0: an open-source dataset collection for benchmarking machine learning methods. J D Romano, T T Le, W La Cava, J T Gregg, D J Goldberg, P Chakraborty, N L Ray, D Himmelstein, W Fu, J H Moore, Bioinformatics. 383J. D. Romano, T. T. Le, W. La Cava, J. T. Gregg, D. J. Goldberg, P. Chakraborty, N. L. Ray, D. Himmelstein, W. Fu, and J. H. Moore, \"PMLB v1.0: an open-source dataset collection for benchmarking machine learning methods,\" Bioinformatics, vol. 38, no. 3, pp. 878- 880, 2021.\n\nCertified robustness to label-flipping attacks via randomized smoothing. E Rosenfeld, E Winston, P Ravikumar, Z Kolter, International Conference on Machine Learning. PMLR, 2020. E. Rosenfeld, E. Winston, P. Ravikumar, and Z. Kolter, \"Certified robustness to label-flipping attacks via randomized smoothing,\" in International Conference on Machine Learning. PMLR, 2020, pp. 8230-8241.\n\nHidden trigger backdoor attacks. A Saha, A Subramanya, H Pirsiavash, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34A. Saha, A. Subramanya, and H. Pirsiavash, \"Hidden trigger back- door attacks,\" in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, 2020, pp. 11 957-11 965.\n\nJust how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks. A Schwarzschild, M Goldblum, A Gupta, J P Dickerson, T Goldstein, Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning. Research, M. Meila and T. Zhangthe 38th International Conference on Machine Learning, ser. Machine LearningPMLR139A. Schwarzschild, M. Goldblum, A. Gupta, J. P. Dickerson, and T. Goldstein, \"Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks,\" in Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, M. Meila and T. Zhang, Eds., vol. 139. PMLR, 18-24 Jul 2021, pp. 9389-9398.\n\nJust how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks. International Conference on Machine Learning. PMLR, 2021. --, \"Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks,\" in International Conference on Machine Learning. PMLR, 2021, pp. 9389-9398.\n\nPoison frogs! targeted clean-label poisoning attacks on neural networks. A Shafahi, W R Huang, M Najibi, O Suciu, C Studer, T Dumitras, T Goldstein, Proceedings of the 32nd International Conference on Neural Information Processing Systems. the 32nd International Conference on Neural Information Processing SystemsA. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras, and T. Goldstein, \"Poison frogs! targeted clean-label poisoning attacks on neural networks,\" in Proceedings of the 32nd International Confer- ence on Neural Information Processing Systems, 2018, p. 6106-6116.\n\nCertified defenses for data poisoning attacks. J Steinhardt, P W Koh, P Liang, Proceedings of the 31st International Conference on Neural Information Processing Systems. the 31st International Conference on Neural Information Processing SystemsJ. Steinhardt, P. W. Koh, and P. Liang, \"Certified defenses for data poi- soning attacks,\" in Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017, pp. 3520-3532.\n\nPoisoned classifiers are not only backdoored, they are fundamentally broken. M Sun, S Agarwal, J Z Kolter, arXiv:2010.09080M. Sun, S. Agarwal, and J. Z. Kolter, \"Poisoned classifiers are not only backdoored, they are fundamentally broken,\" in arXiv:2010.09080, 2020.\n\nDemon in the variant: Statistical analysis of {DNNs} for robust backdoor contamination detection. D Tang, X Wang, H Tang, K Zhang, 30th USENIX Security Symposium. D. Tang, X. Wang, H. Tang, and K. Zhang, \"Demon in the variant: Statistical analysis of {DNNs} for robust backdoor contamination detection,\" in 30th USENIX Security Symposium, 2021, pp. 1541- 1558.\n\nSpectral signatures in backdoor attacks. B Tran, J Li, A Madry, Advances in Neural Information Processing Systems. B. Tran, J. Li, and A. Madry, \"Spectral signatures in backdoor attacks,\" in Advances in Neural Information Processing Systems, 2018, pp. 8000-8010.\n\nOn certifying robustness against backdoor attacks via randomized smoothing. B Wang, X Cao, N Z Gong, CVPR 2020 Workshop on Adversarial Machine Learning in Computer Vision. B. Wang, X. Cao, N. Z. Gong et al., \"On certifying robustness against backdoor attacks via randomized smoothing,\" CVPR 2020 Workshop on Adversarial Machine Learning in Computer Vision, 2020.\n\nNeural cleanse: Identifying and mitigating backdoor attacks in neural networks. B Wang, Y Yao, S Shan, H Li, B Viswanath, H Zheng, B Y Zhao, 2019 IEEE Symposium on Security and Privacy (SP). IEEEB. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao, \"Neural cleanse: Identifying and mitigating backdoor attacks in neural networks,\" in 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019, pp. 707-723.\n\nDefending dnn adversarial attacks with pruning and logits augmentation. S Wang, X Wang, S Ye, P Zhao, X Lin, 2018 IEEE Global Conference on Signal and Information Processing. IEEES. Wang, X. Wang, S. Ye, P. Zhao, and X. Lin, \"Defending dnn adversarial attacks with pruning and logits augmentation,\" in 2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP). IEEE, 2018, pp. 1144-1148.\n\nSha-2 -Wikipedia, the free encyclopedia. 182020Wikipedia contributorsWikipedia contributors, \"Sha-2 -Wikipedia, the free encyclopedia,\" 2020, [Online; accessed 18-March-2020]. [Online]. Available: https://en.wikipedia.org/w/index.php?title=SHA-2&oldid=944705336\n\nGenerating adversarial examples with adversarial networks. C Xiao, B Li, J.-Y Zhu, W He, M Liu, D Song, Proceedings of the 27th International Joint Conference on Artificial Intelligence. the 27th International Joint Conference on Artificial IntelligenceAAAI PressC. Xiao, B. Li, J.-Y. Zhu, W. He, M. Liu, and D. Song, \"Generating adversarial examples with adversarial networks,\" in Proceedings of the 27th International Joint Conference on Artificial Intelligence. AAAI Press, 2018, p. 3905-3911.\n\nCrfl: Certifiably robust federated learning against backdoor attacks. C Xie, M Chen, P.-Y. Chen, B Li, International Conference on Machine Learning. PMLR, 2021. C. Xie, M. Chen, P.-Y. Chen, and B. Li, \"Crfl: Certifiably robust fed- erated learning against backdoor attacks,\" in International Conference on Machine Learning. PMLR, 2021, pp. 11 372-11 382.\n\nFeature squeezing: Detecting adversarial examples in deep neural networks. W Xu, D Evans, Y Qi, 25th Annual Network and Distributed System Security Symposium. W. Xu, D. Evans, and Y. Qi, \"Feature squeezing: Detecting adversarial examples in deep neural networks,\" in 25th Annual Network and Distributed System Security Symposium. The Internet Society, 2018.\n\nDetecting ai trojans using meta neural analysis. X Xu, Q Wang, H Li, N Borisov, C A Gunter, B Li, 2021 IEEE Symposium on Security and Privacy (SP). IEEEX. Xu, Q. Wang, H. Li, N. Borisov, C. A. Gunter, and B. Li, \"Detecting ai trojans using meta neural analysis,\" in 2021 IEEE Symposium on Security and Privacy (SP). IEEE, 2021, pp. 103- 120.\n\nC Yang, Q Wu, H Li, Y Chen, arXiv:1703.01340Generative poisoning attack method against neural networks. C. Yang, Q. Wu, H. Li, and Y. Chen, \"Generative poisoning attack method against neural networks,\" arXiv:1703.01340, 2017.\n\nRandomized smoothing of all shapes and sizes. G Yang, T Duan, J E Hu, H Salman, I Razenshteyn, J Li, International Conference on Machine Learning. PMLR, 2020. 705G. Yang, T. Duan, J. E. Hu, H. Salman, I. Razenshteyn, and J. Li, \"Randomized smoothing of all shapes and sizes,\" in International Conference on Machine Learning. PMLR, 2020, pp. 10 693-10 705.\n\nCharacterizing audio adversarial examples using temporal dependency. Z Yang, B Li, P.-Y. Chen, D Song, International Conference on Learning Representations. Z. Yang, B. Li, P.-Y. Chen, and D. Song, \"Characterizing audio adversarial examples using temporal dependency,\" in International Conference on Learning Representations, 2019.\n\nEnd-to-end robustness for sensing-reasoning machine learning pipelines. Z Yang, Z Zhao, H Pei, B Wang, B Karlas, J Liu, H Guo, B Li, C Zhang, arXiv:2003.00120Z. Yang, Z. Zhao, H. Pei, B. Wang, B. Karlas, J. Liu, H. Guo, B. Li, and C. Zhang, \"End-to-end robustness for sensing-reasoning machine learning pipelines,\" arXiv:2003.00120, 2020.\n\nBackdoor embedding in convolutional neural network models via invisible perturbation. H Zhong, C Liao, A C Squicciarini, S Zhu, D Miller, Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy. the Tenth ACM Conference on Data and Application Security and PrivacyH. Zhong, C. Liao, A. C. Squicciarini, S. Zhu, and D. Miller, \"Backdoor embedding in convolutional neural network models via invisible perturbation,\" in Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy, 2020, pp. 97-108.\n\nTransferable clean-label poisoning attacks on deep neural nets. C Zhu, W R Huang, H Li, G Taylor, C Studer, T Goldstein, Proceedings of the 36th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research. the 36th International Conference on Machine Learning, ser. Machine Learning ResearchPMLR97C. Zhu, W. R. Huang, H. Li, G. Taylor, C. Studer, and T. Goldstein, \"Transferable clean-label poisoning attacks on deep neural nets,\" in Proceedings of the 36th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, vol. 97. PMLR, 2019, pp. 7614-7623.\n", "annotations": {"author": "[{\"end\":141,\"start\":53},{\"end\":223,\"start\":142},{\"end\":305,\"start\":224},{\"end\":384,\"start\":306},{\"end\":439,\"start\":385},{\"end\":490,\"start\":440},{\"end\":550,\"start\":491},{\"end\":611,\"start\":551}]", "publisher": null, "author_last_name": "[{\"end\":66,\"start\":61},{\"end\":152,\"start\":150},{\"end\":236,\"start\":230},{\"end\":314,\"start\":309},{\"end\":390,\"start\":388},{\"end\":501,\"start\":495},{\"end\":562,\"start\":551}]", "author_first_name": "[{\"end\":60,\"start\":53},{\"end\":149,\"start\":142},{\"end\":229,\"start\":224},{\"end\":308,\"start\":306},{\"end\":387,\"start\":385},{\"end\":441,\"start\":440},{\"end\":494,\"start\":491}]", "author_affiliation": "[{\"end\":140,\"start\":94},{\"end\":222,\"start\":176},{\"end\":304,\"start\":258},{\"end\":383,\"start\":337},{\"end\":438,\"start\":392},{\"end\":489,\"start\":443},{\"end\":549,\"start\":503},{\"end\":610,\"start\":564}]", "title": "[{\"end\":50,\"start\":1},{\"end\":661,\"start\":612}]", "venue": null, "abstract": "[{\"end\":2600,\"start\":663}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3113,\"start\":3109},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":3119,\"start\":3115},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":3125,\"start\":3121},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":3131,\"start\":3127},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3233,\"start\":3230},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3239,\"start\":3235},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3245,\"start\":3241},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":3251,\"start\":3247},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3371,\"start\":3367},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3912,\"start\":3908},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4070,\"start\":4067},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4075,\"start\":4072},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4276,\"start\":4272},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4493,\"start\":4489},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4499,\"start\":4495},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":4505,\"start\":4501},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5100,\"start\":5096},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5106,\"start\":5102},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5112,\"start\":5108},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":5118,\"start\":5114},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5775,\"start\":5771},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6336,\"start\":6333},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6342,\"start\":6338},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6447,\"start\":6444},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6453,\"start\":6449},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7146,\"start\":7142},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8952,\"start\":8949},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9349,\"start\":9346},{\"end\":10452,\"start\":10451},{\"end\":10605,\"start\":10604},{\"end\":10771,\"start\":10770},{\"end\":10895,\"start\":10894},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12807,\"start\":12804},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12813,\"start\":12809},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13719,\"start\":13715},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":13725,\"start\":13721},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":13835,\"start\":13831},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13871,\"start\":13868},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14424,\"start\":14421},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14430,\"start\":14426},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14551,\"start\":14547},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14729,\"start\":14725},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23134,\"start\":23130},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26648,\"start\":26644},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27709,\"start\":27705},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28369,\"start\":28366},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":29238,\"start\":29234},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29593,\"start\":29589},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31487,\"start\":31483},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":38188,\"start\":38184},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":38638,\"start\":38634},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":40720,\"start\":40716},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":41425,\"start\":41421},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":44362,\"start\":44358},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":46299,\"start\":46295},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":46730,\"start\":46726},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":49143,\"start\":49139},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":49149,\"start\":49145},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":49155,\"start\":49151},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":49161,\"start\":49157},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":49167,\"start\":49163},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":49205,\"start\":49201},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":49432,\"start\":49428},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":49608,\"start\":49607},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":49613,\"start\":49609},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":49699,\"start\":49695},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":49895,\"start\":49891},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":49972,\"start\":49968},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":50068,\"start\":50064},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":50453,\"start\":50449},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":50490,\"start\":50486},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":50562,\"start\":50558},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":51627,\"start\":51623},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":52117,\"start\":52114},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":52354,\"start\":52350},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":52390,\"start\":52386},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":52575,\"start\":52571},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":52739,\"start\":52735},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":53201,\"start\":53197},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":53584,\"start\":53581},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":54336,\"start\":54332},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":55993,\"start\":55990},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":56528,\"start\":56524},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":57175,\"start\":57171},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":59027,\"start\":59023},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":62528,\"start\":62524},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":66596,\"start\":66593},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":66602,\"start\":66598},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":66621,\"start\":66617},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":66891,\"start\":66888},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":66897,\"start\":66893},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":66971,\"start\":66967},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":67062,\"start\":67059},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":67245,\"start\":67241},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":67404,\"start\":67401},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":67619,\"start\":67616},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":67744,\"start\":67740},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":67842,\"start\":67838},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":68002,\"start\":67999},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":68180,\"start\":68176},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":68483,\"start\":68479},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":68709,\"start\":68705},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":69098,\"start\":69094},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":69373,\"start\":69369},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":69610,\"start\":69606},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":69818,\"start\":69814},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":69922,\"start\":69918},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":70259,\"start\":70255},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":72220,\"start\":72216},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":73511,\"start\":73507},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":74549,\"start\":74545},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":75346,\"start\":75342},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":75472,\"start\":75468},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":75565,\"start\":75561},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":76829,\"start\":76825},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":78392,\"start\":78389},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":79347,\"start\":79343},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":80625,\"start\":80621},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":80702,\"start\":80698},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":91166,\"start\":91162},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":91274,\"start\":91270},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":91626,\"start\":91622},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":93098,\"start\":93094},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":93483,\"start\":93479},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":94050,\"start\":94046},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":103210,\"start\":103209}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":94211,\"start\":94174},{\"attributes\":{\"id\":\"fig_1\"},\"end\":94267,\"start\":94212},{\"attributes\":{\"id\":\"fig_2\"},\"end\":94396,\"start\":94268},{\"attributes\":{\"id\":\"fig_4\"},\"end\":94617,\"start\":94397},{\"attributes\":{\"id\":\"fig_5\"},\"end\":94675,\"start\":94618},{\"attributes\":{\"id\":\"fig_6\"},\"end\":94961,\"start\":94676},{\"attributes\":{\"id\":\"fig_7\"},\"end\":95056,\"start\":94962},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":95703,\"start\":95057},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":95842,\"start\":95704},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":97180,\"start\":95843},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":98002,\"start\":97181},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":101300,\"start\":98003},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":103196,\"start\":101301},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":103678,\"start\":103197}]", "paragraph": "[{\"end\":3132,\"start\":2616},{\"end\":3833,\"start\":3134},{\"end\":4076,\"start\":3835},{\"end\":4933,\"start\":4078},{\"end\":5461,\"start\":4935},{\"end\":6129,\"start\":5463},{\"end\":7007,\"start\":6131},{\"end\":7661,\"start\":7009},{\"end\":9647,\"start\":7663},{\"end\":10066,\"start\":9649},{\"end\":10369,\"start\":10068},{\"end\":11270,\"start\":10371},{\"end\":12307,\"start\":11272},{\"end\":12496,\"start\":12322},{\"end\":13039,\"start\":12517},{\"end\":13442,\"start\":13041},{\"end\":14046,\"start\":13444},{\"end\":14258,\"start\":14048},{\"end\":14671,\"start\":14283},{\"end\":15202,\"start\":14673},{\"end\":15433,\"start\":15204},{\"end\":16019,\"start\":15435},{\"end\":16227,\"start\":16056},{\"end\":17425,\"start\":16240},{\"end\":17668,\"start\":17461},{\"end\":18394,\"start\":17698},{\"end\":18637,\"start\":18465},{\"end\":18913,\"start\":18680},{\"end\":19498,\"start\":18966},{\"end\":19948,\"start\":19518},{\"end\":20432,\"start\":20033},{\"end\":21295,\"start\":20434},{\"end\":22222,\"start\":21297},{\"end\":22621,\"start\":22224},{\"end\":23511,\"start\":22668},{\"end\":24277,\"start\":23557},{\"end\":24772,\"start\":24322},{\"end\":25229,\"start\":24774},{\"end\":26052,\"start\":25265},{\"end\":26282,\"start\":26054},{\"end\":26842,\"start\":26340},{\"end\":27085,\"start\":26844},{\"end\":28537,\"start\":27133},{\"end\":28706,\"start\":28593},{\"end\":28833,\"start\":28768},{\"end\":28946,\"start\":28835},{\"end\":29594,\"start\":28948},{\"end\":29692,\"start\":29596},{\"end\":31289,\"start\":29694},{\"end\":31855,\"start\":31291},{\"end\":32728,\"start\":32007},{\"end\":33591,\"start\":32747},{\"end\":34857,\"start\":33604},{\"end\":35063,\"start\":34921},{\"end\":35300,\"start\":35255},{\"end\":35372,\"start\":35356},{\"end\":35879,\"start\":35416},{\"end\":36110,\"start\":35928},{\"end\":36528,\"start\":36144},{\"end\":36732,\"start\":36591},{\"end\":36956,\"start\":36918},{\"end\":37017,\"start\":37012},{\"end\":37340,\"start\":37102},{\"end\":37673,\"start\":37392},{\"end\":38864,\"start\":37675},{\"end\":39157,\"start\":38928},{\"end\":40154,\"start\":40114},{\"end\":41119,\"start\":40161},{\"end\":41517,\"start\":41144},{\"end\":42535,\"start\":41643},{\"end\":42691,\"start\":42550},{\"end\":43489,\"start\":42753},{\"end\":44321,\"start\":43491},{\"end\":44893,\"start\":44323},{\"end\":45539,\"start\":44917},{\"end\":45892,\"start\":45541},{\"end\":46036,\"start\":45955},{\"end\":46205,\"start\":46038},{\"end\":46445,\"start\":46207},{\"end\":46680,\"start\":46447},{\"end\":47457,\"start\":46682},{\"end\":47634,\"start\":47459},{\"end\":48228,\"start\":47659},{\"end\":49033,\"start\":48230},{\"end\":50576,\"start\":49054},{\"end\":51360,\"start\":50599},{\"end\":52085,\"start\":51410},{\"end\":53798,\"start\":52087},{\"end\":54377,\"start\":53800},{\"end\":55014,\"start\":54401},{\"end\":56462,\"start\":55074},{\"end\":56615,\"start\":56464},{\"end\":57273,\"start\":56617},{\"end\":57658,\"start\":57331},{\"end\":59102,\"start\":57693},{\"end\":59314,\"start\":59116},{\"end\":59993,\"start\":59316},{\"end\":61245,\"start\":60013},{\"end\":61821,\"start\":61247},{\"end\":62160,\"start\":61867},{\"end\":63018,\"start\":62162},{\"end\":63961,\"start\":63072},{\"end\":64736,\"start\":64000},{\"end\":66309,\"start\":64738},{\"end\":68586,\"start\":66326},{\"end\":71019,\"start\":68588},{\"end\":71534,\"start\":71035},{\"end\":72222,\"start\":71536},{\"end\":74069,\"start\":72252},{\"end\":74390,\"start\":74071},{\"end\":74585,\"start\":74470},{\"end\":74910,\"start\":74794},{\"end\":75135,\"start\":74912},{\"end\":75277,\"start\":75161},{\"end\":75936,\"start\":75331},{\"end\":76293,\"start\":75988},{\"end\":76518,\"start\":76346},{\"end\":77032,\"start\":76546},{\"end\":77114,\"start\":77034},{\"end\":77677,\"start\":77257},{\"end\":78065,\"start\":77799},{\"end\":78122,\"start\":78119},{\"end\":78244,\"start\":78184},{\"end\":78508,\"start\":78336},{\"end\":78603,\"start\":78510},{\"end\":78854,\"start\":78633},{\"end\":79093,\"start\":78856},{\"end\":79421,\"start\":79120},{\"end\":79668,\"start\":79531},{\"end\":80269,\"start\":80203},{\"end\":80965,\"start\":80475},{\"end\":81462,\"start\":81361},{\"end\":81915,\"start\":81570},{\"end\":82185,\"start\":82051},{\"end\":82304,\"start\":82275},{\"end\":82420,\"start\":82407},{\"end\":82589,\"start\":82527},{\"end\":83034,\"start\":82690},{\"end\":83209,\"start\":83164},{\"end\":83546,\"start\":83293},{\"end\":83780,\"start\":83606},{\"end\":83902,\"start\":83808},{\"end\":84016,\"start\":83904},{\"end\":84661,\"start\":84077},{\"end\":86204,\"start\":84752},{\"end\":86624,\"start\":86394},{\"end\":86853,\"start\":86723},{\"end\":87383,\"start\":86966},{\"end\":88036,\"start\":87457},{\"end\":88953,\"start\":88038},{\"end\":89681,\"start\":88982},{\"end\":90971,\"start\":89701},{\"end\":91807,\"start\":91012},{\"end\":92421,\"start\":91809},{\"end\":92689,\"start\":92468},{\"end\":92809,\"start\":92703},{\"end\":93196,\"start\":92811},{\"end\":93434,\"start\":93219},{\"end\":94173,\"start\":93476}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17460,\"start\":17426},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17697,\"start\":17678},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18464,\"start\":18395},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18679,\"start\":18638},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18965,\"start\":18914},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20032,\"start\":19949},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23556,\"start\":23528},{\"attributes\":{\"id\":\"formula_7\"},\"end\":24321,\"start\":24278},{\"attributes\":{\"id\":\"formula_9\"},\"end\":26339,\"start\":26283},{\"attributes\":{\"id\":\"formula_11\"},\"end\":28592,\"start\":28538},{\"attributes\":{\"id\":\"formula_12\"},\"end\":28767,\"start\":28707},{\"attributes\":{\"id\":\"formula_13\"},\"end\":31966,\"start\":31856},{\"attributes\":{\"id\":\"formula_14\"},\"end\":34920,\"start\":34858},{\"attributes\":{\"id\":\"formula_15\"},\"end\":35254,\"start\":35064},{\"attributes\":{\"id\":\"formula_16\"},\"end\":35355,\"start\":35301},{\"attributes\":{\"id\":\"formula_17\"},\"end\":35415,\"start\":35373},{\"attributes\":{\"id\":\"formula_18\"},\"end\":35927,\"start\":35880},{\"attributes\":{\"id\":\"formula_19\"},\"end\":36590,\"start\":36529},{\"attributes\":{\"id\":\"formula_20\"},\"end\":36917,\"start\":36733},{\"attributes\":{\"id\":\"formula_21\"},\"end\":37011,\"start\":36957},{\"attributes\":{\"id\":\"formula_22\"},\"end\":37101,\"start\":37018},{\"attributes\":{\"id\":\"formula_23\"},\"end\":37391,\"start\":37341},{\"attributes\":{\"id\":\"formula_24\"},\"end\":40113,\"start\":39158},{\"attributes\":{\"id\":\"formula_25\"},\"end\":41642,\"start\":41518},{\"attributes\":{\"id\":\"formula_26\"},\"end\":42752,\"start\":42692},{\"attributes\":{\"id\":\"formula_27\"},\"end\":45954,\"start\":45893},{\"attributes\":{\"id\":\"formula_29\"},\"end\":55073,\"start\":55015},{\"attributes\":{\"id\":\"formula_30\"},\"end\":74469,\"start\":74391},{\"attributes\":{\"id\":\"formula_31\"},\"end\":74793,\"start\":74586},{\"attributes\":{\"id\":\"formula_32\"},\"end\":75160,\"start\":75136},{\"attributes\":{\"id\":\"formula_33\"},\"end\":75330,\"start\":75278},{\"attributes\":{\"id\":\"formula_34\"},\"end\":75987,\"start\":75937},{\"attributes\":{\"id\":\"formula_35\"},\"end\":76345,\"start\":76294},{\"attributes\":{\"id\":\"formula_36\"},\"end\":77256,\"start\":77115},{\"attributes\":{\"id\":\"formula_37\"},\"end\":77798,\"start\":77678},{\"attributes\":{\"id\":\"formula_38\"},\"end\":78118,\"start\":78066},{\"attributes\":{\"id\":\"formula_39\"},\"end\":78183,\"start\":78123},{\"attributes\":{\"id\":\"formula_40\"},\"end\":78335,\"start\":78245},{\"attributes\":{\"id\":\"formula_41\"},\"end\":79119,\"start\":79094},{\"attributes\":{\"id\":\"formula_42\"},\"end\":79530,\"start\":79422},{\"attributes\":{\"id\":\"formula_43\"},\"end\":79708,\"start\":79669},{\"attributes\":{\"id\":\"formula_44\"},\"end\":80202,\"start\":79708},{\"attributes\":{\"id\":\"formula_45\"},\"end\":80446,\"start\":80270},{\"attributes\":{\"id\":\"formula_46\"},\"end\":81064,\"start\":80966},{\"attributes\":{\"id\":\"formula_47\"},\"end\":81360,\"start\":81064},{\"attributes\":{\"id\":\"formula_48\"},\"end\":81569,\"start\":81463},{\"attributes\":{\"id\":\"formula_49\"},\"end\":81976,\"start\":81916},{\"attributes\":{\"id\":\"formula_50\"},\"end\":82050,\"start\":81976},{\"attributes\":{\"id\":\"formula_51\"},\"end\":82274,\"start\":82186},{\"attributes\":{\"id\":\"formula_52\"},\"end\":82406,\"start\":82305},{\"attributes\":{\"id\":\"formula_53\"},\"end\":82526,\"start\":82421},{\"attributes\":{\"id\":\"formula_54\"},\"end\":82649,\"start\":82590},{\"attributes\":{\"id\":\"formula_55\"},\"end\":83163,\"start\":83035},{\"attributes\":{\"id\":\"formula_56\"},\"end\":83292,\"start\":83210},{\"attributes\":{\"id\":\"formula_57\"},\"end\":83605,\"start\":83547},{\"attributes\":{\"id\":\"formula_59\"},\"end\":84076,\"start\":84017},{\"attributes\":{\"id\":\"formula_60\"},\"end\":84751,\"start\":84662},{\"attributes\":{\"id\":\"formula_61\"},\"end\":86269,\"start\":86205},{\"attributes\":{\"id\":\"formula_62\"},\"end\":86393,\"start\":86269},{\"attributes\":{\"id\":\"formula_63\"},\"end\":86722,\"start\":86625},{\"attributes\":{\"id\":\"formula_64\"},\"end\":86965,\"start\":86854}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":55447,\"start\":55440},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":57914,\"start\":57907},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":59326,\"start\":59319},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":60361,\"start\":60354},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":64188,\"start\":64181},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":89284,\"start\":89277},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":90224,\"start\":90217},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":92836,\"start\":92829}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2614,\"start\":2602},{\"attributes\":{\"n\":\"2.\"},\"end\":12320,\"start\":12310},{\"attributes\":{\"n\":\"2.1.\"},\"end\":12515,\"start\":12499},{\"attributes\":{\"n\":\"2.2.\"},\"end\":14281,\"start\":14261},{\"attributes\":{\"n\":\"3.\"},\"end\":16054,\"start\":16022},{\"attributes\":{\"n\":\"3.1.\"},\"end\":16238,\"start\":16230},{\"attributes\":{\"n\":\"3.2.\"},\"end\":17677,\"start\":17671},{\"attributes\":{\"n\":\"3.3.\"},\"end\":19516,\"start\":19501},{\"attributes\":{\"n\":\"4.\"},\"end\":22666,\"start\":22624},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23527,\"start\":23514},{\"attributes\":{\"n\":\"4.1.2.\"},\"end\":25263,\"start\":25232},{\"attributes\":{\"n\":\"4.2.\"},\"end\":27131,\"start\":27088},{\"attributes\":{\"n\":\"5.\"},\"end\":32005,\"start\":31968},{\"attributes\":{\"n\":\"5.1.\"},\"end\":32745,\"start\":32731},{\"attributes\":{\"n\":\"5.1.2.\"},\"end\":33602,\"start\":33594},{\"attributes\":{\"n\":\"5.2.\"},\"end\":36142,\"start\":36113},{\"attributes\":{\"n\":\"6.\"},\"end\":38926,\"start\":38867},{\"end\":40159,\"start\":40157},{\"attributes\":{\"n\":\"6.1.\"},\"end\":41142,\"start\":41122},{\"attributes\":{\"n\":\"6.1.2.\"},\"end\":42548,\"start\":42538},{\"attributes\":{\"n\":\"6.2.\"},\"end\":44915,\"start\":44896},{\"attributes\":{\"n\":\"7.\"},\"end\":47657,\"start\":47637},{\"attributes\":{\"n\":\"7.1.\"},\"end\":49052,\"start\":49036},{\"attributes\":{\"n\":\"7.1.2.\"},\"end\":50597,\"start\":50579},{\"attributes\":{\"n\":\"7.1.3.\"},\"end\":51408,\"start\":51363},{\"attributes\":{\"n\":\"7.1.4.\"},\"end\":54399,\"start\":54380},{\"attributes\":{\"n\":\"7.2.\"},\"end\":57329,\"start\":57276},{\"attributes\":{\"n\":\"7.2.1.\"},\"end\":57691,\"start\":57661},{\"attributes\":{\"n\":\"7.2.2.\"},\"end\":59114,\"start\":59105},{\"attributes\":{\"n\":\"7.2.3.\"},\"end\":60011,\"start\":59996},{\"attributes\":{\"n\":\"7.2.4.\"},\"end\":61865,\"start\":61824},{\"attributes\":{\"n\":\"7.2.5.\"},\"end\":63070,\"start\":63021},{\"attributes\":{\"n\":\"7.3.\"},\"end\":63998,\"start\":63964},{\"attributes\":{\"n\":\"8.\"},\"end\":66324,\"start\":66312},{\"attributes\":{\"n\":\"9.\"},\"end\":71033,\"start\":71022},{\"attributes\":{\"n\":\"10.\"},\"end\":72250,\"start\":72225},{\"end\":76544,\"start\":76521},{\"end\":78631,\"start\":78606},{\"end\":80473,\"start\":80448},{\"end\":82688,\"start\":82651},{\"end\":83806,\"start\":83783},{\"end\":87429,\"start\":87386},{\"end\":87455,\"start\":87432},{\"end\":88980,\"start\":88956},{\"end\":89699,\"start\":89684},{\"end\":91010,\"start\":90974},{\"end\":92466,\"start\":92424},{\"end\":92701,\"start\":92692},{\"end\":93217,\"start\":93199},{\"end\":93474,\"start\":93437},{\"end\":94223,\"start\":94213},{\"end\":94279,\"start\":94269},{\"end\":94408,\"start\":94398},{\"end\":94629,\"start\":94619},{\"end\":94970,\"start\":94963},{\"end\":95853,\"start\":95844},{\"end\":97191,\"start\":97182},{\"end\":98013,\"start\":98004},{\"end\":101311,\"start\":101302},{\"end\":103207,\"start\":103198}]", "table": "[{\"end\":95703,\"start\":95508},{\"end\":97180,\"start\":96193},{\"end\":98002,\"start\":97513},{\"end\":101300,\"start\":98943},{\"end\":103196,\"start\":101774},{\"end\":103678,\"start\":103286}]", "figure_caption": "[{\"end\":94211,\"start\":94176},{\"end\":94267,\"start\":94225},{\"end\":94396,\"start\":94281},{\"end\":94617,\"start\":94410},{\"end\":94675,\"start\":94631},{\"end\":94961,\"start\":94678},{\"end\":95056,\"start\":94974},{\"end\":95508,\"start\":95059},{\"end\":95842,\"start\":95706},{\"end\":96193,\"start\":95855},{\"end\":97513,\"start\":97193},{\"end\":98943,\"start\":98015},{\"end\":101774,\"start\":101313},{\"end\":103286,\"start\":103209}]", "figure_ref": "[{\"end\":3431,\"start\":3423},{\"end\":6627,\"start\":6619},{\"end\":7943,\"start\":7935},{\"end\":20211,\"start\":20203},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":48021,\"start\":48013},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":56061,\"start\":56055},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":61956,\"start\":61950},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":63196,\"start\":63188},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":65563,\"start\":65555},{\"end\":92731,\"start\":92723},{\"end\":93871,\"start\":93863}]", "bib_author_first_name": "[{\"end\":105013,\"start\":105012},{\"end\":105024,\"start\":105023},{\"end\":105035,\"start\":105034},{\"end\":105363,\"start\":105362},{\"end\":105373,\"start\":105372},{\"end\":105383,\"start\":105382},{\"end\":105793,\"start\":105792},{\"end\":105804,\"start\":105803},{\"end\":105819,\"start\":105818},{\"end\":105827,\"start\":105826},{\"end\":105837,\"start\":105836},{\"end\":105848,\"start\":105847},{\"end\":105860,\"start\":105859},{\"end\":105873,\"start\":105872},{\"end\":106400,\"start\":106399},{\"end\":106407,\"start\":106406},{\"end\":106409,\"start\":106408},{\"end\":106834,\"start\":106833},{\"end\":106845,\"start\":106844},{\"end\":107284,\"start\":107283},{\"end\":107292,\"start\":107291},{\"end\":107304,\"start\":107303},{\"end\":107317,\"start\":107316},{\"end\":107327,\"start\":107326},{\"end\":107338,\"start\":107337},{\"end\":107345,\"start\":107344},{\"end\":107355,\"start\":107354},{\"end\":107675,\"start\":107674},{\"end\":107683,\"start\":107682},{\"end\":107689,\"start\":107688},{\"end\":107697,\"start\":107696},{\"end\":108205,\"start\":108204},{\"end\":108213,\"start\":108212},{\"end\":108220,\"start\":108219},{\"end\":108226,\"start\":108225},{\"end\":108232,\"start\":108231},{\"end\":108481,\"start\":108480},{\"end\":108489,\"start\":108488},{\"end\":108499,\"start\":108498},{\"end\":108815,\"start\":108814},{\"end\":108824,\"start\":108823},{\"end\":108837,\"start\":108836},{\"end\":109367,\"start\":109366},{\"end\":109375,\"start\":109374},{\"end\":109383,\"start\":109382},{\"end\":109396,\"start\":109392},{\"end\":109402,\"start\":109401},{\"end\":109408,\"start\":109407},{\"end\":109732,\"start\":109731},{\"end\":109739,\"start\":109738},{\"end\":109927,\"start\":109926},{\"end\":109934,\"start\":109933},{\"end\":109940,\"start\":109939},{\"end\":109948,\"start\":109947},{\"end\":109956,\"start\":109955},{\"end\":109958,\"start\":109957},{\"end\":109972,\"start\":109971},{\"end\":110496,\"start\":110495},{\"end\":110498,\"start\":110497},{\"end\":110512,\"start\":110511},{\"end\":110522,\"start\":110521},{\"end\":110810,\"start\":110809},{\"end\":110816,\"start\":110815},{\"end\":110823,\"start\":110822},{\"end\":110839,\"start\":110838},{\"end\":111101,\"start\":111100},{\"end\":111107,\"start\":111106},{\"end\":111123,\"start\":111122},{\"end\":111363,\"start\":111362},{\"end\":111373,\"start\":111372},{\"end\":111381,\"start\":111380},{\"end\":111391,\"start\":111390},{\"end\":111985,\"start\":111984},{\"end\":111991,\"start\":111990},{\"end\":112000,\"start\":111999},{\"end\":112007,\"start\":112006},{\"end\":112354,\"start\":112353},{\"end\":112524,\"start\":112523},{\"end\":112531,\"start\":112530},{\"end\":112538,\"start\":112537},{\"end\":112540,\"start\":112539},{\"end\":112763,\"start\":112762},{\"end\":112770,\"start\":112769},{\"end\":112777,\"start\":112776},{\"end\":112784,\"start\":112783},{\"end\":112786,\"start\":112785},{\"end\":113046,\"start\":113045},{\"end\":113056,\"start\":113055},{\"end\":113062,\"start\":113061},{\"end\":113068,\"start\":113067},{\"end\":113070,\"start\":113069},{\"end\":113079,\"start\":113078},{\"end\":113086,\"start\":113085},{\"end\":113092,\"start\":113091},{\"end\":113409,\"start\":113408},{\"end\":113584,\"start\":113583},{\"end\":113593,\"start\":113592},{\"end\":113603,\"start\":113602},{\"end\":113613,\"start\":113612},{\"end\":113896,\"start\":113895},{\"end\":113907,\"start\":113906},{\"end\":113920,\"start\":113919},{\"end\":113932,\"start\":113931},{\"end\":113939,\"start\":113938},{\"end\":114287,\"start\":114286},{\"end\":114297,\"start\":114296},{\"end\":114614,\"start\":114613},{\"end\":114620,\"start\":114619},{\"end\":114628,\"start\":114627},{\"end\":114637,\"start\":114636},{\"end\":114950,\"start\":114949},{\"end\":114956,\"start\":114955},{\"end\":114962,\"start\":114961},{\"end\":114969,\"start\":114968},{\"end\":115172,\"start\":115171},{\"end\":115178,\"start\":115177},{\"end\":115187,\"start\":115186},{\"end\":115193,\"start\":115192},{\"end\":115204,\"start\":115203},{\"end\":115217,\"start\":115216},{\"end\":115224,\"start\":115223},{\"end\":115233,\"start\":115232},{\"end\":115712,\"start\":115711},{\"end\":115719,\"start\":115718},{\"end\":115728,\"start\":115727},{\"end\":115740,\"start\":115736},{\"end\":116132,\"start\":116131},{\"end\":116138,\"start\":116137},{\"end\":116144,\"start\":116143},{\"end\":116152,\"start\":116151},{\"end\":116154,\"start\":116153},{\"end\":116164,\"start\":116163},{\"end\":116180,\"start\":116179},{\"end\":116195,\"start\":116194},{\"end\":116197,\"start\":116196},{\"end\":116206,\"start\":116205},{\"end\":116214,\"start\":116213},{\"end\":116605,\"start\":116604},{\"end\":116611,\"start\":116610},{\"end\":116618,\"start\":116617},{\"end\":117332,\"start\":117331},{\"end\":117341,\"start\":117340},{\"end\":117352,\"start\":117351},{\"end\":117363,\"start\":117362},{\"end\":117374,\"start\":117373},{\"end\":117705,\"start\":117704},{\"end\":117723,\"start\":117722},{\"end\":117733,\"start\":117732},{\"end\":117745,\"start\":117744},{\"end\":117756,\"start\":117755},{\"end\":117772,\"start\":117771},{\"end\":117774,\"start\":117773},{\"end\":117782,\"start\":117781},{\"end\":118291,\"start\":118290},{\"end\":118301,\"start\":118300},{\"end\":118516,\"start\":118512},{\"end\":118527,\"start\":118526},{\"end\":118535,\"start\":118534},{\"end\":118537,\"start\":118536},{\"end\":118545,\"start\":118544},{\"end\":118556,\"start\":118555},{\"end\":118565,\"start\":118564},{\"end\":118576,\"start\":118575},{\"end\":118591,\"start\":118590},{\"end\":118604,\"start\":118603},{\"end\":118612,\"start\":118611},{\"end\":118914,\"start\":118913},{\"end\":118927,\"start\":118926},{\"end\":118940,\"start\":118939},{\"end\":118952,\"start\":118951},{\"end\":118962,\"start\":118961},{\"end\":118973,\"start\":118972},{\"end\":118983,\"start\":118982},{\"end\":118994,\"start\":118993},{\"end\":119010,\"start\":119009},{\"end\":119019,\"start\":119018},{\"end\":119030,\"start\":119029},{\"end\":119044,\"start\":119043},{\"end\":119054,\"start\":119053},{\"end\":119068,\"start\":119067},{\"end\":119079,\"start\":119078},{\"end\":119089,\"start\":119088},{\"end\":119502,\"start\":119501},{\"end\":119509,\"start\":119508},{\"end\":119515,\"start\":119514},{\"end\":119842,\"start\":119841},{\"end\":119844,\"start\":119843},{\"end\":119854,\"start\":119853},{\"end\":119856,\"start\":119855},{\"end\":119862,\"start\":119861},{\"end\":119865,\"start\":119863},{\"end\":119873,\"start\":119872},{\"end\":119875,\"start\":119874},{\"end\":119884,\"start\":119883},{\"end\":119886,\"start\":119885},{\"end\":119898,\"start\":119897},{\"end\":119913,\"start\":119912},{\"end\":119915,\"start\":119914},{\"end\":119922,\"start\":119921},{\"end\":119937,\"start\":119936},{\"end\":119943,\"start\":119942},{\"end\":119945,\"start\":119944},{\"end\":120321,\"start\":120320},{\"end\":120334,\"start\":120333},{\"end\":120345,\"start\":120344},{\"end\":120358,\"start\":120357},{\"end\":120666,\"start\":120665},{\"end\":120674,\"start\":120673},{\"end\":120688,\"start\":120687},{\"end\":121089,\"start\":121088},{\"end\":121106,\"start\":121105},{\"end\":121118,\"start\":121117},{\"end\":121127,\"start\":121126},{\"end\":121129,\"start\":121128},{\"end\":121142,\"start\":121141},{\"end\":122151,\"start\":122150},{\"end\":122162,\"start\":122161},{\"end\":122164,\"start\":122163},{\"end\":122173,\"start\":122172},{\"end\":122183,\"start\":122182},{\"end\":122192,\"start\":122191},{\"end\":122202,\"start\":122201},{\"end\":122214,\"start\":122213},{\"end\":122719,\"start\":122718},{\"end\":122733,\"start\":122732},{\"end\":122735,\"start\":122734},{\"end\":122742,\"start\":122741},{\"end\":123200,\"start\":123199},{\"end\":123207,\"start\":123206},{\"end\":123218,\"start\":123217},{\"end\":123220,\"start\":123219},{\"end\":123489,\"start\":123488},{\"end\":123497,\"start\":123496},{\"end\":123505,\"start\":123504},{\"end\":123513,\"start\":123512},{\"end\":123794,\"start\":123793},{\"end\":123802,\"start\":123801},{\"end\":123808,\"start\":123807},{\"end\":124093,\"start\":124092},{\"end\":124101,\"start\":124100},{\"end\":124108,\"start\":124107},{\"end\":124110,\"start\":124109},{\"end\":124461,\"start\":124460},{\"end\":124469,\"start\":124468},{\"end\":124476,\"start\":124475},{\"end\":124484,\"start\":124483},{\"end\":124490,\"start\":124489},{\"end\":124503,\"start\":124502},{\"end\":124512,\"start\":124511},{\"end\":124514,\"start\":124513},{\"end\":124882,\"start\":124881},{\"end\":124890,\"start\":124889},{\"end\":124898,\"start\":124897},{\"end\":124904,\"start\":124903},{\"end\":124912,\"start\":124911},{\"end\":125540,\"start\":125539},{\"end\":125548,\"start\":125547},{\"end\":125557,\"start\":125553},{\"end\":125564,\"start\":125563},{\"end\":125570,\"start\":125569},{\"end\":125577,\"start\":125576},{\"end\":126049,\"start\":126048},{\"end\":126056,\"start\":126055},{\"end\":126068,\"start\":126063},{\"end\":126076,\"start\":126075},{\"end\":126410,\"start\":126409},{\"end\":126416,\"start\":126415},{\"end\":126425,\"start\":126424},{\"end\":126743,\"start\":126742},{\"end\":126749,\"start\":126748},{\"end\":126757,\"start\":126756},{\"end\":126763,\"start\":126762},{\"end\":126774,\"start\":126773},{\"end\":126776,\"start\":126775},{\"end\":126786,\"start\":126785},{\"end\":127037,\"start\":127036},{\"end\":127045,\"start\":127044},{\"end\":127051,\"start\":127050},{\"end\":127057,\"start\":127056},{\"end\":127310,\"start\":127309},{\"end\":127318,\"start\":127317},{\"end\":127326,\"start\":127325},{\"end\":127328,\"start\":127327},{\"end\":127334,\"start\":127333},{\"end\":127344,\"start\":127343},{\"end\":127359,\"start\":127358},{\"end\":127690,\"start\":127689},{\"end\":127698,\"start\":127697},{\"end\":127708,\"start\":127703},{\"end\":127716,\"start\":127715},{\"end\":128026,\"start\":128025},{\"end\":128034,\"start\":128033},{\"end\":128042,\"start\":128041},{\"end\":128049,\"start\":128048},{\"end\":128057,\"start\":128056},{\"end\":128067,\"start\":128066},{\"end\":128074,\"start\":128073},{\"end\":128081,\"start\":128080},{\"end\":128087,\"start\":128086},{\"end\":128380,\"start\":128379},{\"end\":128389,\"start\":128388},{\"end\":128397,\"start\":128396},{\"end\":128399,\"start\":128398},{\"end\":128415,\"start\":128414},{\"end\":128422,\"start\":128421},{\"end\":128909,\"start\":128908},{\"end\":128916,\"start\":128915},{\"end\":128918,\"start\":128917},{\"end\":128927,\"start\":128926},{\"end\":128933,\"start\":128932},{\"end\":128943,\"start\":128942},{\"end\":128953,\"start\":128952}]", "bib_author_last_name": "[{\"end\":105021,\"start\":105014},{\"end\":105032,\"start\":105025},{\"end\":105042,\"start\":105036},{\"end\":105370,\"start\":105364},{\"end\":105380,\"start\":105374},{\"end\":105390,\"start\":105384},{\"end\":105801,\"start\":105794},{\"end\":105816,\"start\":105805},{\"end\":105824,\"start\":105820},{\"end\":105834,\"start\":105828},{\"end\":105845,\"start\":105838},{\"end\":105857,\"start\":105849},{\"end\":105870,\"start\":105861},{\"end\":105879,\"start\":105874},{\"end\":106404,\"start\":106401},{\"end\":106414,\"start\":106410},{\"end\":106842,\"start\":106835},{\"end\":106852,\"start\":106846},{\"end\":107289,\"start\":107285},{\"end\":107301,\"start\":107293},{\"end\":107314,\"start\":107305},{\"end\":107324,\"start\":107318},{\"end\":107335,\"start\":107328},{\"end\":107342,\"start\":107339},{\"end\":107352,\"start\":107346},{\"end\":107366,\"start\":107356},{\"end\":107680,\"start\":107676},{\"end\":107686,\"start\":107684},{\"end\":107694,\"start\":107690},{\"end\":107708,\"start\":107698},{\"end\":108210,\"start\":108206},{\"end\":108217,\"start\":108214},{\"end\":108223,\"start\":108221},{\"end\":108229,\"start\":108227},{\"end\":108237,\"start\":108233},{\"end\":108486,\"start\":108482},{\"end\":108496,\"start\":108490},{\"end\":108510,\"start\":108500},{\"end\":108821,\"start\":108816},{\"end\":108834,\"start\":108825},{\"end\":108844,\"start\":108838},{\"end\":109372,\"start\":109368},{\"end\":109380,\"start\":109376},{\"end\":109390,\"start\":109384},{\"end\":109399,\"start\":109397},{\"end\":109405,\"start\":109403},{\"end\":109416,\"start\":109409},{\"end\":109736,\"start\":109733},{\"end\":109745,\"start\":109740},{\"end\":109931,\"start\":109928},{\"end\":109937,\"start\":109935},{\"end\":109945,\"start\":109941},{\"end\":109953,\"start\":109949},{\"end\":109969,\"start\":109959},{\"end\":109978,\"start\":109973},{\"end\":110509,\"start\":110499},{\"end\":110519,\"start\":110513},{\"end\":110530,\"start\":110523},{\"end\":110813,\"start\":110811},{\"end\":110820,\"start\":110817},{\"end\":110836,\"start\":110824},{\"end\":110844,\"start\":110840},{\"end\":111104,\"start\":111102},{\"end\":111120,\"start\":111108},{\"end\":111128,\"start\":111124},{\"end\":111370,\"start\":111364},{\"end\":111378,\"start\":111374},{\"end\":111388,\"start\":111382},{\"end\":111394,\"start\":111392},{\"end\":111988,\"start\":111986},{\"end\":111997,\"start\":111992},{\"end\":112004,\"start\":112001},{\"end\":112011,\"start\":112008},{\"end\":112361,\"start\":112355},{\"end\":112528,\"start\":112525},{\"end\":112535,\"start\":112532},{\"end\":112545,\"start\":112541},{\"end\":112767,\"start\":112764},{\"end\":112774,\"start\":112771},{\"end\":112781,\"start\":112778},{\"end\":112791,\"start\":112787},{\"end\":113053,\"start\":113047},{\"end\":113059,\"start\":113057},{\"end\":113065,\"start\":113063},{\"end\":113076,\"start\":113071},{\"end\":113083,\"start\":113080},{\"end\":113089,\"start\":113087},{\"end\":113098,\"start\":113093},{\"end\":113420,\"start\":113410},{\"end\":113590,\"start\":113585},{\"end\":113600,\"start\":113594},{\"end\":113610,\"start\":113604},{\"end\":113621,\"start\":113614},{\"end\":113904,\"start\":113897},{\"end\":113917,\"start\":113908},{\"end\":113929,\"start\":113921},{\"end\":113936,\"start\":113933},{\"end\":113944,\"start\":113940},{\"end\":114294,\"start\":114288},{\"end\":114303,\"start\":114298},{\"end\":114617,\"start\":114615},{\"end\":114625,\"start\":114621},{\"end\":114634,\"start\":114629},{\"end\":114649,\"start\":114638},{\"end\":114953,\"start\":114951},{\"end\":114959,\"start\":114957},{\"end\":114966,\"start\":114963},{\"end\":114972,\"start\":114970},{\"end\":115175,\"start\":115173},{\"end\":115184,\"start\":115179},{\"end\":115190,\"start\":115188},{\"end\":115201,\"start\":115194},{\"end\":115214,\"start\":115205},{\"end\":115221,\"start\":115218},{\"end\":115230,\"start\":115225},{\"end\":115236,\"start\":115234},{\"end\":115716,\"start\":115713},{\"end\":115725,\"start\":115720},{\"end\":115734,\"start\":115729},{\"end\":115746,\"start\":115741},{\"end\":116135,\"start\":116133},{\"end\":116141,\"start\":116139},{\"end\":116149,\"start\":116145},{\"end\":116161,\"start\":116155},{\"end\":116177,\"start\":116165},{\"end\":116192,\"start\":116181},{\"end\":116203,\"start\":116198},{\"end\":116211,\"start\":116207},{\"end\":116221,\"start\":116215},{\"end\":116608,\"start\":116606},{\"end\":116615,\"start\":116612},{\"end\":116622,\"start\":116619},{\"end\":117338,\"start\":117333},{\"end\":117349,\"start\":117342},{\"end\":117360,\"start\":117353},{\"end\":117371,\"start\":117364},{\"end\":117380,\"start\":117375},{\"end\":117720,\"start\":117706},{\"end\":117730,\"start\":117724},{\"end\":117742,\"start\":117734},{\"end\":117753,\"start\":117746},{\"end\":117769,\"start\":117757},{\"end\":117779,\"start\":117775},{\"end\":117787,\"start\":117783},{\"end\":118298,\"start\":118292},{\"end\":118309,\"start\":118302},{\"end\":118524,\"start\":118517},{\"end\":118532,\"start\":118528},{\"end\":118542,\"start\":118538},{\"end\":118553,\"start\":118546},{\"end\":118562,\"start\":118557},{\"end\":118573,\"start\":118566},{\"end\":118588,\"start\":118577},{\"end\":118601,\"start\":118592},{\"end\":118609,\"start\":118605},{\"end\":118619,\"start\":118613},{\"end\":118924,\"start\":118915},{\"end\":118937,\"start\":118928},{\"end\":118949,\"start\":118941},{\"end\":118959,\"start\":118953},{\"end\":118970,\"start\":118963},{\"end\":118980,\"start\":118974},{\"end\":118991,\"start\":118984},{\"end\":119007,\"start\":118995},{\"end\":119016,\"start\":119011},{\"end\":119027,\"start\":119020},{\"end\":119041,\"start\":119031},{\"end\":119051,\"start\":119045},{\"end\":119065,\"start\":119055},{\"end\":119076,\"start\":119069},{\"end\":119086,\"start\":119080},{\"end\":119099,\"start\":119090},{\"end\":119506,\"start\":119503},{\"end\":119512,\"start\":119510},{\"end\":119520,\"start\":119516},{\"end\":119851,\"start\":119845},{\"end\":119859,\"start\":119857},{\"end\":119870,\"start\":119866},{\"end\":119881,\"start\":119876},{\"end\":119895,\"start\":119887},{\"end\":119910,\"start\":119899},{\"end\":119919,\"start\":119916},{\"end\":119934,\"start\":119923},{\"end\":119940,\"start\":119938},{\"end\":119951,\"start\":119946},{\"end\":120331,\"start\":120322},{\"end\":120342,\"start\":120335},{\"end\":120355,\"start\":120346},{\"end\":120365,\"start\":120359},{\"end\":120671,\"start\":120667},{\"end\":120685,\"start\":120675},{\"end\":120699,\"start\":120689},{\"end\":121103,\"start\":121090},{\"end\":121115,\"start\":121107},{\"end\":121124,\"start\":121119},{\"end\":121139,\"start\":121130},{\"end\":121152,\"start\":121143},{\"end\":122159,\"start\":122152},{\"end\":122170,\"start\":122165},{\"end\":122180,\"start\":122174},{\"end\":122189,\"start\":122184},{\"end\":122199,\"start\":122193},{\"end\":122211,\"start\":122203},{\"end\":122224,\"start\":122215},{\"end\":122730,\"start\":122720},{\"end\":122739,\"start\":122736},{\"end\":122748,\"start\":122743},{\"end\":123204,\"start\":123201},{\"end\":123215,\"start\":123208},{\"end\":123227,\"start\":123221},{\"end\":123494,\"start\":123490},{\"end\":123502,\"start\":123498},{\"end\":123510,\"start\":123506},{\"end\":123519,\"start\":123514},{\"end\":123799,\"start\":123795},{\"end\":123805,\"start\":123803},{\"end\":123814,\"start\":123809},{\"end\":124098,\"start\":124094},{\"end\":124105,\"start\":124102},{\"end\":124115,\"start\":124111},{\"end\":124466,\"start\":124462},{\"end\":124473,\"start\":124470},{\"end\":124481,\"start\":124477},{\"end\":124487,\"start\":124485},{\"end\":124500,\"start\":124491},{\"end\":124509,\"start\":124504},{\"end\":124519,\"start\":124515},{\"end\":124887,\"start\":124883},{\"end\":124895,\"start\":124891},{\"end\":124901,\"start\":124899},{\"end\":124909,\"start\":124905},{\"end\":124916,\"start\":124913},{\"end\":125545,\"start\":125541},{\"end\":125551,\"start\":125549},{\"end\":125561,\"start\":125558},{\"end\":125567,\"start\":125565},{\"end\":125574,\"start\":125571},{\"end\":125582,\"start\":125578},{\"end\":126053,\"start\":126050},{\"end\":126061,\"start\":126057},{\"end\":126073,\"start\":126069},{\"end\":126079,\"start\":126077},{\"end\":126413,\"start\":126411},{\"end\":126422,\"start\":126417},{\"end\":126428,\"start\":126426},{\"end\":126746,\"start\":126744},{\"end\":126754,\"start\":126750},{\"end\":126760,\"start\":126758},{\"end\":126771,\"start\":126764},{\"end\":126783,\"start\":126777},{\"end\":126789,\"start\":126787},{\"end\":127042,\"start\":127038},{\"end\":127048,\"start\":127046},{\"end\":127054,\"start\":127052},{\"end\":127062,\"start\":127058},{\"end\":127315,\"start\":127311},{\"end\":127323,\"start\":127319},{\"end\":127331,\"start\":127329},{\"end\":127341,\"start\":127335},{\"end\":127356,\"start\":127345},{\"end\":127362,\"start\":127360},{\"end\":127695,\"start\":127691},{\"end\":127701,\"start\":127699},{\"end\":127713,\"start\":127709},{\"end\":127721,\"start\":127717},{\"end\":128031,\"start\":128027},{\"end\":128039,\"start\":128035},{\"end\":128046,\"start\":128043},{\"end\":128054,\"start\":128050},{\"end\":128064,\"start\":128058},{\"end\":128071,\"start\":128068},{\"end\":128078,\"start\":128075},{\"end\":128084,\"start\":128082},{\"end\":128093,\"start\":128088},{\"end\":128386,\"start\":128381},{\"end\":128394,\"start\":128390},{\"end\":128412,\"start\":128400},{\"end\":128419,\"start\":128416},{\"end\":128429,\"start\":128423},{\"end\":128913,\"start\":128910},{\"end\":128924,\"start\":128919},{\"end\":128930,\"start\":128928},{\"end\":128940,\"start\":128934},{\"end\":128950,\"start\":128944},{\"end\":128963,\"start\":128954}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3310672},\"end\":105309,\"start\":104911},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9089716},\"end\":105694,\"start\":105311},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":227054251},\"end\":106313,\"start\":105696},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1213397},\"end\":106752,\"start\":106315},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":207599948},\"end\":107204,\"start\":106754},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":53250337},\"end\":107580,\"start\":107206},{\"attributes\":{\"id\":\"b6\"},\"end\":108129,\"start\":107582},{\"attributes\":{\"doi\":\"arXiv:1712.05526\",\"id\":\"b7\"},\"end\":108399,\"start\":108131},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":215856644},\"end\":108753,\"start\":108401},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":59842968},\"end\":109311,\"start\":108755},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":57246310},\"end\":109696,\"start\":109313},{\"attributes\":{\"id\":\"b11\"},\"end\":109859,\"start\":109698},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":62841494},\"end\":110445,\"start\":109861},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6706414},\"end\":110742,\"start\":110447},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":131777414},\"end\":111017,\"start\":110744},{\"attributes\":{\"doi\":\"arXiv:1708.06733\",\"id\":\"b15\"},\"end\":111291,\"start\":111019},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":233387945},\"end\":111936,\"start\":111293},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206594692},\"end\":112339,\"start\":111938},{\"attributes\":{\"id\":\"b18\"},\"end\":112447,\"start\":112341},{\"attributes\":{\"id\":\"b19\"},\"end\":112673,\"start\":112449},{\"attributes\":{\"id\":\"b20\"},\"end\":112940,\"start\":112675},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":218581207},\"end\":113351,\"start\":112942},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":18268744},\"end\":113524,\"start\":113353},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14542261},\"end\":113821,\"start\":113526},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":49431481},\"end\":114203,\"start\":113823},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":220128149},\"end\":114540,\"start\":114205},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":16687466},\"end\":114895,\"start\":114542},{\"attributes\":{\"doi\":\"arXiv:2009.04131\",\"id\":\"b27\"},\"end\":115100,\"start\":114897},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":237563316},\"end\":115652,\"start\":115102},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":12455403},\"end\":116054,\"start\":115654},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1248661},\"end\":116525,\"start\":116056},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":85498668},\"end\":117266,\"start\":116527},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3488815},\"end\":117623,\"start\":117268},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":12424035},\"end\":118213,\"start\":117625},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":85550403},\"end\":118508,\"start\":118215},{\"attributes\":{\"doi\":\"arXiv:1807.01069\",\"id\":\"b35\"},\"end\":118869,\"start\":118510},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":10659969},\"end\":119462,\"start\":118871},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":238713127},\"end\":119717,\"start\":119464},{\"attributes\":{\"id\":\"b38\"},\"end\":119751,\"start\":119719},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":224996407},\"end\":120245,\"start\":119753},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":211069632},\"end\":120630,\"start\":120247},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":203610516},\"end\":120991,\"start\":120632},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":219980448},\"end\":121744,\"start\":120993},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":219980448},\"end\":122075,\"start\":121746},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":4626477},\"end\":122669,\"start\":122077},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":35426171},\"end\":123120,\"start\":122671},{\"attributes\":{\"doi\":\"arXiv:2010.09080\",\"id\":\"b46\"},\"end\":123388,\"start\":123122},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":199405468},\"end\":123750,\"start\":123390},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":53298804},\"end\":124014,\"start\":123752},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":211532446},\"end\":124378,\"start\":124016},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":67846878},\"end\":124807,\"start\":124380},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":67874979},\"end\":125215,\"start\":124809},{\"attributes\":{\"id\":\"b52\"},\"end\":125478,\"start\":125217},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":21949142},\"end\":125976,\"start\":125480},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":235435833},\"end\":126332,\"start\":125978},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":3851184},\"end\":126691,\"start\":126334},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":203902799},\"end\":127034,\"start\":126693},{\"attributes\":{\"doi\":\"arXiv:1703.01340\",\"id\":\"b57\"},\"end\":127261,\"start\":127036},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":211171876},\"end\":127618,\"start\":127263},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":52891543},\"end\":127951,\"start\":127620},{\"attributes\":{\"doi\":\"arXiv:2003.00120\",\"id\":\"b60\"},\"end\":128291,\"start\":127953},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":52138086},\"end\":128842,\"start\":128293},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":155092992},\"end\":129462,\"start\":128844}]", "bib_title": "[{\"end\":105010,\"start\":104911},{\"end\":105360,\"start\":105311},{\"end\":105790,\"start\":105696},{\"end\":106397,\"start\":106315},{\"end\":106831,\"start\":106754},{\"end\":107281,\"start\":107206},{\"end\":107672,\"start\":107582},{\"end\":108478,\"start\":108401},{\"end\":108812,\"start\":108755},{\"end\":109364,\"start\":109313},{\"end\":109924,\"start\":109861},{\"end\":110493,\"start\":110447},{\"end\":110807,\"start\":110744},{\"end\":111360,\"start\":111293},{\"end\":111982,\"start\":111938},{\"end\":113043,\"start\":112942},{\"end\":113406,\"start\":113353},{\"end\":113581,\"start\":113526},{\"end\":113893,\"start\":113823},{\"end\":114284,\"start\":114205},{\"end\":114611,\"start\":114542},{\"end\":115169,\"start\":115102},{\"end\":115709,\"start\":115654},{\"end\":116129,\"start\":116056},{\"end\":116602,\"start\":116527},{\"end\":117329,\"start\":117268},{\"end\":117702,\"start\":117625},{\"end\":118288,\"start\":118215},{\"end\":118911,\"start\":118871},{\"end\":119499,\"start\":119464},{\"end\":119839,\"start\":119753},{\"end\":120318,\"start\":120247},{\"end\":120663,\"start\":120632},{\"end\":121086,\"start\":120993},{\"end\":121839,\"start\":121746},{\"end\":122148,\"start\":122077},{\"end\":122716,\"start\":122671},{\"end\":123486,\"start\":123390},{\"end\":123791,\"start\":123752},{\"end\":124090,\"start\":124016},{\"end\":124458,\"start\":124380},{\"end\":124879,\"start\":124809},{\"end\":125537,\"start\":125480},{\"end\":126046,\"start\":125978},{\"end\":126407,\"start\":126334},{\"end\":126740,\"start\":126693},{\"end\":127307,\"start\":127263},{\"end\":127687,\"start\":127620},{\"end\":128377,\"start\":128293},{\"end\":128906,\"start\":128844}]", "bib_author": "[{\"end\":105023,\"start\":105012},{\"end\":105034,\"start\":105023},{\"end\":105044,\"start\":105034},{\"end\":105372,\"start\":105362},{\"end\":105382,\"start\":105372},{\"end\":105392,\"start\":105382},{\"end\":105803,\"start\":105792},{\"end\":105818,\"start\":105803},{\"end\":105826,\"start\":105818},{\"end\":105836,\"start\":105826},{\"end\":105847,\"start\":105836},{\"end\":105859,\"start\":105847},{\"end\":105872,\"start\":105859},{\"end\":105881,\"start\":105872},{\"end\":106406,\"start\":106399},{\"end\":106416,\"start\":106406},{\"end\":106844,\"start\":106833},{\"end\":106854,\"start\":106844},{\"end\":107291,\"start\":107283},{\"end\":107303,\"start\":107291},{\"end\":107316,\"start\":107303},{\"end\":107326,\"start\":107316},{\"end\":107337,\"start\":107326},{\"end\":107344,\"start\":107337},{\"end\":107354,\"start\":107344},{\"end\":107368,\"start\":107354},{\"end\":107682,\"start\":107674},{\"end\":107688,\"start\":107682},{\"end\":107696,\"start\":107688},{\"end\":107710,\"start\":107696},{\"end\":108212,\"start\":108204},{\"end\":108219,\"start\":108212},{\"end\":108225,\"start\":108219},{\"end\":108231,\"start\":108225},{\"end\":108239,\"start\":108231},{\"end\":108488,\"start\":108480},{\"end\":108498,\"start\":108488},{\"end\":108512,\"start\":108498},{\"end\":108823,\"start\":108814},{\"end\":108836,\"start\":108823},{\"end\":108846,\"start\":108836},{\"end\":109374,\"start\":109366},{\"end\":109382,\"start\":109374},{\"end\":109392,\"start\":109382},{\"end\":109401,\"start\":109392},{\"end\":109407,\"start\":109401},{\"end\":109418,\"start\":109407},{\"end\":109738,\"start\":109731},{\"end\":109747,\"start\":109738},{\"end\":109933,\"start\":109926},{\"end\":109939,\"start\":109933},{\"end\":109947,\"start\":109939},{\"end\":109955,\"start\":109947},{\"end\":109971,\"start\":109955},{\"end\":109980,\"start\":109971},{\"end\":110511,\"start\":110495},{\"end\":110521,\"start\":110511},{\"end\":110532,\"start\":110521},{\"end\":110815,\"start\":110809},{\"end\":110822,\"start\":110815},{\"end\":110838,\"start\":110822},{\"end\":110846,\"start\":110838},{\"end\":111106,\"start\":111100},{\"end\":111122,\"start\":111106},{\"end\":111130,\"start\":111122},{\"end\":111372,\"start\":111362},{\"end\":111380,\"start\":111372},{\"end\":111390,\"start\":111380},{\"end\":111396,\"start\":111390},{\"end\":111990,\"start\":111984},{\"end\":111999,\"start\":111990},{\"end\":112006,\"start\":111999},{\"end\":112013,\"start\":112006},{\"end\":112363,\"start\":112353},{\"end\":112530,\"start\":112523},{\"end\":112537,\"start\":112530},{\"end\":112547,\"start\":112537},{\"end\":112769,\"start\":112762},{\"end\":112776,\"start\":112769},{\"end\":112783,\"start\":112776},{\"end\":112793,\"start\":112783},{\"end\":113055,\"start\":113045},{\"end\":113061,\"start\":113055},{\"end\":113067,\"start\":113061},{\"end\":113078,\"start\":113067},{\"end\":113085,\"start\":113078},{\"end\":113091,\"start\":113085},{\"end\":113100,\"start\":113091},{\"end\":113422,\"start\":113408},{\"end\":113592,\"start\":113583},{\"end\":113602,\"start\":113592},{\"end\":113612,\"start\":113602},{\"end\":113623,\"start\":113612},{\"end\":113906,\"start\":113895},{\"end\":113919,\"start\":113906},{\"end\":113931,\"start\":113919},{\"end\":113938,\"start\":113931},{\"end\":113946,\"start\":113938},{\"end\":114296,\"start\":114286},{\"end\":114305,\"start\":114296},{\"end\":114619,\"start\":114613},{\"end\":114627,\"start\":114619},{\"end\":114636,\"start\":114627},{\"end\":114651,\"start\":114636},{\"end\":114955,\"start\":114949},{\"end\":114961,\"start\":114955},{\"end\":114968,\"start\":114961},{\"end\":114974,\"start\":114968},{\"end\":115177,\"start\":115171},{\"end\":115186,\"start\":115177},{\"end\":115192,\"start\":115186},{\"end\":115203,\"start\":115192},{\"end\":115216,\"start\":115203},{\"end\":115223,\"start\":115216},{\"end\":115232,\"start\":115223},{\"end\":115238,\"start\":115232},{\"end\":115718,\"start\":115711},{\"end\":115727,\"start\":115718},{\"end\":115736,\"start\":115727},{\"end\":115748,\"start\":115736},{\"end\":116137,\"start\":116131},{\"end\":116143,\"start\":116137},{\"end\":116151,\"start\":116143},{\"end\":116163,\"start\":116151},{\"end\":116179,\"start\":116163},{\"end\":116194,\"start\":116179},{\"end\":116205,\"start\":116194},{\"end\":116213,\"start\":116205},{\"end\":116223,\"start\":116213},{\"end\":116610,\"start\":116604},{\"end\":116617,\"start\":116610},{\"end\":116624,\"start\":116617},{\"end\":117340,\"start\":117331},{\"end\":117351,\"start\":117340},{\"end\":117362,\"start\":117351},{\"end\":117373,\"start\":117362},{\"end\":117382,\"start\":117373},{\"end\":117722,\"start\":117704},{\"end\":117732,\"start\":117722},{\"end\":117744,\"start\":117732},{\"end\":117755,\"start\":117744},{\"end\":117771,\"start\":117755},{\"end\":117781,\"start\":117771},{\"end\":117789,\"start\":117781},{\"end\":118300,\"start\":118290},{\"end\":118311,\"start\":118300},{\"end\":118526,\"start\":118512},{\"end\":118534,\"start\":118526},{\"end\":118544,\"start\":118534},{\"end\":118555,\"start\":118544},{\"end\":118564,\"start\":118555},{\"end\":118575,\"start\":118564},{\"end\":118590,\"start\":118575},{\"end\":118603,\"start\":118590},{\"end\":118611,\"start\":118603},{\"end\":118621,\"start\":118611},{\"end\":118926,\"start\":118913},{\"end\":118939,\"start\":118926},{\"end\":118951,\"start\":118939},{\"end\":118961,\"start\":118951},{\"end\":118972,\"start\":118961},{\"end\":118982,\"start\":118972},{\"end\":118993,\"start\":118982},{\"end\":119009,\"start\":118993},{\"end\":119018,\"start\":119009},{\"end\":119029,\"start\":119018},{\"end\":119043,\"start\":119029},{\"end\":119053,\"start\":119043},{\"end\":119067,\"start\":119053},{\"end\":119078,\"start\":119067},{\"end\":119088,\"start\":119078},{\"end\":119101,\"start\":119088},{\"end\":119508,\"start\":119501},{\"end\":119514,\"start\":119508},{\"end\":119522,\"start\":119514},{\"end\":119853,\"start\":119841},{\"end\":119861,\"start\":119853},{\"end\":119872,\"start\":119861},{\"end\":119883,\"start\":119872},{\"end\":119897,\"start\":119883},{\"end\":119912,\"start\":119897},{\"end\":119921,\"start\":119912},{\"end\":119936,\"start\":119921},{\"end\":119942,\"start\":119936},{\"end\":119953,\"start\":119942},{\"end\":120333,\"start\":120320},{\"end\":120344,\"start\":120333},{\"end\":120357,\"start\":120344},{\"end\":120367,\"start\":120357},{\"end\":120673,\"start\":120665},{\"end\":120687,\"start\":120673},{\"end\":120701,\"start\":120687},{\"end\":121105,\"start\":121088},{\"end\":121117,\"start\":121105},{\"end\":121126,\"start\":121117},{\"end\":121141,\"start\":121126},{\"end\":121154,\"start\":121141},{\"end\":122161,\"start\":122150},{\"end\":122172,\"start\":122161},{\"end\":122182,\"start\":122172},{\"end\":122191,\"start\":122182},{\"end\":122201,\"start\":122191},{\"end\":122213,\"start\":122201},{\"end\":122226,\"start\":122213},{\"end\":122732,\"start\":122718},{\"end\":122741,\"start\":122732},{\"end\":122750,\"start\":122741},{\"end\":123206,\"start\":123199},{\"end\":123217,\"start\":123206},{\"end\":123229,\"start\":123217},{\"end\":123496,\"start\":123488},{\"end\":123504,\"start\":123496},{\"end\":123512,\"start\":123504},{\"end\":123521,\"start\":123512},{\"end\":123801,\"start\":123793},{\"end\":123807,\"start\":123801},{\"end\":123816,\"start\":123807},{\"end\":124100,\"start\":124092},{\"end\":124107,\"start\":124100},{\"end\":124117,\"start\":124107},{\"end\":124468,\"start\":124460},{\"end\":124475,\"start\":124468},{\"end\":124483,\"start\":124475},{\"end\":124489,\"start\":124483},{\"end\":124502,\"start\":124489},{\"end\":124511,\"start\":124502},{\"end\":124521,\"start\":124511},{\"end\":124889,\"start\":124881},{\"end\":124897,\"start\":124889},{\"end\":124903,\"start\":124897},{\"end\":124911,\"start\":124903},{\"end\":124918,\"start\":124911},{\"end\":125547,\"start\":125539},{\"end\":125553,\"start\":125547},{\"end\":125563,\"start\":125553},{\"end\":125569,\"start\":125563},{\"end\":125576,\"start\":125569},{\"end\":125584,\"start\":125576},{\"end\":126055,\"start\":126048},{\"end\":126063,\"start\":126055},{\"end\":126075,\"start\":126063},{\"end\":126081,\"start\":126075},{\"end\":126415,\"start\":126409},{\"end\":126424,\"start\":126415},{\"end\":126430,\"start\":126424},{\"end\":126748,\"start\":126742},{\"end\":126756,\"start\":126748},{\"end\":126762,\"start\":126756},{\"end\":126773,\"start\":126762},{\"end\":126785,\"start\":126773},{\"end\":126791,\"start\":126785},{\"end\":127044,\"start\":127036},{\"end\":127050,\"start\":127044},{\"end\":127056,\"start\":127050},{\"end\":127064,\"start\":127056},{\"end\":127317,\"start\":127309},{\"end\":127325,\"start\":127317},{\"end\":127333,\"start\":127325},{\"end\":127343,\"start\":127333},{\"end\":127358,\"start\":127343},{\"end\":127364,\"start\":127358},{\"end\":127697,\"start\":127689},{\"end\":127703,\"start\":127697},{\"end\":127715,\"start\":127703},{\"end\":127723,\"start\":127715},{\"end\":128033,\"start\":128025},{\"end\":128041,\"start\":128033},{\"end\":128048,\"start\":128041},{\"end\":128056,\"start\":128048},{\"end\":128066,\"start\":128056},{\"end\":128073,\"start\":128066},{\"end\":128080,\"start\":128073},{\"end\":128086,\"start\":128080},{\"end\":128095,\"start\":128086},{\"end\":128388,\"start\":128379},{\"end\":128396,\"start\":128388},{\"end\":128414,\"start\":128396},{\"end\":128421,\"start\":128414},{\"end\":128431,\"start\":128421},{\"end\":128915,\"start\":128908},{\"end\":128926,\"start\":128915},{\"end\":128932,\"start\":128926},{\"end\":128942,\"start\":128932},{\"end\":128952,\"start\":128942},{\"end\":128965,\"start\":128952}]", "bib_venue": "[{\"end\":105088,\"start\":105044},{\"end\":105459,\"start\":105392},{\"end\":105970,\"start\":105881},{\"end\":106488,\"start\":106416},{\"end\":106930,\"start\":106854},{\"end\":107380,\"start\":107368},{\"end\":107791,\"start\":107710},{\"end\":108202,\"start\":108131},{\"end\":108552,\"start\":108512},{\"end\":108961,\"start\":108846},{\"end\":109481,\"start\":109418},{\"end\":109729,\"start\":109698},{\"end\":110052,\"start\":109980},{\"end\":110584,\"start\":110532},{\"end\":110857,\"start\":110846},{\"end\":111098,\"start\":111019},{\"end\":111502,\"start\":111396},{\"end\":112090,\"start\":112013},{\"end\":112351,\"start\":112341},{\"end\":112521,\"start\":112449},{\"end\":112760,\"start\":112675},{\"end\":113116,\"start\":113100},{\"end\":113431,\"start\":113422},{\"end\":113646,\"start\":113623},{\"end\":113994,\"start\":113946},{\"end\":114361,\"start\":114305},{\"end\":114700,\"start\":114651},{\"end\":114947,\"start\":114897},{\"end\":115323,\"start\":115238},{\"end\":115812,\"start\":115748},{\"end\":116275,\"start\":116223},{\"end\":116797,\"start\":116624},{\"end\":117434,\"start\":117382},{\"end\":117865,\"start\":117789},{\"end\":118347,\"start\":118311},{\"end\":119137,\"start\":119101},{\"end\":119583,\"start\":119522},{\"end\":119725,\"start\":119721},{\"end\":119967,\"start\":119953},{\"end\":120423,\"start\":120367},{\"end\":120762,\"start\":120701},{\"end\":121260,\"start\":121154},{\"end\":121897,\"start\":121841},{\"end\":122315,\"start\":122226},{\"end\":122839,\"start\":122750},{\"end\":123197,\"start\":123122},{\"end\":123551,\"start\":123521},{\"end\":123865,\"start\":123816},{\"end\":124186,\"start\":124117},{\"end\":124569,\"start\":124521},{\"end\":124982,\"start\":124918},{\"end\":125256,\"start\":125217},{\"end\":125665,\"start\":125584},{\"end\":126137,\"start\":126081},{\"end\":126491,\"start\":126430},{\"end\":126839,\"start\":126791},{\"end\":127138,\"start\":127080},{\"end\":127420,\"start\":127364},{\"end\":127775,\"start\":127723},{\"end\":128023,\"start\":127953},{\"end\":128515,\"start\":128431},{\"end\":129080,\"start\":128965},{\"end\":105513,\"start\":105461},{\"end\":106547,\"start\":106490},{\"end\":106993,\"start\":106932},{\"end\":107859,\"start\":107793},{\"end\":109048,\"start\":108963},{\"end\":110128,\"start\":110054},{\"end\":111611,\"start\":111535},{\"end\":112154,\"start\":112092},{\"end\":115395,\"start\":115325},{\"end\":115863,\"start\":115814},{\"end\":116957,\"start\":116799},{\"end\":117928,\"start\":117867},{\"end\":120810,\"start\":120764},{\"end\":121369,\"start\":121293},{\"end\":122391,\"start\":122317},{\"end\":122915,\"start\":122841},{\"end\":125733,\"start\":125667},{\"end\":128586,\"start\":128517},{\"end\":129167,\"start\":129082}]"}}}, "year": 2023, "month": 12, "day": 17}