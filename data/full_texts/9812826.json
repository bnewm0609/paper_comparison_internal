{"id": 9812826, "updated": "2023-03-22 16:44:10.465", "metadata": {"title": "A Robust and Energy-Efficient Classifier Using Brain-Inspired Hyperdimensional Computing", "authors": "[{\"first\":\"Abbas\",\"last\":\"Rahimi\",\"middle\":[]},{\"first\":\"Pentti\",\"last\":\"Kanerva\",\"middle\":[]},{\"first\":\"Jan\",\"last\":\"Rabaey\",\"middle\":[\"M.\"]}]", "venue": null, "journal": "Proceedings of the 2016 International Symposium on Low Power Electronics and Design", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": "The mathematical properties of high-dimensional (HD) spaces show remarkable agreement with behaviors controlled by the brain. Computing with HD vectors, referred to as \"hypervectors,\" is a brain-inspired alternative to computing with numbers. Hypervectors are high-dimensional, holographic, and (pseudo)random with independent and identically distributed (i.i.d.) components. They provide for energy-efficient computing while tolerating hardware variation typical of nanoscale fabrics. We describe a hardware architecture for a hypervector-based classifier and demonstrate it with language identification from letter trigrams. The HD classifier is 96.7% accurate, 1.2% lower than a conventional machine learning method, operating with half the energy. Moreover, the HD classifier is able to tolerate 8.8-fold probability of failure of memory cells while maintaining 94% accuracy. This robust behavior with erroneous memory cells can significantly improve energy efficiency.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2476008461", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/islped/RahimiKR16", "doi": "10.1145/2934583.2934624"}}, "content": {"source": {"pdf_hash": "d62982265d03ee9b348ce0653b7a79cab3809180", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4d242a9f2f174fefdf8f476a314d5b8b8deb0432", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d62982265d03ee9b348ce0653b7a79cab3809180.txt", "contents": "\nA Robust and Energy-Efficient Classifier Using Brain-Inspired Hyperdimensional Computing\n\n\nAbbas Rahimi \nEECS Department\nEECS Department\nRedwood Center for Theoretical Neuroscience University of California Berkeley\nUniversity of California Berkeley\nUniversity of California Berkeley\n\n\nPentti Kanerva pkanerva@berkeley.edu \nEECS Department\nEECS Department\nRedwood Center for Theoretical Neuroscience University of California Berkeley\nUniversity of California Berkeley\nUniversity of California Berkeley\n\n\nJan M Rabaey \nEECS Department\nEECS Department\nRedwood Center for Theoretical Neuroscience University of California Berkeley\nUniversity of California Berkeley\nUniversity of California Berkeley\n\n\nA Robust and Energy-Efficient Classifier Using Brain-Inspired Hyperdimensional Computing\n10.1145/2934583.2934624\nThe mathematical properties of high-dimensional (HD) spaces show remarkable agreement with behaviors controlled by the brain. Computing with HD vectors, referred to as \"hypervectors,\" is a brain-inspired alternative to computing with numbers. Hypervectors are high-dimensional, holographic, and (pseudo)random with independent and identically distributed (i.i.d.) components. They provide for energy-efficient computing while tolerating hardware variation typical of nanoscale fabrics. We describe a hardware architecture for a hypervector-based classifier and demonstrate it with language identification from letter trigrams. The HD classifier is 96.7% accurate, 1.2% lower than a conventional machine learning method, operating with half the energy. Moreover, the HD classifier is able to tolerate 8.8-fold probability of failure of memory cells while maintaining 94% accuracy. This robust behavior with erroneous memory cells can significantly improve energy efficiency.\n\nINTRODUCTION\n\nReducing the size of CMOS transistors no longer guarantees the customary gains in performance and energy efficiency of integrated computing platforms. The manufacture of devices near atomic feature dimensions is particularly challenging. Any variation in dimensions, doping, etc. has a large effect on the resulting device and circuit behavior [1]. Solutions that improve energy efficiency -performance per Watt -in the presence of such variations, are highly desirable.\n\nBio-and brain-inspired information processing architectures are a promising new avenue to energy efficiency, asymptotically approaching the efficiency of brain computation, while tolerating variations in nanoscale fabrics [2,3,4]. Among brain-inspired computing paradigms, hyperdimensional computing is founded on the mathematical properties of high-dimensional spaces which show remarkable agreement with behaviors controlled by the brain [5,6,7,8,9]. Instead of computing with numbers, we compute with hypervectors that are high-dimensional (HD; e.g., 10,000-D) and holographic, i.e., every piece of information contained in the vector is distributed equally over all the components Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. of the vector. These features allow hyperdimensional computing to achieve robustness in the presence of hardware-induced errors, greatly improving energy efficiency at the expense of slight, or no functional performance degradation.\n\nThe algorithm that forms the basis of this paper is available on [9]. It identifies the language of text samples based on letter Ngrams. Here, we introduce an HD classifier for that task as the first energy-efficient and robust hardware design of hyperdimensional computing. We propose a modular, scalable, and memory-centric architecture where a hypervector for a text sample is produced and compared concurrently with a set of trained hypervectors. Our design optimizations include substituting a high-precision hypervector of integers with a low-precision binary hypervector, reducing switching activities, and lowering the complexity of search operations. We compare classification accuracy, energy consumption and robustness of the HD classifier with a conventional machine learning method of the same hardware complexity. Compared to this conventional method: 1) the HD classifier enables 53% energy saving at the expense of 1.2% lower accuracy in language recognition. 2) the HD classifier exhibits extremely robust behavior with low-precision components and tolerates hardware-induced errors in them; it tolerates 8.8-fold probability of failure per individual memory cells, while maintaining a recognition accuracy of above 94%. In addition, the same architecture can be retrained to perform other tasks such as text classification by topic with similar success rates [10].\n\nIn Section 2, we present the concepts of hyperdimensional computing. Our proposed memory-centric architecture with energy optimizations for HD classifier are described in Section 3. In Section 4, we present experimental results followed by discussion. Section 5 concludes this paper.\n\n\nHYPERDIMENSIONAL COMPUTING\n\nThe brain's circuits are massive in terms of numbers of neurons and synapses, suggesting that large circuits are fundamental to the brain's computing. Hyperdimensional computing [7] is based on the understanding that brains compute with patterns of neural activity that are not readily associated with numbers. In fact, the brain's ability to calculate with numbers is feeble. However, due to the very size of the brain's circuits, we can model neural activity patterns with points of a high-dimensional space, that is, with hypervectors. When the dimensionality is in the thousands (e.g., D =10,000), it is called hyperdimensional.\n\nHypervectors are holographic, and (pseudo)random with i.i.d. components. A hypervector contains all the information combined and spread across all its bits in a full holistic representation so that no bit is more responsible to store any piece of information than an-other. Hypervectors are combined with operations akin to addition, multiplication, and permutation that form an algebra over the vector space (e.g., a field). Hypervectors can be compared for similarity using a distance metric over the vector space. These operations on hypervectors can be combined into interesting computational behavior with unique features that make them robust and efficient. In this paper, we target an application of hyperdimensional computing for identifying the language of text samples, based on encoding consecutive letters into hypervectors.\n\nRecognizing the language of a given text is the first step in all sorts of language processing, such as text analysis, categorization, translation, etc. High-dimensional vector models are popular in natural-language processing and are used to capture word meaning from word-use statistics. The vectors are often called semantic vectors. Ideally, words with a similar meaning are represented by semantic vectors that are close to each other in the vector space, while dissimilar meanings are represented by semantic vectors far from each other [11]. Latent semantic analysis [11] is a standard way of making semantic vectors. It relies on singular value decomposition of a large matrix of word frequencies. It is computationally heavy and scales poorly.\n\nRandom indexing [5,6] is an algorithm based on high dimensionality and randomness and it provides a simple and scalable alternative to methods based on principal components, including latent semantic analysis. It is incremental and computes semantic vectors in a single pass over the text data. With the dimensionality in the thousands it is possible to calculate useful representations with fast, and highly scalable algorithms. We use random indexing for identifying the source language of text samples by compiling their N-grams -N consecutive letters -into hypervectors, and by comparing the vectors to each other.\n\n\nRandom Indexing\n\nRandom indexing represents information by projecting data onto vectors in a hyperdimensional space. There exist a huge number of different, nearly orthogonal hypervectors in such a space [12]. This lets us combine two such hypervectors into a new hypervector using well-defined vector-space operations, while keeping the information of the original two with high probability. We consider a variant of the multiplication, addition, and permutation (MAP) coding described in [13] to define the hyperdimensional vector space. The hypervectors are initially taken from a 10,000-dimensional space and have an equal number of randomly placed 1s and 1s. Such hypervectors are used to represent the basic elements, i.e., the 26 letters of the Latin alphabet and the (ASCII) space.\n\nThe MAP operations on the hypervectors are defined as follows. Componentwise addition of two hypervectors A and B, is denoted by A + B. Information from a pair of hypervectors A and B is stored and utilized in a single hypervector by exploiting the addition operation. That is, the sum of two separate hypervectors naturally preserves unique information from each hypervector because of the mathematical properties of vector addition. The vector addition is well suited for representing sets. Componentwise multiplication is denoted by A \u21e4 B. Multiplication of two hypervectors produces a vector that is dissimilar to its constituent vectors; hence it is well suited for binding hypervectors. The third operation is a permutation, r, that rotates the hypervector coordinates. The permutation operation generates a dissimilar vector by scrambling that is good for storing a sequence of hypervectors. For example, the sequence trigram of A-B-C, is stored as the following hypervector, r(rA \u21e4 B) \u21e4 C = rrA \u21e4 rB \u21e4 C. This efficiently distinguishes the sequence A-B-C from A-C-B, since a rotated hypervector is uncorrelated with all the other hypervectors.\n\nCosine similarity is used to measure similarity between two hypervectors by measuring the cosine of the angle between them using a dot product. It is defined as cos(A, B) = |A 0 \u21e4 B 0 |, where A 0 and B 0 are the normalized vectors of A and B, respectively, and |C| denotes the sum of the elements in C.\n\n\nMEMORY-CENTRIC ARCHITECTURE FOR HD CLASSIFIER\n\nIn this section, we first describe our proposed architecture for the HD classifier and in Sections 3.1 and 3.2 describe its main modules. In Section 3.3, we show how the hyperdimensional computing for language recognition is very robust to low-precision binary components, followed by our optimizations for energy efficiency in Section 3.4.\n\nThe proposed design uses the same strategy as presented in [9] for recognizing a text's language by generating and comparing text hypervectors: the text hypervector of an unknown text sample is compared for similarity to precomputed text hypervectors of known language samples -the former is referred to as a query hypervector, while the latter are referred to as language hypervectors. As shown in Figure 1, the design is based on a memory-centric architecture where logic is tightly integrated with the memory and all computation is fully distributed. The architecture has two main modules: encoding and similarity search. The encoding module projects an input text, composed of a stream of letters, to a hypervector in highdimensional space. Then this hypervector is broadcast to the similarity search module for comparing with a set of precomputed language hypervectors. The search module returns the language that has the closet match. In the following two sections, we describe the architectural details of these two modules.\n\n\nEncoding Module\n\nThe encoding module accepts a stream of letters from a text and computes a hypervector that represents the text. First, an item memory assigns a unique but random hypervector, that is called letter hypervector, to an input letter. The item memory is a catalog of meaningful patterns, and it is implemented as a lookup table. In the binary implementation of our encoding module, a hypervector has an equal number of randomly placed 1s and 0s. This assignment is fixed throughout the computation, and formed 27 approximately orthogonal hypervectors as the basic elements of our alphabet here with 27 symbols.\n\nSecond, we need to compute a hypervector for a block of N consecutive letters, for example, a window of three letters or a trigram. Hence, we consider three stages of memory, in the FIFO style, each of which stores a letter hypervector. A trigram hypervector is created by permuting the letter hypervectors and multiplying them as described earlier. The random permutation operation r is fixed, and implemented as a cyclic rotation to right by 1 position as shown in Figure 1. In geometry sense, this permutation rotates the vector in the space. For instance, considering the trigram of A-B-C, A hypervector is rotated twice (rrA), B hypervector is rotated once (rB), and there is no rotation for C hypervector. Once the letter C is reached, its corresponding C hypervector is fetched from the item memory and is directly written to the first stage of encoder (i.e., letter 3 hypervector in Figure 1). To apply r(rA, B) on the two previous letters, they are rotated as they pass through the encoder stages. The pointwise multiplications are then applied between these new hypervectors to compute the trigram hypervector, i.e., rrA \u21e4 rB \u21e4C. Since the trigram hypervector is binary, the multiplication between two hypervectors is implemented with D XOR gates. Third, a text hypervector for an input text is computed by adding all the trigram hypervectors using a sliding window of three letters across the text. This pointwise addition, or summation, produces another D-dimensional hypervector where each component is an integer value. Section 3.4.2 shows how we can substitute such a high-precision text hypervector with a binary hypervector. The output of the encoding module is the text hypervector.\n\nThe encoding module is used for both training and testing. During training when the language of the input text in known, we refer to the text hypervector as a language hypervector. Such language hypervectors are stored in the search module. When the language of a text is unknown, as it is during testing, we call the text hypervector as a query hypervector. The query hypervector is sent to the similarity search module to identify its source language.\n\n\nSimilarity Search Module\n\nThe search module stores a set of language hypervectors that are precomputed by the encoding module. These language hypervectors are made in exactly the same way described above, by making the text hypervectors from samples of a known language. Therefore, during the training phase, we feed texts of a known language to the encoding module and save the resulting text hypervector as a language hypervector in the search module. We consider 21 European languages, consequently at the end of the training phase, we will have 21 language hypervectors, each of which is stored separately in a row of the search module.\n\nDetermining the language of an unknown text is done by comparing its query hypervector to all the language hypervectors. This comparison is effectively performed in a distributed fashion using an associative memory. The cosine similarity is used as the similarity metric [8,9]. It measures distance cos between a language hypervector (LV i ) and an unknown query hypervector (QV) as follows:\ndistance cos = LV i \u00b7 QV | LV i || QV |(1)\nwhere LV i \u00b7 QV is the dot product between the two hypervectors, |LV i | and | QV | are the magnitudes of LV i and QV, respectively. If distance cos is close to 1, it means that the trigram frequencies of the unknown text presented in QV are similar to the trigram frequencies of the language vector i , and therefore the text is likely to  be written in the same language. We design a modular similaritymeasurement block that calculates such distance cos between a precomputed LV i and QV. This block is replicated L times within the search module; L is the number of languages in our application. The QV is broadcast across the search module, hence all the similarity-measurement blocks compute their cosines concurrently. Finally, a combinational comparison block selects the highest cosine and returns its associated language as the language that the unknown text has been written in.\n\n\nRobustness in the Presence of Low-Precision Components\n\nHere we assess the robustness of hyperdimensional computing for the language recognition by replacing high-precision components with low-precision components. As described in Section 3.1, a text hypervector contains integer components due to the addition operation that accumulates the trigram hypervectors over the text. Hence, each component in the hypervector requires a multibit cell memory. For learning a megabyte of text, each hypervector component will need 19 bits precision for summing a million of randomly placed 0s and 1s. Figure 2 shows the accuracy of language recognition as a function of bitwidth of the hypervector components. As shown, the recognition accuracy is slightly decreased by reducing the bitwidth (i.e., the precision of each component). Such a robust behavior enables us to turn the high-precision hypervec-  tors to binary hypervectors, with the same dimensionality, while slightly degrading the recognition accuracy from 97.4% to 96.7%. This bitwidth reduction saves the required memory for the search module by a factor of 19\u21e5. Moreover, computing with such binary hypervectors requires fewer hardware resources in both encoding and search modules, motivating us to look for further optimizations presented in the following section.\n\n\nOptimizations for Energy Efficiency\n\nIn this section, we describe our design optimization techniques for energy-efficient hyperdimensional computing with binary components. Our energy analysis in Section 4.2.2 shows that more than 55% of the total power consumption goes to the encoding module. This is because the encoding module, shown in Figure 1, involves power-hungry operations: trigram hypervector generation, and text hypervector generation. The former requires high amount of switching activity, and the latter requires a large number of resources for accumulation and thresholding. In the following two sections, we provide effective solutions to address each of these concerns. In Section 3.4.1, we describe a technique for reducing memory switching activity during trigram hypervector generation. In Section 3.4.2, we demonstrate savings in resources by using binary hypervectors rather than high-precision hypervectors. In Section 3.4.3, we reduce the complexity of hardware implementation of the similarity search module.\n\n\nEncoding Trigrams with Minimal Switching\n\nGenerating a trigram hypervector involves permutation and multiplication operations. As we describe in Section 3.1, the permutation is implemented as a cyclic 1-bit rotation to right. This rotation operation imposes high amount of switching activity in the memory stages where the hypervectors for the letters are stored. Because the hypervectors have equal number of randomly placed 0s and 1s, rotating them in the memory consumes a lot of energy. To address this issue, we propose a new design for trigram encoding that avoids such high switching activity in the memory. Figure 3 illustrates the proposed encoding of trigrams. The letter hypervectors, retrieved from the item memory, are stored in a separate letter memory in their arrival order. The design uses three Barrel shifters to rotate the letter hypervectors as desired, before sending them into the multipliers as opposed to rotating them in each cycle and storing the rotated hypervectors in the memory stages (see Figure 1). The design includes a set of rotating pointers (P0, P1, and P2) that rotate values of 0, 1, and 2 among themselves. These pointer values for the Barrel shifters implement the no rotate, 1-bit rotate (r), and 2-bit rotate (rr) operations. Every Barrel shifter rotates the letter hypervector based on the assigned pointer value.\n\nThe rotated letter hypervectors are multiplied (i.e., XORed) and the resulting hypervector is written to the trigram memory. This design inhibits the undesirable switching activities due to the rotate operations in the memory while generating the trigrams as accurately as the naive encoder, therefore it does not degrade the recognition accuracy.\n\n\nBinary Hypervector Generation\n\nHere, we focus on resource optimizations for the second part of the encoder that uses the trigrams. As described in Section 2.1, a text hypervector is computed by adding all the trigrams over the input text. To produce a binary hypervector, we implement such pointwise addition through a set of D accumulators (ACC) and threshold units (THR) as shown in Figure 1. Every accumulator is assigned to a dimension of the hypervector, and counts the number of 1s in that component location. Once a new trigram hypervector is generated, i accumulators will be accordingly incremented where i is the number of 1s in the generated trigram hypervector. An input text with k letters generates k 2 trigram vectors. Finally, to compute the corresponding binary text hypervector, the encoding module applies a majority function of (k, k/2) to every accumulator value. The accumulator values are compared to a threshold of k/2 by the encoding module and passed on to the text hypervector as either 0 or 1. Left side of Figure 1 shows such a dedicated accumulation and thresholding for every hypervector component.\n\n\nLow-Cost Modular Similarity Search\n\nThe last optimization focuses on the similarity search module that is composed of a set of similarity-measurement blocks. In a similarity-measurement block, the broadcast query hypervector (QV) is compared to a precomputed language hypervector, LV i . Our optimization reduces the complexity of hardware resources that are required to compare these two hypervectors while providing a reliable distance measurement. In Section 3.2, the cosine is suggested as a measure of similarity between hypervectors [7,9]. The cosine is a non-Euclidean distance that is based on angles between vectors and not their \"locations\" in space. We find that Hamming distance measures the similarity of hypervectors as reliably as the cosine without any degradation in the recognition accuracy.\n\nHamming distance counts the number of components at which two binary hypervectors disagree. Hamming distance reduces the energy consumption of similarity-measurement block since it does not require any normalization calculation as opposed to the cosine. We use a set of D XOR gates to identify mismatches between QV and LV i . To ensure the scalability, the module compares only one component each clock cycle. Hence, the similarity-measurement block takes O(D) cycles to compute the Hamming distance between the two hypervectors. Thanks to its modularity, this block is replicated L times in the search module as shown in Figure 1. The search module selects a language that has the minimum Hamming distance with QV.\n\n\nEXPERIMENTAL RESULTS\n\nIn this section, we first present our application of language recognition and its dataset. Next, we describe a conventional machine learning method as a baseline to compare our HD classifier to. We provide 1 both Matlab and RTL implementations for these two classifiers. We then compare their classification accuracy, memory footprints, energy consumption and robustness. Finally, we discuss our observations.\n\n\nLanguage Recognition Dataset\n\nWe consider an application for recognition of 21 European languages. The sample texts are taken from the Wortschatz Corpora [14] where large numbers of sentences in these languages are available. We train each language hypervector based on about a million bytes of text. To test the ability of identifying the language of unseen text samples, we select test sentences from Europarl Parallel Corpus [15] as an independent text source. This corpus provides 1,000 samples of each language, and each sample is a single sentence. The accuracy recognition metric used throughout this paper is the percentage of these 21,000 test samples that are identified correctly. This accuracy is measured as the microaveraging that gives equal weight to each per-sentence classification decision, rather than per-class.\n\n\nBaseline Machine Learning Method\n\nAs the baseline technique, we choose a nearest neighbor classifier that uses histograms of N-grams. To compute distance between histograms, the dot product is used. A histogram is generated for each language to capture the frequency of N-grams that are observed during the training text. Hence, the outcome of the training phase is a set of 21 histograms that represent the language profiles. In the same vein, a histogram is generated from a test sentence. To find out the language of the test sentence, we compute the dot product of its histogram with the 21 precomputed histograms. The highest dot product score identifies the language that the test sentence is written in. Considering N-grams as the input features, a histogram requires #elements N integer components where #elements is 27 in our application. To reduce this memory footprint, we convert the integer components of histograms to binary using their mean value as the threshold.\n\nWe choose such a nearest-neighbor classifier among histograms as the baseline for two reasons. First, the histogram has full information about the N-gram statistics, so it sets the highest standard of comparison. Second, from a hardware point of view, this baseline shares similarity with the HD classifier. Both use N-grams as input for the encoding, and involve operations with the same complexity. For instance, computing the frequency of an N-gram in the baseline is a lookup action followed by addition. During search operations, both use dot product to measure the distances; it is then simplified to Hamming distance for the binary components. Essentially, this baseline uses the same hardware components as the HD classifier does, but excludes the item memory. In the following sections we compare them in detail. Table 1 compares the two classifiers when using binary components with different N-grams. The first two columns summarize the classification accuracy; the last two columns list the memory footprint with the binary components. Using bigrams of letters, the baseline has 2.3% lower recognition accuracy compared to the HD classifier. However, using N-grams with N 3, the baseline dis-  Figure 4: Accuracy of classifiers with faulty memory cells.\n\n\nClassification Accuracy and Memory Usage\n\nplays slightly higher accuracies. For example, the baseline shows 97.9% recognition while the HD classifier shows 96.7% using trigrams. In this case, the HD classifier requires 1.2\u21e5 as many memory cells as the baseline. On the upside, the HD classifier is able to represent many more N-grams within the same hardware structure. It scales very well: for instance, by moving from trigrams (N = 3) to pentagrams (N = 5), the HD classifier must add memory cells only for 2 extra hypervectors whereas the memory required by the baseline grows exponentially with N. Using pentagrams of letters, the baseline shows an accuracy of 99.8% (4.8% higher than the HD classifier), at the expense of 500\u21e5 larger memory size. Of course the exact counts of all pentagrams that appear in a million bytes of text can be captured in much less memory than that but the algorithm is no longer as simple.\n\n\nEnergy Efficiency\n\nWe use a standard ASIC flow to design dedicated hardware for these two classifiers. We describe the classifiers, in a fully parameterized manner, using RTL SystemVerilog. We apply the identical constraints and flow to both designs. For the synthesis, we use Synopsys Design Compiler with the TSMC 65-nm technology library, the low-power process with high V T H cells. The designs are optimized for a cycle time of 1 ns. We extract the switching activity of these classifiers during postsynthesis simulations in ModelSim by applying the test sentences. Finally, we measure their power consumptions using Synopsys PrimeTime at (1.2V, 25 C, TT) corner.\n\nWe compare the power consumption for trigrams only, since with N-grams of N 4, the baseline classifier becomes increasingly less efficient compared to the HD classifier due to exponential growth in the amount of memory required. With 47% of the energy required by the baseline classifier, the HD classifier is only 1.2% less accurate: 96.7% versus 97.9% for the baseline. Although both designs use binary components and low-cost Hamming distance for similarity measurements, the HD classifier achieves higher energy efficiency thanks to its one-shot computation with highly scalable and local operations, in addition to the optimizations presented in Section 3.4.\n\n\nRobustness Against Memory Errors\n\nHere we assess the classifiers' tolerance for memory errors. We target RTL fault simulations where we inject memory bit flips during every clock cycle of execution. We consider a wider range of probability of failures for each memory cell; the fault simulations cover all the memory elements in both designs. Fig. 4 shows the recognition accuracy with the erroneous memory cells; the X-axis displays the probability of failure for each memory cell in every clock cycle. The baseline is able to maintain its high accuracy of 97% using faulty memory cells with the probability of failure at 3.16E-08, and lower values. At 3.17E-08 the accuracy falls sharply to below 46%. However, the HD classifier exhibits a very robust behavior: it maintains the recognition accuracy of 94% and higher for the probability of failure up to 2.78E-07. At or near peek performance (94% for the HD classifier and 97% for the baseline), the HD classifier tolerates 8.8-fold probability of failure compared to the baseline. By further increasing the probability of failure by 2.8\u21e5 to 7.69E-07, the HD classifier is still 80% accurate or better. Finally, the accuracy of the HD classifier drops to 43% when using memory cells with probability of failure at 4.00E-06, i.e., \u21e1120\u21e5 higher that the failure rate that the baseline could tolerate for the same accuracy.\n\n\nDiscussion\n\nHere, we further discuss energy efficiency and robustness benefits of hyperdimensional computing. At its very core, hyperdimensional computing is about manipulating and comparing large patterns, stored in memory. The operations are either local or can be performed in a distributed fashion leading to a substantial energy reduction. These properties of hyperdimensional computing make it an excellent match to emerging 3D nanoscale device platforms. This presents a fundamental departure from traditional computational architectures, where data has to be transported to the processing unit and back, creating the infamous memory wall.\n\nHyperdimensional computing also exhibits a robust behavior enabling further energy saving by operating in low signal-to-noise ratio conditions, or by utilizing emerging imprecise nanoscale devices. Such robustness to the low-precision and faulty components is achieved thanks to the special brain-inspired properties of hyperdimensional computing: (pseudo)randomness with i.i.d. components, high-dimensionality, and holographic representations. In the following, we briefly discuss their contributions to such robustness.\n\nThe algorithm starts with seed letter vectors with i.i.d. components and combines them with the MAP operations. Componentwise multiplication and addition are i.i.d.-preserving. When the permutation is combined with the multiplications to encode Ngrams, we end up with vectors whose components are identically distributed and nearly independent. This means that the components of the language vectors are identically distributed and nearly independent; hence, a failure in a component is not contagious. At the same time, failures in a subset of components are compensated by the holographic representation, i.e., the error-free components can still provide a useful representation that is \"good enough\" for distinction. This property inherently eliminates any needs for the asymmetric error protection in the memory units.\n\nFurther, the algorithm for hyperdimensional computing is oneshot and incremental. It involves componentwise and local operations to compute and compare the hypervectors without any control flow conditions, which brings another degree of robustness for the algorithm.\n\n\nCONCLUSION\n\nWe propose a robust and energy-efficient hardware design for hyperdimensional computing. The proposed HD classifier forms a memory-centric architecture with modular and scalable components. We compare it with the conventional nearest neighbor classifier that uses histograms of trigrams: the HD classifier uses half the energy and tolerates 8.8-fold probability of failure for individual memory cells, while displaying a recognition accuracy of 94% (at maximum, 3% lower than the conventional method). This excellent performance with low-precision and faulty components is accomplished by appeal to the mathematical properties of high-dimensional spaces, including the high-dimensional, holographic, and (pseudo)random representation with i.i.d. components, in addition to the absence of control flow during execution. Our ongoing work is focused on efficient encoding with hierarchical searches, as well as their realization on 3D nanofabrics.\n\n\nACKNOWLEDGMENT\n\nThis work was supported by Systems on Nanoscale Information fabriCs (SONIC), one of the six SRC STARnet Centers, sponsored by MARCO and DARPA.\n\nISLPED ' 16 ,\n16August 08-10, 2016, San Francisco Airport, CA, USA c 2016 ACM. ISBN 978-1-4503-4185-1/16/08. . . $15.00 DOI: http://dx.doi.org/10.1145/2934583.2934624\n\nFigure 1 :\n1Memory-centric architecture for hyperdimensional computing: encoding module and search module.\n\nFigure 2 :\n2Recognition accuracy while varying the component bitwidth of 10,000-dimensional hypervectors.\n\nFigure 3 :\n3Barrel shifters for trigram hypervector generation.\n\nTable 1 :\n1Classification accuracy and memory footprint of HD and baseline classifiers.Accuracy \nMemory (Kb) \nHD \nBaseline HD Baseline \nBigrams (N=2) \n93.2% 90.9% \n670 39 \nTrigrams (N=3) \n96.7% 97.9% \n680 532 \nTetragrams (N=4) 97.1% 99.2% \n690 13837 \nPentagrams (N=5) 95.0% 99.8% \n700 373092 \n\n\nAvailable for download at https://github.com/abbas-rahimi/ HDC-Language-Recognition\n\nParameter variations and impact on circuits and microarchitecture. S Borkar, Proc. of the Design Automation Conference. of the Design Automation ConferenceS. Borkar, et. al. Parameter variations and impact on circuits and microarchitecture. In Proc. of the Design Automation Conference, pages 338-342, June 2003.\n\nA 0.25 V 460 nW asynchronous neural signal processor with inherent leakage suppression. Solid-State Circuits. T.-T Liu, J M Rabaey, IEEE Journal. 484T.-T. Liu and J.M. Rabaey. A 0.25 V 460 nW asynchronous neural signal processor with inherent leakage suppression. Solid-State Circuits, IEEE Journal of, 48(4):897-906, 2013.\n\nLow-energy robust neuromorphic computation using synaptic devices. Electron Devices. D Kuzum, IEEE Transactions on. 5912D. Kuzum, et. al. Low-energy robust neuromorphic computation using synaptic devices. Electron Devices, IEEE Transactions on, 59(12):3489-3494, Dec 2012.\n\nA neuromorphic neural spike clustering processor for deep-brain sensing and stimulation systems. Beinuo Zhang, Zhewei Jiang, Qi Wang, Jae Sun Seo, Mingoo Seok, Proc. of the International Symposium on Low Power Electronics and Design. of the International Symposium on Low Power Electronics and DesignBeinuo Zhang, Zhewei Jiang, Qi Wang, Jae sun Seo, and Mingoo Seok. A neuromorphic neural spike clustering processor for deep-brain sensing and stimulation systems. In Proc. of the International Symposium on Low Power Electronics and Design, 2015.\n\nRandom indexing of text samples for latent semantic analysis. Pentti Kanerva, Jan Kristoferson, Anders Holst, Proc. of the Conference of the Cognitive Science Society. of the Conference of the Cognitive Science SocietyPentti Kanerva, Jan Kristoferson, and Anders Holst. Random indexing of text samples for latent semantic analysis. In Proc. of the Conference of the Cognitive Science Society, 2000.\n\nAn introduction to random indexing. Magnus Sahlgren, Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering. Magnus Sahlgren. An introduction to random indexing. In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, TKE 2005, 2005.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Pentti Kanerva, Cognitive Computation. 12Pentti Kanerva. Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Cognitive Computation, 1(2):139-159, 2009.\n\nComputing with 10,000-bitwords. Pentti Kanerva, Proc. 52nd Annual Allerton Conference on Communication, Control, and Computing. 52nd Annual Allerton Conference on Communication, Control, and ComputingPentti Kanerva. Computing with 10,000-bitwords. In Proc. 52nd Annual Allerton Conference on Communication, Control, and Computing, 2014.\n\nLanguage geometry using random indexing. Aditya Joshi, Johan Halseth, Pentti Kanerva, Quantum Interaction 2016 Conference Proceedings. in pressAditya Joshi, Johan Halseth, and Pentti Kanerva. Language geometry using random indexing. In Quantum Interaction 2016 Conference Proceedings, in press.\n\nHyperdimensional computing for text classification. Design. Abbas Fateme Rasti Najafabadi, Pentti Rahimi, Jan M Kanerva, Rabaey, Automation Test in Europe Conference Exhibition (DATE), University BoothFateme Rasti Najafabadi, Abbas Rahimi, Pentti Kanerva, and Jan M. Rabaey. Hyperdimensional computing for text classification. Design, Automation Test in Europe Conference Exhibition (DATE), University Booth, March 2016.\n\nA solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. T K Landauer, S T Dumais, Psychological Review. 1042T.K. Landauer and S.T. Dumais. A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211-240, 1997.\n\nPentti Kanerva, Sparse Distributed Memory. Cambridge, MA, USAMIT PressPentti Kanerva. Sparse Distributed Memory. MIT Press, Cambridge, MA, USA, 1988.\n\nMultiplicative binding, representation operators & analogy. Advances in analogy research. Ross W Gayler, Ross W. Gayler. Multiplicative binding, representation operators & analogy. Advances in analogy research, 1998.\n\nCorpus portal for search in monolingual corpora. Uwe Quasthoff, Matthias Richter, Christian Biemann, Proc. of the International Conference on Language Resources and Evaluation. of the International Conference on Language Resources and EvaluationUwe Quasthoff, Matthias Richter, and Christian Biemann. Corpus portal for search in monolingual corpora. In Proc. of the International Conference on Language Resources and Evaluation, 2006.\n\nEuroparl: A parallel corpus for statistical machine translation. Philipp Koehn, Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. http://www.statmt.org/europarl/, 2005.\n", "annotations": {"author": "[{\"end\":285,\"start\":92},{\"end\":503,\"start\":286},{\"end\":697,\"start\":504}]", "publisher": null, "author_last_name": "[{\"end\":104,\"start\":98},{\"end\":300,\"start\":293},{\"end\":516,\"start\":510}]", "author_first_name": "[{\"end\":97,\"start\":92},{\"end\":292,\"start\":286},{\"end\":507,\"start\":504},{\"end\":509,\"start\":508}]", "author_affiliation": "[{\"end\":284,\"start\":106},{\"end\":502,\"start\":324},{\"end\":696,\"start\":518}]", "title": "[{\"end\":89,\"start\":1},{\"end\":786,\"start\":698}]", "venue": null, "abstract": "[{\"end\":1784,\"start\":811}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2147,\"start\":2144},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2497,\"start\":2494},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2499,\"start\":2497},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2501,\"start\":2499},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2715,\"start\":2712},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2717,\"start\":2715},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2719,\"start\":2717},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2721,\"start\":2719},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2723,\"start\":2721},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3835,\"start\":3832},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5148,\"start\":5144},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5646,\"start\":5643},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7484,\"start\":7480},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7515,\"start\":7511},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7710,\"start\":7707},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7712,\"start\":7710},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8520,\"start\":8516},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8806,\"start\":8802},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11013,\"start\":11010},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15684,\"start\":15681},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15686,\"start\":15684},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22483,\"start\":22480},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22485,\"start\":22483},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24063,\"start\":24059},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24337,\"start\":24333}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34176,\"start\":34009},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34284,\"start\":34177},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34391,\"start\":34285},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34456,\"start\":34392},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34752,\"start\":34457}]", "paragraph": "[{\"end\":2270,\"start\":1800},{\"end\":3765,\"start\":2272},{\"end\":5149,\"start\":3767},{\"end\":5434,\"start\":5151},{\"end\":6097,\"start\":5465},{\"end\":6935,\"start\":6099},{\"end\":7689,\"start\":6937},{\"end\":8309,\"start\":7691},{\"end\":9101,\"start\":8329},{\"end\":10254,\"start\":9103},{\"end\":10559,\"start\":10256},{\"end\":10949,\"start\":10609},{\"end\":11982,\"start\":10951},{\"end\":12608,\"start\":12002},{\"end\":14310,\"start\":12610},{\"end\":14765,\"start\":14312},{\"end\":15408,\"start\":14794},{\"end\":15801,\"start\":15410},{\"end\":16733,\"start\":15845},{\"end\":18058,\"start\":16792},{\"end\":19096,\"start\":18098},{\"end\":20457,\"start\":19141},{\"end\":20806,\"start\":20459},{\"end\":21938,\"start\":20840},{\"end\":22750,\"start\":21977},{\"end\":23468,\"start\":22752},{\"end\":23902,\"start\":23493},{\"end\":24737,\"start\":23935},{\"end\":25719,\"start\":24774},{\"end\":26986,\"start\":25721},{\"end\":27912,\"start\":27031},{\"end\":28583,\"start\":27934},{\"end\":29248,\"start\":28585},{\"end\":30624,\"start\":29285},{\"end\":31273,\"start\":30639},{\"end\":31796,\"start\":31275},{\"end\":32620,\"start\":31798},{\"end\":32888,\"start\":32622},{\"end\":33847,\"start\":32903},{\"end\":34008,\"start\":33866}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15844,\"start\":15802}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26550,\"start\":26543}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1798,\"start\":1786},{\"attributes\":{\"n\":\"2.\"},\"end\":5463,\"start\":5437},{\"attributes\":{\"n\":\"2.1\"},\"end\":8327,\"start\":8312},{\"attributes\":{\"n\":\"3.\"},\"end\":10607,\"start\":10562},{\"attributes\":{\"n\":\"3.1\"},\"end\":12000,\"start\":11985},{\"attributes\":{\"n\":\"3.2\"},\"end\":14792,\"start\":14768},{\"attributes\":{\"n\":\"3.3\"},\"end\":16790,\"start\":16736},{\"attributes\":{\"n\":\"3.4\"},\"end\":18096,\"start\":18061},{\"attributes\":{\"n\":\"3.4.1\"},\"end\":19139,\"start\":19099},{\"attributes\":{\"n\":\"3.4.2\"},\"end\":20838,\"start\":20809},{\"attributes\":{\"n\":\"3.4.3\"},\"end\":21975,\"start\":21941},{\"attributes\":{\"n\":\"4.\"},\"end\":23491,\"start\":23471},{\"attributes\":{\"n\":\"4.1\"},\"end\":23933,\"start\":23905},{\"attributes\":{\"n\":\"4.2\"},\"end\":24772,\"start\":24740},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":27029,\"start\":26989},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":27932,\"start\":27915},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":29283,\"start\":29251},{\"attributes\":{\"n\":\"4.3\"},\"end\":30637,\"start\":30627},{\"attributes\":{\"n\":\"5.\"},\"end\":32901,\"start\":32891},{\"attributes\":{\"n\":\"6.\"},\"end\":33864,\"start\":33850},{\"end\":34023,\"start\":34010},{\"end\":34188,\"start\":34178},{\"end\":34296,\"start\":34286},{\"end\":34403,\"start\":34393},{\"end\":34467,\"start\":34458}]", "table": "[{\"end\":34752,\"start\":34545}]", "figure_caption": "[{\"end\":34176,\"start\":34026},{\"end\":34284,\"start\":34190},{\"end\":34391,\"start\":34298},{\"end\":34456,\"start\":34405},{\"end\":34545,\"start\":34469}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11358,\"start\":11350},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13085,\"start\":13077},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13509,\"start\":13501},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17336,\"start\":17328},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18410,\"start\":18402},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19722,\"start\":19714},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20128,\"start\":20120},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21202,\"start\":21194},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21852,\"start\":21844},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23383,\"start\":23375},{\"end\":26935,\"start\":26927},{\"end\":29600,\"start\":29594}]", "bib_author_first_name": "[{\"end\":34906,\"start\":34905},{\"end\":35266,\"start\":35262},{\"end\":35273,\"start\":35272},{\"end\":35275,\"start\":35274},{\"end\":35563,\"start\":35562},{\"end\":35854,\"start\":35848},{\"end\":35868,\"start\":35862},{\"end\":35878,\"start\":35876},{\"end\":35888,\"start\":35885},{\"end\":35904,\"start\":35898},{\"end\":36367,\"start\":36361},{\"end\":36380,\"start\":36377},{\"end\":36401,\"start\":36395},{\"end\":36741,\"start\":36735},{\"end\":37222,\"start\":37216},{\"end\":37480,\"start\":37474},{\"end\":37827,\"start\":37821},{\"end\":37840,\"start\":37835},{\"end\":37856,\"start\":37850},{\"end\":38141,\"start\":38136},{\"end\":38173,\"start\":38167},{\"end\":38185,\"start\":38182},{\"end\":38187,\"start\":38186},{\"end\":38626,\"start\":38625},{\"end\":38628,\"start\":38627},{\"end\":38640,\"start\":38639},{\"end\":38642,\"start\":38641},{\"end\":38886,\"start\":38880},{\"end\":39125,\"start\":39121},{\"end\":39127,\"start\":39126},{\"end\":39301,\"start\":39298},{\"end\":39321,\"start\":39313},{\"end\":39340,\"start\":39331},{\"end\":39757,\"start\":39750}]", "bib_author_last_name": "[{\"end\":34913,\"start\":34907},{\"end\":35270,\"start\":35267},{\"end\":35282,\"start\":35276},{\"end\":35569,\"start\":35564},{\"end\":35860,\"start\":35855},{\"end\":35874,\"start\":35869},{\"end\":35883,\"start\":35879},{\"end\":35896,\"start\":35889},{\"end\":35909,\"start\":35905},{\"end\":36375,\"start\":36368},{\"end\":36393,\"start\":36381},{\"end\":36407,\"start\":36402},{\"end\":36750,\"start\":36742},{\"end\":37230,\"start\":37223},{\"end\":37488,\"start\":37481},{\"end\":37833,\"start\":37828},{\"end\":37848,\"start\":37841},{\"end\":37864,\"start\":37857},{\"end\":38165,\"start\":38142},{\"end\":38180,\"start\":38174},{\"end\":38195,\"start\":38188},{\"end\":38203,\"start\":38197},{\"end\":38637,\"start\":38629},{\"end\":38649,\"start\":38643},{\"end\":38894,\"start\":38887},{\"end\":39134,\"start\":39128},{\"end\":39311,\"start\":39302},{\"end\":39329,\"start\":39322},{\"end\":39348,\"start\":39341},{\"end\":39763,\"start\":39758}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7863591},\"end\":35150,\"start\":34838},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14731196},\"end\":35475,\"start\":35152},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7153834},\"end\":35749,\"start\":35477},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":22036965},\"end\":36297,\"start\":35751},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":60571601},\"end\":36697,\"start\":36299},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":17228581},\"end\":37089,\"start\":36699},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":733980},\"end\":37440,\"start\":37091},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":13894815},\"end\":37778,\"start\":37442},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":39020350},\"end\":38074,\"start\":37780},{\"attributes\":{\"id\":\"b9\"},\"end\":38496,\"start\":38076},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1144461},\"end\":38878,\"start\":38498},{\"attributes\":{\"id\":\"b11\"},\"end\":39029,\"start\":38880},{\"attributes\":{\"id\":\"b12\"},\"end\":39247,\"start\":39031},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":17089484},\"end\":39683,\"start\":39249},{\"attributes\":{\"id\":\"b14\"},\"end\":39883,\"start\":39685}]", "bib_title": "[{\"end\":34903,\"start\":34838},{\"end\":35260,\"start\":35152},{\"end\":35560,\"start\":35477},{\"end\":35846,\"start\":35751},{\"end\":36359,\"start\":36299},{\"end\":36733,\"start\":36699},{\"end\":37214,\"start\":37091},{\"end\":37472,\"start\":37442},{\"end\":37819,\"start\":37780},{\"end\":38623,\"start\":38498},{\"end\":39296,\"start\":39249}]", "bib_author": "[{\"end\":34915,\"start\":34905},{\"end\":35272,\"start\":35262},{\"end\":35284,\"start\":35272},{\"end\":35571,\"start\":35562},{\"end\":35862,\"start\":35848},{\"end\":35876,\"start\":35862},{\"end\":35885,\"start\":35876},{\"end\":35898,\"start\":35885},{\"end\":35911,\"start\":35898},{\"end\":36377,\"start\":36361},{\"end\":36395,\"start\":36377},{\"end\":36409,\"start\":36395},{\"end\":36752,\"start\":36735},{\"end\":37232,\"start\":37216},{\"end\":37490,\"start\":37474},{\"end\":37835,\"start\":37821},{\"end\":37850,\"start\":37835},{\"end\":37866,\"start\":37850},{\"end\":38167,\"start\":38136},{\"end\":38182,\"start\":38167},{\"end\":38197,\"start\":38182},{\"end\":38205,\"start\":38197},{\"end\":38639,\"start\":38625},{\"end\":38651,\"start\":38639},{\"end\":38896,\"start\":38880},{\"end\":39136,\"start\":39121},{\"end\":39313,\"start\":39298},{\"end\":39331,\"start\":39313},{\"end\":39350,\"start\":39331},{\"end\":39765,\"start\":39750}]", "bib_venue": "[{\"end\":34956,\"start\":34915},{\"end\":35296,\"start\":35284},{\"end\":35591,\"start\":35571},{\"end\":35983,\"start\":35911},{\"end\":36465,\"start\":36409},{\"end\":36883,\"start\":36752},{\"end\":37253,\"start\":37232},{\"end\":37568,\"start\":37490},{\"end\":37913,\"start\":37866},{\"end\":38134,\"start\":38076},{\"end\":38671,\"start\":38651},{\"end\":38921,\"start\":38896},{\"end\":39119,\"start\":39031},{\"end\":39424,\"start\":39350},{\"end\":39748,\"start\":39685},{\"end\":34993,\"start\":34958},{\"end\":36051,\"start\":35985},{\"end\":36517,\"start\":36467},{\"end\":37642,\"start\":37570},{\"end\":38941,\"start\":38923},{\"end\":39494,\"start\":39426}]"}}}, "year": 2023, "month": 12, "day": 17}