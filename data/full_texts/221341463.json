{"id": 221341463, "updated": "2022-01-22 13:29:40.701", "metadata": {"title": "Sub-center ArcFace: Boosting Face Recognition by Large-scale Noisy Web Faces", "authors": "[{\"middle\":[],\"last\":\"Deng\",\"first\":\"Jiankang\"},{\"middle\":[],\"last\":\"Guo\",\"first\":\"Jia\"},{\"middle\":[],\"last\":\"Liu\",\"first\":\"Tongliang\"},{\"middle\":[],\"last\":\"Gong\",\"first\":\"Mingming\"},{\"middle\":[],\"last\":\"Zafeiriou\",\"first\":\"Stefanos\"}]", "venue": "Computer Vision \u2013 ECCV 2020", "journal": "Computer Vision \u2013 ECCV 2020", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Margin-based deep face recognition methods (e.g. SphereFace, CosFace, and ArcFace) have achieved remarkable success in unconstrained face recognition. However, these methods are susceptible to the massive label noise in the training data and thus require laborious human effort to clean the datasets. In this paper, we relax the intra-class constraint of ArcFace to improve the robustness to label noise. More specifically, we design K sub-centers for each class and the training sample only needs to be close to any of the K positive subcenters instead of the only one positive center. The proposed sub-center ArcFace encourages one dominant sub-class that contains the majority of clean faces and non-dominant sub-classes that include hard or noisy faces. Extensive experiments confirm the robustness of sub-center ArcFace under massive real-world noise. After the model achieves enough discriminative power, we directly drop non-dominant sub-centers and high-confident noisy samples, which helps recapture intra-compactness, decrease the influence from noise, and achieve comparable performance compared to ArcFace trained on the manually cleaned dataset. By taking advantage of the large-scale raw web faces (Celeb500K), sub-center Arcface achieves state-of-the-art performance on IJB-B, IJB-C, MegaFace, and FRVT.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3109225549", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/DengGLGZ20", "doi": "10.1007/978-3-030-58621-8_43"}}, "content": {"source": {"pdf_hash": "6201cf3773a2ed565ae7f60271b1824536a245f4", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b2a5d0b75f58e69cad436e87e95ffbad5b464cdd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6201cf3773a2ed565ae7f60271b1824536a245f4.txt", "contents": "\nSub-center ArcFace: Boosting Face Recognition by Large-scale Noisy Web Faces\n\n\nJiankang Deng \nImperial College\n\n\nJia Guo guojia@gmail.com \nInsightFace\n\n\nTongliang Liu tongliang.liu@sydney.edu.au \nUniversity of Sydney\n\n\nMingming Gong mingming.gong@unimelb.edu.au \nUniversity of Melbourne\n\n\nStefanos Zafeiriou s.zafeiriou@imperial.ac.uk \nImperial College\n\n\nSub-center ArcFace: Boosting Face Recognition by Large-scale Noisy Web Faces\nFace RecognitionSub-classSub-centerLarge-scaleNoisy Data\nMargin-based deep face recognition methods (e.g. SphereFace, Cos-Face, and ArcFace) have achieved remarkable success in unconstrained face recognition. However, these methods are susceptible to the massive label noise in the training data and thus require laborious human effort to clean the datasets. In this paper, we relax the intra-class constraint of ArcFace to improve the robustness to label noise. More specifically, we design K sub-centers for each class and the training sample only needs to be close to any of the K positive subcenters instead of the only one positive center. The proposed sub-center Arc-Face encourages one dominant sub-class that contains the majority of clean faces and non-dominant sub-classes that include hard or noisy faces. Extensive experiments confirm the robustness of sub-center ArcFace under massive real-world noise. After the model achieves enough discriminative power, we directly drop non-dominant sub-centers and high-confident noisy samples, which helps recapture intra-compactness, decrease the influence from noise, and achieve comparable performance compared to ArcFace trained on the manually cleaned dataset. By taking advantage of the large-scale raw web faces (Celeb500K), sub-center Arcface achieves state-of-the-art performance on IJB-B, IJB-C, MegaFace, and FRVT. * Equal contributions. InsightFace is a nonprofit Github project for 2D and 3D face analysis.\n\nIntroduction\n\nFace representation using Deep Convolutional Neural Network (DCNN) embedding with margin penalty [26,15,32,5] to simultaneously achieve intra-class compactness and inter-class discrepancy is the method of choice for state-of-the-art face recognition. To avoid the sampling problem in the Triplet loss [26], margin-based softmax methods [15,32,31,5] focused on incorporating margin penalty into a more feasible framework, the softmax loss, which has global sample-to-class comparisons within the multiplication step between the embedding feature and the linear transformation matrix. Naturally, each column of the linear transformation matrix is viewed as a class center representing a certain class [5].  In this paper, we introduce sub-class into ArcFace to relax the intra-class constraint, which can effectively improve robustness under noise. (b) The sub-classes of one identity from the CASIA dataset [40] after using the sub-center ArcFace loss (K = 10). Noisy samples and hard samples (e.g. profile and occluded faces) are automatically separated from the majority of clean samples.\n\nEven though remarkable advances have been achieved by the margin-based softmax methods [8,15,32,31,5,39], they all need to be trained on well-annotated clean datasets [30,5], which require intensive human efforts. Wang et al. [30] found that faces with label noise significantly degenerate the recognition accuracy and manually built a high-quality dataset including 1.7M images of 59K celebrities. However, it took 50 annotators to work continuously for one month to clean the dataset, which further demonstrates the difficulty of obtaining a large-scale clean dataset for face recognition.\n\nSince accurate manual annotations can be expensive [30], learning with massive noisy data 1 has recently attracted much attention [14,4,11,41,33]. However, computing time-varying weights for samples [11] or designing piece-wise loss functions [41] based on the current model's predictions can only alleviate the influence from noisy data to some extent as the robustness and improvement depend on the initial performance of the model. Besides, the co-mining method [33] requires to train twin networks together thus it is less practical for training large models on large-scale datasets.\n\nAs shown in Fig. 1(a), the objective of ArcFace [5] has two parts: (1) intra-class compactness: pushing the sample close to the corresponding positive center; and (2) inter-class discrepancy: pushing the sample away from all other negative centers. If a face is a noisy sample, it does not belong to the corresponding positive class. In Arc-Face, this noisy sample generates a large wrong loss value, which impairs the model training. In this paper, we relax the intra-class constraint of forcing all samples close to the corresponding positive center by introducing sub-classes into ArcFace. For each class, we design K sub-centers and the training sample only needs to be close to any of the K positive sub-centers instead of the only one positive center. As illustrated in Fig. 1(b), the proposed sub-center ArcFace will encourage one dominant sub-class that contains the majority clean faces and multiple non-dominant sub-classes that include hard or noisy faces. This happens because the intra-class constraint of sub-center Arc-Face enforces the training sample to be close to one of the multiple positive sub-classes but not all of them. The noise is likely to form a non-dominant sub-class and will not be enforced into the dominant sub-class. Therefore, sub-center ArcFace is more robust to noise. Extensive experimental results in this paper indicate that the proposed sub-center ArcFace is more robust than ArcFace [5] under massive real-world noises.\n\nAlthough the proposed sub-center ArcFace can effectively separate clean data from noisy data. However, hard samples are also kept away. The existing of sub-centers can improve the robustness but also undermine the intra-class compactness, which is important for face recognition [34]. As the devil of face recognition is in the noise [30], we directly drop non-dominant sub-centers and high-confident noisy samples after the model achieves enough discriminative power. By pushing hard samples close to the dominant sub-center, we gradually recapture intra-class compactness and further improve the accuracy.\n\nTo summarise, our key contributions are as follows:\n\n-We introduce sub-class into ArcFace to improve its robustness on noisy training data. The proposed sub-center ArcFace consistently outperforms ArcFace under massive real-world noises. -By dropping non-dominant sub-centers and high-confident noisy samples, our method can achieve comparable performance compared to ArcFace trained on the manually cleaned dataset. -Sub-center Arcface can be easily implemented by using the parallel toolkit and thus enjoys scalability to large-scale datasets. By taking advantage of the largescale raw web faces (e.g. Celeb500K [1]), the proposed sub-center Arcface achieves state-of-the-art performance on IJB-B, IJB-C, MegaFace, and FRVT 1:1 Verification.\n\n\nRelated work\n\nFace Recognition with Margin Penalty. The pioneering work [26] uses the Triplet loss to exploit triplet data such that faces from the same class are closer than faces from different classes by a clear Euclidean distance margin. Even though the Triplet loss makes perfect sense for face recognition, the sample-to-sample comparisons are local within mini-batch and the training procedure for the Triplet loss is very challenging as there is a combinatorial explosion in the number of triplets especially for large-scale datasets, requiring effective sampling strategies to select informative mini-batch [25,26] and choose representative triplets within the mini-batch [36,21,28]. Some works tried to reduce the total number of triplets with proxies [19,23], i.e., sample-to-sample comparison is changed into sample-to-proxy comparison. However, sampling and proxy methods only optimise the embedding of partial classes instead of all classes in one iteration step.\n\nMargin-based softmax methods [15,8,32,31,5] focused on incorporating margin penalty into a more feasible framework, softmax loss, which has extensive sample-to-class comparisons. Compared to deep metric learning methods (e.g., Triplet [26], Tuplet [21,28]), margin-based softmax methods conduct global comparisons at the cost of memory consumption on holding the center of each class. Sample-to-class comparison is more efficient and stable than sample-to-sample comparison as (1) the class number is much smaller than sample number, and (2) each class can be represented by a smoothed center vector which can be updated during training. Face Recognition under Noise. Most of the face recognition datasets [40,9,2,1] are downloaded from the Internet by searching a pre-defined celebrity list, and the original labels are likely to be ambiguous and inaccurate [30]. Learning with massive noisy data has recently drawn much attention in face recognition [37,11,41,33] as accurate manual annotations can be expensive [30] or even unavailable.\n\nWu et al. [37] proposed a semantic bootstrap strategy, which re-labels the noisy samples according to the probabilities of the softmax function. However, automatic cleaning by the bootstrapping rule requires time-consuming iterations (e.g. twice refinement steps are used in [37]) and the labelling quality is affected by the capacity of the original model. Hu et al. [11] found that the cleanness possibility of a sample can be dynamically reflected by its position in the target logit distribution and presented a noise-tolerant end-to-end paradigm by employing the idea of weighting training samples. Zhong et al. [41] devised a noise-resistant loss by introducing a hypothetical training label, which is a convex combination of the original label with probability \u03c1 and the predicted label by the current model with probability 1 \u2212 \u03c1. However, computing time-varying fusion weight [11] and designing piece-wise loss [41] contain many hand-designed hyperparameters. Besides, re-weighting methods are susceptible to the performance of the initial model. Wang et al. [33] proposed a co-mining strategy which uses the loss values as the cue to simultaneously detect noisy labels, exchange the high-confidence clean faces to alleviate the error accumulation caused by the sampling bias, and re-weight the predicted clean faces to make them dominate the discriminative model training. However, the co-mining method requires training twin networks simultaneously and it is challenging to train large networks (e.g. ResNet100 [10]) on a large-scale dataset (e.g. MS1M [9] and Celeb500K [1]). Face Recognition with Sub-classes. Practices and theories that lead to \"sub-class\" have been studied for a long time [42,43]. The concept of \"sub-class\" applied in face recognition was first introduced in [42,43], where a mixture of Gaussians was used to approximate the underlying distribution of each class. For instance, a person's face images may be frontal view or side view, resulting in different modalities when all images are represented in the same data space. In [42,43], experimental results showed that subclass divisions can be used to effectively adapt to different face modalities thus improve the performance of face recognition. Wan et al. [29] further proposed a separability criterion to divide every class into sub-classes, which have much less overlaps. The new within-class scatter can represent multi-modality information, therefore optimising this within-class scatter will separate different modalities more clearly and further increase the accuracy of face recognition. However, these work [42,43,29] only employed handdesigned feature descriptor on tiny under-controlled datasets.\n\nConcurrent with our work, Softtriple [22] presents a multi-center softmax loss with class-wise regularizer. These multi-centers can capture the hidden distribution of the data better [20] due to the fact that they can capture the complex geometry of the orig-inal data and help reduce the intra-class variance. On the fine-grained visual retrieval problem, the Softtriple [22] loss achieves better performance than the softmax loss as capturing local clusters is essential for this task. Even though the concept of \"sub-class\" has been employed in face recognition [42,43,29] and fine-grained visual retrieval [22], none of these work has considered the large-scale (e.g. 0.5 million classes) face recognition problem under massive noise (e.g. around 50% noisy samples within the training data).\n\n\nThe Proposed Approach\n\n\nArcFace\n\nArcFace [5] introduced an additive angular margin penalty into the softmax loss,\nArcFace = \u2212 log e s cos(\u03b8y i +m) e s cos(\u03b8y i +m) + N j=1,j =yi e s cos \u03b8j ,(1)\nwhere \u03b8 j is the angle between the embedding feature x i \u2208 R 512\u00d71 of the i-th face sample and the j-th class center W j \u2208 R 512\u00d71 . Given that the corresponding class label of x i is y i , \u03b8 yi represents the angle between x i and the ground-truth center W yi . m = 0.5 is the angular margin parameter, s = 64 is the feature re-scale parameter, and N is the total class number. As there is a 2 normalisation step on both x i and W j , \u03b8 j = arccos W T j x i . Taking advantage of parallel acceleration on both x i and W j , the implementation of ArcFace 2 can efficiently handle million-level identities on a single server with 8 GPUs (11GB 1080ti). This straightforward solution has changed the ingrained belief that large-scale global comparison with all classes is usually not attainable due to the bottleneck of GPU memory [26,28].\n\n\nSub-center ArcFace\n\nEven though ArcFace [5] has shown its power in efficient and effective face feature embedding, this method assumes that training data are clean [5,30]. However, this is not true especially when the dataset is in large scale. How to enable ArcFace to be robust to noise is one of the main challenges that impeding the development of face representation and recognition [30]. In this paper, we address this problem by proposing the idea of using sub-classes for each identity, which can be directly adopted by ArcFace and will significantly increase its robustness. Foster Sub-classes. As illustrated in Fig. 2, we set a sufficiently large K for each identity. Based on a 2 normalisation step on both embedding feature x i \u2208 R 512\u00d71 and all sub-centers W \u2208 R N \u00d7K\u00d7512 , we get the subclass-wise similarity scores S \u2208 R N \u00d7K by a matrix multiplication W T x i . Then, we employ a max pooling step on the subclasswise similarity score S \u2208 R N \u00d7K to get the class-wise similarity score S \u2208 R N \u00d71 . The The main contribution in this paper is highlighted by the blue dashed box. Based on a 2 normalisation step on both embedding feature xi \u2208 R 512\u00d71 and all sub-centers W \u2208 R N \u00d7K\u00d7512 , we get the subclass-wise similarity score S \u2208 R N \u00d7K by a matrix multiplication W T xi. After a max pooling step, we can easily get the class-wise similarity score S \u2208 R N \u00d71 . The following steps are same as ArcFace [5]. Table 1. The strictness and robustness analysis of different comparison strategies. In the angular space, \"Min\" is closest and \"Max\" is farest. \"intra\" refers to comparison between the training sample and the positive sub-centers (K). \"inter\" refers to comparison between the training sample and all negative sub-centers ((N \u2212 1) \u00d7 K). \"outlier\" denotes the open-set noise and \"label flip\" denotes the close-set noise.\n\n\nConstraints\n\nSub-center? Strictness? Robustness to outlier? Robustness to label flip?   proposed sub-center ArcFace loss can be formulated as:\n(1) Min(inter) -Min(intra) \u2265 m \u221a +++ ++ + (2) Max(inter) -Min(intra) \u2265 m \u221a + ++ ++ (3) Min(inter) -Max(intra) \u2265 m ++++ (4) Max(inter) -Max(intra) \u2265 m ++ +(d) K = 3 \u2193 1, Non-dominantArcFace subcenter = \u2212 log e s cos(\u03b8i,y i +m) e s cos(\u03b8i,y i +m) + N j=1,j =yi e s cos \u03b8i,j ,(2)\nwhere \u03b8 i,j = arccos max k W T j k x i , k \u2208 {1, \u00b7 \u00b7 \u00b7 , K}. Robustness and Strictness Analysis. Given a large K, sub-classes are able to capture the complex distribution of the whole training data. Except for applying max pooling on the subclass-wise cosine similarity score, we can also consider other different comparison strategies. In Tab. 1, we give the strictness and robustness analysis of four comparison strategies. (1) adds angular margin between the closest inter-class sub-center and the closest intra-class sub-center. For intra-class comparison, choosing the closest positive sub-center can relax the intra-class constraint and improve the robustness under noise. For inter-class comparison, choosing the closest negative sub-center will enhance the inter-class constraint as sub-centers can better capture the complex geometric distributions of the whole data set compared to a single center for each class. However, the enhanced inter-class comparison is less robust under the close-set noise. The training procedure of (2) can not converge as the initial status between inter-classes is orthogonal and relaxing both of the inter-class and intra-class comparisons will disorient the training, as there is no loss from inter-class comparisons. (3) and (4) can not foster subclasses as stiffening intra-class comparison will compress sub-centers into one point in the high-dimension feature space thus undermine the robustness to noise.\n\nDominant and Non-dominant Sub-classes. In Fig. 1(b), we have visualised the clustering results of one identity from the CASIA dataset [40] after employing the subcenter ArcFace loss (K = 10) for training. It is obvious that the proposed sub-center ArcFace loss can automatically cluster faces such that hard samples and noisy samples are separated away from the dominant clean samples. Note that some sub-classes are empty as K = 10 is too large for a particular identity. In Fig. 3(a) and Fig. 3(b), we show the angle distribution on the CASIA dataset [40]. We use the pre-trained ArcFace model [5] to predict the feature center of each identity and then calculate the angle between the sample and its corresponding feature center. As we can see from Fig. 3(a), most of the samples are close to their centers, however, there are some noisy samples which are far away from their centers. This observation on the CASIA dataset matches the noise percentage estimation (9.3% \u223c 13.0%) in [30]. To automatically obtain a clean training dataset, the noisy tail is usually removed by a hard threshold (e.g. angle \u2265 77 \u2022 or cosine \u2264 0.225). Since sub-center ArcFace can automatically divide the training samples into dominant sub-classes and non-dominant sub-classes, we visualise these two different kinds of samples in Fig. 3(b). As we can see from the two histograms, sub-center ArcFace can automatically separate clean samples from hard and noisy samples. More specifically, the majority of clean faces (85.6%) go to the dominant sub-class, while the rest hard and noisy faces go to the non-dominant sub-classes.\n\nDrop Non-dominant Sub-centers and High-confident Noises. Even though using sub-classes can improve the robustness under noise, it undermines the intra-class compactness as hard samples are also kept away as shown in Fig. 3(b). In [9], MS1MV0 (around 10M images of 100K identities) is released with the estimated noise percentage around 47.1% \u223c 54.4% [30]. In [6], MS1MV0 is refined by a semi-automatic approach into a clean dataset named MS1MV3 (around 5.1M images of 93K identities). Based on these two datasets, we can get clean and noisy labels on MS1MV0. In Fig. 4(a) and Fig. 4(b), we show the angle distributions of samples to their closest sub-centers (training settings: [MS1MV0, ResNet-50, Sub-center ArcFace K=3]). In general, there are four categories of samples: (1) easy clean samples belonging to dominant sub-classes (57.24%), (2) hard noisy samples belonging to dominant sub-classes (12.40%), (3) hard clean samples belonging to non-dominant sub-classes (4.28%), and (4) easy noisy samples belonging to non-dominant sub-classes (26.08%). In Fig. 4(c), we show the angle distribution of samples to their corresponding centers from the ArcFace model (training settings: [MS1MV0, ResNet50, ArcFace K=1]). By comparing the percentages of noisy sample in Fig. 4(a) and Fig. 4(c), we find that sub-center ArcFace can significantly decrease the noise rate to around one third (from 38.47% to 12.40%) and this is the reason why sub-center ArcFace is more robust under noise. During the training of sub-center ArcFace, samples belonging to non-dominant sub-classes are pushed to be close to these non-dominant sub-centers as shown in Fig. 4(b). Since we have not set any constraint on sub-centers, the sub-centers of each identity can be quite different and even orthogonal. In Fig. 4(d), we show the angle distributions of non-dominant samples to their dominant sub-centers. By combining Fig. 4(a) and Fig. 4(d), we find that even though the clean and noisy data have some overlaps, a constant angle threshold (between 70 \u2022 and 80 \u2022 ) can be easily searched to drop most of high-confident noisy samples.\n\nBased on the above observations, we propose a straightforward approach to recapture intra-class compactness. We directly drop non-dominant sub-centers after the network has enough discriminative power. Meanwhile, we introduce a constant angle threshold to drop high-confident noisy data. After that, we retrain the model from scratch on the automatically cleaned dataset.\n\n\nComparison with Softtriple and Re-weighting Methods\n\nThe proposed sub-center ArcFace is different from Softtriple [22] in the following aspects:\n\n-Softtriple shows improvement in fine-grained retrieval by employing multi-centers.\n\nHowever, we have not found obvious improvement when we directly use subcenters on the clean dataset as sub-centers can undermine the intra-class compactness which is important for the face recognition problem. Our experimental analysis indicates that sub-centers can increase robustness under noise such that sub-center ArcFace can be trained on raw web faces without any manual cleaning step. -Softtriple employs the softmax pooling (from sub-class similarity to class similarity) considering the smoothness. By contrast, we use the built-in max pooling without any performance drop. Max pooling is much more efficient than softmax pooling, especially for large-scale classification problem. -Softtriple adds a similarity regularization between sub-centers. However, it is not reasonable that noisy data should be similar in our case. To enhance intra-class compactness, we only keep the dominant sub-center and drop the non-dominant sub-centers after the model has enough discriminative power. To decrease the affection from noisy data, we directly drop high-confident noisy data instead of employing complicated re-weighting strategies [41,11]. -Softriple only employs a small cosine margin (0.01) to explicitly break the tie during training. On the contrary, we use a large angular margin (0.5) setting as done by ArcFace.\n\nThe main difference between the proposed sub-center ArcFace and re-weighting methods [11,41] is that sub-center ArcFace is less affected by the noisy data from the beginning of the model training. By contrast, the discriminative power of the initial model is important for both NT [11] and NR [41] methods as their adaptive weights are predicted from the model.\n\nOur sub-center ArcFace achieves high accuracy in face recognition while keeps extreme simplicity, only adding two hyper-parameters: the sub-center number and the constant threshold to drop high-confident noisy data.\n\n\nExperiments\n\n\nExperimental Settings\n\nDatasets. Our training datasets include MS1MV0 (\u223c10M images of 100K identities) [9], MS1MV3 (\u223c5.1M faces of 91K identities) [6], and Celeb500K [1]. MS1MV0 is a raw data with the estimated noise percentage around 47.1% \u223c 54.4% [30]. MS1MV3 is cleaned from MS1MV0 by a semi-automatic approach [6]. Celeb500K [1] is collected as MS1MV0 [9], using half of the MS1M name list [9] to search identities from Google and download the top-ranked face images. Our testing datasets consist of IJB-B [35], IJB-C [17], MegaFace [13], and Face Recognition Vendor Test (FRVT). Besides, we also report our final results on widely used verification datasets (e.g. LFW [12], CFP-FP [27], and AgeDB-30 [18]). Implementation Details. For data pre-possessing, we follow ArcFace [5] to generate the normalised face crops (112 \u00d7 112) by utilising five facial points predicted by RetinaFace [7]. We employ ResNet-50 and ResNet-100 [10,5] to get the 512-D face embedding feature. Following [5], the feature scale s is set to 64 and the angular margin m is set to 0.5. All experiments in this paper are implemented by MXNet [3]. We set the batch size for back-propagation as 512 and train models on 8 NVIDIA Tesla P40 (24GB) GPUs. We set momentum to 0.9 and weight decay to 5e \u2212 4. For the training of ArcFace on MS1MV0 and MS1MV3, the learning rate starts from 0.1 and is divided by 10 at the 100K, 160K, and 220K iteration steps. We finish the training process at 240K steps. For the training of the proposed sub-center ArcFace, we also employ the same learning rate schedule to train the first round of model (K=3). Then, we drop nondominant sub-centers (K = 3 \u2193 1) and high-confident noisy data (> 75 \u2022 ) by using the first round model through an off-line way. Finally, we retrain the model from scratch using the automatically cleaned data.\n\n\nAblation Study\n\nTo facilitate comparisons, we abbreviate different settings by the experiment number (E*) in the table and only focus on the TAR@FAR=1e-4 of IJB-C, which is more objective and less affected by the noise within the test data [38]. Real-world Noise. In Tab. 2, we conduct extensive experiments to investigate the proposed Sub-center ArcFace. We train ResNet-50 networks on different datasets (MS1MV0, MS1MV3 and Celeb500K) with different settings. From Tab. 2, we have the following observations: (a) ArcFace has an obvious performance drop (from E14 96.50% to E1 90.27%) when the training data is changed from the clean MS1MV3 to the noisy MS1MV0. By contrast, sub-center ArcFace is more robust (E2 93.72%) under massive noise. (b) Too many sub-centers (too large K) can obviously undermine the intra-class compactness and decrease the accuracy (from E2 93.72% to E5 67.94%). This observation indicates that robustness and strictness should be balanced during training, thus we select K=3 in this paper. (c) The nearest sub-center assignment by the max pooling is slightly better than the softmax pooling [22] (E2 93.72% vs. E3 93.55%). Thus, we choose the more efficient max pooling operator in the following experiments. (d) Dropping non-dominant sub-centers and high-confident noisy samples can achieve better performance than adding regularization [22] to enforce compactness between subcenters (E7 95.92% vs. E10 93.64%). Besides, The performance of our method is not very sensitive to the constant threshold (E6 95.91%, E7 95.92% and E8 95.74%), and we select 75 \u2022 as the threshold for dropping high-confident noisy samples in the following experiments. (e) Co-mining [33] and re-weighting methods [11,41] can also improve the robustness under massive noise, but sub-center ArcFace can do better through automatic clean and noisy data isolation during training (E7 ArcFace under synthetic open-set and close-set noise. We train ResNet-50 networks on MS1MV3 with different noise types and levels. To simulate the training data with controlled open-set noise, we randomly select 75% and 50% identities from MS1MV3 [6] and the face images of the rest identities are assigned with random labels of selected identities. To simulate the training data with controlled close-set noise, we use all iden- tities (\u223c 100K) from MS1MV3 [6] but randomly select 25% and 50% face images of each identity and assign random labels to these face images.\n\nFrom Tab. 3, we have the following observations: (a) Performance drops as the ratio of synthetic noise increases, especially for the close-set noise (E2 94.52% vs. E6 92.25% and E10 94.28% vs. E14 75.80%). In fact, close-set noise is also found to be more harmful than open-set noise in [30]. (b) Under the open-set noise, the proposed sub-center can effectively enhance the robustness of ArcFace (E3 95.57% vs. E2 94.52% and E7 94.50% vs. E6 92.25%). By dropping non-dominant sub-centers and high-confident noisy samples, the performance of sub-center arcface can even approach Arcface trained on the clean dataset (E4 95.89% vs. E1 96.00% and E8 95.50% vs. E5 95.65%). (c) Under the close-set noise, the performance of sub-center Arcface is worse than ArcFace (E11 93.57% vs. E10 94.28% and E15 72.04% vs. E14 75.80%), as the inter-class constraint of sub-center Arcface is more strict than ArcFace. By dropping non-dominant sub-centers and high-confident noisy samples, the performance of subcenter Arcface outperforms ArcFace (E12 95.13% vs. E10 94.28% and E16 78.12% vs. E14 75.80%) but still lags behind ArcFace trained on the clean dataset (E12 95.13% vs. E9 96.19% and E16 78.12% vs. E13 96.08%), which indicates the capacity of the network to drop noisy samples depends on its initial discriminative power. Sub-center ArcFace trained on 50% close-set noise is far from accurate (E15 72.04%) and the step of dropping noisy samples is also not accurate. Therefore, it is hard to catch up with ArcFace trained on the clean dataset. However, in the real-world data, close-set noise Table 4. Column 2-3: 1:1 verification TAR (@FAR=1e-4) on the IJB-B and IJB-C dataset. Column 4-5: Face identification and verification evaluation on MegaFace Challenge1 using Face-Scrub as the probe set. \"Id\" refers to the rank-1 face identification accuracy with 1M distractors, and \"Ver\" refers to the face verification TAR at 10 \u22126 FAR. Column 6-8:  is not dominant, much less than 50% (e.g. only a small part of celebrities frequently appear in others' album).\n\n\nBenchmark Results\n\nResults on IJB-B [35] and IJB-C [35]. We employ the face detection scores and the feature norms to re-weigh faces within templates [24,16]. In Tab. 4, we compare the TAR (@FAR=1e-4) of ArcFace and the proposed sub-center ArcFace trained on noisy data (e.g. MS1MV0 and Celeb500K). The performance of ArcFace significantly drops from 96.61% to 90.42% on the IJB-C dataset when the training data is changed from the manually cleaned data (MS1MV3) to the raw noisy data (MS1MV0). By contrast, the proposed sub-center ArcFace is robust to massive noise and can achieve similar results compared with ArcFace trained on the clean data (96.28% vs. 96.61%). When we apply sub-center ArcFace on large-scale training data (Celeb500K), we further improve the TAR (@FAR=1e-4) to 95.75% and 96.96% on IJB-B and IJB-C, respectively.\n\nResults on MegaFace [13]. We adopt the refined version of MegaFace [5] to give a fair evaluation. As shown in Tab. 4, the identification accuracy of ArcFace obviously drops from 98.40% to 96.52% when the training data is changed from MS1MV3 to MS1MV0, while the proposed sub-center ArcFace is more robust under massive noise within MS1MV0, achieving the identification accuracy of 98.16%. ArcFace trained on MS1MV3 only slightly outperforms our method trained on MS1MV0 under both verification and identification protocols. Finally, the sub-center ArcFace model trained on the large-scale Celeb500K dataset achieves state-of-the-art identification accuracy of 98.78% on the MegaFace dataset. Results on LFW [12], CFP-FP [27], and AgeDB-30 [18]. We follow the unrestricted with labelled outside data protocol to report the verification performance. As reported in Tab. 4, sub-center ArcFace trained on noisy MS1MV0 achieves comparable performance compared to ArcFace trained on clean MS1MV3. Moreover, our method trained on the noisy Celeb500K outperforms ArcFace [5], achieving the verification accuracy of 99.86%, 99.11%, 98.35% on LFW, CFP-FP and AgeDB-30, respectively. Results on FRVT. The Face Recognition Vendor Test (FRVT) is the most strict industrylevel face recognition test, and the participants need to submit the whole face recognition system (e.g. face detection, alignment and feature embedding) to the organiser. No test image has been released for hyper-parameter searching and the submission interval is no less than three months. Besides, the submitted face recognition system should complete face detection and face feature embedding within 1000ms on Intel Xeon CPU (E5-2630 v4 @ 2.20GHz processors) by using the single-thread inference. We build our face recognition system by RetinaFace (ResNet-50) [7] and sub-center ArcFace (ResNet-100), and accelerate the inference by the openVINO toolkit. In Tab. 5, we show the top-performing 1:1 algorithms measured on false non-match rate (FNMR) across several different tracks. As we can see from the results, the proposed sub-center ArcFace trained on the Celeb500K dataset achieves state-of-the-art performance on the wild track (0.0303, rank 3rd). Considering several hundred of industry submissions to FRVT, the overall performance of our single model is very impressive.\n\n\nConclusion\n\nIn this paper, we have proposed sub-center ArcFace which first enforces sub-classes by nearest sub-center selection and then only keeps the dominant sub-center to achieve intra-class compactness. As we relax the intra-class compactness from beginning, the proposed sub-center ArcFace is robust under massive label noise and can easily train face recognition models from raw downloaded data. Extensive experimental results show that our method consistently outperforms ArcFace on real-world noisy datasets and achieve comparable performance compared to using manually refined data. Acknowledgements. Jiankang Deng acknowledges the Imperial President's PhD Scholarship. Tongliang Liu acknowledges support from the Australian Research Council Project DE-190101473. Stefanos Zafeiriou acknowledges support from the Google Faculty Fellowship, EPSRC DEFORM (EP/S010203/1) and FACER2VM (EP/N007743/1). We are thankful to Nvidia for the GPU donations.\n\nFig. 1 .\n1(a) Difference between ArcFace and the proposed sub-center ArcFace.\n\nFig. 2 .\n2Training the deep face recognition model by minimizing the proposed sub-center ArcFace loss.\n\nFig. 3 .\n3(a) Angle distribution of samples to their corresponding centers predicted by the pretrained ArcFace model[5]. Noise exists in the CASIA dataset[40,30]. (b) Angle distribution of samples from the dominant and non-dominant sub-classes. Clean data are automatically isolated by sub-center ArcFace (K=10).\n\nFig. 4 .\n4Data distribution of ArcFace (K=1) and the proposed sub-center ArcFace (K=3) before and after dropping non-dominant sub-centers. MS1MV0[9] is used here. K = 3 \u2193 1 denotes sub-center ArcFace with non-dominant sub-centers dropping.\n\nTable 2 .\n2Ablation experiments of different settings on MS1MV0, MS1MV3 and Celeb500K. The 1:1 verification accuracy (TAR@FAR) is reported on the IJB-B and IJB-C datasets. ResNet-50 is used for training.Settings \nIJB-B \nIJB-C \n\n1e\u22126 1e\u22125 1e-4 1e\u22123 1e\u22122 1e\u22126 1e\u22125 1e-4 1e\u22123 1e\u22122 \n\n(1) MS1MV0,K=1 \n34.14 74.74 87.87 93.27 96.40 67.08 81.11 90.27 94.59 97.08 \n(2) MS1MV0,K=3 \n40.89 85.62 91.70 94.88 96.93 86.18 90.59 93.72 95.98 97.60 \n(3) MS1MV0,K=3, softmax pooling [22] 38.4 85.49 91.53 94.76 96.83 85.43 90.40 93.55 95.87 97.36 \n(4) MS1MV0,K=5 \n39.24 85.48 91.47 94.68 96.96 85.49 90.38 93.62 95.88 97.59 \n(5) MS1MV0,K=10 \n19.81 49.03 63.84 76.09 87.73 45.98 55.74 67.94 79.44 89.29 \n(6) MS1MV0, K = 3 \u2193 1, drop > 70 \u2022 47.61 90.60 94.44 96.44 97.71 90.40 94.05 95.91 97.42 98.42 \n(7) MS1MV0, K = 3 \u2193 1, drop > 75 \u2022 46.78 89.40 94.56 96.49 97.83 89.17 94.03 95.92 97.40 98.41 \n(8) MS1MV0, K = 3 \u2193 1, drop > 80 \u2022 38.05 88.26 94.04 96.19 97.64 86.16 93.09 95.74 97.19 98.33 \n(9) MS1MV0, K = 3 \u2193 1, drop > 85 \u2022 42.89 87.06 93.33 96.05 97.59 81.53 92.01 95.10 97.01 98.24 \n(10) MS1MV0, K=3, regularizer [22] \n39.92 85.51 91.53 94.77 96.92 85.44 90.41 93.64 95.85 97.40 \n(11) MS1MV0,Co-mining [33] \n40.96 85.57 91.80 94.99 97.10 86.31 90.71 93.82 95.95 97.63 \n(12) MS1MV0,NT [11] \n40.84 85.56 91.57 94.79 96.83 86.14 90.48 93.65 95.86 97.54 \n(13) MS1MV0,NR [41] \n40.86 85.53 91.58 94.77 96.80 86.07 90.41 93.60 95.88 97.44 \n(14) MS1MV3, K=1 \n35.86 91.52 95.13 96.61 97.65 90.16 94.75 96.50 97.61 98.40 \n(15) MS1MV3, K=3 \n40.16 91.30 94.84 96.66 97.74 90.64 94.68 96.35 97.66 98.48 \n(16) MS1MV3, K = 3 \u2193 1 \n40.18 91.32 94.87 96.70 97.81 90.67 94.74 96.43 97.66 98.47 \n(17) Celeb500K, K=1 \n42.42 88.18 90.96 92.19 93.00 88.18 90.87 92.15 95.47 97.64 \n(18) Celeb500K, K=3 \n43.84 90.91 93.76 95.12 96.00 90.92 93.66 94.90 96.21 98.02 \n(19) Celeb500K, K = 3 \u2193 1 \n44.64 92.71 95.65 96.94 97.89 92.73 95.52 96.91 97.87 98.42 \n\n\n\nTable 3 .\n3Ablation experiments of different settings under synthetic open-set and close-set noise. 50%CleanIM+50%NoisyIM,K = 3 \u2193 1 22.19 68.11 85.86 88.13 95.08 44.34 69.25 78.12 90.51 96.16The 1:1 verification accuracy (TAR@FAR) is reported on the IJB-B and IJB-C datasets. ResNet-\n50 is used for training. \n\nSettings \nIJB-B \nIJB-C \n\n1e\u22126 1e\u22125 1e-4 1e\u22123 1e\u22122 1e\u22126 1e\u22125 1e-4 1e\u22123 1e\u22122 \n\nSynthetic Open-set Noise \n(1) 75%CleanID,K=1 \n37.49 90.02 94.48 96.48 97.72 90.10 94.18 96.00 97.45 98.38 \n(2) 75%CleanID+25%NoisyID,K=1 \n37.80 86.68 92.96 94.72 95.80 86.19 92.03 94.52 95.89 97.29 \n(3) 75%CleanID+25%NoisyID,K=3 \n38.31 87.87 94.17 95.83 97.15 87.23 93.01 95.57 96.95 97.75 \n(4) 75%CleanID+25%NoisyID,K = 3 \u2193 1 38.36 88.14 94.20 96.15 97.94 87.51 93.27 95.89 97.29 98.43 \n(5) 50%CleanID,K=1 \n34.43 89.36 93.97 96.26 97.63 88.35 93.49 95.65 97.28 98.35 \n(6) 50%CleanID+50%NoisyID,K=1 \n35.96 81.45 90.77 92.69 94.56 80.97 88.49 92.25 93.84 95.10 \n(7) 50%CleanID+50%NoisyID,K=3 \n34.15 85.13 92.62 94.98 96.77 84.43 91.00 94.50 95.79 97.33 \n(8) 50%CleanID+50%NoisyID,K = 3 \u2193 1 34.55 86.43 93.85 96.13 97.37 85.22 91.82 95.50 96.73 98.16 \nSynthetic Close-set Noise \n(9) 75%CleanIM,K=1 \n38.44 89.41 94.76 96.42 97.71 89.31 94.19 96.19 97.39 98.43 \n(10) 75%CleanIM+25%NoisyIM,K=1 \n36.16 83.46 92.29 94.85 95.61 82.20 91.24 94.28 95.58 97.58 \n(11) 75%CleanIM+25%NoisyIM,K=3 \n36.09 83.16 91.45 94.33 95.23 81.28 90.02 93.57 94.96 96.32 \n(12) 75%CleanIM+25%NoisyIM,K = 3 \u2193 1 37.79 85.50 94.03 95.53 97.42 84.09 93.17 95.13 96.85 97.61 \n(13) 50%CleanIM,K=1 \n36.85 90.50 94.59 96.49 97.65 90.46 94.32 96.08 97.44 98.33 \n(14) 50%CleanIM+50%NoisyIM,K=1 \n17.54 43.10 71.76 82.08 93.38 28.40 55.46 75.80 88.22 94.68 \n(15) 50%CleanIM+50%NoisyIM,K=3 \n17.47 41.63 66.42 78.70 91.37 26.03 54.23 72.04 86.36 94.19 \n(16) \n\n\nThe 1:1 verification accuracy on the LFW, CFP-FP and AgeDB-30 datasets. ResNet-100 is used for training. Celeb500K, K = 3 \u2193 1 95.75 96.96 98.78 98.69 99.86 99.11 98.35Settings \nIJB \nMegaFace Quick Verification Datasets \n\nIJB-B IJB-C Id Ver LFW CFP-FP AgeDB-30 \nMS1MV0, K=1 \n87.91 90.42 96.52 96.75 99.75 97.17 \n97.26 \nMS1MV0, K = 3 \u2193 1 94.94 96.28 98.16 98.36 99.80 98.80 \n98.31 \nMS1MV3, K=1 [5,6] 95.25 96.61 98.40 98.51 99.83 98.80 \n98.45 \n\n\nTable 5 .\n5FRVT 1:1 verification results. Sub-center ArcFace (K = 3 \u2193 1) employs ResNet-100 and is trained on the Celeb500K dataset. FNMR is the proportion of mated comparisons below a threshold set to achieve the false match rate (FMR) specified. FMR is the proportion of impostor comparisons at or above that threshold. @FMR \u2264 1e-5 @FMR \u2264 1e-6 @FMR \u2264 1e-4 @FMR \u2264 1e-5 @FMR \u2264 1e-5 @FMR \u2264 1e-6Rank \nSubmissions \nWILD \nVISA \nVISA \nMUGSHOT \nMUGSHOT VISABORDER \nFNMR \nFNMR \nFNMR \nFNMR \nFNMR \nFNMR \nDT=14 YRS \n1 \ndeepglint-002 \n0.0301 \n0.0027 \n0.0004 \n0.0032 \n0.0041 \n0.0043 \n2 everai-paravision-003 \n0.0302 \n0.0050 \n0.0011 \n0.0036 \n0.0053 \n0.0092 \n3 Sub-center ArcFace \n0.0303 \n0.0081 \n0.0027 \n0.0055 \n0.0087 \n0.0083 \n4 \ndahua-004 \n0.0304 \n0.0058 \n0.0019 \n0.0036 \n0.0051 \n0.0051 \n5 \nxforwardai-000 \n0.0305 \n0.0072 \n0.0018 \n0.0036 \n0.0051 \n0.0074 \n6 \nvisionlabs-008 \n0.0308 \n0.0036 \n0.0007 \n0.0031 \n0.0044 \n0.0045 \n7 \ndidiglobalface-001 \n0.0308 \n0.0092 \n0.0016 \n0.0030 \n0.0048 \n0.0088 \n8 \nvocord-008 \n0.0310 \n0.0038 \n0.0008 \n0.0042 \n0.0054 \n0.0045 \n9 \nparavision-004 \n0.0311 \n0.0046 \n0.0012 \n0.0030 \n0.0041 \n0.0091 \n10 \nntechlab-008 \n0.0312 \n0.0061 \n0.0011 \n0.0056 \n0.0106 \n0.0042 \n11 \ntevian-005 \n0.0325 \n0.0062 \n0.0020 \n0.0057 \n0.0081 \n0.0070 \n12 \nsensetime-003 \n0.0355 \n0.0027 \n0.0005 \n0.0027 \n0.0033 \n0.0051 \n13 \nyitu-003 \n0.0360 \n0.0026 \n0.0003 \n0.0066 \n0.0083 \n0.0064 \n\n\nGenerally, there are two types of label noise in face recognition[30,11,41,33]: one is open-set label noise, i.e., faces whose true labels are out of the training label set but are wrongly labeled to be within the set; and the other one is close-set label noise, i.e., faces whose true labels are in the training label set but are wrongly labeled.\nhttps://github.com/deepinsight/insightface/tree/master/recognition\n\nCeleb-500k: A large training dataset for face recognition. J Cao, Y Li, Z Zhang, ICIPCao, J., Li, Y., Zhang, Z.: Celeb-500k: A large training dataset for face recognition. In: ICIP (2018)\n\nVggface2: A dataset for recognising faces across pose and age. Q Cao, L Shen, W Xie, O M Parkhi, A Zisserman, FGCao, Q., Shen, L., Xie, W., Parkhi, O.M., Zisserman, A.: Vggface2: A dataset for recognising faces across pose and age. In: FG (2018)\n\nMxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. T Chen, M Li, Y Li, M Lin, N Wang, M Wang, T Xiao, B Xu, C Zhang, Z Zhang, arXiv:1512.01274Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang, C., Zhang, Z.: Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. arXiv:1512.01274 (2015)\n\nLearning with bounded instance-and labeldependent label noise. J Cheng, T Liu, K Ramamohanarao, D Tao, Cheng, J., Liu, T., Ramamohanarao, K., Tao, D.: Learning with bounded instance-and label- dependent label noise. ICML (2020)\n\nArcface: Additive angular margin loss for deep face recognition. J Deng, J Guo, N Xue, S Zafeiriou, CVPRDeng, J., Guo, J., Xue, N., Zafeiriou, S.: Arcface: Additive angular margin loss for deep face recognition. In: CVPR (2019)\n\nLightweight face recognition challenge. J Deng, J Guo, D Zhang, Y Deng, X Lu, S Shi, ICCV Workshops. Deng, J., Guo, J., Zhang, D., Deng, Y., Lu, X., Shi, S.: Lightweight face recognition chal- lenge. In: ICCV Workshops (2019)\n\nJ Deng, J Guo, Y Zhou, J Yu, I Kotsia, S Zafeiriou, arXiv:1905.00641Retinaface: Single-stage dense face localisation in the wild. Deng, J., Guo, J., Zhou, Y., Yu, J., Kotsia, I., Zafeiriou, S.: Retinaface: Single-stage dense face localisation in the wild. arXiv:1905.00641 (2019)\n\nMarginal loss for deep face recognition. J Deng, Y Zhou, S Zafeiriou, CVPR Workshops. Deng, J., Zhou, Y., Zafeiriou, S.: Marginal loss for deep face recognition. In: CVPR Work- shops (2017)\n\nMs-celeb-1m: A dataset and benchmark for large-scale face recognition. Y Guo, L Zhang, Y Hu, X He, J Gao, ECCVGuo, Y., Zhang, L., Hu, Y., He, X., Gao, J.: Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In: ECCV (2016)\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPRHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)\n\nNoise-tolerant paradigm for training face recognition cnns. W Hu, Y Huang, F Zhang, R Li, CVPRHu, W., Huang, Y., Zhang, F., Li, R.: Noise-tolerant paradigm for training face recognition cnns. In: CVPR (2019)\n\nLabeled faces in the wild: A database for studying face recognition in unconstrained environments. G B Huang, M Ramesh, T Berg, E Learned-Miller, Tech. rep. Huang, G.B., Ramesh, M., Berg, T., Learned-Miller, E.: Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Tech. rep. (2007)\n\nThe megaface benchmark: 1 million faces for recognition at scale. I Kemelmacher-Shlizerman, S M Seitz, D Miller, E Brossard, CVPRKemelmacher-Shlizerman, I., Seitz, S.M., Miller, D., Brossard, E.: The megaface bench- mark: 1 million faces for recognition at scale. In: CVPR (2016)\n\nClassification with noisy labels by importance reweighting. T Liu, D Tao, TPAMILiu, T., Tao, D.: Classification with noisy labels by importance reweighting. TPAMI (2015)\n\nSphereface: Deep hypersphere embedding for face recognition. W Liu, Y Wen, Z Yu, M Li, B Raj, L Song, CVPRLiu, W., Wen, Y., Yu, Z., Li, M., Raj, B., Song, L.: Sphereface: Deep hypersphere embedding for face recognition. In: CVPR (2017)\n\nFace-specific data augmentation for unconstrained face recognition. I Masi, A T Tran, T Hassner, G Sahin, G Medioni, IJCVMasi, I., Tran, A.T., Hassner, T., Sahin, G., Medioni, G.: Face-specific data augmentation for unconstrained face recognition. IJCV (2019)\n\nIarpa janus benchmark-c: Face dataset and protocol. B Maze, J Adams, J A Duncan, N Kalka, T Miller, C Otto, A K Jain, W T Niggel, J Anderson, J Cheney, ICBMaze, B., Adams, J., Duncan, J.A., Kalka, N., Miller, T., Otto, C., Jain, A.K., Niggel, W.T., Anderson, J., Cheney, J.: Iarpa janus benchmark-c: Face dataset and protocol. In: ICB (2018)\n\nAgedb: The first manually collected in-the-wild age database. S Moschoglou, A Papaioannou, C Sagonas, J Deng, I Kotsia, S Zafeiriou, CVPR Workshops. Moschoglou, S., Papaioannou, A., Sagonas, C., Deng, J., Kotsia, I., Zafeiriou, S.: Agedb: The first manually collected in-the-wild age database. In: CVPR Workshops (2017)\n\nNo fuss distance metric learning using proxies. Y Movshovitz-Attias, A Toshev, T K Leung, S Ioffe, S Singh, Movshovitz-Attias, Y., Toshev, A., Leung, T.K., Ioffe, S., Singh, S.: No fuss distance metric learning using proxies. In: ICCV (2017)\n\nR M\u00fcller, S Kornblith, G Hinton, arXiv:2002.03936Subclass distillation. M\u00fcller, R., Kornblith, S., Hinton, G.: Subclass distillation. arXiv:2002.03936 (2020)\n\nDeep metric learning via lifted structured feature embedding. Oh Song, H Xiang, Y Jegelka, S Savarese, S , CVPROh Song, H., Xiang, Y., Jegelka, S., Savarese, S.: Deep metric learning via lifted structured feature embedding. In: CVPR (2016)\n\nSofttriple loss: Deep metric learning without triplet sampling. Q Qian, L Shang, B Sun, J Hu, H Li, R Jin, ICCVQian, Q., Shang, L., Sun, B., Hu, J., Li, H., Jin, R.: Softtriple loss: Deep metric learning without triplet sampling. In: ICCV (2019)\n\nLarge-scale distance metric learning with uncertainty. Q Qian, J Tang, H Li, S Zhu, R Jin, CVPRQian, Q., Tang, J., Li, H., Zhu, S., Jin, R.: Large-scale distance metric learning with uncer- tainty. In: CVPR (2018)\n\nR Ranjan, A Bansal, H Xu, S Sankaranarayanan, J C Chen, C D Castillo, R Chellappa, arXiv:1804.01159Crystal loss and quality pooling for unconstrained face verification and recognition. Ranjan, R., Bansal, A., Xu, H., Sankaranarayanan, S., Chen, J.C., Castillo, C.D., Chellappa, R.: Crystal loss and quality pooling for unconstrained face verification and recognition. arXiv:1804.01159 (2018)\n\nMetric learning with adaptive density discrimination. O Rippel, M Paluri, P Dollar, L Bourdev, ICLRRippel, O., Paluri, M., Dollar, P., Bourdev, L.: Metric learning with adaptive density discrim- ination. In: ICLR (2016)\n\nFacenet: A unified embedding for face recognition and clustering. F Schroff, D Kalenichenko, J Philbin, CVPRSchroff, F., Kalenichenko, D., Philbin, J.: Facenet: A unified embedding for face recognition and clustering. In: CVPR (2015)\n\nFrontal to profile face verification in the wild. S Sengupta, J C Chen, C Castillo, V M Patel, R Chellappa, D W Jacobs, WACVSengupta, S., Chen, J.C., Castillo, C., Patel, V.M., Chellappa, R., Jacobs, D.W.: Frontal to profile face verification in the wild. In: WACV (2016)\n\nImproved deep metric learning with multi-class n-pair loss objective. K Sohn, NeurIPSSohn, K.: Improved deep metric learning with multi-class n-pair loss objective. In: NeurIPS (2016)\n\nSeparability-oriented subclass discriminant analysis. H Wan, H Wang, G Guo, X Wei, TPAMIWan, H., Wang, H., Guo, G., Wei, X.: Separability-oriented subclass discriminant analysis. TPAMI (2017)\n\nThe devil of face recognition is in the noise. F Wang, L Chen, C Li, S Huang, Y Chen, C Qian, C C Loy, ECCVWang, F., Chen, L., Li, C., Huang, S., Chen, Y., Qian, C., Loy, C.C.: The devil of face recognition is in the noise. In: ECCV (2018)\n\nAdditive margin softmax for face verification. F Wang, J Cheng, W Liu, H Liu, SPL. Wang, F., Cheng, J., Liu, W., Liu, H.: Additive margin softmax for face verification. SPL (2018)\n\nCosface: Large margin cosine loss for deep face recognition. H Wang, Y Wang, Z Zhou, X Ji, D Gong, J Zhou, Z Li, W Liu, CVPRWang, H., Wang, Y., Zhou, Z., Ji, X., Gong, D., Zhou, J., Li, Z., Liu, W.: Cosface: Large margin cosine loss for deep face recognition. In: CVPR (2018)\n\nCo-mining: Deep face recognition with noisy labels. X Wang, S Wang, J Wang, H Shi, T Mei, ICCVWang, X., Wang, S., Wang, J., Shi, H., Mei, T.: Co-mining: Deep face recognition with noisy labels. In: ICCV (2019)\n\nA discriminative feature learning approach for deep face recognition. Y Wen, K Zhang, Z Li, Y Qiao, ECCVWen, Y., Zhang, K., Li, Z., Qiao, Y.: A discriminative feature learning approach for deep face recognition. In: ECCV (2016)\n\nC Whitelam, E Taborsky, A Blanton, B Maze, J C Adams, T Miller, N D Kalka, A K Jain, J A Duncan, K Allen, Iarpa janus benchmark-b face dataset. In: CVPR Workshops. Whitelam, C., Taborsky, E., Blanton, A., Maze, B., Adams, J.C., Miller, T., Kalka, N.D., Jain, A.K., Duncan, J.A., Allen, K.: Iarpa janus benchmark-b face dataset. In: CVPR Workshops (2017)\n\nSampling matters in deep embedding learning. C Y Wu, R Manmatha, A J Smola, P Krahenbuhl, Wu, C.Y., Manmatha, R., Smola, A.J., Krahenbuhl, P.: Sampling matters in deep embedding learning. In: ICCV (2017)\n\nA light cnn for deep face representation with noisy labels. X Wu, R He, Z Sun, T Tan, TIFS. Wu, X., He, R., Sun, Z., Tan, T.: A light cnn for deep face representation with noisy labels. TIFS (2018)\n\nComparator networks. W Xie, S Li, A Zisserman, ECCVXie, W., Li, S., Zisserman, A.: Comparator networks. In: ECCV (2018)\n\nFan-face: a simple orthogonal improvement to deep face recognition. J Yang, A Bulat, G Tzimiropoulos, AAAIYang, J., Bulat, A., Tzimiropoulos, G.: Fan-face: a simple orthogonal improvement to deep face recognition. In: AAAI (2020)\n\nD Yi, Z Lei, S Liao, S Z Li, arXiv:1411.7923Learning face representation from scratch. Yi, D., Lei, Z., Liao, S., Li, S.Z.: Learning face representation from scratch. arXiv:1411.7923 (2014)\n\nUnequal-training for deep face recognition with long-tailed noisy data. Y Zhong, W Deng, M Wang, J Hu, J Peng, X Tao, Y Huang, CVPRZhong, Y., Deng, W., Wang, M., Hu, J., Peng, J., Tao, X., Huang, Y.: Unequal-training for deep face recognition with long-tailed noisy data. In: CVPR (2019)\n\nOptimal subclass discovery for discriminant analysis. M Zhu, A M Mart\u00ednez, CVPR Workshops. Zhu, M., Mart\u00ednez, A.M.: Optimal subclass discovery for discriminant analysis. In: CVPR Workshops (2004)\n\nM Zhu, A M Martinez, Subclass discriminant analysis. TPAMI. Zhu, M., Martinez, A.M.: Subclass discriminant analysis. TPAMI (2006)\n", "annotations": {"author": "[{\"start\":\"80\",\"end\":\"113\"},{\"start\":\"114\",\"end\":\"153\"},{\"start\":\"154\",\"end\":\"219\"},{\"start\":\"220\",\"end\":\"289\"},{\"start\":\"290\",\"end\":\"355\"}]", "publisher": null, "author_last_name": "[{\"start\":\"89\",\"end\":\"93\"},{\"start\":\"118\",\"end\":\"121\"},{\"start\":\"164\",\"end\":\"167\"},{\"start\":\"229\",\"end\":\"233\"},{\"start\":\"299\",\"end\":\"308\"}]", "author_first_name": "[{\"start\":\"80\",\"end\":\"88\"},{\"start\":\"114\",\"end\":\"117\"},{\"start\":\"154\",\"end\":\"163\"},{\"start\":\"220\",\"end\":\"228\"},{\"start\":\"290\",\"end\":\"298\"}]", "author_affiliation": "[{\"start\":\"95\",\"end\":\"112\"},{\"start\":\"140\",\"end\":\"152\"},{\"start\":\"197\",\"end\":\"218\"},{\"start\":\"264\",\"end\":\"288\"},{\"start\":\"337\",\"end\":\"354\"}]", "title": "[{\"start\":\"1\",\"end\":\"77\"},{\"start\":\"356\",\"end\":\"432\"}]", "venue": null, "abstract": "[{\"start\":\"490\",\"end\":\"1904\"}]", "bib_ref": "[{\"start\":\"2017\",\"end\":\"2021\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"2021\",\"end\":\"2024\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"2024\",\"end\":\"2027\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"2027\",\"end\":\"2029\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2221\",\"end\":\"2225\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"2256\",\"end\":\"2260\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"2260\",\"end\":\"2263\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"2263\",\"end\":\"2266\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"2266\",\"end\":\"2268\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2619\",\"end\":\"2622\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2826\",\"end\":\"2830\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"3098\",\"end\":\"3101\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"3101\",\"end\":\"3104\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"3104\",\"end\":\"3107\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"3107\",\"end\":\"3110\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"3110\",\"end\":\"3112\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"3112\",\"end\":\"3115\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"3178\",\"end\":\"3182\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"3182\",\"end\":\"3184\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"3237\",\"end\":\"3241\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"3655\",\"end\":\"3659\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"3734\",\"end\":\"3738\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"3738\",\"end\":\"3740\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"3740\",\"end\":\"3743\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"3743\",\"end\":\"3746\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"3746\",\"end\":\"3749\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"3803\",\"end\":\"3807\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"3847\",\"end\":\"3851\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"4069\",\"end\":\"4073\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"4241\",\"end\":\"4244\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"4356\",\"end\":\"4359\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"5619\",\"end\":\"5622\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"5936\",\"end\":\"5940\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"5991\",\"end\":\"5995\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"6880\",\"end\":\"6883\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"7084\",\"end\":\"7088\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"7628\",\"end\":\"7632\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"7632\",\"end\":\"7635\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"7693\",\"end\":\"7697\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"7697\",\"end\":\"7700\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"7700\",\"end\":\"7703\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"7774\",\"end\":\"7778\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"7778\",\"end\":\"7781\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"8020\",\"end\":\"8024\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"8024\",\"end\":\"8026\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"8026\",\"end\":\"8029\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"8029\",\"end\":\"8032\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"8032\",\"end\":\"8034\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"8226\",\"end\":\"8230\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"8239\",\"end\":\"8243\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"8243\",\"end\":\"8246\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"8697\",\"end\":\"8701\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"8701\",\"end\":\"8703\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"8703\",\"end\":\"8705\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"8705\",\"end\":\"8707\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"8850\",\"end\":\"8854\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"8943\",\"end\":\"8947\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"8947\",\"end\":\"8950\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"8950\",\"end\":\"8953\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"8953\",\"end\":\"8956\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"9005\",\"end\":\"9009\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"9042\",\"end\":\"9046\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"9307\",\"end\":\"9311\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"9400\",\"end\":\"9404\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"9649\",\"end\":\"9653\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"9917\",\"end\":\"9921\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"9952\",\"end\":\"9956\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"10100\",\"end\":\"10104\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"10554\",\"end\":\"10558\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"10596\",\"end\":\"10599\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"10614\",\"end\":\"10617\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"10737\",\"end\":\"10741\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"10741\",\"end\":\"10744\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"10825\",\"end\":\"10829\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"10829\",\"end\":\"10832\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"11094\",\"end\":\"11098\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"11098\",\"end\":\"11101\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"11278\",\"end\":\"11282\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"11637\",\"end\":\"11641\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"11641\",\"end\":\"11644\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"11644\",\"end\":\"11647\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"11767\",\"end\":\"11771\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"11913\",\"end\":\"11917\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"12102\",\"end\":\"12106\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"12295\",\"end\":\"12299\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"12299\",\"end\":\"12302\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"12302\",\"end\":\"12305\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"12340\",\"end\":\"12344\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"12569\",\"end\":\"12572\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"13550\",\"end\":\"13554\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"13554\",\"end\":\"13557\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"13601\",\"end\":\"13604\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"13725\",\"end\":\"13728\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"13728\",\"end\":\"13731\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"13949\",\"end\":\"13953\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"14979\",\"end\":\"14982\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"17412\",\"end\":\"17416\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"17831\",\"end\":\"17835\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"17874\",\"end\":\"17877\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"18262\",\"end\":\"18266\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"19118\",\"end\":\"19121\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"19238\",\"end\":\"19242\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"19247\",\"end\":\"19250\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"21489\",\"end\":\"21493\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"22745\",\"end\":\"22749\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"22749\",\"end\":\"22752\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"23019\",\"end\":\"23023\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"23023\",\"end\":\"23026\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"23215\",\"end\":\"23219\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"23227\",\"end\":\"23231\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"23632\",\"end\":\"23635\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"23676\",\"end\":\"23679\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"23695\",\"end\":\"23698\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"23778\",\"end\":\"23782\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"23843\",\"end\":\"23846\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"23858\",\"end\":\"23861\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"23885\",\"end\":\"23888\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"23923\",\"end\":\"23926\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"24039\",\"end\":\"24043\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"24051\",\"end\":\"24055\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"24066\",\"end\":\"24070\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"24202\",\"end\":\"24206\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"24215\",\"end\":\"24219\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"24234\",\"end\":\"24238\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"24308\",\"end\":\"24311\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"24418\",\"end\":\"24421\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"24458\",\"end\":\"24462\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"24462\",\"end\":\"24464\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"24516\",\"end\":\"24519\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"24649\",\"end\":\"24652\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"25613\",\"end\":\"25617\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"26493\",\"end\":\"26497\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"26740\",\"end\":\"26744\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"27062\",\"end\":\"27066\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"27092\",\"end\":\"27096\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"27096\",\"end\":\"27099\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"27506\",\"end\":\"27509\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"27717\",\"end\":\"27720\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"28117\",\"end\":\"28121\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"29920\",\"end\":\"29924\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"29935\",\"end\":\"29939\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"30034\",\"end\":\"30038\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"30038\",\"end\":\"30041\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"30742\",\"end\":\"30746\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"30789\",\"end\":\"30792\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"31429\",\"end\":\"31433\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"31442\",\"end\":\"31446\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"31461\",\"end\":\"31465\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"31785\",\"end\":\"31788\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"32543\",\"end\":\"32546\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"33824\",\"end\":\"33836\"},{\"start\":\"34320\",\"end\":\"34323\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"34358\",\"end\":\"34362\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"34362\",\"end\":\"34365\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"34663\",\"end\":\"34666\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"40365\",\"end\":\"40369\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"40369\",\"end\":\"40372\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"40372\",\"end\":\"40375\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"40375\",\"end\":\"40378\",\"attributes\":{\"ref_id\":\"b32\"}}]", "figure": "[{\"start\":\"34020\",\"end\":\"34098\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"34099\",\"end\":\"34202\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"34203\",\"end\":\"34516\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"34517\",\"end\":\"34757\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"34758\",\"end\":\"36675\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"36676\",\"end\":\"38480\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}},{\"start\":\"38481\",\"end\":\"38925\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}},{\"start\":\"38926\",\"end\":\"40299\",\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1920\",\"end\":\"3009\"},{\"start\":\"3011\",\"end\":\"3602\"},{\"start\":\"3604\",\"end\":\"4191\"},{\"start\":\"4193\",\"end\":\"5655\"},{\"start\":\"5657\",\"end\":\"6264\"},{\"start\":\"6266\",\"end\":\"6317\"},{\"start\":\"6319\",\"end\":\"7009\"},{\"start\":\"7026\",\"end\":\"7989\"},{\"start\":\"7991\",\"end\":\"9030\"},{\"start\":\"9032\",\"end\":\"11728\"},{\"start\":\"11730\",\"end\":\"12525\"},{\"start\":\"12561\",\"end\":\"12641\"},{\"start\":\"12722\",\"end\":\"13558\"},{\"start\":\"13581\",\"end\":\"15402\"},{\"start\":\"15418\",\"end\":\"15547\"},{\"start\":\"15825\",\"end\":\"17276\"},{\"start\":\"17278\",\"end\":\"18886\"},{\"start\":\"18888\",\"end\":\"20999\"},{\"start\":\"21001\",\"end\":\"21372\"},{\"start\":\"21428\",\"end\":\"21519\"},{\"start\":\"21521\",\"end\":\"21604\"},{\"start\":\"21606\",\"end\":\"22932\"},{\"start\":\"22934\",\"end\":\"23295\"},{\"start\":\"23297\",\"end\":\"23512\"},{\"start\":\"23552\",\"end\":\"25370\"},{\"start\":\"25389\",\"end\":\"27828\"},{\"start\":\"27830\",\"end\":\"29881\"},{\"start\":\"29903\",\"end\":\"30720\"},{\"start\":\"30722\",\"end\":\"33061\"},{\"start\":\"33076\",\"end\":\"34019\"}]", "formula": "[{\"start\":\"12642\",\"end\":\"12721\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"15548\",\"end\":\"15702\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"15702\",\"end\":\"15729\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"15729\",\"end\":\"15824\",\"attributes\":{\"id\":\"formula_3\"}}]", "table_ref": "[{\"start\":\"14984\",\"end\":\"14991\"},{\"start\":\"29417\",\"end\":\"29424\"}]", "section_header": "[{\"start\":\"1906\",\"end\":\"1918\",\"attributes\":{\"n\":\"1\"}},{\"start\":\"7012\",\"end\":\"7024\",\"attributes\":{\"n\":\"2\"}},{\"start\":\"12528\",\"end\":\"12549\",\"attributes\":{\"n\":\"3\"}},{\"start\":\"12552\",\"end\":\"12559\",\"attributes\":{\"n\":\"3.1\"}},{\"start\":\"13561\",\"end\":\"13579\",\"attributes\":{\"n\":\"3.2\"}},{\"start\":\"15405\",\"end\":\"15416\"},{\"start\":\"21375\",\"end\":\"21426\",\"attributes\":{\"n\":\"3.3\"}},{\"start\":\"23515\",\"end\":\"23526\",\"attributes\":{\"n\":\"4\"}},{\"start\":\"23529\",\"end\":\"23550\",\"attributes\":{\"n\":\"4.1\"}},{\"start\":\"25373\",\"end\":\"25387\",\"attributes\":{\"n\":\"4.2\"}},{\"start\":\"29884\",\"end\":\"29901\",\"attributes\":{\"n\":\"4.3\"}},{\"start\":\"33064\",\"end\":\"33074\",\"attributes\":{\"n\":\"5\"}},{\"start\":\"34021\",\"end\":\"34029\"},{\"start\":\"34100\",\"end\":\"34108\"},{\"start\":\"34204\",\"end\":\"34212\"},{\"start\":\"34518\",\"end\":\"34526\"},{\"start\":\"34759\",\"end\":\"34768\"},{\"start\":\"36677\",\"end\":\"36686\"},{\"start\":\"38927\",\"end\":\"38936\"}]", "table": "[{\"start\":\"34962\",\"end\":\"36675\"},{\"start\":\"36868\",\"end\":\"38480\"},{\"start\":\"38650\",\"end\":\"38925\"},{\"start\":\"39320\",\"end\":\"40299\"}]", "figure_caption": "[{\"start\":\"34031\",\"end\":\"34098\"},{\"start\":\"34110\",\"end\":\"34202\"},{\"start\":\"34214\",\"end\":\"34516\"},{\"start\":\"34528\",\"end\":\"34757\"},{\"start\":\"34770\",\"end\":\"34962\"},{\"start\":\"36688\",\"end\":\"36868\"},{\"start\":\"38483\",\"end\":\"38650\"},{\"start\":\"38938\",\"end\":\"39320\"}]", "figure_ref": "[{\"start\":\"4205\",\"end\":\"4214\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"4969\",\"end\":\"4978\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"14183\",\"end\":\"14189\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"17320\",\"end\":\"17329\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"17754\",\"end\":\"17763\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"17768\",\"end\":\"17777\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"18030\",\"end\":\"18039\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"18591\",\"end\":\"18600\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"19104\",\"end\":\"19113\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"19450\",\"end\":\"19459\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"19464\",\"end\":\"19473\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"19945\",\"end\":\"19954\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"20154\",\"end\":\"20163\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"20168\",\"end\":\"20177\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"20529\",\"end\":\"20538\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"20673\",\"end\":\"20682\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"20784\",\"end\":\"20793\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"20798\",\"end\":\"20807\",\"attributes\":{\"ref_id\":\"fig_5\"}}]", "bib_author_first_name": "[{\"start\":\"40775\",\"end\":\"40776\"},{\"start\":\"40782\",\"end\":\"40783\"},{\"start\":\"40788\",\"end\":\"40789\"},{\"start\":\"40968\",\"end\":\"40969\"},{\"start\":\"40975\",\"end\":\"40976\"},{\"start\":\"40983\",\"end\":\"40984\"},{\"start\":\"40990\",\"end\":\"40991\"},{\"start\":\"40992\",\"end\":\"40993\"},{\"start\":\"41002\",\"end\":\"41003\"},{\"start\":\"41248\",\"end\":\"41249\"},{\"start\":\"41256\",\"end\":\"41257\"},{\"start\":\"41262\",\"end\":\"41263\"},{\"start\":\"41268\",\"end\":\"41269\"},{\"start\":\"41275\",\"end\":\"41276\"},{\"start\":\"41283\",\"end\":\"41284\"},{\"start\":\"41291\",\"end\":\"41292\"},{\"start\":\"41299\",\"end\":\"41300\"},{\"start\":\"41305\",\"end\":\"41306\"},{\"start\":\"41314\",\"end\":\"41315\"},{\"start\":\"41618\",\"end\":\"41619\"},{\"start\":\"41627\",\"end\":\"41628\"},{\"start\":\"41634\",\"end\":\"41635\"},{\"start\":\"41651\",\"end\":\"41652\"},{\"start\":\"41849\",\"end\":\"41850\"},{\"start\":\"41857\",\"end\":\"41858\"},{\"start\":\"41864\",\"end\":\"41865\"},{\"start\":\"41871\",\"end\":\"41872\"},{\"start\":\"42053\",\"end\":\"42054\"},{\"start\":\"42061\",\"end\":\"42062\"},{\"start\":\"42068\",\"end\":\"42069\"},{\"start\":\"42077\",\"end\":\"42078\"},{\"start\":\"42085\",\"end\":\"42086\"},{\"start\":\"42091\",\"end\":\"42092\"},{\"start\":\"42240\",\"end\":\"42241\"},{\"start\":\"42248\",\"end\":\"42249\"},{\"start\":\"42255\",\"end\":\"42256\"},{\"start\":\"42263\",\"end\":\"42264\"},{\"start\":\"42269\",\"end\":\"42270\"},{\"start\":\"42279\",\"end\":\"42280\"},{\"start\":\"42562\",\"end\":\"42563\"},{\"start\":\"42570\",\"end\":\"42571\"},{\"start\":\"42578\",\"end\":\"42579\"},{\"start\":\"42783\",\"end\":\"42784\"},{\"start\":\"42790\",\"end\":\"42791\"},{\"start\":\"42799\",\"end\":\"42800\"},{\"start\":\"42805\",\"end\":\"42806\"},{\"start\":\"42811\",\"end\":\"42812\"},{\"start\":\"43001\",\"end\":\"43002\"},{\"start\":\"43007\",\"end\":\"43008\"},{\"start\":\"43016\",\"end\":\"43017\"},{\"start\":\"43023\",\"end\":\"43024\"},{\"start\":\"43194\",\"end\":\"43195\"},{\"start\":\"43200\",\"end\":\"43201\"},{\"start\":\"43209\",\"end\":\"43210\"},{\"start\":\"43218\",\"end\":\"43219\"},{\"start\":\"43442\",\"end\":\"43443\"},{\"start\":\"43444\",\"end\":\"43445\"},{\"start\":\"43453\",\"end\":\"43454\"},{\"start\":\"43463\",\"end\":\"43464\"},{\"start\":\"43471\",\"end\":\"43472\"},{\"start\":\"43739\",\"end\":\"43740\"},{\"start\":\"43765\",\"end\":\"43766\"},{\"start\":\"43767\",\"end\":\"43768\"},{\"start\":\"43776\",\"end\":\"43777\"},{\"start\":\"43786\",\"end\":\"43787\"},{\"start\":\"44014\",\"end\":\"44015\"},{\"start\":\"44021\",\"end\":\"44022\"},{\"start\":\"44186\",\"end\":\"44187\"},{\"start\":\"44193\",\"end\":\"44194\"},{\"start\":\"44200\",\"end\":\"44201\"},{\"start\":\"44206\",\"end\":\"44207\"},{\"start\":\"44212\",\"end\":\"44213\"},{\"start\":\"44219\",\"end\":\"44220\"},{\"start\":\"44430\",\"end\":\"44431\"},{\"start\":\"44438\",\"end\":\"44439\"},{\"start\":\"44440\",\"end\":\"44441\"},{\"start\":\"44448\",\"end\":\"44449\"},{\"start\":\"44459\",\"end\":\"44460\"},{\"start\":\"44468\",\"end\":\"44469\"},{\"start\":\"44675\",\"end\":\"44676\"},{\"start\":\"44683\",\"end\":\"44684\"},{\"start\":\"44692\",\"end\":\"44693\"},{\"start\":\"44694\",\"end\":\"44695\"},{\"start\":\"44704\",\"end\":\"44705\"},{\"start\":\"44713\",\"end\":\"44714\"},{\"start\":\"44723\",\"end\":\"44724\"},{\"start\":\"44731\",\"end\":\"44732\"},{\"start\":\"44733\",\"end\":\"44734\"},{\"start\":\"44741\",\"end\":\"44742\"},{\"start\":\"44743\",\"end\":\"44744\"},{\"start\":\"44753\",\"end\":\"44754\"},{\"start\":\"44765\",\"end\":\"44766\"},{\"start\":\"45028\",\"end\":\"45029\"},{\"start\":\"45042\",\"end\":\"45043\"},{\"start\":\"45057\",\"end\":\"45058\"},{\"start\":\"45068\",\"end\":\"45069\"},{\"start\":\"45076\",\"end\":\"45077\"},{\"start\":\"45086\",\"end\":\"45087\"},{\"start\":\"45335\",\"end\":\"45336\"},{\"start\":\"45356\",\"end\":\"45357\"},{\"start\":\"45366\",\"end\":\"45367\"},{\"start\":\"45368\",\"end\":\"45369\"},{\"start\":\"45377\",\"end\":\"45378\"},{\"start\":\"45386\",\"end\":\"45387\"},{\"start\":\"45530\",\"end\":\"45531\"},{\"start\":\"45540\",\"end\":\"45541\"},{\"start\":\"45553\",\"end\":\"45554\"},{\"start\":\"45751\",\"end\":\"45753\"},{\"start\":\"45760\",\"end\":\"45761\"},{\"start\":\"45769\",\"end\":\"45770\"},{\"start\":\"45780\",\"end\":\"45781\"},{\"start\":\"45792\",\"end\":\"45793\"},{\"start\":\"45994\",\"end\":\"45995\"},{\"start\":\"46002\",\"end\":\"46003\"},{\"start\":\"46011\",\"end\":\"46012\"},{\"start\":\"46018\",\"end\":\"46019\"},{\"start\":\"46024\",\"end\":\"46025\"},{\"start\":\"46030\",\"end\":\"46031\"},{\"start\":\"46232\",\"end\":\"46233\"},{\"start\":\"46240\",\"end\":\"46241\"},{\"start\":\"46248\",\"end\":\"46249\"},{\"start\":\"46254\",\"end\":\"46255\"},{\"start\":\"46261\",\"end\":\"46262\"},{\"start\":\"46392\",\"end\":\"46393\"},{\"start\":\"46402\",\"end\":\"46403\"},{\"start\":\"46412\",\"end\":\"46413\"},{\"start\":\"46418\",\"end\":\"46419\"},{\"start\":\"46438\",\"end\":\"46439\"},{\"start\":\"46440\",\"end\":\"46441\"},{\"start\":\"46448\",\"end\":\"46449\"},{\"start\":\"46450\",\"end\":\"46451\"},{\"start\":\"46462\",\"end\":\"46463\"},{\"start\":\"46839\",\"end\":\"46840\"},{\"start\":\"46849\",\"end\":\"46850\"},{\"start\":\"46859\",\"end\":\"46860\"},{\"start\":\"46869\",\"end\":\"46870\"},{\"start\":\"47072\",\"end\":\"47073\"},{\"start\":\"47083\",\"end\":\"47084\"},{\"start\":\"47099\",\"end\":\"47100\"},{\"start\":\"47291\",\"end\":\"47292\"},{\"start\":\"47303\",\"end\":\"47304\"},{\"start\":\"47305\",\"end\":\"47306\"},{\"start\":\"47313\",\"end\":\"47314\"},{\"start\":\"47325\",\"end\":\"47326\"},{\"start\":\"47327\",\"end\":\"47328\"},{\"start\":\"47336\",\"end\":\"47337\"},{\"start\":\"47349\",\"end\":\"47350\"},{\"start\":\"47351\",\"end\":\"47352\"},{\"start\":\"47584\",\"end\":\"47585\"},{\"start\":\"47753\",\"end\":\"47754\"},{\"start\":\"47760\",\"end\":\"47761\"},{\"start\":\"47768\",\"end\":\"47769\"},{\"start\":\"47775\",\"end\":\"47776\"},{\"start\":\"47939\",\"end\":\"47940\"},{\"start\":\"47947\",\"end\":\"47948\"},{\"start\":\"47955\",\"end\":\"47956\"},{\"start\":\"47961\",\"end\":\"47962\"},{\"start\":\"47970\",\"end\":\"47971\"},{\"start\":\"47978\",\"end\":\"47979\"},{\"start\":\"47986\",\"end\":\"47987\"},{\"start\":\"47988\",\"end\":\"47989\"},{\"start\":\"48180\",\"end\":\"48181\"},{\"start\":\"48188\",\"end\":\"48189\"},{\"start\":\"48197\",\"end\":\"48198\"},{\"start\":\"48204\",\"end\":\"48205\"},{\"start\":\"48375\",\"end\":\"48376\"},{\"start\":\"48383\",\"end\":\"48384\"},{\"start\":\"48391\",\"end\":\"48392\"},{\"start\":\"48399\",\"end\":\"48400\"},{\"start\":\"48405\",\"end\":\"48406\"},{\"start\":\"48413\",\"end\":\"48414\"},{\"start\":\"48421\",\"end\":\"48422\"},{\"start\":\"48427\",\"end\":\"48428\"},{\"start\":\"48643\",\"end\":\"48644\"},{\"start\":\"48651\",\"end\":\"48652\"},{\"start\":\"48659\",\"end\":\"48660\"},{\"start\":\"48667\",\"end\":\"48668\"},{\"start\":\"48674\",\"end\":\"48675\"},{\"start\":\"48872\",\"end\":\"48873\"},{\"start\":\"48879\",\"end\":\"48880\"},{\"start\":\"48888\",\"end\":\"48889\"},{\"start\":\"48894\",\"end\":\"48895\"},{\"start\":\"49031\",\"end\":\"49032\"},{\"start\":\"49043\",\"end\":\"49044\"},{\"start\":\"49055\",\"end\":\"49056\"},{\"start\":\"49066\",\"end\":\"49067\"},{\"start\":\"49074\",\"end\":\"49075\"},{\"start\":\"49076\",\"end\":\"49077\"},{\"start\":\"49085\",\"end\":\"49086\"},{\"start\":\"49095\",\"end\":\"49096\"},{\"start\":\"49097\",\"end\":\"49098\"},{\"start\":\"49106\",\"end\":\"49107\"},{\"start\":\"49108\",\"end\":\"49109\"},{\"start\":\"49116\",\"end\":\"49117\"},{\"start\":\"49118\",\"end\":\"49119\"},{\"start\":\"49128\",\"end\":\"49129\"},{\"start\":\"49431\",\"end\":\"49432\"},{\"start\":\"49433\",\"end\":\"49434\"},{\"start\":\"49439\",\"end\":\"49440\"},{\"start\":\"49451\",\"end\":\"49452\"},{\"start\":\"49453\",\"end\":\"49454\"},{\"start\":\"49462\",\"end\":\"49463\"},{\"start\":\"49651\",\"end\":\"49652\"},{\"start\":\"49657\",\"end\":\"49658\"},{\"start\":\"49663\",\"end\":\"49664\"},{\"start\":\"49670\",\"end\":\"49671\"},{\"start\":\"49811\",\"end\":\"49812\"},{\"start\":\"49818\",\"end\":\"49819\"},{\"start\":\"49824\",\"end\":\"49825\"},{\"start\":\"49979\",\"end\":\"49980\"},{\"start\":\"49987\",\"end\":\"49988\"},{\"start\":\"49996\",\"end\":\"49997\"},{\"start\":\"50142\",\"end\":\"50143\"},{\"start\":\"50148\",\"end\":\"50149\"},{\"start\":\"50155\",\"end\":\"50156\"},{\"start\":\"50163\",\"end\":\"50164\"},{\"start\":\"50165\",\"end\":\"50166\"},{\"start\":\"50405\",\"end\":\"50406\"},{\"start\":\"50414\",\"end\":\"50415\"},{\"start\":\"50422\",\"end\":\"50423\"},{\"start\":\"50430\",\"end\":\"50431\"},{\"start\":\"50436\",\"end\":\"50437\"},{\"start\":\"50444\",\"end\":\"50445\"},{\"start\":\"50451\",\"end\":\"50452\"},{\"start\":\"50676\",\"end\":\"50677\"},{\"start\":\"50683\",\"end\":\"50684\"},{\"start\":\"50685\",\"end\":\"50686\"},{\"start\":\"50819\",\"end\":\"50820\"},{\"start\":\"50826\",\"end\":\"50827\"},{\"start\":\"50828\",\"end\":\"50829\"}]", "bib_author_last_name": "[{\"start\":\"40777\",\"end\":\"40780\"},{\"start\":\"40784\",\"end\":\"40786\"},{\"start\":\"40790\",\"end\":\"40795\"},{\"start\":\"40970\",\"end\":\"40973\"},{\"start\":\"40977\",\"end\":\"40981\"},{\"start\":\"40985\",\"end\":\"40988\"},{\"start\":\"40994\",\"end\":\"41000\"},{\"start\":\"41004\",\"end\":\"41013\"},{\"start\":\"41250\",\"end\":\"41254\"},{\"start\":\"41258\",\"end\":\"41260\"},{\"start\":\"41264\",\"end\":\"41266\"},{\"start\":\"41270\",\"end\":\"41273\"},{\"start\":\"41277\",\"end\":\"41281\"},{\"start\":\"41285\",\"end\":\"41289\"},{\"start\":\"41293\",\"end\":\"41297\"},{\"start\":\"41301\",\"end\":\"41303\"},{\"start\":\"41307\",\"end\":\"41312\"},{\"start\":\"41316\",\"end\":\"41321\"},{\"start\":\"41620\",\"end\":\"41625\"},{\"start\":\"41629\",\"end\":\"41632\"},{\"start\":\"41636\",\"end\":\"41649\"},{\"start\":\"41653\",\"end\":\"41656\"},{\"start\":\"41851\",\"end\":\"41855\"},{\"start\":\"41859\",\"end\":\"41862\"},{\"start\":\"41866\",\"end\":\"41869\"},{\"start\":\"41873\",\"end\":\"41882\"},{\"start\":\"42055\",\"end\":\"42059\"},{\"start\":\"42063\",\"end\":\"42066\"},{\"start\":\"42070\",\"end\":\"42075\"},{\"start\":\"42079\",\"end\":\"42083\"},{\"start\":\"42087\",\"end\":\"42089\"},{\"start\":\"42093\",\"end\":\"42096\"},{\"start\":\"42242\",\"end\":\"42246\"},{\"start\":\"42250\",\"end\":\"42253\"},{\"start\":\"42257\",\"end\":\"42261\"},{\"start\":\"42265\",\"end\":\"42267\"},{\"start\":\"42271\",\"end\":\"42277\"},{\"start\":\"42281\",\"end\":\"42290\"},{\"start\":\"42564\",\"end\":\"42568\"},{\"start\":\"42572\",\"end\":\"42576\"},{\"start\":\"42580\",\"end\":\"42589\"},{\"start\":\"42785\",\"end\":\"42788\"},{\"start\":\"42792\",\"end\":\"42797\"},{\"start\":\"42801\",\"end\":\"42803\"},{\"start\":\"42807\",\"end\":\"42809\"},{\"start\":\"42813\",\"end\":\"42816\"},{\"start\":\"43003\",\"end\":\"43005\"},{\"start\":\"43009\",\"end\":\"43014\"},{\"start\":\"43018\",\"end\":\"43021\"},{\"start\":\"43025\",\"end\":\"43028\"},{\"start\":\"43196\",\"end\":\"43198\"},{\"start\":\"43202\",\"end\":\"43207\"},{\"start\":\"43211\",\"end\":\"43216\"},{\"start\":\"43220\",\"end\":\"43222\"},{\"start\":\"43446\",\"end\":\"43451\"},{\"start\":\"43455\",\"end\":\"43461\"},{\"start\":\"43465\",\"end\":\"43469\"},{\"start\":\"43473\",\"end\":\"43487\"},{\"start\":\"43741\",\"end\":\"43763\"},{\"start\":\"43769\",\"end\":\"43774\"},{\"start\":\"43778\",\"end\":\"43784\"},{\"start\":\"43788\",\"end\":\"43796\"},{\"start\":\"44016\",\"end\":\"44019\"},{\"start\":\"44023\",\"end\":\"44026\"},{\"start\":\"44188\",\"end\":\"44191\"},{\"start\":\"44195\",\"end\":\"44198\"},{\"start\":\"44202\",\"end\":\"44204\"},{\"start\":\"44208\",\"end\":\"44210\"},{\"start\":\"44214\",\"end\":\"44217\"},{\"start\":\"44221\",\"end\":\"44225\"},{\"start\":\"44432\",\"end\":\"44436\"},{\"start\":\"44442\",\"end\":\"44446\"},{\"start\":\"44450\",\"end\":\"44457\"},{\"start\":\"44461\",\"end\":\"44466\"},{\"start\":\"44470\",\"end\":\"44477\"},{\"start\":\"44677\",\"end\":\"44681\"},{\"start\":\"44685\",\"end\":\"44690\"},{\"start\":\"44696\",\"end\":\"44702\"},{\"start\":\"44706\",\"end\":\"44711\"},{\"start\":\"44715\",\"end\":\"44721\"},{\"start\":\"44725\",\"end\":\"44729\"},{\"start\":\"44735\",\"end\":\"44739\"},{\"start\":\"44745\",\"end\":\"44751\"},{\"start\":\"44755\",\"end\":\"44763\"},{\"start\":\"44767\",\"end\":\"44773\"},{\"start\":\"45030\",\"end\":\"45040\"},{\"start\":\"45044\",\"end\":\"45055\"},{\"start\":\"45059\",\"end\":\"45066\"},{\"start\":\"45070\",\"end\":\"45074\"},{\"start\":\"45078\",\"end\":\"45084\"},{\"start\":\"45088\",\"end\":\"45097\"},{\"start\":\"45337\",\"end\":\"45354\"},{\"start\":\"45358\",\"end\":\"45364\"},{\"start\":\"45370\",\"end\":\"45375\"},{\"start\":\"45379\",\"end\":\"45384\"},{\"start\":\"45388\",\"end\":\"45393\"},{\"start\":\"45532\",\"end\":\"45538\"},{\"start\":\"45542\",\"end\":\"45551\"},{\"start\":\"45555\",\"end\":\"45561\"},{\"start\":\"45754\",\"end\":\"45758\"},{\"start\":\"45762\",\"end\":\"45767\"},{\"start\":\"45771\",\"end\":\"45778\"},{\"start\":\"45782\",\"end\":\"45790\"},{\"start\":\"45996\",\"end\":\"46000\"},{\"start\":\"46004\",\"end\":\"46009\"},{\"start\":\"46013\",\"end\":\"46016\"},{\"start\":\"46020\",\"end\":\"46022\"},{\"start\":\"46026\",\"end\":\"46028\"},{\"start\":\"46032\",\"end\":\"46035\"},{\"start\":\"46234\",\"end\":\"46238\"},{\"start\":\"46242\",\"end\":\"46246\"},{\"start\":\"46250\",\"end\":\"46252\"},{\"start\":\"46256\",\"end\":\"46259\"},{\"start\":\"46263\",\"end\":\"46266\"},{\"start\":\"46394\",\"end\":\"46400\"},{\"start\":\"46404\",\"end\":\"46410\"},{\"start\":\"46414\",\"end\":\"46416\"},{\"start\":\"46420\",\"end\":\"46436\"},{\"start\":\"46442\",\"end\":\"46446\"},{\"start\":\"46452\",\"end\":\"46460\"},{\"start\":\"46464\",\"end\":\"46473\"},{\"start\":\"46841\",\"end\":\"46847\"},{\"start\":\"46851\",\"end\":\"46857\"},{\"start\":\"46861\",\"end\":\"46867\"},{\"start\":\"46871\",\"end\":\"46878\"},{\"start\":\"47074\",\"end\":\"47081\"},{\"start\":\"47085\",\"end\":\"47097\"},{\"start\":\"47101\",\"end\":\"47108\"},{\"start\":\"47293\",\"end\":\"47301\"},{\"start\":\"47307\",\"end\":\"47311\"},{\"start\":\"47315\",\"end\":\"47323\"},{\"start\":\"47329\",\"end\":\"47334\"},{\"start\":\"47338\",\"end\":\"47347\"},{\"start\":\"47353\",\"end\":\"47359\"},{\"start\":\"47586\",\"end\":\"47590\"},{\"start\":\"47755\",\"end\":\"47758\"},{\"start\":\"47762\",\"end\":\"47766\"},{\"start\":\"47770\",\"end\":\"47773\"},{\"start\":\"47777\",\"end\":\"47780\"},{\"start\":\"47941\",\"end\":\"47945\"},{\"start\":\"47949\",\"end\":\"47953\"},{\"start\":\"47957\",\"end\":\"47959\"},{\"start\":\"47963\",\"end\":\"47968\"},{\"start\":\"47972\",\"end\":\"47976\"},{\"start\":\"47980\",\"end\":\"47984\"},{\"start\":\"47990\",\"end\":\"47993\"},{\"start\":\"48182\",\"end\":\"48186\"},{\"start\":\"48190\",\"end\":\"48195\"},{\"start\":\"48199\",\"end\":\"48202\"},{\"start\":\"48206\",\"end\":\"48209\"},{\"start\":\"48377\",\"end\":\"48381\"},{\"start\":\"48385\",\"end\":\"48389\"},{\"start\":\"48393\",\"end\":\"48397\"},{\"start\":\"48401\",\"end\":\"48403\"},{\"start\":\"48407\",\"end\":\"48411\"},{\"start\":\"48415\",\"end\":\"48419\"},{\"start\":\"48423\",\"end\":\"48425\"},{\"start\":\"48429\",\"end\":\"48432\"},{\"start\":\"48645\",\"end\":\"48649\"},{\"start\":\"48653\",\"end\":\"48657\"},{\"start\":\"48661\",\"end\":\"48665\"},{\"start\":\"48669\",\"end\":\"48672\"},{\"start\":\"48676\",\"end\":\"48679\"},{\"start\":\"48874\",\"end\":\"48877\"},{\"start\":\"48881\",\"end\":\"48886\"},{\"start\":\"48890\",\"end\":\"48892\"},{\"start\":\"48896\",\"end\":\"48900\"},{\"start\":\"49033\",\"end\":\"49041\"},{\"start\":\"49045\",\"end\":\"49053\"},{\"start\":\"49057\",\"end\":\"49064\"},{\"start\":\"49068\",\"end\":\"49072\"},{\"start\":\"49078\",\"end\":\"49083\"},{\"start\":\"49087\",\"end\":\"49093\"},{\"start\":\"49099\",\"end\":\"49104\"},{\"start\":\"49110\",\"end\":\"49114\"},{\"start\":\"49120\",\"end\":\"49126\"},{\"start\":\"49130\",\"end\":\"49135\"},{\"start\":\"49435\",\"end\":\"49437\"},{\"start\":\"49441\",\"end\":\"49449\"},{\"start\":\"49455\",\"end\":\"49460\"},{\"start\":\"49464\",\"end\":\"49474\"},{\"start\":\"49653\",\"end\":\"49655\"},{\"start\":\"49659\",\"end\":\"49661\"},{\"start\":\"49665\",\"end\":\"49668\"},{\"start\":\"49672\",\"end\":\"49675\"},{\"start\":\"49813\",\"end\":\"49816\"},{\"start\":\"49820\",\"end\":\"49822\"},{\"start\":\"49826\",\"end\":\"49835\"},{\"start\":\"49981\",\"end\":\"49985\"},{\"start\":\"49989\",\"end\":\"49994\"},{\"start\":\"49998\",\"end\":\"50011\"},{\"start\":\"50144\",\"end\":\"50146\"},{\"start\":\"50150\",\"end\":\"50153\"},{\"start\":\"50157\",\"end\":\"50161\"},{\"start\":\"50167\",\"end\":\"50169\"},{\"start\":\"50407\",\"end\":\"50412\"},{\"start\":\"50416\",\"end\":\"50420\"},{\"start\":\"50424\",\"end\":\"50428\"},{\"start\":\"50432\",\"end\":\"50434\"},{\"start\":\"50438\",\"end\":\"50442\"},{\"start\":\"50446\",\"end\":\"50449\"},{\"start\":\"50453\",\"end\":\"50458\"},{\"start\":\"50678\",\"end\":\"50681\"},{\"start\":\"50687\",\"end\":\"50695\"},{\"start\":\"50821\",\"end\":\"50824\"},{\"start\":\"50830\",\"end\":\"50838\"}]", "bib_entry": "[{\"start\":\"40716\",\"end\":\"40903\",\"attributes\":{\"id\":\"b0\"}},{\"start\":\"40905\",\"end\":\"41150\",\"attributes\":{\"id\":\"b1\"}},{\"start\":\"41152\",\"end\":\"41553\",\"attributes\":{\"id\":\"b2\",\"doi\":\"arXiv:1512.01274\"}},{\"start\":\"41555\",\"end\":\"41782\",\"attributes\":{\"id\":\"b3\"}},{\"start\":\"41784\",\"end\":\"42011\",\"attributes\":{\"id\":\"b4\"}},{\"start\":\"42013\",\"end\":\"42238\",\"attributes\":{\"matched_paper_id\":\"207891997\",\"id\":\"b5\"}},{\"start\":\"42240\",\"end\":\"42519\",\"attributes\":{\"id\":\"b6\",\"doi\":\"arXiv:1905.00641\"}},{\"start\":\"42521\",\"end\":\"42710\",\"attributes\":{\"matched_paper_id\":\"29573685\",\"id\":\"b7\"}},{\"start\":\"42712\",\"end\":\"42953\",\"attributes\":{\"id\":\"b8\"}},{\"start\":\"42955\",\"end\":\"43132\",\"attributes\":{\"id\":\"b9\"}},{\"start\":\"43134\",\"end\":\"43341\",\"attributes\":{\"id\":\"b10\"}},{\"start\":\"43343\",\"end\":\"43671\",\"attributes\":{\"matched_paper_id\":\"88166\",\"id\":\"b11\"}},{\"start\":\"43673\",\"end\":\"43952\",\"attributes\":{\"id\":\"b12\"}},{\"start\":\"43954\",\"end\":\"44123\",\"attributes\":{\"id\":\"b13\"}},{\"start\":\"44125\",\"end\":\"44360\",\"attributes\":{\"id\":\"b14\"}},{\"start\":\"44362\",\"end\":\"44621\",\"attributes\":{\"id\":\"b15\"}},{\"start\":\"44623\",\"end\":\"44964\",\"attributes\":{\"id\":\"b16\"}},{\"start\":\"44966\",\"end\":\"45285\",\"attributes\":{\"matched_paper_id\":\"1755257\",\"id\":\"b17\"}},{\"start\":\"45287\",\"end\":\"45528\",\"attributes\":{\"id\":\"b18\"}},{\"start\":\"45530\",\"end\":\"45687\",\"attributes\":{\"id\":\"b19\",\"doi\":\"arXiv:2002.03936\"}},{\"start\":\"45689\",\"end\":\"45928\",\"attributes\":{\"id\":\"b20\"}},{\"start\":\"45930\",\"end\":\"46175\",\"attributes\":{\"id\":\"b21\"}},{\"start\":\"46177\",\"end\":\"46390\",\"attributes\":{\"id\":\"b22\"}},{\"start\":\"46392\",\"end\":\"46783\",\"attributes\":{\"id\":\"b23\",\"doi\":\"arXiv:1804.01159\"}},{\"start\":\"46785\",\"end\":\"47004\",\"attributes\":{\"id\":\"b24\"}},{\"start\":\"47006\",\"end\":\"47239\",\"attributes\":{\"id\":\"b25\"}},{\"start\":\"47241\",\"end\":\"47512\",\"attributes\":{\"id\":\"b26\"}},{\"start\":\"47514\",\"end\":\"47697\",\"attributes\":{\"id\":\"b27\"}},{\"start\":\"47699\",\"end\":\"47890\",\"attributes\":{\"id\":\"b28\"}},{\"start\":\"47892\",\"end\":\"48131\",\"attributes\":{\"id\":\"b29\"}},{\"start\":\"48133\",\"end\":\"48312\",\"attributes\":{\"matched_paper_id\":\"9683805\",\"id\":\"b30\"}},{\"start\":\"48314\",\"end\":\"48589\",\"attributes\":{\"id\":\"b31\"}},{\"start\":\"48591\",\"end\":\"48800\",\"attributes\":{\"id\":\"b32\"}},{\"start\":\"48802\",\"end\":\"49029\",\"attributes\":{\"id\":\"b33\"}},{\"start\":\"49031\",\"end\":\"49384\",\"attributes\":{\"id\":\"b34\"}},{\"start\":\"49386\",\"end\":\"49589\",\"attributes\":{\"id\":\"b35\"}},{\"start\":\"49591\",\"end\":\"49788\",\"attributes\":{\"matched_paper_id\":\"5351802\",\"id\":\"b36\"}},{\"start\":\"49790\",\"end\":\"49909\",\"attributes\":{\"id\":\"b37\"}},{\"start\":\"49911\",\"end\":\"50140\",\"attributes\":{\"id\":\"b38\"}},{\"start\":\"50142\",\"end\":\"50331\",\"attributes\":{\"id\":\"b39\",\"doi\":\"arXiv:1411.7923\"}},{\"start\":\"50333\",\"end\":\"50620\",\"attributes\":{\"id\":\"b40\"}},{\"start\":\"50622\",\"end\":\"50817\",\"attributes\":{\"matched_paper_id\":\"17002642\",\"id\":\"b41\"}},{\"start\":\"50819\",\"end\":\"50948\",\"attributes\":{\"id\":\"b42\"}}]", "bib_title": "[{\"start\":\"42013\",\"end\":\"42051\"},{\"start\":\"42521\",\"end\":\"42560\"},{\"start\":\"43343\",\"end\":\"43440\"},{\"start\":\"44966\",\"end\":\"45026\"},{\"start\":\"48133\",\"end\":\"48178\"},{\"start\":\"49591\",\"end\":\"49649\"},{\"start\":\"50622\",\"end\":\"50674\"}]", "bib_author": "[{\"start\":\"40775\",\"end\":\"40782\"},{\"start\":\"40782\",\"end\":\"40788\"},{\"start\":\"40788\",\"end\":\"40797\"},{\"start\":\"40968\",\"end\":\"40975\"},{\"start\":\"40975\",\"end\":\"40983\"},{\"start\":\"40983\",\"end\":\"40990\"},{\"start\":\"40990\",\"end\":\"41002\"},{\"start\":\"41002\",\"end\":\"41015\"},{\"start\":\"41248\",\"end\":\"41256\"},{\"start\":\"41256\",\"end\":\"41262\"},{\"start\":\"41262\",\"end\":\"41268\"},{\"start\":\"41268\",\"end\":\"41275\"},{\"start\":\"41275\",\"end\":\"41283\"},{\"start\":\"41283\",\"end\":\"41291\"},{\"start\":\"41291\",\"end\":\"41299\"},{\"start\":\"41299\",\"end\":\"41305\"},{\"start\":\"41305\",\"end\":\"41314\"},{\"start\":\"41314\",\"end\":\"41323\"},{\"start\":\"41618\",\"end\":\"41627\"},{\"start\":\"41627\",\"end\":\"41634\"},{\"start\":\"41634\",\"end\":\"41651\"},{\"start\":\"41651\",\"end\":\"41658\"},{\"start\":\"41849\",\"end\":\"41857\"},{\"start\":\"41857\",\"end\":\"41864\"},{\"start\":\"41864\",\"end\":\"41871\"},{\"start\":\"41871\",\"end\":\"41884\"},{\"start\":\"42053\",\"end\":\"42061\"},{\"start\":\"42061\",\"end\":\"42068\"},{\"start\":\"42068\",\"end\":\"42077\"},{\"start\":\"42077\",\"end\":\"42085\"},{\"start\":\"42085\",\"end\":\"42091\"},{\"start\":\"42091\",\"end\":\"42098\"},{\"start\":\"42240\",\"end\":\"42248\"},{\"start\":\"42248\",\"end\":\"42255\"},{\"start\":\"42255\",\"end\":\"42263\"},{\"start\":\"42263\",\"end\":\"42269\"},{\"start\":\"42269\",\"end\":\"42279\"},{\"start\":\"42279\",\"end\":\"42292\"},{\"start\":\"42562\",\"end\":\"42570\"},{\"start\":\"42570\",\"end\":\"42578\"},{\"start\":\"42578\",\"end\":\"42591\"},{\"start\":\"42783\",\"end\":\"42790\"},{\"start\":\"42790\",\"end\":\"42799\"},{\"start\":\"42799\",\"end\":\"42805\"},{\"start\":\"42805\",\"end\":\"42811\"},{\"start\":\"42811\",\"end\":\"42818\"},{\"start\":\"43001\",\"end\":\"43007\"},{\"start\":\"43007\",\"end\":\"43016\"},{\"start\":\"43016\",\"end\":\"43023\"},{\"start\":\"43023\",\"end\":\"43030\"},{\"start\":\"43194\",\"end\":\"43200\"},{\"start\":\"43200\",\"end\":\"43209\"},{\"start\":\"43209\",\"end\":\"43218\"},{\"start\":\"43218\",\"end\":\"43224\"},{\"start\":\"43442\",\"end\":\"43453\"},{\"start\":\"43453\",\"end\":\"43463\"},{\"start\":\"43463\",\"end\":\"43471\"},{\"start\":\"43471\",\"end\":\"43489\"},{\"start\":\"43739\",\"end\":\"43765\"},{\"start\":\"43765\",\"end\":\"43776\"},{\"start\":\"43776\",\"end\":\"43786\"},{\"start\":\"43786\",\"end\":\"43798\"},{\"start\":\"44014\",\"end\":\"44021\"},{\"start\":\"44021\",\"end\":\"44028\"},{\"start\":\"44186\",\"end\":\"44193\"},{\"start\":\"44193\",\"end\":\"44200\"},{\"start\":\"44200\",\"end\":\"44206\"},{\"start\":\"44206\",\"end\":\"44212\"},{\"start\":\"44212\",\"end\":\"44219\"},{\"start\":\"44219\",\"end\":\"44227\"},{\"start\":\"44430\",\"end\":\"44438\"},{\"start\":\"44438\",\"end\":\"44448\"},{\"start\":\"44448\",\"end\":\"44459\"},{\"start\":\"44459\",\"end\":\"44468\"},{\"start\":\"44468\",\"end\":\"44479\"},{\"start\":\"44675\",\"end\":\"44683\"},{\"start\":\"44683\",\"end\":\"44692\"},{\"start\":\"44692\",\"end\":\"44704\"},{\"start\":\"44704\",\"end\":\"44713\"},{\"start\":\"44713\",\"end\":\"44723\"},{\"start\":\"44723\",\"end\":\"44731\"},{\"start\":\"44731\",\"end\":\"44741\"},{\"start\":\"44741\",\"end\":\"44753\"},{\"start\":\"44753\",\"end\":\"44765\"},{\"start\":\"44765\",\"end\":\"44775\"},{\"start\":\"45028\",\"end\":\"45042\"},{\"start\":\"45042\",\"end\":\"45057\"},{\"start\":\"45057\",\"end\":\"45068\"},{\"start\":\"45068\",\"end\":\"45076\"},{\"start\":\"45076\",\"end\":\"45086\"},{\"start\":\"45086\",\"end\":\"45099\"},{\"start\":\"45335\",\"end\":\"45356\"},{\"start\":\"45356\",\"end\":\"45366\"},{\"start\":\"45366\",\"end\":\"45377\"},{\"start\":\"45377\",\"end\":\"45386\"},{\"start\":\"45386\",\"end\":\"45395\"},{\"start\":\"45530\",\"end\":\"45540\"},{\"start\":\"45540\",\"end\":\"45553\"},{\"start\":\"45553\",\"end\":\"45563\"},{\"start\":\"45751\",\"end\":\"45760\"},{\"start\":\"45760\",\"end\":\"45769\"},{\"start\":\"45769\",\"end\":\"45780\"},{\"start\":\"45780\",\"end\":\"45792\"},{\"start\":\"45792\",\"end\":\"45796\"},{\"start\":\"45994\",\"end\":\"46002\"},{\"start\":\"46002\",\"end\":\"46011\"},{\"start\":\"46011\",\"end\":\"46018\"},{\"start\":\"46018\",\"end\":\"46024\"},{\"start\":\"46024\",\"end\":\"46030\"},{\"start\":\"46030\",\"end\":\"46037\"},{\"start\":\"46232\",\"end\":\"46240\"},{\"start\":\"46240\",\"end\":\"46248\"},{\"start\":\"46248\",\"end\":\"46254\"},{\"start\":\"46254\",\"end\":\"46261\"},{\"start\":\"46261\",\"end\":\"46268\"},{\"start\":\"46392\",\"end\":\"46402\"},{\"start\":\"46402\",\"end\":\"46412\"},{\"start\":\"46412\",\"end\":\"46418\"},{\"start\":\"46418\",\"end\":\"46438\"},{\"start\":\"46438\",\"end\":\"46448\"},{\"start\":\"46448\",\"end\":\"46462\"},{\"start\":\"46462\",\"end\":\"46475\"},{\"start\":\"46839\",\"end\":\"46849\"},{\"start\":\"46849\",\"end\":\"46859\"},{\"start\":\"46859\",\"end\":\"46869\"},{\"start\":\"46869\",\"end\":\"46880\"},{\"start\":\"47072\",\"end\":\"47083\"},{\"start\":\"47083\",\"end\":\"47099\"},{\"start\":\"47099\",\"end\":\"47110\"},{\"start\":\"47291\",\"end\":\"47303\"},{\"start\":\"47303\",\"end\":\"47313\"},{\"start\":\"47313\",\"end\":\"47325\"},{\"start\":\"47325\",\"end\":\"47336\"},{\"start\":\"47336\",\"end\":\"47349\"},{\"start\":\"47349\",\"end\":\"47361\"},{\"start\":\"47584\",\"end\":\"47592\"},{\"start\":\"47753\",\"end\":\"47760\"},{\"start\":\"47760\",\"end\":\"47768\"},{\"start\":\"47768\",\"end\":\"47775\"},{\"start\":\"47775\",\"end\":\"47782\"},{\"start\":\"47939\",\"end\":\"47947\"},{\"start\":\"47947\",\"end\":\"47955\"},{\"start\":\"47955\",\"end\":\"47961\"},{\"start\":\"47961\",\"end\":\"47970\"},{\"start\":\"47970\",\"end\":\"47978\"},{\"start\":\"47978\",\"end\":\"47986\"},{\"start\":\"47986\",\"end\":\"47995\"},{\"start\":\"48180\",\"end\":\"48188\"},{\"start\":\"48188\",\"end\":\"48197\"},{\"start\":\"48197\",\"end\":\"48204\"},{\"start\":\"48204\",\"end\":\"48211\"},{\"start\":\"48375\",\"end\":\"48383\"},{\"start\":\"48383\",\"end\":\"48391\"},{\"start\":\"48391\",\"end\":\"48399\"},{\"start\":\"48399\",\"end\":\"48405\"},{\"start\":\"48405\",\"end\":\"48413\"},{\"start\":\"48413\",\"end\":\"48421\"},{\"start\":\"48421\",\"end\":\"48427\"},{\"start\":\"48427\",\"end\":\"48434\"},{\"start\":\"48643\",\"end\":\"48651\"},{\"start\":\"48651\",\"end\":\"48659\"},{\"start\":\"48659\",\"end\":\"48667\"},{\"start\":\"48667\",\"end\":\"48674\"},{\"start\":\"48674\",\"end\":\"48681\"},{\"start\":\"48872\",\"end\":\"48879\"},{\"start\":\"48879\",\"end\":\"48888\"},{\"start\":\"48888\",\"end\":\"48894\"},{\"start\":\"48894\",\"end\":\"48902\"},{\"start\":\"49031\",\"end\":\"49043\"},{\"start\":\"49043\",\"end\":\"49055\"},{\"start\":\"49055\",\"end\":\"49066\"},{\"start\":\"49066\",\"end\":\"49074\"},{\"start\":\"49074\",\"end\":\"49085\"},{\"start\":\"49085\",\"end\":\"49095\"},{\"start\":\"49095\",\"end\":\"49106\"},{\"start\":\"49106\",\"end\":\"49116\"},{\"start\":\"49116\",\"end\":\"49128\"},{\"start\":\"49128\",\"end\":\"49137\"},{\"start\":\"49431\",\"end\":\"49439\"},{\"start\":\"49439\",\"end\":\"49451\"},{\"start\":\"49451\",\"end\":\"49462\"},{\"start\":\"49462\",\"end\":\"49476\"},{\"start\":\"49651\",\"end\":\"49657\"},{\"start\":\"49657\",\"end\":\"49663\"},{\"start\":\"49663\",\"end\":\"49670\"},{\"start\":\"49670\",\"end\":\"49677\"},{\"start\":\"49811\",\"end\":\"49818\"},{\"start\":\"49818\",\"end\":\"49824\"},{\"start\":\"49824\",\"end\":\"49837\"},{\"start\":\"49979\",\"end\":\"49987\"},{\"start\":\"49987\",\"end\":\"49996\"},{\"start\":\"49996\",\"end\":\"50013\"},{\"start\":\"50142\",\"end\":\"50148\"},{\"start\":\"50148\",\"end\":\"50155\"},{\"start\":\"50155\",\"end\":\"50163\"},{\"start\":\"50163\",\"end\":\"50171\"},{\"start\":\"50405\",\"end\":\"50414\"},{\"start\":\"50414\",\"end\":\"50422\"},{\"start\":\"50422\",\"end\":\"50430\"},{\"start\":\"50430\",\"end\":\"50436\"},{\"start\":\"50436\",\"end\":\"50444\"},{\"start\":\"50444\",\"end\":\"50451\"},{\"start\":\"50451\",\"end\":\"50460\"},{\"start\":\"50676\",\"end\":\"50683\"},{\"start\":\"50683\",\"end\":\"50697\"},{\"start\":\"50819\",\"end\":\"50826\"},{\"start\":\"50826\",\"end\":\"50840\"}]", "bib_venue": "[{\"start\":\"40716\",\"end\":\"40773\"},{\"start\":\"40905\",\"end\":\"40966\"},{\"start\":\"41152\",\"end\":\"41246\"},{\"start\":\"41555\",\"end\":\"41616\"},{\"start\":\"41784\",\"end\":\"41847\"},{\"start\":\"42098\",\"end\":\"42112\"},{\"start\":\"42308\",\"end\":\"42368\"},{\"start\":\"42591\",\"end\":\"42605\"},{\"start\":\"42712\",\"end\":\"42781\"},{\"start\":\"42955\",\"end\":\"42999\"},{\"start\":\"43134\",\"end\":\"43192\"},{\"start\":\"43489\",\"end\":\"43498\"},{\"start\":\"43673\",\"end\":\"43737\"},{\"start\":\"43954\",\"end\":\"44012\"},{\"start\":\"44125\",\"end\":\"44184\"},{\"start\":\"44362\",\"end\":\"44428\"},{\"start\":\"44623\",\"end\":\"44673\"},{\"start\":\"45099\",\"end\":\"45113\"},{\"start\":\"45287\",\"end\":\"45333\"},{\"start\":\"45579\",\"end\":\"45600\"},{\"start\":\"45689\",\"end\":\"45749\"},{\"start\":\"45930\",\"end\":\"45992\"},{\"start\":\"46177\",\"end\":\"46230\"},{\"start\":\"46491\",\"end\":\"46575\"},{\"start\":\"46785\",\"end\":\"46837\"},{\"start\":\"47006\",\"end\":\"47070\"},{\"start\":\"47241\",\"end\":\"47289\"},{\"start\":\"47514\",\"end\":\"47582\"},{\"start\":\"47699\",\"end\":\"47751\"},{\"start\":\"47892\",\"end\":\"47937\"},{\"start\":\"48211\",\"end\":\"48214\"},{\"start\":\"48314\",\"end\":\"48373\"},{\"start\":\"48591\",\"end\":\"48641\"},{\"start\":\"48802\",\"end\":\"48870\"},{\"start\":\"49137\",\"end\":\"49193\"},{\"start\":\"49386\",\"end\":\"49429\"},{\"start\":\"49677\",\"end\":\"49681\"},{\"start\":\"49790\",\"end\":\"49809\"},{\"start\":\"49911\",\"end\":\"49977\"},{\"start\":\"50186\",\"end\":\"50227\"},{\"start\":\"50333\",\"end\":\"50403\"},{\"start\":\"50697\",\"end\":\"50711\"},{\"start\":\"50840\",\"end\":\"50877\"}]"}}}, "year": 2023, "month": 12, "day": 17}