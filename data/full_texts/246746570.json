{"id": 246746570, "updated": "2023-10-05 16:49:44.158", "metadata": {"title": "Snow Coverage Mapping by Learning from Sentinel-2 Satellite Multispectral Images via Machine Learning Algorithms", "authors": "[{\"first\":\"Yucheng\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Jinya\",\"last\":\"Su\",\"middle\":[]},{\"first\":\"Xiaojun\",\"last\":\"Zhai\",\"middle\":[]},{\"first\":\"Fanlin\",\"last\":\"Meng\",\"middle\":[]},{\"first\":\"Cunjia\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "Remote Sensing", "journal": "Remote Sensing", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": ": Snow coverage mapping plays a vital role not only in studying hydrology and climatology, but also in investigating crop disease overwintering for smart agriculture management. This work investigates snow coverage mapping by learning from Sentinel-2 satellite multispectral images via machine-learning methods. To this end, the largest dataset for snow coverage mapping (to our best knowledge) with three typical classes (snow, cloud and background) is \ufb01rst collected and labeled via the semi-automatic classi\ufb01cation plugin in QGIS. Then, both random forest-based conventional machine learning and U-Net-based deep learning are applied to the semantic segmentation challenge in this work. The effects of various input band combinations are also investigated so that the most suitable one can be identi\ufb01ed. Experimental results show that (1) both conventional machine-learning and advanced deep-learning methods signi\ufb01cantly outperform the existing rule-based Sen2Cor product for snow mapping; (2) U-Net generally outperforms the random forest since both spectral and spatial information is incorporated in U-Net via convolution operations; (3) the best spectral band combination for U-Net is B2, B11, B4 and B9. It is concluded that a U-Net-based deep-learning classi\ufb01er with four informative spectral bands is suitable for snow coverage mapping.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/remotesensing/WangSZML22", "doi": "10.3390/rs14030782"}}, "content": {"source": {"pdf_hash": "a7d21b546999c90f058dc01c5163fd5de43f2559", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.mdpi.com/2072-4292/14/3/782/pdf", "status": "GOLD"}}, "grobid": {"id": "693fdb01bd65cd5ed48d438927f70ba8dd0504fd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a7d21b546999c90f058dc01c5163fd5de43f2559.txt", "contents": "\nCitation: Mapping by Learning from Sentinel-2 Satellite Multispectral Images via Machine Learning Algorithms\nPublished: 8 February 2022\n\nY ; Wang \nJ ; Su \nX ; Zhai \nF ; Meng \nC Liu \nSnow Coverage \n\nSchool of Computer Science and Electronic Engineering\nUniversity of Essex\nCO4 3SQColchesterUK\n\n\nDepartment of Mathematical Sciences\nUniversity of Essex\nCO4 3SQColchesterUK\n\n\nDepartment of Aeronautical and Automotive Engineering\nLoughborough University\nLE11 3TULoughboroughUK\n\nCitation: Mapping by Learning from Sentinel-2 Satellite Multispectral Images via Machine Learning Algorithms\nPublished: 8 February 202210.3390/rs14030782Received: 31 December 2021 Accepted: 1 February 2022Academic Editor: Karem Chokmani\n\n\nIntroduction\n\nRemote sensing technology (e.g., satellite-based, unmanned aerial vehicle (UAV)based, ground-based) can provide a nondestructive way of spatially and temporally monitoring the targets of interest remotely, which has drawn increasing attention recently with the rapid development of autonomous system innovation, sensing technology and imageprocessing algorithms, especially with convolutional neural network-based deep-learning approaches. Taking remote sensing in geosciences as an example, UAV multispectral images are used in [1] for bulk drag prediction of Riparian Arundo donax stands so that their impact on vegetated drainage channel can be assessed. Terrestrial laser scanning is also used in [2] to investigate physically based characterization of mixed floodplain vegetation. Meanwhile, the interest in agricultural applications of remote sensing technology has also been exponentially growing since 2013 [3], where the main applications of remote sensing in agriculture include phenotyping, land-use monitoring, crop yield forecasting, precision farming and the provision of ecosystem services [3][4][5][6].\n\nCrop disease monitoring and, more importantly, the accurate early forecasting of crop disease outbreak by making use of the remote sensing data has also attracted much attention in recent years [6][7][8]. In particular, temperature and humidity are the two most important environmental factors determining the activities of pathogenic microorganisms. A good example is the wheat stripe rust, one of the most destructive diseases in wheat that the U-Net model with the four informative bands (including B2, B11, B4 and B9) as inputs gave the best classification performance on our test dataset.\n\n\nMaterials and Methods\n\nTo make the work readable, the entire workflow is illustrated in Figure 1. \n\n\nModel evaluation 3. Data exploration\n\nClipped into patches  (Google Earth Engine) Figure 1. The entire workflow is divided into data collection, data labeling, data exploration, model training and model evaluation.\n\n\nSatellite Image Collection\n\nSentinel-2 satellite images are freely accessible from several platforms, such as the Copernicus Open Access Hub [24], USGS EROS Archive [25] and Google Earth Engine [26] among others. In this study, all Sentinel-2 satellite images were directly downloaded from the Google Earth Engine via some basic scripts in Java. Our main purpose is to conduct snow mapping of Earth's surface, therefore, we focused on the corrected Sentinel-2 Level-2A product instead of the Level-1C product, as the Level-2A provides Orthoimage Bottom Of Atmosphere (BOA) corrected reflectance products. Moreover, the Level-2A product has itself included a scene classification map, including cloud and snow probabilities at 60 m resolution, which are derived from the Sen2Cor algorithm. However, it should also be noted that Level-2A products are only available after December 2018, although a separate program is available to generate Level-2A products from Level-1C product.\n\nThere are a total of 12 spectral bands for Sentinel-2 Level-2A product, which include B1 (Aerosols), B2 (Blue), B3 (Green), B4 (Red), B5 (Red Edge 1), B6 (Red Edge 2), B7 (Red Edge 3), B8 (NIR), B8A (Red Edge 4), B9 (Water vapor), B11 (SWIR 1) and B12 (SWIR 2). The cirrus B10 is omitted as it does not contain surface information. Of the 12 available spectral bands, B2, B3, B4 and B8 are all at 10 m resolution, the resolutions of B5, B6, B7, B8A, B9 and B12 are 20 m, and the remaining two bands of B1 and B19 have 60 m resolution. Within all our downstream analyses, all spectral bands with resolutions different from 10 m were re-sampled to 10 m to achieve an equal spatial resolution across all spectral bands.\n\nDuring our scene collection, we specifically choose scenes that include humanidentifiable snow, cloud, or both snow and cloud. It is important to ensure that the snow and cloud scenarios are human identifiable, as we are doing a supervised machine-learning task and our next step data annotation is to label each pixel into three classes. To ensure that the collected dataset includes a large diversity of scenes (to be representative), we selected the scenes to cover different continents, years, months and land-cover classes. Lastly, we only kept a representative 1000 pixels \u00d7 1000 pixels region for each scene; this is, on the one hand, to reduce the redundant contents of a whole product and, on the other hand, to greatly reduce the amount of time needed for the following data annotation step.\n\n\nData Annotation\n\nUpon downloading the representative images, the next step is to manually label them into different classes for machine/deep-learning model construction. The data annotation step involves manually labeling every pixel into one of three classes (i.e., snow, cloud and background) by human experts. As the multispectral satellite images are not readily human-readable, we first extract the B4, B3 and B2 bands and re-scale them into the three channels of a typical RGB image (i.e., false-color RGB image). However, it is obvious that snow and cloud share very similar colors (i.e., close to white) and texture across many scenes, thus it is almost impossible to distinguish them, especially when there are overlaps between snow and cloud. B12 (SWIR 2) is known to have a relatively better separation between snow and cloud than other bands, thus we also created a false-color image, with B2, B3 and B12 as the R, G, B channels, for each scene. Then, all the downstream labeling processes are performed on the false-color images.\n\nThe pixel-level labeling process was performed in QGIS platform (Version: 3.18.2) [27]. Recently, Luca Congedo developed a Semi-Automatic Classification Plugin for QGIS, which is reported to be able to ease and automate the phases of land-cover classification [28]. Therefore, our labeling processes were completed with the help of the Semi-Automatic Classification Plugin (Version: 7.9) [28]. Specifically, for each image, we first select and define several representative regions for each target class; then, we use the Minimum Distance algorithm of this plugin to group all other pixels into the pre-defined classes. All final generated classification masks were carefully checked by two human experts to make sure the label for snow and cloud is as correct as possible.\n\n\nSen2Cor Cloud Mask and Snow Mask\n\nThe Sentinel-2 Level-2A product itself includes a cloud confidence mask and snow confidence mask, which are both derived from the Sen2Cor system. The algorithm used by Sen2Cor to detect snow or cloud is based on a series of threshold tests that use top-ofatmosphere reflectance from the spectral bands as input; the thresholds are also applied on band ratios and several spectral indices, such as Normalized Difference Vegetation Index (NDVI) and Normalized Difference Snow Index (NDSI). In addition, a level of confidence is associated with each of these threshold tests, and the final output of the series of threshold tests are a probabilistic (0-100%) snow mask quality map and a cloud mask quality map [29]. In our study, the snow confidence mask and cloud confidence mask of each scene were directly downloaded from Google Earth Engine along with its Level-2A spectral band data. For a better visualization of the Sen2Cor classification performance, for each satellite scene, we put the cloud confidence mask, snow confidence mask and snow confidence mask into the three channels of a color image to generate the final Sen2Cor classification mask.\n\n\nRandom Forest with Bayesian Hyperparameter Optimization\n\nThe 'sklearn.ensemble.RandomForestClassifier' function in sklearn library (Version: 0.24.2) [30] in Python (Version: 3.8.0) is used to construct Random Forest (RF) models to evaluate the performance of traditional machine-learning algorithms in classifying snow and cloud with the inputs of independent BOA-corrected reluctance data from different spectral band combinations.\n\nRF is a decision tree-based algorithm, which has been widely applied in crop disease detection [31]. To improve the prediction accuracy and control the problem of overfitting, we need to fine-tune several key hyper-parameters for the training of each RF model [32]. In this study, we mainly fine-tuned three hyper-parameters: the number of trees in the forest, the maximum depth of the tree and the minimum number of samples required to split an internal node. Instead of using random or grid search for the optimal hyperparameter combination, we applied the Bayesian Optimization [33] to find the optimal parameter combination for each RF model. Bayesian optimization enables finding out the optimal parameter combination in as few iterations as possible, which works by sequentially constructing a posterior distribution of functions (Gaussian process) that best describes the function you want to optimize. Here, we used the average of five-fold cross-validation scores, which resulted from training a random forest model with weighted F1 score as the loss function, as the optimization function of Bayesian optimization. After the Bayesian optimization, a random forest model with the optical parameters setting is trained.\n\nWe then applied both forward sequential feature selection (FSFS) and backward sequential feature selection (BSFS) to rank the importance of each spectral band and more importantly to find out the optimal band combination, which has fewer bands and at the same time can capture the most informative features. FSFS sequentially adds features and BSFS sequentially removes features to form a feature subset in a greedy fashion. FSFS starts with zero features; at each stage, it chooses the best feature to add based on the cross-validation score of an estimator (RF classifier in this study). By contrast, BSFS starts with full features and sequentially selects the least important features to be removed at each stage.\n\n\nU-Net Training\n\nU-Net is a convolutional network architecture for fast and precise segmentation of images [34], which has been applied for yellow rust disease mapping in [5,32]. In this study, U-Net is used as the representative deep-learning model for satellite image classification. It is noted that our collected satellite images are in the size of 1000 \u00d7 1000 pixels. However, to make our deep-learning models rely less on large-size images and to greatly increase the samples size of training dataset, we set the model input to have a width of 256, and height of 256 and channels of N, where N depends on the used combination bands in different models. For each satellite image in the training dataset, we clipped it into small patches in a sliding window way-with a window size of 256 \u00d7 256 pixels and a sliding step of 128 pixels. As a result, each satellite image will yield around 49 patches of size 256 \u00d7 256 \u00d7 N.\n\nThe details of the U-Net network architecture are shown in Figure 2. It consists of an encoding path (left side) and a decoding path (right side). Every block of the encoding path consists of two repeated 3 \u00d7 3 unpadded convolutions, each followed by a batch normalization (BN) layer and a rectified linear unit (ReLU), then a 2 \u00d7 2 max pooling operation with stride 2 is applied for downsampling. Each block of the decoding path includes a 2 \u00d7 2 transpose convolution for feature upsampling, followed by a concatenation with the corresponding feature map from the encoding path, then two 3 \u00d7 3 convolutions, each followed by a BN and a ReLU. The final layer is a 1 \u00d7 1 convolution to map each pixel in the input to the desired number of classes.\n\nThe U-Net model is constructed and trained based on the Pytorch deep leaning framework (Version: 1.7.1) [35]. For model training, we take the patches located in the first column or first row of the generated patch matrix of each training satellite image into the validation set and the remaining patches excluding those that have overlap with validation patches are selected as the training data, the ratio of the number of validation patches to training patches is about 19.1%. The model stops training until the loss of the validation data does not decrease after 20 epochs. To train the U-Net models, we used the weighted cross-entropy loss as the loss function, stochastic gradient descent as the optimizer with learning rate of 0.01 and momentum ratio of 0.9. The input batch size is set to be 4. The loss curves of the training processes for U-Net with different input bands are shown in Figure 3. \n\n\nEvaluation Matrix\n\nTo systematically compare the classification performance of the different models, we have used the following evaluation matrices including prevision, recall, F1 score and Intersection Over Union(IoU) and Accuracy\nPrecision = TP TP + FP (1) Recall = TP TP + FN (2) F1_score = 2 \u00d7 Precision \u00d7 Recall Precision + Recal(3)\nIntersection over Union(IoU) = Area of Overlap Area of Union (4)\nAccuracy = TP + TN TP + TN + FP + FN (5)\nwhere TP represents true positive numbers, FP represents false positive numbers and FN represents false negative numbers.\nLoss (weighted cross entropy loss) = \u2212 M \u2211 c=1 w c y o,c log(p o,c )(6)\nwhere M = 3 is the number of classes (snow, cloud and background), w c is the weight value for class c which is defined as the ratio between the total number of pixels in the training set and the number of pixels in class c, y o,c is binary indicator (0 or 1) if class label c is the correct classification for observation o, y o,c is the predicted probability of observation o is of class c.\n\n\nResults\n\n\nLargest Snow-Mapping Satellite Image Dataset\n\nTo validate and compare different methods in classifying snow and cloud for satellite images, we carefully searched and selected 40 Sentinel-2 L2A scenes across the globe as displayed in Figure 4. In addition, the details about their product ID, coordinates and timing are listed in Table. 1.  The 40 sites have been chosen to ensure scene diversity. In particular, the 40 sites are distributed across all six continents except for Antarctica. With the constant high temperature in the low latitudes, our selected snow and cloud scenes are all distributed in middle-and high-latitude areas. Since the Sentinel-2 Level-2A products have only been available since December 2018, our collected scenes are all dated from the last three years, i.e., 2019, 2020 and 2021. For each scene, all 12 atmospheric corrected spectral bands, i.e., B1, B2, B3, B4, B5, B6, B7, B8, B8A, B9, B11 and B12, are collected, and the cloud confidence mask and the snow confidence mask which are derived from the Sen2Cor algorithm are also downloaded along with the spectral bands by the Google Earth Engine. Each band of the scene is re-sampled to 10 m resolution. For each scene product, we only kept a representative region, where the sizes of width and height are both 1000 pixels, which contains human identifiable snow or cloud.\n\n\n2020 2021\n\nEvery pixel of all 40 collected satellite images were labeled into three classes including snow, cloud and background using the Semi-Automatic Classification Plugin [28] in QGIS. We took six representative scenes as the test dataset, and their false-color RGB images and classification masks are shown in Figure 12. The remaining 34 scenes were put into the training dataset, and their RGB images and classification masks are shown in Figures 5 and 6, respectively.  \n\n\nSpectral Band Comparison\n\nSnow and cloud are both white bright objects seen from the satellite RGB images, and they are often indistinguishable in most scenarios by only looking at RGB images. We first compared the reflectance distributions of snow, cloud and background in the 12 spectral bands from the Sentinel-2 L2A product. From the boxplots in Figure 7, we could first observe that the background pixels have relative low reflectance values across all 12 spectral bands, and the median reflectance values are all less than 2500. Snow and cloud showed similar and relatively high reflectance values (median reflectance values are greater than 5000) in the first ten spectral bands; however, they also both have high reflectance variations in these ten bands.\n\nRegarding snow and cloud, B12 and B11 are the top two bands that best separate snow and cloud, with a median reflectance around 950 in snow, compared to that of around 2500 in the cloud. This is in line with our expectations, as B12 and B11 are both designed to measure short-wave infrared (SWIR) wavelengths, and they are often used to differentiate between snow and cloud. However, the distribution of background is very similar to snow in B11 and B12 although with larger fluctuations. B9, B1 and B2 are the three next bands that have a relatively larger distribution difference between snow and cloud. Interestingly, even though snow and cloud have very similar reflectance distributions in the first 10 bands, the cloud has a slightly higher median value than snow in nine out of the ten bands (except B4). In summary, there are several spectral bands that have good separation between any two of the three classes; however, there is no single band that clearly separates the three classes simultaneously.  The Normalized Difference Snow Index (NDSI) has been suggested to be useful in estimating fractional snow cover [36,37]. It measures the relative magnitude of the reflectance difference between the visible green band and SWIR band. Here we also compared the NDSI distribution of snow, cloud and background in the training dataset, where the results are displayed in Figure 8. Our results showed that despite there being three major different spikes representing the three classes respectively, the huge overlaps between the snow spike and the cloud suggest that NDSI (though being the best index for snow mapping) is not a very accurate index to distinguish snow and cloud.\n\n\nOptimal Band Combination\n\nOur results in the previous section have demonstrated that no single spectral band (or index) is able to give clean separations between the three classes, i.e., snow, cloud and background. However, combining several bands is very promising. For example, the background pixels have clear separations with snow and cloud across the first ten bands, and the reflectance distribution of cloud is largely different from that in background and the snow within B12 or B11. Among the 12 spectral bands of the Sentinel-2 Level-2A product, some bands may capture similar features and thus may be redundant when used to distinguish the three classes. To find out the optimal band combination that captures the most useful information to discriminate the three classes that at the same time has fewer (saving computation resources) bands included, we applied both Forward Sequential As shown in Figure 9, the B2 (Blue) band is ranked as the most important band by both forward and backward sequential feature selection. B12 and B11 are two SWIR bands, and they are listed as the second most important bands by FSFS and BSFS, respectively. However, the band combination of B2 and B12 slightly outperforms the B2 and B11 combination when used as an input for constructing models to separate the three classes (OOB errors 0.109 vs. 0.123). FSFS and BSFS both identified B4 (Red) as the third most important band, and again the combination of B2, B11 and B4 identified by BSFS demonstrated as the best three-band combination. The sequential addition of more bands into the model input subset gives minimal improvements, especially when the top four bands have already been included. As a result, we take the combination of B2, B11, B4 and B9 as the most informative band set of Sentinel-2 Level-2A products for separating snow, cloud and background. It should be noted, although we re-sampled each band to the highest 10 meter resolution, the original resolutions for B2 and B4 are 10 m, B11 is 20 m and B9 is 60 m.  \n+\n\nPerformance Comparison for RF Models with Various Band Combinations\n\nWe trained three RF models, each with a different band combination as input, and compared their performance in classifying snow, cloud and background. The three-band combinations are RGB bands (B4, B3 and B2), the informative four bands (B2, B11, B4 and B9) and all 12 bands (B1, B2, B3, B4, B5, B6, B7, B8, B8A, B9, B11 and B12). The hyper-parameters of each random forest model were optimized independently by Bayesian optimization to achieve each model's best performance.\n\nThe comparison of four evaluation scores (precision, F1 score, recall and IoU) between the three RF models is demonstrated in Figures 10 (on the training dataset) and 11 (on the test dataset). The three RF models showed very close and good performance (all above 0.86) in predicting background pixels across all four evaluation scores. This is in line with our previous observation that background pixels have distinct BOA reflectance distribution compared to cloud and snow in each of the first ten bands (Figure 7). The most apparent difference between the three models is that the RF model trained on RGB bands (RF RGB ) exhibited very poor performance in predicting both cloud and snow. For example, the IoU of RF RGB in predicting cloud and snow are both below 0.35, and F1 scores are both only around 0.5. These results demonstrate the RGB bands (i.e., B4, B3 and B2) do not contain enough information to discriminate snow from cloud. This finding is also reflected by the fact that snow and cloud share similar reflectance distribution patterns across the three bands.\n\nThe RF model trained on the four informative bands (RF 4bands ) and the RF model trained on all 12 bands (RF 12bands ) exhibited very close performance in predicting all three classes, and they both significantly outperform the RF RGB model in predicting both cloud and snow. The four bands input (B2, B11, B4 and B9) for RF 4bands was selected by BSFS to maximize the informative features and simultaneously minimize the number of bands. The previous section (Figure 9) has demonstrated that the top 4 important bands combined accounted for almost all the informative features in classifying the three classes. This explains why RF 4bands and RF 12bands showed close performance and are much better than RF RGB . A striking finding is that RF 4bands is even marginally better than RF 12bands in classifying the three classes based on all four evaluation scores, except for the precision of cloud and recall of snow. This may be due to the fact that the inclusion of more similar or non-relevant bands may make the machine-learning model at higher risk of overfitting on the training dataset, and this point is supported by the finding that RF 12bands slightly outperformed RF 4bands in classifying all three classes across the four evaluation scores in the training dataset (please refer to Figure 10).\n\n\nPerformance Comparison for U-Net with Various Band Combinations\n\nIn addition to the RF models, we also trained three U-Net models with RGB bands, informative four bands and all 12 bands as inputs. Except for the input layer, all other layers of the U-Net structure are the same for the three U-Net models. We then compared their classification performance based on the four evaluation metrics.\n\nSimilar to the RF models, the three U-Net models achieved close and good performance (all above 0.84) in classifying background pixels according to all four evaluation scores. The U-Net model with informative four bands as inputs (U-Net 4bands ) and the U-Net model with all bands as inputs (U-Net 12bands model) also exhibited close performance in predicting snow pixels and cloud pixels according to the four scores, even though the U-Net 4bands model slightly and consistently outperformed the U-Net 12bands model. The U-Net model with RGB bands as inputs (U-Net RGB ) apparently fell behind U-Net 4bands and U-Net 12bands in classifying snow and cloud in almost all evaluation scores, except that the three models all achieved nearly perfect scores (all are greater than 0.987) on precision for cloud. \n\n\nComparison between Sen2Cor, RF and U-Net Models\n\nWe then compared the classification performance of Sen2Cor, RF and U-Net models. In terms of overall accuracy, Sen2Cor gave very poor classification results (only 58.06%) on our selected six test scenes, apparently falling far behind other models ( Table 2). The poor classification performance of Sen2Cor is also reflected in its generated classification masks, which are listed in the third column of Figure 12. The Sen2Cor misclassified almost all cloud pixels in the first scene and most cloud pixels in the third scene into snow pixels; it also misclassified many snow pixels, which are mainly located at the boundaries between snow and background, in the last two scenes into clouds.\n\n\nScene1\n\n\nRGB image\n\n\nManual mask Sen2Cor\n\n\nRF-RGB RF-4bands RF-12bands UNet-RGB UNet-4bands UNet_12bands\n\nScene2 Scene3 Scene4 Scene5 Scene6 Background Cloud Snow Figure 12. Visual comparisons of the classification performance in six independent scenes for different methods. Each row represents an independent test scene, and each column represents a different method. Except for the plots in the first column, the three target classes are represented by different colors, where black denotes background, red denotes cloud and cyan denotes snow. The three U-Net models all clearly outperformed their corresponding RF models. The greatest improvement came from the comparison between RF RGB and U-Net RGB . The overall accuracy for RF RGB is 69.06%, while it significantly increased to 87.72% using U-Net RGB which is even closer to the performance of RF 12bands (89.65%). As with Sen2Cor, RF RGB misclassified almost all cloud pixels in the first scene into snow; however, U-Net RGB managed to correctly classify around 80% of the cloud pixels in the first scene. Both RF RGB and U-Net RGB tend to predict all snow and cloud pixels in the third scene as snow pixels. Furthermore, RF RGB misclassified a lot of cloud pixels in the second scene and the fourth scene into snow and predicted many snow pixels in the last scene as cloud pixels, while U-Net RGB does not have such issues (Figure 12). The above results indicate that the pure pixel-level reflectance from the RGB bands does not contain enough informative features to discriminate snow from cloud; the additional addition of spatial information as employed by U-Net model greatly improved the classification results.\n\nEven though the overall accuracy of RF 4bands and RF 12bands reached around 90%, which is a huge improvement over RF RGB , they still both misclassified many cloud pixels into snow in the first and fourth scene. U-Net 4bands and U-Net 12bands further increased the overall accuracy to above 93%, and with U-Net RGB they all avoided such \"cloud to snow\"misclassification issues in the fourth scene, U-Net 4bands even further uniquely correctly classified all cloud pixels in the first scene ( Figure 12). U-Net 4bands achieved the highest overall accuracy of 93.89%, and this, combined with its outstanding score in the other four evaluation matrices (Figure 11), makes it the best model among the six models we have studied to do snow mapping for Sentinel-2 imagery.\n\n\nDiscussion\n\nSnow coverage information is important for a wide range of applications. In agriculture, in particular, accurate snow mapping could be a vital factor for developing models to predict future disease development. However, accurate snow mapping from satellite images is still a challenging task, as cloud and snow share similar spectral reflectance distribution (visible spectral bands in particular), and therefore they are not easily distinguishable. To our best knowledge, there is no large annotated satellite image dataset especially for the task of snow mapping that is currently publicly available. Although Hollstein et al. [16] manually labeled dozens of small polygonal regions from scenes of Sentinel-2 Level-1C products across the globe at 20 m resolution, the small isolated irregular polygons are not useful enough to train convolutional neural network-based models. Baetens et al. [20] annotated 32 scenes of Sentinel-2 Level-2A products in 10 locations at 60 m resolution; however, they were mainly focused on generating cloud masks, and snow has very limited representation. As a result, we carefully collected and labeled 40 scenes of Sentinel-2 Level-2A products at the highest 10 m resolution, which includes a wide representative of snow, cloud and background landscape across the globe. The proposed database would on the one hand be used to evaluate the performance of different snow prediction tools, and on the other hand enable the future development of more advanced algorithms for snow coverage mapping.\n\nThreshold tests-based tools (such as Sen2Cor) could be used to make fast and rough estimations of snow or cloud. However, our results have demonstrated that they can be misleading under some circumstances. In particular, Sen2Cor tends to mis-classify the cloud under a near-freezing environment temperature into snow, such as the case in the first and fourth scene of the test dataset (Figure 7). The thin layer of snow located in the junction between snow and background are also often misclassified to be cloud. Thus, accurate snow coverage mapping requires much better snow and cloud classification tools.\n\nThe Sentinel-2 Level-2A product includes 12 BOA-corrected reflectance bands. Our results show that no single band can provide clean separations between snow, cloud and background; each of the 12 bands may contain redundant or unique features that are useful to classify the three classes. Including too many features, such as including all 12 bands, may easily lead to overfitting on training data for most machine-learning and deep-learning algorithms, especially when the training dataset is not large enough. Thus, identifying the optimal band combination that contains most of the informative features while also containing a few bands is essential. Our forward feature selection and backward feature selection both agreed B2 (blue) is the most important band to separate the three classes. The combination of B2, B11, B4 and B9 reserves almost all informative features among all 12 bands for separating the three classes. Therefore, our results provide guidance for selecting bands for the following studies aimed at developing better snow-mapping tools.\n\nRandom Forest as the representative traditional machine-learning algorithm provides much better classification performance than the threshold test-based method Sen2Cor, especially when feeding the RF model with the four informative bands or all 12 bands. However, all three RF models have the issue of \"salt-and-pepper\" noise on their classification masks [32]. This issue does not only reflect the high variance of spectral reluctance values of each band within the three classes but also reflects the limitations of the traditional machine-learning algorithms. Traditional machine-learning algorithms, such as RF, only use pixel-level information, i.e., the reflectance values of each band for the same pixel, to make class predictions. They failed to make use of the information from the surrounding pixels or the broad spatial information. In contrast, the convolutional neural network-based deep-learning models such as U-Net exploit the surrounding pixel information by convolution operations and take advantage of the broad spatial information by repeated convolution and pooling operations. Therefore, the U-Net models all bypass the \"salt-and-pepper\" issue and give even better classification performance than the RF models.\n\nThe important role of spatial information in distinguishing snow and cloud is further highlighted when comparing classification performance between U-Net RGB and RF RGB . The large overlaps of the reflectance distribution of the RGB bands between snow and cloud and the poor classification performance of RF RGB demonstrate that pixel-level information of only RGB bands contains very limited features that can be used to separate the three classes. In contrast, U-Net RGB , also only fed by RGB bands but incorporating spatial information by the neural networks, can achieve significant improvements in classification performance compared to RF RGB . This raises an interesting open question for future studies, i.e., with a larger training dataset and improved neural network algorithms, is it possible to build satisfactory models with inputs of only RGB bands?\n\nIn terms of practical applications, although we have demonstrated that the U-Net model fed with the four informative bands (RF 4bands ) achieved the best prediction performance in our test dataset, and is much better than Sen2Cor and RF models, we should acknowledge that the efficient execution of deep-learning models often requires advanced hardware (e.g., GPU) and higher computation demands, thus making it less convenient to implement than the threshold test-based methods. However, with technology development and algorithm evolution, the application of deep-learning models in satellite images will become mainstream in the future.\n\n\nConclusions and Future Work\n\nThis work investigates the problem of snow coverage mapping by learning from Sentinel-2 multispectral satellite images via conventional machine-and recent deep-learning methods. To this end, the largest (to our best knowledge) satellite image dataset for snow coverage mapping is first collected by downloading Sentinel-2 satellite images at different locations and times, followed by manual data labeling via a semi-automatic data labeling tool in QGIS. Then, both a random forest-based conventional machine-learning approach and a U-Net-based deep-learning approach are applied to the labeled dataset so that their performance can be compared. In addition, different band inputs are also compared including a RGB three-band image, selected four bands via feature selection, and full multispectral bands. It is shown that (1) both conventional machine-learning and recent deep-learning methods significantly outperform the existing rule-based Sen2Cor product for snow mapping; (2) U-Net generally outperforms the random forest since both spectral and spatial information is incorporated in U-Net; (3) the best spectral band combination for snow coverage mapping is B2, B11, B4 and B9, even outperforming all spectral band combinations.\n\nAlthough the results in this study are very encouraging, there is still much room for further development. For example, (1) in terms of data source, more labeled images from different locations and under diverse background conditions are required to generate a more representative dataset; (2) in terms of algorithm, the representative machine-learning algorithm (e.g., random forest, U-Net) are compared to obtain a baseline performance in this study, and more advanced deep-learning algorithms should be further considered/developed to further improve the performance; (3) in terms of practical application, a supervised learning approach is adopted in this study, and semi-supervised or even unsupervised algorithm should also be exploited so that the workload on data labeling can be significantly reduced.\n\nFigure 2 .Figure 3 .\n23U-Net architecture used in this study. The blue boxes represent different multi-channel feature maps, with the numbers on the top and left edge of the box indicate the number of channels and the feature size (width and height) separately. Each white box represents a copied feature map. The arrows with different colors denote different operations. The number of channels is denoted on the top of the box. Loss curves for training data (blue) and validation data (orange) in training process of (A) U-Net RGB , (B) U-Net 4bands and (C) U-Net 12bands . The dashed line indicates the epoch with smallest validation loss and the loss in the Y-axis represents the weighted cross-entropy.\n\nFigure 4 .\n4Geographical distribution of the 40 selected sites denoted by empty triangles, with different colors representing scenes obtained from different years, i.e., cyan, red and green denotes scenes dated from the years in 2019, 2020 and 2021.\n\nFigure 5 .\n5Visualization of all 40 scenes via RGB bands, with the above numbers being the scene captured date.BackgroundCloud Snow\n\nFigure 6 .\n6Labeled classification masks of all 40 collected scenes. The three target classes are represented by different colors: black denotes background, red denotes cloud and cyan denotes snow.\n\nFigure 7 .\n7Boxplots comparing the bottom of atmosphere corrected reflectance of 12 spectral bands from Sentinel-2 L2A products for background (white), cloud (red) and snow (cyan). Note: the outliers of each boxplot are not displayed.\n\nFeatureFigure 8 .\n8NDSI distribution of snow (cyan), cloud (red) and background (black) pixels, where the NDSI is defined as NDSI = (B3 \u2212 B12)/(B3 + B12)\n\nFigure 9 .\n9Feature selection. (A) Forward sequential feature selection, where the tick name of the x-axis means sequentially adding the specified bands into the inputs of the model. (B) Backward sequential feature selection, where the tick name of the x-axis means sequentially removing the specified bands from the inputs of the model.\n\nFigure 10 .Figure 11 .\n1011Classification performance comparisons for different models applied in a training dataset images (n = 34) based on (A) precision, (B) F1 score, (C) recall and (D) IoU. The bars with three different colors, i.e., violet, green and blue, represent models with input subset made up of RGB bands, informative four bands and all 12 bands, respectively. The bar without texture denotes random forest model, while the bar with diagonal texture symbols U-Net model. Note: the evaluation was performed on image level, therefore the validation dataset paths are also included. Classification performance comparisons for different models applied in testing dataset images (n = 6) based on (A) precision, (B) F1 score, (C) recall and (D) IoU. The bars with three different colors, i.e., violet, green and blue, represent models with input subset made up of RGB bands, informative four bands and all 12 bands respectively. The bar without texture denotes random forest model, while the bar with diagonal texture symbols U-Net model.\n\nTable 1 .\n1Product IDs and coordinates of the 40 collected scenes.Product ID \nX Coordinate Y Coordinate Date \n\n20190110T112439_20190110T112436_T30UVG \n\u22122.96 \n55.25 \n10 January 2019 \n20190113T113429_20190113T113432_T30UVG \n\u22124.09 \n55.17 \n13 January 2019 \n20190129T151649_20190129T151651_T20UNA \n\u221262.2 \n50.16 \n29 January 2019 \n20190205T055031_20190205T055025_T45VUE \n85.08 \n57.71 \n5 February 2019 \n20190205T164459_20190205T164614_T17ULA \n\u221282.53 \n54.86 \n5 February 2019 \n20190205T164459_20190205T164614_T17ULA \n\u221283.13 \n54.47 \n5 February 2019 \n20190727T035549_20190727T040012_T47SPT \n100.5 \n33.99 \n27 July 2019 \n20190730T040549_20190730T041756_T47SMS \n98.79 \n33.4 \n30 July 2019 \n20190730T040549_20190730T041756_T47SMT \n98.77 \n33.68 \n30 July 2019 \n20191121T062151_20191121T062148_T42UWV \n69.8 \n48.97 \n21 November 2019 \n20191124T005709_20191124T010446_T53HNE \n136.05 \n\u221231.79 \n24 November 2019 \n20191127T041109_20191127T041653_T47SMS \n98.95 \n33.38 \n27 November 2019 \n20191127T041109_20191127T041653_T47SPT \n100.16 \n34.18 \n27 November 2019 \n20200129T151641_20200129T151643_T20UNA \n\u221261.71 \n50.32 \n29 January 2020 \n20200215T054929_20200215T054925_T45VUE \n84.81 \n57.94 \n15 February 2020 \n20200701T095031_20200701T095034_T34VFN \n24.22 \n60.46 \n1 July 2020 \n20200724T142739_20200724T143750_T18FXK \n\u221272.5 \n\u221250.18 \n24 July 2020 \n20200729T142741_20200729T143445_T19GCR \n\u221270.2 \n\u221240.51 \n29 July 2020 \n20200801T182919_20200801T183807_T12VWH \n\u2212110.14 \n56.28 \n1 August 2020 \n20200805T085601_20200805T085807_T35UQS \n30.37 \n51.06 \n5 August 20205 \n20200810T141739_20200810T142950_T19GCP \n\u221270.8 \n\u221242.4 \n10 August 2020 \n20200817T084559_20200817T085235_T36UXD \n35.63 \n52.73 \n17 August 2020 \n20201113T005711_20201113T005712_T53JNF \n135.86 \n\u221231.65 \n13 November 2020 \n20201123T005711_20201123T010434_T53HNE \n135.79 \n\u221231.71 \n23 November 2020 \n20201206T041141_20201206T041138_T47SNT \n99.74 \n33.72 \n6 December 2020 \n20201222T111501_20201222T111456_T29RPQ \n\u22127.74 \n31.1 \n22 December 2020 \n20210105T050209_20210105T050811_T45SUR \n85.74 \n32.43 \n5 January 2021 \n20210110T182731_20210110T182953_T12TVR \n\u2212111.66 \n45.12 \n10 January 2021 \n20210207T151649_20210207T151817_T20UNA \n\u221262.05 \n50.29 \n7 February 2021 \n20210208T112129_20210208T112318_T30VWJ \n\u22121.57 \n57.14 \n8 February 2021 \n20210211T113319_20210211T113447_T30VWJ \n\u22122.02 \n57.31 \n11 February 2021 \n20210708T141051_20210708T142222_T20HLB \n\u221264.31 \n\u221239.38 \n8 July 2021 \n20210712T082611_20210712T084900_T34JBM \n18.11 \n\u221230.39 \n12 July 2021 \n20210724T081609_20210724T083856_T34HCJ \n19.67 \n\u221233.41 \n24 July 2021 \n20191123T111259_20191123T112151_T29RNQ \n\u22128.3 \n31.22 \n23 November 2019 \n20200713T103031_20200713T103026_T33VWH 15.31 \n60.46 \n13 July 2020 \n20200804T223709_20200804T223712_T59GLM \n169.47 \n\u221244.02 \n4 August 2020 \n20200805T001109_20200805T001647_T55GCN \n145.04 \n\u221242.94 \n5 August 2020 \n20201126T041121_20201126T041842_T47SNT \n100.09 \n33.93 \n26 November 2020 \n20210714T081609_20210714T083805_T34HCJ \n19.16 \n\u221233.13 \n14 July 2021 \n\n\n\nTable 2 .\n2Overall accuracy of different models.Overall Accuracy (%) \nSen2Cor \nRF \nU-Net \n\nRGB \n-\n69.06% \n87.72% \nInformative 4 bands \n-\n90.03% \n93.89% \nAll 12 bands \n58.06% \n89.65% \n93.21% \n\n\nData Availability Statement:The dataset in this study will be openly shared upon publication at https://github.com/yiluyucheng/SnowCoverage, accessed on 31 December 2021.Conflicts of Interest:The authors declare no conflict of interest.\nBulk Drag Predictions of Riparian Arundo donax Stands through UAV-Acquired Multispectral Images. G F C Lama, M Crimaldi, V Pasquino, R Padulano, G B Chirico, 10.3390/w13101333131333Lama, G.F.C.; Crimaldi, M.; Pasquino, V.; Padulano, R.; Chirico, G.B. Bulk Drag Predictions of Riparian Arundo donax Stands through UAV-Acquired Multispectral Images. Water 2021, 13, 1333. [CrossRef]\n\nDetermining characteristic vegetation areas by terrestrial laser scanning for floodplain flow modeling. J Jalonen, J J\u00e4rvel\u00e4, J P Virtanen, M Vaaja, M Kurkela, H Hyypp\u00e4, 10.3390/w70204207Jalonen, J.; J\u00e4rvel\u00e4, J.; Virtanen, J.P.; Vaaja, M.; Kurkela, M.; Hyypp\u00e4, H. Determining characteristic vegetation areas by terrestrial laser scanning for floodplain flow modeling. Water 2015, 7, 420-437. [CrossRef]\n\nRemote sensing for agricultural applications: A meta-review. M Weiss, F Jacob, G Duveiller, 10.1016/j.rse.2019.111402Remote Sens. Environ. 2020, 236, 111402. [CrossRefWeiss, M.; Jacob, F.; Duveiller, G. Remote sensing for agricultural applications: A meta-review. Remote Sens. Environ. 2020, 236, 111402. [CrossRef]\n\nMonitoring plant diseases and pests through remote sensing technology: A review. J Zhang, Y Huang, R Pu, P Gonzalez-Moreno, L Yuan, K Wu, W Huang, 10.1016/j.compag.2019.104943Comput. Electron. Agric. 165Zhang, J.; Huang, Y.; Pu, R.; Gonzalez-Moreno, P.; Yuan, L.; Wu, K.; Huang, W. Monitoring plant diseases and pests through remote sensing technology: A review. Comput. Electron. Agric. 2019, 165, 104943. [CrossRef]\n\nIr-UNet: Irregular Segmentation U-Shape Network for Wheat Yellow Rust Detection by UAV Multispectral Imagery. T Zhang, Z Xu, J Su, Z Yang, C Liu, W H Chen, J Li, 10.3390/rs13193892Remote Sens. 2021, 13, 3892. [CrossRefZhang, T.; Xu, Z.; Su, J.; Yang, Z.; Liu, C.; Chen, W.H.; Li, J. Ir-UNet: Irregular Segmentation U-Shape Network for Wheat Yellow Rust Detection by UAV Multispectral Imagery. Remote Sens. 2021, 13, 3892. [CrossRef]\n\nWheat yellow rust monitoring by learning from multispectral UAV aerial imagery. J Su, C Liu, M Coombes, X Hu, C Wang, X Xu, Q Li, L Guo, W H Chen, 10.1016/j.compag.2018.10.017Comput. Electron. Agric. 155Su, J.; Liu, C.; Coombes, M.; Hu, X.; Wang, C.; Xu, X.; Li, Q.; Guo, L.; Chen, W.H. Wheat yellow rust monitoring by learning from multispectral UAV aerial imagery. Comput. Electron. Agric. 2018, 155, 157-166. [CrossRef]\n\nAutomatic System for Crop Pest and Disease Dynamic Monitoring and Early Forecasting. Y Dong, F Xu, L Liu, X Du, B Ren, A Guo, Y Geng, C Ruan, H Ye, W Huang, 10.1109/JSTARS.2020.3013340IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 13Dong, Y.; Xu, F.; Liu, L.; Du, X.; Ren, B.; Guo, A.; Geng, Y.; Ruan, C.; Ye, H.; Huang, W.; et al. Automatic System for Crop Pest and Disease Dynamic Monitoring and Early Forecasting. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2020, 13, 4410-4418. [CrossRef]\n\nPredicting overwintering of wheat stripe rust in central and northwestern China. X Hu, S Cao, A Cornelius, X Xu, 10.1094/PDIS-06-19-1148-REPlant Dis. 104Hu, X.; Cao, S.; Cornelius, A.; Xu, X. Predicting overwintering of wheat stripe rust in central and northwestern China. Plant Dis. 2020, 104, 44-51. [CrossRef]\n\nPotential oversummering and overwintering regions for the wheat stripe rust pathogen in the contiguous United States. D Sharma-Poudyal, X Chen, R A Rupp, 10.1007/s00484-013-0683-6Int. J. Biometeorol. 58PubMedSharma-Poudyal, D.; Chen, X.; Rupp, R.A. Potential oversummering and overwintering regions for the wheat stripe rust pathogen in the contiguous United States. Int. J. Biometeorol. 2014, 58, 987-997. [CrossRef] [PubMed]\n\nSentinel-2: ESA's optical high-resolution mission for GMES operational services. M Drusch, U Bello, S Carlier, O Colin, V Fernandez, F Gascon, B Hoersch, C Isola, P Laberinti, P Martimort, 10.1016/j.rse.2011.11.026Remote Sens. Environ. 120Drusch, M.; Del Bello, U.; Carlier, S.; Colin, O.; Fernandez, V.; Gascon, F.; Hoersch, B.; Isola, C.; Laberinti, P.; Martimort, P.; et al. Sentinel-2: ESA's optical high-resolution mission for GMES operational services. Remote Sens. Environ. 2012, 120, 25-36. [CrossRef]\n\nImprovement and expansion of the Fmask algorithm: Cloud, cloud shadow, and snow detection for Landsats 4-7, 8, and Sentinel 2 images. Z Zhu, S Wang, C E Woodcock, 10.1016/j.rse.2014.12.014Remote Sens. Environ. 159Zhu, Z.; Wang, S.; Woodcock, C.E. Improvement and expansion of the Fmask algorithm: Cloud, cloud shadow, and snow detection for Landsats 4-7, 8, and Sentinel 2 images. Remote Sens. Environ. 2015, 159, 269-277. [CrossRef]\n\nAtmospheric and topographic correction (ATCOR theoretical background document). R Richter, D Schl\u00e4pfer, DLR IB 2019. 1Richter, R.; Schl\u00e4pfer, D. Atmospheric and topographic correction (ATCOR theoretical background document). DLR IB 2019, 1, 0564-03.\n\nSentinel-2 Sen2Cor: L2A processor for users. J Louis, V Debaecker, B Pflug, M Main-Knorn, J Bieniarz, U Mueller-Wilm, E Cadau, F Gascon, Proceedings of the Living Planet Symposium. the Living Planet SymposiumPrague, Czech Republic 9Louis, J.; Debaecker, V.; Pflug, B.; Main-Knorn, M.; Bieniarz, J.; Mueller-Wilm, U.; Cadau, E.; Gascon, F. Sentinel-2 Sen2Cor: L2A processor for users. In Proceedings of the Living Planet Symposium 2016, Prague, Czech Republic 9-13 May 2016; pp. 1-8.\n\nDeep learning based cloud detection for medium and high resolution remote sensing images of different sensors. Z Li, H Shen, Q Cheng, Y Liu, S You, Z He, 10.1016/j.isprsjprs.2019.02.017ISPRS J. Photogramm. Remote Sens. 150Li, Z.; Shen, H.; Cheng, Q.; Liu, Y.; You, S.; He, Z. Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors. ISPRS J. Photogramm. Remote Sens. 2019, 150, 197-212. [CrossRef]\n\nDeep learning for multi-modal classification of cloud, shadow and land cover scenes in PlanetScope and Sentinel-2 imagery. Y Shendryk, Y Rist, C Ticehurst, P Thorburn, 10.1016/j.isprsjprs.2019.08.018ISPRS J. Photogramm. Remote Sens. 157Shendryk, Y.; Rist, Y.; Ticehurst, C.; Thorburn, P. Deep learning for multi-modal classification of cloud, shadow and land cover scenes in PlanetScope and Sentinel-2 imagery. ISPRS J. Photogramm. Remote Sens. 2019, 157, 124-136. [CrossRef]\n\nReady-to-use methods for the detection of clouds, cirrus, snow, shadow, water and clear sky pixels in Sentinel-2 MSI images. A Hollstein, K Segl, L Guanter, M Brell, M Enesco, 10.3390/rs8080666Remote Sens. 2016, 8, 666. [CrossRefHollstein, A.; Segl, K.; Guanter, L.; Brell, M.; Enesco, M. Ready-to-use methods for the detection of clouds, cirrus, snow, shadow, water and clear sky pixels in Sentinel-2 MSI images. Remote Sens. 2016, 8, 666. [CrossRef]\n\nEuroSAT: A novel dataset and deep learning benchmark for land use and land cover classification. P Helber, B Bischke, A Dengel, D Borth, 10.1109/JSTARS.2019.2918242IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 12Helber, P.; Bischke, B.; Dengel, A.; Borth, D. EuroSAT: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2019, 12, 2217-2226. [CrossRef]\n\nAutomated cloud, cloud shadow, and snow detection in multitemporal Landsat data: An algorithm designed specifically for monitoring land cover change. Z Zhu, C E Woodcock, 10.1016/j.rse.2014.06.012Remote Sens. Environ. 152Zhu, Z.; Woodcock, C.E. Automated cloud, cloud shadow, and snow detection in multitemporal Landsat data: An algorithm designed specifically for monitoring land cover change. Remote Sens. Environ. 2014, 152, 217-234. [CrossRef]\n\nDistinguishing Cloud and Snow in Satellite Images via Deep Convolutional Network. Y Zhan, J Wang, J Shi, G Cheng, L Yao, W Sun, 10.1109/LGRS.2017.2735801IEEE Geosci. Remote Sens. Lett. 14Zhan, Y.; Wang, J.; Shi, J.; Cheng, G.; Yao, L.; Sun, W. Distinguishing Cloud and Snow in Satellite Images via Deep Convolutional Network. IEEE Geosci. Remote Sens. Lett. 2017, 14, 1785-1789. [CrossRef]\n\nValidation of copernicus Sentinel-2 cloud masks obtained from MAJA, Sen2Cor, and FMask processors using reference cloud masks generated with a supervised active learning procedure. L Baetens, C Desjardins, O Hagolle, 10.3390/rs11040433Remote Sens. 2019, 11, 433. [CrossRefBaetens, L.; Desjardins, C.; Hagolle, O. Validation of copernicus Sentinel-2 cloud masks obtained from MAJA, Sen2Cor, and FMask processors using reference cloud masks generated with a supervised active learning procedure. Remote Sens. 2019, 11, 433. [CrossRef]\n\nComparison of masking algorithms for sentinel-2 imagery. V Zekoll, M Main-Knorn, J Louis, D Frantz, R Richter, B Pflug, 10.3390/rs13010137Remote Sens. 2021, 13, 137. [CrossRefZekoll, V.; Main-Knorn, M.; Louis, J.; Frantz, D.; Richter, R.; Pflug, B. Comparison of masking algorithms for sentinel-2 imagery. Remote Sens. 2021, 13, 137. [CrossRef]\n\nDeep learning in remote sensing applications: A meta-analysis and review. L Ma, Y Liu, X Zhang, Y Ye, G Yin, B A Johnson, 10.1016/j.isprsjprs.2019.04.015ISPRS J. Photogramm. Remote Sens. 152Ma, L.; Liu, Y.; Zhang, X.; Ye, Y.; Yin, G.; Johnson, B.A. Deep learning in remote sensing applications: A meta-analysis and review. ISPRS J. Photogramm. Remote Sens. 2019, 152, 166-177. [CrossRef]\n\nWave height predictions in complex sea flows through soft-computing models: Case study of Persian Gulf. T Sadeghifar, G Lama, P Sihag, A Bayram, O Kisi, 10.1016/j.oceaneng.2021.110467Ocean Eng. 2022, 245, 110467. [CrossRefSadeghifar, T.; Lama, G.; Sihag, P.; Bayram, A.; Kisi, O. Wave height predictions in complex sea flows through soft-computing models: Case study of Persian Gulf. Ocean Eng. 2022, 245, 110467. [CrossRef]\n\nThe Copernicus Open Access Hub. 20The Copernicus Open Access Hub. Available online: https://scihub.copernicus.eu/ (accessed on 20 December 2021).\n\n. Usgs Eros Archive, 20USGS EROS Archive. Available online: https://www.usgs.gov/centers/eros/science/usgs-eros-archive-sentinel-2 (accessed on 20 December 2021).\n\nQGIS Association. 20QGIS Association. Available online: https://www.qgis.org/en/site//getinvolved/governance/charter/index.html (accessed on 20 December 2021).\n\nAutomatic Classification Plugin: A Python tool for the download and processing of remote sensing images in QGIS. L Congedo, Semi, 10.21105/joss.03172J. Open Source Softw. 63172Congedo, L. Semi-Automatic Classification Plugin: A Python tool for the download and processing of remote sensing images in QGIS. J. Open Source Softw. 2021, 6, 3172. [CrossRef]\n\nSen2Cor Configuration and User Manual. U M\u00fcller-Wilm, S2-PDGS-MPC-L2A-SUM-V2.4.0.pdf20M\u00fcller-Wilm, U. Sen2Cor Configuration and User Manual. Available online: https://step.esa.int/thirdparties/sen2cor/2.4.0/ Sen2Cor_240_Documenation_PDF/S2-PDGS-MPC-L2A-SUM-V2.4.0.pdf (accessed on 20 December 2021).\n\nScikit-learn: Machine Learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J. Mach. Learn. Res. 12Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.; Weiss, R.; Dubourg, V.; et al. Scikit-learn: Machine Learning in Python. J. Mach. Learn. Res. 2011, 12, 2825-2830.\n\nSpectral analysis and mapping of blackgrass weed by leveraging machine learning and UAV multispectral imagery. J Su, D Yi, M Coombes, C Liu, X Zhai, K Mcdonald-Maier, W H Chen, 10.1016/j.compag.2021.106621Comput. Electron. Agric. Su, J.; Yi, D.; Coombes, M.; Liu, C.; Zhai, X.; McDonald-Maier, K.; Chen, W.H. Spectral analysis and mapping of blackgrass weed by leveraging machine learning and UAV multispectral imagery. Comput. Electron. Agric. 2022, 192, 106621. [CrossRef]\n\nAerial visual perception in smart farming: Field study of wheat yellow rust monitoring. J Su, D Yi, B Su, Z Mi, C Liu, X Hu, X Xu, L Guo, W H Chen, 10.1109/TII.2020.2979237IEEE Trans. Ind. Inform. 17Su, J.; Yi, D.; Su, B.; Mi, Z.; Liu, C.; Hu, X.; Xu, X.; Guo, L.; Chen, W.H. Aerial visual perception in smart farming: Field study of wheat yellow rust monitoring. IEEE Trans. Ind. Inform. 2020, 17, 2242-2249. [CrossRef]\n\nOptimization: Open Source Constrained Global Optimization Tool for Python. F Nogueira, Bayesian, 20Nogueira, F. Bayesian Optimization: Open Source Constrained Global Optimization Tool for Python. Available online: https: //github.com/fmfn/BayesianOptimization (accessed on 20 December 2021).\n\nConvolutional Networks for Biomedical Image Segmentation. O Ronneberger, P Fischer, T Brox, U-Net, In Medical Image Computing and Computer-Assisted Intervention-MICCAI. Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.Springer International PublishingRonneberger, O.; Fischer, P.; Brox, T. U-Net: Convolutional Networks for Biomedical Image Segmentation. In Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015; Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F., Eds.; Springer International Publishing: Cham, Switzerland, 2015; pp. 234-241.\n\nPyTorch: An Imperative Style, High-Performance Deep Learning Library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsVancouver, BC, Canada32Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.; et al. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Proceedings of the Advances in Neural Information Processing Systems 32, Vancouver, BC, Canada, 8-14 December 2019.\n\nEstimating fractional snow cover from MODIS using the normalized difference snow index. Remote Sens. Environ. V V Salomonson, I Appel, 10.1016/j.rse.2003.10.01689Salomonson, V.V.; Appel, I. Estimating fractional snow cover from MODIS using the normalized difference snow index. Remote Sens. Environ. 2004, 89, 351-360. [CrossRef]\n\nDevelopment of the Aqua MODIS NDSI fractional snow cover algorithm and validation results. V V Salomonson, I Appel, 10.1109/TGRS.2006.876029IEEE Trans. Geosci. Remote Sens. 44Salomonson, V.V.; Appel, I. Development of the Aqua MODIS NDSI fractional snow cover algorithm and validation results. IEEE Trans. Geosci. Remote Sens. 2006, 44, 1747-1756. [CrossRef]\n", "annotations": {"author": "[{\"end\":147,\"start\":138},{\"end\":155,\"start\":148},{\"end\":165,\"start\":156},{\"end\":175,\"start\":166},{\"end\":182,\"start\":176},{\"end\":197,\"start\":183},{\"end\":293,\"start\":198},{\"end\":371,\"start\":294},{\"end\":474,\"start\":372}]", "publisher": null, "author_last_name": "[{\"end\":146,\"start\":142},{\"end\":154,\"start\":152},{\"end\":164,\"start\":160},{\"end\":174,\"start\":170},{\"end\":181,\"start\":178},{\"end\":196,\"start\":183}]", "author_first_name": "[{\"end\":139,\"start\":138},{\"end\":141,\"start\":140},{\"end\":149,\"start\":148},{\"end\":151,\"start\":150},{\"end\":157,\"start\":156},{\"end\":159,\"start\":158},{\"end\":167,\"start\":166},{\"end\":169,\"start\":168},{\"end\":177,\"start\":176}]", "author_affiliation": "[{\"end\":292,\"start\":199},{\"end\":370,\"start\":295},{\"end\":473,\"start\":373}]", "title": "[{\"end\":109,\"start\":1},{\"end\":583,\"start\":475}]", "venue": null, "abstract": null, "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1432,\"start\":1429},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1646,\"start\":1643},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1836,\"start\":1833},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1839,\"start\":1836},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1842,\"start\":1839},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1845,\"start\":1842},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2045,\"start\":2042},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2048,\"start\":2045},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2051,\"start\":2048},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2907,\"start\":2903},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2931,\"start\":2927},{\"end\":2960,\"start\":2956},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6394,\"start\":6390},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6572,\"start\":6568},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6700,\"start\":6696},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7829,\"start\":7825},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8427,\"start\":8423},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8807,\"start\":8803},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8972,\"start\":8968},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9293,\"start\":9289},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10766,\"start\":10762},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10829,\"start\":10826},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10832,\"start\":10829},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12437,\"start\":12433},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15816,\"start\":15812},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18010,\"start\":18006},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18013,\"start\":18010},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28567,\"start\":28563},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28831,\"start\":28827},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31495,\"start\":31491},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":35438,\"start\":35435}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36663,\"start\":35956},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36914,\"start\":36664},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37047,\"start\":36915},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37246,\"start\":37048},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37482,\"start\":37247},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37637,\"start\":37483},{\"attributes\":{\"id\":\"fig_6\"},\"end\":37976,\"start\":37638},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39024,\"start\":37977},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":41966,\"start\":39025},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":42160,\"start\":41967}]", "paragraph": "[{\"end\":1846,\"start\":728},{\"end\":2441,\"start\":1848},{\"end\":2542,\"start\":2467},{\"end\":2759,\"start\":2583},{\"end\":3740,\"start\":2790},{\"end\":4458,\"start\":3742},{\"end\":5261,\"start\":4460},{\"end\":6306,\"start\":5281},{\"end\":7081,\"start\":6308},{\"end\":8271,\"start\":7118},{\"end\":8706,\"start\":8331},{\"end\":9935,\"start\":8708},{\"end\":10653,\"start\":9937},{\"end\":11579,\"start\":10672},{\"end\":12327,\"start\":11581},{\"end\":13233,\"start\":12329},{\"end\":13467,\"start\":13255},{\"end\":13638,\"start\":13574},{\"end\":13801,\"start\":13680},{\"end\":14266,\"start\":13874},{\"end\":15633,\"start\":14325},{\"end\":16114,\"start\":15647},{\"end\":16880,\"start\":16143},{\"end\":18567,\"start\":16882},{\"end\":20597,\"start\":18596},{\"end\":21145,\"start\":20670},{\"end\":22222,\"start\":21147},{\"end\":23527,\"start\":22224},{\"end\":23923,\"start\":23595},{\"end\":24731,\"start\":23925},{\"end\":25472,\"start\":24783},{\"end\":27151,\"start\":25581},{\"end\":27919,\"start\":27153},{\"end\":29462,\"start\":27934},{\"end\":30072,\"start\":29464},{\"end\":31133,\"start\":30074},{\"end\":32368,\"start\":31135},{\"end\":33234,\"start\":32370},{\"end\":33875,\"start\":33236},{\"end\":35143,\"start\":33907},{\"end\":35955,\"start\":35145}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13573,\"start\":13468},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13679,\"start\":13639},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13873,\"start\":13802},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20599,\"start\":20598}]", "table_ref": "[{\"end\":14614,\"start\":14608},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25039,\"start\":25032}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":726,\"start\":714},{\"attributes\":{\"n\":\"2.\"},\"end\":2465,\"start\":2444},{\"attributes\":{\"n\":\"5.\"},\"end\":2581,\"start\":2545},{\"attributes\":{\"n\":\"2.1.\"},\"end\":2788,\"start\":2762},{\"attributes\":{\"n\":\"2.2.\"},\"end\":5279,\"start\":5264},{\"attributes\":{\"n\":\"2.3.\"},\"end\":7116,\"start\":7084},{\"attributes\":{\"n\":\"2.4.\"},\"end\":8329,\"start\":8274},{\"attributes\":{\"n\":\"2.5.\"},\"end\":10670,\"start\":10656},{\"attributes\":{\"n\":\"2.6.\"},\"end\":13253,\"start\":13236},{\"attributes\":{\"n\":\"3.\"},\"end\":14276,\"start\":14269},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14323,\"start\":14279},{\"attributes\":{\"n\":\"2019\"},\"end\":15645,\"start\":15636},{\"attributes\":{\"n\":\"3.2.\"},\"end\":16141,\"start\":16117},{\"attributes\":{\"n\":\"3.3.\"},\"end\":18594,\"start\":18570},{\"attributes\":{\"n\":\"3.4.\"},\"end\":20668,\"start\":20601},{\"attributes\":{\"n\":\"3.5.\"},\"end\":23593,\"start\":23530},{\"attributes\":{\"n\":\"3.6.\"},\"end\":24781,\"start\":24734},{\"end\":25481,\"start\":25475},{\"end\":25493,\"start\":25484},{\"end\":25515,\"start\":25496},{\"end\":25579,\"start\":25518},{\"attributes\":{\"n\":\"4.\"},\"end\":27932,\"start\":27922},{\"attributes\":{\"n\":\"5.\"},\"end\":33905,\"start\":33878},{\"end\":35977,\"start\":35957},{\"end\":36675,\"start\":36665},{\"end\":36926,\"start\":36916},{\"end\":37059,\"start\":37049},{\"end\":37258,\"start\":37248},{\"end\":37501,\"start\":37484},{\"end\":37649,\"start\":37639},{\"end\":38000,\"start\":37978},{\"end\":39035,\"start\":39026},{\"end\":41977,\"start\":41968}]", "table": "[{\"end\":41966,\"start\":39092},{\"end\":42160,\"start\":42016}]", "figure_caption": "[{\"end\":36663,\"start\":35980},{\"end\":36914,\"start\":36677},{\"end\":37047,\"start\":36928},{\"end\":37246,\"start\":37061},{\"end\":37482,\"start\":37260},{\"end\":37637,\"start\":37503},{\"end\":37976,\"start\":37651},{\"end\":39024,\"start\":38005},{\"end\":39092,\"start\":39037},{\"end\":42016,\"start\":41979}]", "figure_ref": "[{\"end\":2540,\"start\":2532},{\"end\":2635,\"start\":2627},{\"end\":11648,\"start\":11640},{\"end\":13231,\"start\":13223},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14520,\"start\":14512},{\"end\":15961,\"start\":15952},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16097,\"start\":16082},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16475,\"start\":16467},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":18268,\"start\":18260},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":19487,\"start\":19479},{\"end\":21309,\"start\":21273},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21663,\"start\":21653},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22693,\"start\":22684},{\"end\":23525,\"start\":23516},{\"end\":25195,\"start\":25186},{\"end\":25647,\"start\":25638},{\"end\":26868,\"start\":26858},{\"end\":27654,\"start\":27645},{\"end\":27813,\"start\":27803},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29859,\"start\":29849}]", "bib_author_first_name": "[{\"end\":42496,\"start\":42495},{\"end\":42500,\"start\":42497},{\"end\":42508,\"start\":42507},{\"end\":42520,\"start\":42519},{\"end\":42532,\"start\":42531},{\"end\":42544,\"start\":42543},{\"end\":42546,\"start\":42545},{\"end\":42885,\"start\":42884},{\"end\":42896,\"start\":42895},{\"end\":42907,\"start\":42906},{\"end\":42909,\"start\":42908},{\"end\":42921,\"start\":42920},{\"end\":42930,\"start\":42929},{\"end\":42941,\"start\":42940},{\"end\":43246,\"start\":43245},{\"end\":43255,\"start\":43254},{\"end\":43264,\"start\":43263},{\"end\":43583,\"start\":43582},{\"end\":43592,\"start\":43591},{\"end\":43601,\"start\":43600},{\"end\":43607,\"start\":43606},{\"end\":43626,\"start\":43625},{\"end\":43634,\"start\":43633},{\"end\":43640,\"start\":43639},{\"end\":44031,\"start\":44030},{\"end\":44040,\"start\":44039},{\"end\":44046,\"start\":44045},{\"end\":44052,\"start\":44051},{\"end\":44060,\"start\":44059},{\"end\":44067,\"start\":44066},{\"end\":44069,\"start\":44068},{\"end\":44077,\"start\":44076},{\"end\":44435,\"start\":44434},{\"end\":44441,\"start\":44440},{\"end\":44448,\"start\":44447},{\"end\":44459,\"start\":44458},{\"end\":44465,\"start\":44464},{\"end\":44473,\"start\":44472},{\"end\":44479,\"start\":44478},{\"end\":44485,\"start\":44484},{\"end\":44492,\"start\":44491},{\"end\":44494,\"start\":44493},{\"end\":44864,\"start\":44863},{\"end\":44872,\"start\":44871},{\"end\":44878,\"start\":44877},{\"end\":44885,\"start\":44884},{\"end\":44891,\"start\":44890},{\"end\":44898,\"start\":44897},{\"end\":44905,\"start\":44904},{\"end\":44913,\"start\":44912},{\"end\":44921,\"start\":44920},{\"end\":44927,\"start\":44926},{\"end\":45359,\"start\":45358},{\"end\":45365,\"start\":45364},{\"end\":45372,\"start\":45371},{\"end\":45385,\"start\":45384},{\"end\":45710,\"start\":45709},{\"end\":45728,\"start\":45727},{\"end\":45736,\"start\":45735},{\"end\":45738,\"start\":45737},{\"end\":46101,\"start\":46100},{\"end\":46111,\"start\":46110},{\"end\":46120,\"start\":46119},{\"end\":46131,\"start\":46130},{\"end\":46140,\"start\":46139},{\"end\":46153,\"start\":46152},{\"end\":46163,\"start\":46162},{\"end\":46174,\"start\":46173},{\"end\":46183,\"start\":46182},{\"end\":46196,\"start\":46195},{\"end\":46665,\"start\":46664},{\"end\":46672,\"start\":46671},{\"end\":46680,\"start\":46679},{\"end\":46682,\"start\":46681},{\"end\":47046,\"start\":47045},{\"end\":47057,\"start\":47056},{\"end\":47262,\"start\":47261},{\"end\":47271,\"start\":47270},{\"end\":47284,\"start\":47283},{\"end\":47293,\"start\":47292},{\"end\":47307,\"start\":47306},{\"end\":47319,\"start\":47318},{\"end\":47335,\"start\":47334},{\"end\":47344,\"start\":47343},{\"end\":47812,\"start\":47811},{\"end\":47818,\"start\":47817},{\"end\":47826,\"start\":47825},{\"end\":47835,\"start\":47834},{\"end\":47842,\"start\":47841},{\"end\":47849,\"start\":47848},{\"end\":48277,\"start\":48276},{\"end\":48289,\"start\":48288},{\"end\":48297,\"start\":48296},{\"end\":48310,\"start\":48309},{\"end\":48756,\"start\":48755},{\"end\":48769,\"start\":48768},{\"end\":48777,\"start\":48776},{\"end\":48788,\"start\":48787},{\"end\":48797,\"start\":48796},{\"end\":49181,\"start\":49180},{\"end\":49191,\"start\":49190},{\"end\":49202,\"start\":49201},{\"end\":49212,\"start\":49211},{\"end\":49673,\"start\":49672},{\"end\":49680,\"start\":49679},{\"end\":49682,\"start\":49681},{\"end\":50054,\"start\":50053},{\"end\":50062,\"start\":50061},{\"end\":50070,\"start\":50069},{\"end\":50077,\"start\":50076},{\"end\":50086,\"start\":50085},{\"end\":50093,\"start\":50092},{\"end\":50544,\"start\":50543},{\"end\":50555,\"start\":50554},{\"end\":50569,\"start\":50568},{\"end\":50954,\"start\":50953},{\"end\":50964,\"start\":50963},{\"end\":50978,\"start\":50977},{\"end\":50987,\"start\":50986},{\"end\":50997,\"start\":50996},{\"end\":51008,\"start\":51007},{\"end\":51317,\"start\":51316},{\"end\":51323,\"start\":51322},{\"end\":51330,\"start\":51329},{\"end\":51339,\"start\":51338},{\"end\":51345,\"start\":51344},{\"end\":51352,\"start\":51351},{\"end\":51354,\"start\":51353},{\"end\":51736,\"start\":51735},{\"end\":51750,\"start\":51749},{\"end\":51758,\"start\":51757},{\"end\":51767,\"start\":51766},{\"end\":51777,\"start\":51776},{\"end\":52643,\"start\":52642},{\"end\":52924,\"start\":52923},{\"end\":53228,\"start\":53227},{\"end\":53241,\"start\":53240},{\"end\":53254,\"start\":53253},{\"end\":53266,\"start\":53265},{\"end\":53276,\"start\":53275},{\"end\":53287,\"start\":53286},{\"end\":53297,\"start\":53296},{\"end\":53308,\"start\":53307},{\"end\":53324,\"start\":53323},{\"end\":53333,\"start\":53332},{\"end\":53706,\"start\":53705},{\"end\":53712,\"start\":53711},{\"end\":53718,\"start\":53717},{\"end\":53729,\"start\":53728},{\"end\":53736,\"start\":53735},{\"end\":53744,\"start\":53743},{\"end\":53762,\"start\":53761},{\"end\":53764,\"start\":53763},{\"end\":54159,\"start\":54158},{\"end\":54165,\"start\":54164},{\"end\":54171,\"start\":54170},{\"end\":54177,\"start\":54176},{\"end\":54183,\"start\":54182},{\"end\":54190,\"start\":54189},{\"end\":54196,\"start\":54195},{\"end\":54202,\"start\":54201},{\"end\":54209,\"start\":54208},{\"end\":54211,\"start\":54210},{\"end\":54568,\"start\":54567},{\"end\":54844,\"start\":54843},{\"end\":54859,\"start\":54858},{\"end\":54870,\"start\":54869},{\"end\":55421,\"start\":55420},{\"end\":55431,\"start\":55430},{\"end\":55440,\"start\":55439},{\"end\":55449,\"start\":55448},{\"end\":55458,\"start\":55457},{\"end\":55470,\"start\":55469},{\"end\":55480,\"start\":55479},{\"end\":55491,\"start\":55490},{\"end\":55498,\"start\":55497},{\"end\":55512,\"start\":55511},{\"end\":56096,\"start\":56095},{\"end\":56098,\"start\":56097},{\"end\":56112,\"start\":56111},{\"end\":56408,\"start\":56407},{\"end\":56410,\"start\":56409},{\"end\":56424,\"start\":56423}]", "bib_author_last_name": "[{\"end\":42505,\"start\":42501},{\"end\":42517,\"start\":42509},{\"end\":42529,\"start\":42521},{\"end\":42541,\"start\":42533},{\"end\":42554,\"start\":42547},{\"end\":42893,\"start\":42886},{\"end\":42904,\"start\":42897},{\"end\":42918,\"start\":42910},{\"end\":42927,\"start\":42922},{\"end\":42938,\"start\":42931},{\"end\":42948,\"start\":42942},{\"end\":43252,\"start\":43247},{\"end\":43261,\"start\":43256},{\"end\":43274,\"start\":43265},{\"end\":43589,\"start\":43584},{\"end\":43598,\"start\":43593},{\"end\":43604,\"start\":43602},{\"end\":43623,\"start\":43608},{\"end\":43631,\"start\":43627},{\"end\":43637,\"start\":43635},{\"end\":43646,\"start\":43641},{\"end\":44037,\"start\":44032},{\"end\":44043,\"start\":44041},{\"end\":44049,\"start\":44047},{\"end\":44057,\"start\":44053},{\"end\":44064,\"start\":44061},{\"end\":44074,\"start\":44070},{\"end\":44080,\"start\":44078},{\"end\":44438,\"start\":44436},{\"end\":44445,\"start\":44442},{\"end\":44456,\"start\":44449},{\"end\":44462,\"start\":44460},{\"end\":44470,\"start\":44466},{\"end\":44476,\"start\":44474},{\"end\":44482,\"start\":44480},{\"end\":44489,\"start\":44486},{\"end\":44499,\"start\":44495},{\"end\":44869,\"start\":44865},{\"end\":44875,\"start\":44873},{\"end\":44882,\"start\":44879},{\"end\":44888,\"start\":44886},{\"end\":44895,\"start\":44892},{\"end\":44902,\"start\":44899},{\"end\":44910,\"start\":44906},{\"end\":44918,\"start\":44914},{\"end\":44924,\"start\":44922},{\"end\":44933,\"start\":44928},{\"end\":45362,\"start\":45360},{\"end\":45369,\"start\":45366},{\"end\":45382,\"start\":45373},{\"end\":45388,\"start\":45386},{\"end\":45725,\"start\":45711},{\"end\":45733,\"start\":45729},{\"end\":45743,\"start\":45739},{\"end\":46108,\"start\":46102},{\"end\":46117,\"start\":46112},{\"end\":46128,\"start\":46121},{\"end\":46137,\"start\":46132},{\"end\":46150,\"start\":46141},{\"end\":46160,\"start\":46154},{\"end\":46171,\"start\":46164},{\"end\":46180,\"start\":46175},{\"end\":46193,\"start\":46184},{\"end\":46206,\"start\":46197},{\"end\":46669,\"start\":46666},{\"end\":46677,\"start\":46673},{\"end\":46691,\"start\":46683},{\"end\":47054,\"start\":47047},{\"end\":47067,\"start\":47058},{\"end\":47268,\"start\":47263},{\"end\":47281,\"start\":47272},{\"end\":47290,\"start\":47285},{\"end\":47304,\"start\":47294},{\"end\":47316,\"start\":47308},{\"end\":47332,\"start\":47320},{\"end\":47341,\"start\":47336},{\"end\":47351,\"start\":47345},{\"end\":47815,\"start\":47813},{\"end\":47823,\"start\":47819},{\"end\":47832,\"start\":47827},{\"end\":47839,\"start\":47836},{\"end\":47846,\"start\":47843},{\"end\":47852,\"start\":47850},{\"end\":48286,\"start\":48278},{\"end\":48294,\"start\":48290},{\"end\":48307,\"start\":48298},{\"end\":48319,\"start\":48311},{\"end\":48766,\"start\":48757},{\"end\":48774,\"start\":48770},{\"end\":48785,\"start\":48778},{\"end\":48794,\"start\":48789},{\"end\":48804,\"start\":48798},{\"end\":49188,\"start\":49182},{\"end\":49199,\"start\":49192},{\"end\":49209,\"start\":49203},{\"end\":49218,\"start\":49213},{\"end\":49677,\"start\":49674},{\"end\":49691,\"start\":49683},{\"end\":50059,\"start\":50055},{\"end\":50067,\"start\":50063},{\"end\":50074,\"start\":50071},{\"end\":50083,\"start\":50078},{\"end\":50090,\"start\":50087},{\"end\":50097,\"start\":50094},{\"end\":50552,\"start\":50545},{\"end\":50566,\"start\":50556},{\"end\":50577,\"start\":50570},{\"end\":50961,\"start\":50955},{\"end\":50975,\"start\":50965},{\"end\":50984,\"start\":50979},{\"end\":50994,\"start\":50988},{\"end\":51005,\"start\":50998},{\"end\":51014,\"start\":51009},{\"end\":51320,\"start\":51318},{\"end\":51327,\"start\":51324},{\"end\":51336,\"start\":51331},{\"end\":51342,\"start\":51340},{\"end\":51349,\"start\":51346},{\"end\":51362,\"start\":51355},{\"end\":51747,\"start\":51737},{\"end\":51755,\"start\":51751},{\"end\":51764,\"start\":51759},{\"end\":51774,\"start\":51768},{\"end\":51782,\"start\":51778},{\"end\":52223,\"start\":52206},{\"end\":52651,\"start\":52644},{\"end\":52657,\"start\":52653},{\"end\":52936,\"start\":52925},{\"end\":53238,\"start\":53229},{\"end\":53251,\"start\":53242},{\"end\":53263,\"start\":53255},{\"end\":53273,\"start\":53267},{\"end\":53284,\"start\":53277},{\"end\":53294,\"start\":53288},{\"end\":53305,\"start\":53298},{\"end\":53321,\"start\":53309},{\"end\":53330,\"start\":53325},{\"end\":53341,\"start\":53334},{\"end\":53709,\"start\":53707},{\"end\":53715,\"start\":53713},{\"end\":53726,\"start\":53719},{\"end\":53733,\"start\":53730},{\"end\":53741,\"start\":53737},{\"end\":53759,\"start\":53745},{\"end\":53769,\"start\":53765},{\"end\":54162,\"start\":54160},{\"end\":54168,\"start\":54166},{\"end\":54174,\"start\":54172},{\"end\":54180,\"start\":54178},{\"end\":54187,\"start\":54184},{\"end\":54193,\"start\":54191},{\"end\":54199,\"start\":54197},{\"end\":54206,\"start\":54203},{\"end\":54216,\"start\":54212},{\"end\":54577,\"start\":54569},{\"end\":54587,\"start\":54579},{\"end\":54856,\"start\":54845},{\"end\":54867,\"start\":54860},{\"end\":54875,\"start\":54871},{\"end\":54882,\"start\":54877},{\"end\":55428,\"start\":55422},{\"end\":55437,\"start\":55432},{\"end\":55446,\"start\":55441},{\"end\":55455,\"start\":55450},{\"end\":55467,\"start\":55459},{\"end\":55477,\"start\":55471},{\"end\":55488,\"start\":55481},{\"end\":55495,\"start\":55492},{\"end\":55509,\"start\":55499},{\"end\":55519,\"start\":55513},{\"end\":56109,\"start\":56099},{\"end\":56118,\"start\":56113},{\"end\":56421,\"start\":56411},{\"end\":56430,\"start\":56425}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.3390/w13101333\",\"id\":\"b0\"},\"end\":42778,\"start\":42398},{\"attributes\":{\"doi\":\"10.3390/w7020420\",\"id\":\"b1\"},\"end\":43182,\"start\":42780},{\"attributes\":{\"doi\":\"10.1016/j.rse.2019.111402\",\"id\":\"b2\"},\"end\":43499,\"start\":43184},{\"attributes\":{\"doi\":\"10.1016/j.compag.2019.104943\",\"id\":\"b3\",\"matched_paper_id\":202101773},\"end\":43918,\"start\":43501},{\"attributes\":{\"doi\":\"10.3390/rs13193892\",\"id\":\"b4\"},\"end\":44352,\"start\":43920},{\"attributes\":{\"doi\":\"10.1016/j.compag.2018.10.017\",\"id\":\"b5\",\"matched_paper_id\":53749851},\"end\":44776,\"start\":44354},{\"attributes\":{\"doi\":\"10.1109/JSTARS.2020.3013340\",\"id\":\"b6\",\"matched_paper_id\":221161417},\"end\":45275,\"start\":44778},{\"attributes\":{\"doi\":\"10.1094/PDIS-06-19-1148-RE\",\"id\":\"b7\",\"matched_paper_id\":202022000},\"end\":45589,\"start\":45277},{\"attributes\":{\"doi\":\"10.1007/s00484-013-0683-6\",\"id\":\"b8\",\"matched_paper_id\":33968271},\"end\":46017,\"start\":45591},{\"attributes\":{\"doi\":\"10.1016/j.rse.2011.11.026\",\"id\":\"b9\",\"matched_paper_id\":129494315},\"end\":46528,\"start\":46019},{\"attributes\":{\"doi\":\"10.1016/j.rse.2014.12.014\",\"id\":\"b10\",\"matched_paper_id\":129762270},\"end\":46963,\"start\":46530},{\"attributes\":{\"id\":\"b11\"},\"end\":47214,\"start\":46965},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":114110450},\"end\":47698,\"start\":47216},{\"attributes\":{\"doi\":\"10.1016/j.isprsjprs.2019.02.017\",\"id\":\"b13\",\"matched_paper_id\":58007019},\"end\":48151,\"start\":47700},{\"attributes\":{\"doi\":\"10.1016/j.isprsjprs.2019.08.018\",\"id\":\"b14\",\"matched_paper_id\":204200645},\"end\":48628,\"start\":48153},{\"attributes\":{\"doi\":\"10.3390/rs8080666\",\"id\":\"b15\"},\"end\":49081,\"start\":48630},{\"attributes\":{\"doi\":\"10.1109/JSTARS.2019.2918242\",\"id\":\"b16\",\"matched_paper_id\":11810992},\"end\":49520,\"start\":49083},{\"attributes\":{\"doi\":\"10.1016/j.rse.2014.06.012\",\"id\":\"b17\",\"matched_paper_id\":128475301},\"end\":49969,\"start\":49522},{\"attributes\":{\"doi\":\"10.1109/LGRS.2017.2735801\",\"id\":\"b18\",\"matched_paper_id\":11701463},\"end\":50360,\"start\":49971},{\"attributes\":{\"doi\":\"10.3390/rs11040433\",\"id\":\"b19\"},\"end\":50894,\"start\":50362},{\"attributes\":{\"doi\":\"10.3390/rs13010137\",\"id\":\"b20\"},\"end\":51240,\"start\":50896},{\"attributes\":{\"doi\":\"10.1016/j.isprsjprs.2019.04.015\",\"id\":\"b21\",\"matched_paper_id\":149822779},\"end\":51629,\"start\":51242},{\"attributes\":{\"doi\":\"10.1016/j.oceaneng.2021.110467\",\"id\":\"b22\"},\"end\":52055,\"start\":51631},{\"attributes\":{\"id\":\"b23\"},\"end\":52202,\"start\":52057},{\"attributes\":{\"id\":\"b24\"},\"end\":52366,\"start\":52204},{\"attributes\":{\"id\":\"b25\"},\"end\":52527,\"start\":52368},{\"attributes\":{\"doi\":\"10.21105/joss.03172\",\"id\":\"b26\",\"matched_paper_id\":237413293},\"end\":52882,\"start\":52529},{\"attributes\":{\"doi\":\"S2-PDGS-MPC-L2A-SUM-V2.4.0.pdf\",\"id\":\"b27\"},\"end\":53183,\"start\":52884},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":10659969},\"end\":53592,\"start\":53185},{\"attributes\":{\"doi\":\"10.1016/j.compag.2021.106621\",\"id\":\"b29\",\"matched_paper_id\":245218123},\"end\":54068,\"start\":53594},{\"attributes\":{\"doi\":\"10.1109/TII.2020.2979237\",\"id\":\"b30\",\"matched_paper_id\":214703258},\"end\":54490,\"start\":54070},{\"attributes\":{\"id\":\"b31\"},\"end\":54783,\"start\":54492},{\"attributes\":{\"id\":\"b32\"},\"end\":55348,\"start\":54785},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":202786778},\"end\":55983,\"start\":55350},{\"attributes\":{\"doi\":\"10.1016/j.rse.2003.10.016\",\"id\":\"b34\"},\"end\":56314,\"start\":55985},{\"attributes\":{\"doi\":\"10.1109/TGRS.2006.876029\",\"id\":\"b35\",\"matched_paper_id\":6476200},\"end\":56674,\"start\":56316}]", "bib_title": "[{\"end\":43580,\"start\":43501},{\"end\":44432,\"start\":44354},{\"end\":44861,\"start\":44778},{\"end\":45356,\"start\":45277},{\"end\":45707,\"start\":45591},{\"end\":46098,\"start\":46019},{\"end\":46662,\"start\":46530},{\"end\":47043,\"start\":46965},{\"end\":47259,\"start\":47216},{\"end\":47809,\"start\":47700},{\"end\":48274,\"start\":48153},{\"end\":49178,\"start\":49083},{\"end\":49670,\"start\":49522},{\"end\":50051,\"start\":49971},{\"end\":51314,\"start\":51242},{\"end\":52640,\"start\":52529},{\"end\":53225,\"start\":53185},{\"end\":53703,\"start\":53594},{\"end\":54156,\"start\":54070},{\"end\":54841,\"start\":54785},{\"end\":55418,\"start\":55350},{\"end\":56405,\"start\":56316}]", "bib_author": "[{\"end\":42507,\"start\":42495},{\"end\":42519,\"start\":42507},{\"end\":42531,\"start\":42519},{\"end\":42543,\"start\":42531},{\"end\":42556,\"start\":42543},{\"end\":42895,\"start\":42884},{\"end\":42906,\"start\":42895},{\"end\":42920,\"start\":42906},{\"end\":42929,\"start\":42920},{\"end\":42940,\"start\":42929},{\"end\":42950,\"start\":42940},{\"end\":43254,\"start\":43245},{\"end\":43263,\"start\":43254},{\"end\":43276,\"start\":43263},{\"end\":43591,\"start\":43582},{\"end\":43600,\"start\":43591},{\"end\":43606,\"start\":43600},{\"end\":43625,\"start\":43606},{\"end\":43633,\"start\":43625},{\"end\":43639,\"start\":43633},{\"end\":43648,\"start\":43639},{\"end\":44039,\"start\":44030},{\"end\":44045,\"start\":44039},{\"end\":44051,\"start\":44045},{\"end\":44059,\"start\":44051},{\"end\":44066,\"start\":44059},{\"end\":44076,\"start\":44066},{\"end\":44082,\"start\":44076},{\"end\":44440,\"start\":44434},{\"end\":44447,\"start\":44440},{\"end\":44458,\"start\":44447},{\"end\":44464,\"start\":44458},{\"end\":44472,\"start\":44464},{\"end\":44478,\"start\":44472},{\"end\":44484,\"start\":44478},{\"end\":44491,\"start\":44484},{\"end\":44501,\"start\":44491},{\"end\":44871,\"start\":44863},{\"end\":44877,\"start\":44871},{\"end\":44884,\"start\":44877},{\"end\":44890,\"start\":44884},{\"end\":44897,\"start\":44890},{\"end\":44904,\"start\":44897},{\"end\":44912,\"start\":44904},{\"end\":44920,\"start\":44912},{\"end\":44926,\"start\":44920},{\"end\":44935,\"start\":44926},{\"end\":45364,\"start\":45358},{\"end\":45371,\"start\":45364},{\"end\":45384,\"start\":45371},{\"end\":45390,\"start\":45384},{\"end\":45727,\"start\":45709},{\"end\":45735,\"start\":45727},{\"end\":45745,\"start\":45735},{\"end\":46110,\"start\":46100},{\"end\":46119,\"start\":46110},{\"end\":46130,\"start\":46119},{\"end\":46139,\"start\":46130},{\"end\":46152,\"start\":46139},{\"end\":46162,\"start\":46152},{\"end\":46173,\"start\":46162},{\"end\":46182,\"start\":46173},{\"end\":46195,\"start\":46182},{\"end\":46208,\"start\":46195},{\"end\":46671,\"start\":46664},{\"end\":46679,\"start\":46671},{\"end\":46693,\"start\":46679},{\"end\":47056,\"start\":47045},{\"end\":47069,\"start\":47056},{\"end\":47270,\"start\":47261},{\"end\":47283,\"start\":47270},{\"end\":47292,\"start\":47283},{\"end\":47306,\"start\":47292},{\"end\":47318,\"start\":47306},{\"end\":47334,\"start\":47318},{\"end\":47343,\"start\":47334},{\"end\":47353,\"start\":47343},{\"end\":47817,\"start\":47811},{\"end\":47825,\"start\":47817},{\"end\":47834,\"start\":47825},{\"end\":47841,\"start\":47834},{\"end\":47848,\"start\":47841},{\"end\":47854,\"start\":47848},{\"end\":48288,\"start\":48276},{\"end\":48296,\"start\":48288},{\"end\":48309,\"start\":48296},{\"end\":48321,\"start\":48309},{\"end\":48768,\"start\":48755},{\"end\":48776,\"start\":48768},{\"end\":48787,\"start\":48776},{\"end\":48796,\"start\":48787},{\"end\":48806,\"start\":48796},{\"end\":49190,\"start\":49180},{\"end\":49201,\"start\":49190},{\"end\":49211,\"start\":49201},{\"end\":49220,\"start\":49211},{\"end\":49679,\"start\":49672},{\"end\":49693,\"start\":49679},{\"end\":50061,\"start\":50053},{\"end\":50069,\"start\":50061},{\"end\":50076,\"start\":50069},{\"end\":50085,\"start\":50076},{\"end\":50092,\"start\":50085},{\"end\":50099,\"start\":50092},{\"end\":50554,\"start\":50543},{\"end\":50568,\"start\":50554},{\"end\":50579,\"start\":50568},{\"end\":50963,\"start\":50953},{\"end\":50977,\"start\":50963},{\"end\":50986,\"start\":50977},{\"end\":50996,\"start\":50986},{\"end\":51007,\"start\":50996},{\"end\":51016,\"start\":51007},{\"end\":51322,\"start\":51316},{\"end\":51329,\"start\":51322},{\"end\":51338,\"start\":51329},{\"end\":51344,\"start\":51338},{\"end\":51351,\"start\":51344},{\"end\":51364,\"start\":51351},{\"end\":51749,\"start\":51735},{\"end\":51757,\"start\":51749},{\"end\":51766,\"start\":51757},{\"end\":51776,\"start\":51766},{\"end\":51784,\"start\":51776},{\"end\":52225,\"start\":52206},{\"end\":52653,\"start\":52642},{\"end\":52659,\"start\":52653},{\"end\":52938,\"start\":52923},{\"end\":53240,\"start\":53227},{\"end\":53253,\"start\":53240},{\"end\":53265,\"start\":53253},{\"end\":53275,\"start\":53265},{\"end\":53286,\"start\":53275},{\"end\":53296,\"start\":53286},{\"end\":53307,\"start\":53296},{\"end\":53323,\"start\":53307},{\"end\":53332,\"start\":53323},{\"end\":53343,\"start\":53332},{\"end\":53711,\"start\":53705},{\"end\":53717,\"start\":53711},{\"end\":53728,\"start\":53717},{\"end\":53735,\"start\":53728},{\"end\":53743,\"start\":53735},{\"end\":53761,\"start\":53743},{\"end\":53771,\"start\":53761},{\"end\":54164,\"start\":54158},{\"end\":54170,\"start\":54164},{\"end\":54176,\"start\":54170},{\"end\":54182,\"start\":54176},{\"end\":54189,\"start\":54182},{\"end\":54195,\"start\":54189},{\"end\":54201,\"start\":54195},{\"end\":54208,\"start\":54201},{\"end\":54218,\"start\":54208},{\"end\":54579,\"start\":54567},{\"end\":54589,\"start\":54579},{\"end\":54858,\"start\":54843},{\"end\":54869,\"start\":54858},{\"end\":54877,\"start\":54869},{\"end\":54884,\"start\":54877},{\"end\":55430,\"start\":55420},{\"end\":55439,\"start\":55430},{\"end\":55448,\"start\":55439},{\"end\":55457,\"start\":55448},{\"end\":55469,\"start\":55457},{\"end\":55479,\"start\":55469},{\"end\":55490,\"start\":55479},{\"end\":55497,\"start\":55490},{\"end\":55511,\"start\":55497},{\"end\":55521,\"start\":55511},{\"end\":56111,\"start\":56095},{\"end\":56120,\"start\":56111},{\"end\":56423,\"start\":56407},{\"end\":56432,\"start\":56423}]", "bib_venue": "[{\"end\":42493,\"start\":42398},{\"end\":42882,\"start\":42780},{\"end\":43243,\"start\":43184},{\"end\":43699,\"start\":43676},{\"end\":44028,\"start\":43920},{\"end\":44552,\"start\":44529},{\"end\":45008,\"start\":44962},{\"end\":45425,\"start\":45416},{\"end\":45789,\"start\":45770},{\"end\":46253,\"start\":46233},{\"end\":46738,\"start\":46718},{\"end\":47080,\"start\":47069},{\"end\":47395,\"start\":47353},{\"end\":47917,\"start\":47885},{\"end\":48384,\"start\":48352},{\"end\":48753,\"start\":48630},{\"end\":49293,\"start\":49247},{\"end\":49738,\"start\":49718},{\"end\":50154,\"start\":50124},{\"end\":50541,\"start\":50362},{\"end\":50951,\"start\":50896},{\"end\":51427,\"start\":51395},{\"end\":51733,\"start\":51631},{\"end\":52087,\"start\":52057},{\"end\":52384,\"start\":52368},{\"end\":52698,\"start\":52678},{\"end\":52921,\"start\":52884},{\"end\":53362,\"start\":53343},{\"end\":53822,\"start\":53799},{\"end\":54265,\"start\":54242},{\"end\":54565,\"start\":54492},{\"end\":54952,\"start\":54884},{\"end\":55589,\"start\":55521},{\"end\":56093,\"start\":55985},{\"end\":56487,\"start\":56456},{\"end\":47448,\"start\":47397},{\"end\":55665,\"start\":55591}]"}}}, "year": 2023, "month": 12, "day": 17}