{"id": 255440689, "updated": "2023-10-05 05:26:30.561", "metadata": {"title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval", "authors": "[{\"first\":\"Vitor\",\"last\":\"Jeronymo\",\"middle\":[]},{\"first\":\"Luiz\",\"last\":\"Bonifacio\",\"middle\":[]},{\"first\":\"Hugo\",\"last\":\"Abonizio\",\"middle\":[]},{\"first\":\"Marzieh\",\"last\":\"Fadaee\",\"middle\":[]},{\"first\":\"Roberto\",\"last\":\"Lotufo\",\"middle\":[]},{\"first\":\"Jakub\",\"last\":\"Zavrel\",\"middle\":[]},{\"first\":\"Rodrigo\",\"last\":\"Nogueira\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2301.01820", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2301-01820", "doi": "10.48550/arxiv.2301.01820"}}, "content": {"source": {"pdf_hash": "76427fe94e4564fd5df2177bb259d93527fddca5", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2301.01820v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ce85ab3bfcf3028d14cf6148f5a6efa89f29c7b6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/76427fe94e4564fd5df2177bb259d93527fddca5.txt", "contents": "\nInPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval\n26 May 2023\n\nJakub Zavrel \nFEEC-UNICAMP\nVitor Jeronymo NeuralMind\nBrazil, Brazil\n\nFEEC-UNICAMP\nLuiz Bonifacio NeuralMind\nBrazil, Brazil\n\nFEEC-UNICAMP\nMarzieh Fadaee Zeta Alpha\nHugo Abonizio NeuralMind\nBrazil, Brazil\n\nNetherlands Roberto Lotufo NeuralMind\nBrazil\n\nFEEC-UNICAMP\nFEEC-UNICAMP\nNetherlands Rodrigo Nogueira NeuralMindBrazil, Brazil, Brazil Zeta Alpha, Netherlands\n\nInPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval\n26 May 2023\nRecently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https\n\nIntroduction and Background\n\nData augmentation has been a reliable tool to improve the effectiveness of AI models in the face of the scarcity of high-quality in-domain training data, which is a common problem in practical applications. Previous work by Bonifacio et al. [1] and Dai et al. [2] successfully leveraged the few-shot capabilities of LLMs to generate reliable synthetic training data for information retrieval models. These training data helped their models achieve state-of-the-art (SOTA) results on the BEIR benchmark [6].\n\nBonifacio et al. [1] propose InPars where they generate queries from documents in the corpus using LLMs. Similarly to Bonifacio et al. [1], the recently published Promptagator [2] model also feeds prompts to LLMs in order to generate alternative queries for a given document in an unsupervised manner. It differs primarily from InPars in that it uses dataset-specific prompts, a larger LLM to generate queries, and a fully trainable retrieval pipeline with smaller models.\n\nThis work extends the method of Bonifacio et al. [1] by using a reranker as a filtering mechanism to select the best synthetically generated examples and further improving retrieval effectiveness on BEIR. We also use an open-source query generator as opposed to the proprietary one used by Bonifacio et al. and provide the source code and data to reproduce our results on TPUs. We refer to Bonifacio et al. [1] model as Inpars-v1 and the model presented in this paper as Inpars-v2.\n\n\nMethodology\n\nIn this section, we explain the experiments we performed and how they differ from InPars-v1 [1].\n\nTo generate synthetic queries, we use the open-source GPT-J [8] with 6B parameters to replace OpenAI's curie model used in InPars-v1. For each dataset in the BEIR benchmark, we sample 100k documents from its corpus and generate one synthetic query per document using GPT-J prompted with 3 examples from MS MARCO. We use greedy decoding and the \"gbq\" prompt template from InPars-v1. Some corpora in BEIR such as ArguAna [7] have less than 100k documents. In these cases, we generate as many synthetic queries as there are documents in the corpus. It takes on average 30 hours on an A100 GPU to generate 100k queries.\n\nOnce the synthetic queries are generated, we apply a filtering step to select query-document pairs that are more likely to be relevant to each other. In InPars-v1, this filtering step consisted of selecting the top 10k query-document pairs with the highest log probabilities of generating a query given the 3-shot examples and the document as input. In InPars-v2, we use monoT5-3B [4] already finetuned on MS MARCO for one epoch 1 to estimate a relevancy score for each of the 100k query-document pairs. Then, we keep only the top 10k pairs with the highest scores as our positive query-document pairs for training. It takes approximately 1.5 hours to score 100k query-document pairs on a TPU v3-8. It should take twice as much on a A100.\n\nTo obtain negatives (i.e., non-relevant) query-document pairs, we randomly sample one document from the top 1000 retrieved by BM25 when issued the synthetic query. Thus, our training set consists of 10k positive query-document pairs and 10k negative query-document pairs.\n\nThe rerankers are finetuned in the same manner as in InPars-v1: monoT5-3B is finetuned on MS MARCO for one epoch and then further finetuned for one epoch on the synthetic data. We use the Adafactor optimizer [5] with a constant learning rate of 1e-3. Each batch has 64 positive and 64 negative query-document pairs randomly sampled from the training dataset. We finetune one model on each synthetic dataset from BEIR, that is, we end up with 18 different rerankers, one per dataset, which are then evaluated on the corresponding test sets. Finetuning on each synthetic dataset takes less than 10 minutes on a TPU v3-8.\n\nEvaluation is performed using the following pipeline: first we use Pyserini's [3] flat indexes 2 to retrieve a thousand documents for each query using BM25 with default parameters (k1=0.9, b=0.4), for each dataset. Then we use the finetuned monoT5-3B models to rerank these documents. Table 1 presents results for BM25 (2nd column), monoT5-3B finetuned on MS MARCO (3rd column), monoT5-3b finetuned on MS MARCO and further finetuned on InPars-v1 (4th column), and monoT5-3B finetuned on MS MARCO and then finetuned on InPars-v2 data (5th column). Compared to InPars-v1, our approach is substantially better on TREC-News, Climate-FEVER, Robust and Touche. Additionally, we compare our method with Promptagator [2] and RankT5 [10]. Taking into account the average of all BEIR datasets, these results represent a new state of the art on BEIR.\n\n\nResults\n\nPromptagator and RankT5 strive on datasets that monoT5 and InPars-v2 cannot even surpass BM25, such as Touche and ArguAna. Note that these datasets focus on argument retrieval, which is slightly different from other datasets in the BEIR benchmark. As a result, they benefit from using custom prompts. 3 Promptagator does this without using supervised data from MS MARCO and using smaller T5 models with 110M parameters for the retrieval and reranking steps.  Promptagator uses a proprietary model, FLAN [9], to generate synthetic queries. The RankT5 model is a modified version of the monoT5 reranker, but its checkpoint and code are not published. In this work, we make the code, models, and data open-source and publicly available.\n\n\nConclusion\n\nIn this work, we presented InPars-v2, an improved version of InPars [1] that uses a publicly available language model to generate queries and a better query-document pair selection process. Our results show that we achieve effectiveness on par with the state of the art on BEIR. The synthetic data and finetuned models were publicly released.\n\nBM25\n\n\n\nTable 1: nDCG@10 on BEIR. \"Avg PrGator\" is the average of datasets reported by Promptagator.monoT5-3B \nPrGator RankT5 \nMARCO +InPars-v1 +InPars-v2 \n\nTREC-Covid \n0.594 \n0.801 \n0.846 \n0.846 \n0.762 \n0.823 \nRobust \n0.407 \n0.615 \n0.610 \n0.632 \n-\n-\nFiQA \n0.236 \n0.509 \n0.492 \n0.509 \n0.494 \n0.493 \nDBPedia \n0.318 \n0.472 \n0.494 \n0.498 \n0.434 \n0.459 \nSciDocs \n0.149 \n0.197 \n0.206 \n0.208 \n0.201 \n0.191 \nSciFact \n0.678 \n0.774 \n0.774 \n0.774 \n0.731 \n0.760 \nNFCorpus \n0.321 \n0.383 \n0.385 \n0.385 \n0.370 \n0.399 \nBioASQ \n0.522 \n0.566 \n0.607 \n0.595 \n-\n0.579 \nNatural Questions 0.305 \n0.625 \n0.625 \n0.638 \n-\n0.647 \nHotpotQA \n0.633 \n0.760 \n0.790 \n0.791 \n0.736 \n0.753 \nTREC-News \n0.395 \n0.477 \n0.458 \n0.490 \n-\n-\nQuora \n0.788 \n0.835 \n0.874 \n0.845 \n-\n0.819 \nFEVER \n0.651 \n0.848 \n0.852 \n0.872 \n0.866 \n0.848 \nClimate-FEVER \n0.165 \n0.288 \n0.287 \n0.323 \n0.241 \n0.275 \nSignal \n0.328 \n0.302 \n0.319 \n0.308 \n-\n0.319 \nArguAna \n0.397 \n0.379 \n0.371 \n0.369 \n0.630 \n0.406 \nTouche \n0.442 \n0.309 \n0.260 \n0.291 \n0.381 \n0.486 \nCQADupstack \n0.302 \n0.449 \n0.449 \n0.448 \n-\n-\n\nAvg \n0.424 \n0.533 \n0.539 \n0.545 \n-\n-\nAvg PrGator \n0.417 \n0.520 \n0.523 \n0.533 \n0.531 \n0.536 \n\n\nPreprint.\nhttps://huggingface.co/castorini/monot5-3b-msmarco-10k 2 As opposed to the multifield index.3 In preliminary experiments, we also observed an improvement of more than 10 nDCG@10 points on ArguAna by using a dataset-specific prompt to generate synthetic queries. More details and results on the full BEIR benchmark will appear in an upcoming paper.\nAcknowledgmentsThis research was partially supported by Funda\u00e7\u00e3o de Amparo \u00e0 Pesquisa do Estado de S\u00e3o Paulo (FAPESP) (project id 2022/01640-2). We also thank Centro Nacional de Processamento de Alto Desempenho (CENAPAD-SP) and Google Cloud for computing credits.\nInpars: Data augmentation for information retrieval using large language models. L Bonifacio, H Abonizio, M Fadaee, R Nogueira, arXiv:2202.05144arXiv preprintL. Bonifacio, H. Abonizio, M. Fadaee, and R. Nogueira. Inpars: Data augmentation for infor- mation retrieval using large language models. arXiv preprint arXiv:2202.05144, 2022.\n\nZ Dai, V Y Zhao, J Ma, Y Luan, J Ni, J Lu, A Bakalov, K Guu, K B Hall, M.-W Chang, arXiv:2209.11755Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprintZ. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu, A. Bakalov, K. Guu, K. B. Hall, and M.-W. Chang. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755, 2022.\n\nPyserini: An easy-to-use python toolkit to support replicable ir research with sparse and dense representations. J Lin, X Ma, S.-C Lin, J.-H Yang, R Pradeep, R Nogueira, arXiv:2102.10073arXiv preprintJ. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. Nogueira. Pyserini: An easy-to-use python toolkit to support replicable ir research with sparse and dense representations. arXiv preprint arXiv:2102.10073, 2021.\n\nDocument ranking with a pretrained sequenceto-sequence model. R Nogueira, Z Jiang, R Pradeep, J Lin, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. the 2020 Conference on Empirical Methods in Natural Language Processing: FindingsR. Nogueira, Z. Jiang, R. Pradeep, and J. Lin. Document ranking with a pretrained sequence- to-sequence model. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 708-718, 2020.\n\nAdafactor: Adaptive learning rates with sublinear memory cost. N Shazeer, M Stern, International Conference on Machine Learning. PMLRN. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596-4604. PMLR, 2018.\n\nBeir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. N Thakur, N Reimers, A R\u00fcckl\u00e9, A Srivastava, I Gurevych, arXiv:2104.08663arXiv preprintN. Thakur, N. Reimers, A. R\u00fcckl\u00e9, A. Srivastava, and I. Gurevych. Beir: A heteroge- neous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021.\n\nRetrieval of the best counterargument without prior topic knowledge. H Wachsmuth, S Syed, B Stein, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics1H. Wachsmuth, S. Syed, and B. Stein. Retrieval of the best counterargument without prior topic knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 241-251, Melbourne, Australia, July 2018. Association for Computational Linguistics.\n\nGPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. B Wang, A Komatsuzaki, B. Wang and A. Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\n\nFinetuned language models are zero-shot learners. J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, arXiv:2109.01652arXiv preprintJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\n\nH Zhuang, Z Qin, R Jagerman, K Hui, J Ma, J Lu, J Ni, X Wang, M Bendersky, arXiv:2210.10634Rankt5: Fine-tuning t5 for text ranking with ranking losses. arXiv preprintH. Zhuang, Z. Qin, R. Jagerman, K. Hui, J. Ma, J. Lu, J. Ni, X. Wang, and M. Bendersky. Rankt5: Fine-tuning t5 for text ranking with ranking losses. arXiv preprint arXiv:2210.10634, 2022.\n", "annotations": {"author": "[{\"end\":467,\"start\":105}]", "publisher": null, "author_last_name": "[{\"end\":117,\"start\":111}]", "author_first_name": "[{\"end\":110,\"start\":105}]", "author_affiliation": "[{\"end\":172,\"start\":119},{\"end\":227,\"start\":174},{\"end\":307,\"start\":229},{\"end\":353,\"start\":309},{\"end\":466,\"start\":355}]", "title": "[{\"end\":91,\"start\":1},{\"end\":558,\"start\":468}]", "venue": null, "abstract": "[{\"end\":1417,\"start\":571}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1692,\"start\":1689},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1711,\"start\":1708},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1953,\"start\":1950},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1976,\"start\":1973},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2094,\"start\":2091},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2135,\"start\":2132},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2482,\"start\":2479},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2840,\"start\":2837},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3022,\"start\":3019},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3088,\"start\":3085},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3447,\"start\":3444},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4026,\"start\":4023},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4866,\"start\":4863},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5356,\"start\":5353},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5987,\"start\":5984},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6003,\"start\":5999},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6428,\"start\":6427},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6632,\"start\":6629},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6945,\"start\":6942},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8456,\"start\":8455}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":7223,\"start\":7217},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":8352,\"start\":7224}]", "paragraph": "[{\"end\":1954,\"start\":1448},{\"end\":2428,\"start\":1956},{\"end\":2911,\"start\":2430},{\"end\":3023,\"start\":2927},{\"end\":3640,\"start\":3025},{\"end\":4380,\"start\":3642},{\"end\":4653,\"start\":4382},{\"end\":5273,\"start\":4655},{\"end\":6114,\"start\":5275},{\"end\":6859,\"start\":6126},{\"end\":7216,\"start\":6874}]", "formula": null, "table_ref": "[{\"end\":5567,\"start\":5560}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1446,\"start\":1419},{\"attributes\":{\"n\":\"2\"},\"end\":2925,\"start\":2914},{\"attributes\":{\"n\":\"3\"},\"end\":6124,\"start\":6117},{\"attributes\":{\"n\":\"4\"},\"end\":6872,\"start\":6862},{\"end\":7222,\"start\":7218}]", "table": "[{\"end\":8352,\"start\":7318}]", "figure_caption": "[{\"end\":7318,\"start\":7226}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":9057,\"start\":9056},{\"end\":9070,\"start\":9069},{\"end\":9082,\"start\":9081},{\"end\":9092,\"start\":9091},{\"end\":9312,\"start\":9311},{\"end\":9319,\"start\":9318},{\"end\":9321,\"start\":9320},{\"end\":9329,\"start\":9328},{\"end\":9335,\"start\":9334},{\"end\":9343,\"start\":9342},{\"end\":9349,\"start\":9348},{\"end\":9355,\"start\":9354},{\"end\":9366,\"start\":9365},{\"end\":9373,\"start\":9372},{\"end\":9375,\"start\":9374},{\"end\":9386,\"start\":9382},{\"end\":9789,\"start\":9788},{\"end\":9796,\"start\":9795},{\"end\":9805,\"start\":9801},{\"end\":9815,\"start\":9811},{\"end\":9823,\"start\":9822},{\"end\":9834,\"start\":9833},{\"end\":10158,\"start\":10157},{\"end\":10170,\"start\":10169},{\"end\":10179,\"start\":10178},{\"end\":10190,\"start\":10189},{\"end\":10673,\"start\":10672},{\"end\":10684,\"start\":10683},{\"end\":11000,\"start\":10999},{\"end\":11010,\"start\":11009},{\"end\":11021,\"start\":11020},{\"end\":11031,\"start\":11030},{\"end\":11045,\"start\":11044},{\"end\":11354,\"start\":11353},{\"end\":11367,\"start\":11366},{\"end\":11375,\"start\":11374},{\"end\":11986,\"start\":11985},{\"end\":11994,\"start\":11993},{\"end\":12213,\"start\":12212},{\"end\":12220,\"start\":12219},{\"end\":12229,\"start\":12228},{\"end\":12231,\"start\":12230},{\"end\":12239,\"start\":12238},{\"end\":12246,\"start\":12245},{\"end\":12248,\"start\":12247},{\"end\":12254,\"start\":12253},{\"end\":12264,\"start\":12263},{\"end\":12270,\"start\":12269},{\"end\":12272,\"start\":12271},{\"end\":12279,\"start\":12278},{\"end\":12281,\"start\":12280},{\"end\":12498,\"start\":12497},{\"end\":12508,\"start\":12507},{\"end\":12515,\"start\":12514},{\"end\":12527,\"start\":12526},{\"end\":12534,\"start\":12533},{\"end\":12540,\"start\":12539},{\"end\":12546,\"start\":12545},{\"end\":12552,\"start\":12551},{\"end\":12560,\"start\":12559}]", "bib_author_last_name": "[{\"end\":9067,\"start\":9058},{\"end\":9079,\"start\":9071},{\"end\":9089,\"start\":9083},{\"end\":9101,\"start\":9093},{\"end\":9316,\"start\":9313},{\"end\":9326,\"start\":9322},{\"end\":9332,\"start\":9330},{\"end\":9340,\"start\":9336},{\"end\":9346,\"start\":9344},{\"end\":9352,\"start\":9350},{\"end\":9363,\"start\":9356},{\"end\":9370,\"start\":9367},{\"end\":9380,\"start\":9376},{\"end\":9392,\"start\":9387},{\"end\":9793,\"start\":9790},{\"end\":9799,\"start\":9797},{\"end\":9809,\"start\":9806},{\"end\":9820,\"start\":9816},{\"end\":9831,\"start\":9824},{\"end\":9843,\"start\":9835},{\"end\":10167,\"start\":10159},{\"end\":10176,\"start\":10171},{\"end\":10187,\"start\":10180},{\"end\":10194,\"start\":10191},{\"end\":10681,\"start\":10674},{\"end\":10690,\"start\":10685},{\"end\":11007,\"start\":11001},{\"end\":11018,\"start\":11011},{\"end\":11028,\"start\":11022},{\"end\":11042,\"start\":11032},{\"end\":11054,\"start\":11046},{\"end\":11364,\"start\":11355},{\"end\":11372,\"start\":11368},{\"end\":11381,\"start\":11376},{\"end\":11991,\"start\":11987},{\"end\":12006,\"start\":11995},{\"end\":12217,\"start\":12214},{\"end\":12226,\"start\":12221},{\"end\":12236,\"start\":12232},{\"end\":12243,\"start\":12240},{\"end\":12251,\"start\":12249},{\"end\":12261,\"start\":12255},{\"end\":12267,\"start\":12265},{\"end\":12276,\"start\":12273},{\"end\":12284,\"start\":12282},{\"end\":12505,\"start\":12499},{\"end\":12512,\"start\":12509},{\"end\":12524,\"start\":12516},{\"end\":12531,\"start\":12528},{\"end\":12537,\"start\":12535},{\"end\":12543,\"start\":12541},{\"end\":12549,\"start\":12547},{\"end\":12557,\"start\":12553},{\"end\":12570,\"start\":12561}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2202.05144\",\"id\":\"b0\"},\"end\":9309,\"start\":8975},{\"attributes\":{\"doi\":\"arXiv:2209.11755\",\"id\":\"b1\"},\"end\":9673,\"start\":9311},{\"attributes\":{\"doi\":\"arXiv:2102.10073\",\"id\":\"b2\"},\"end\":10093,\"start\":9675},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":212725651},\"end\":10607,\"start\":10095},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4786918},\"end\":10907,\"start\":10609},{\"attributes\":{\"doi\":\"arXiv:2104.08663\",\"id\":\"b5\"},\"end\":11282,\"start\":10909},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":51880268},\"end\":11920,\"start\":11284},{\"attributes\":{\"id\":\"b7\"},\"end\":12160,\"start\":11922},{\"attributes\":{\"doi\":\"arXiv:2109.01652\",\"id\":\"b8\"},\"end\":12495,\"start\":12162},{\"attributes\":{\"doi\":\"arXiv:2210.10634\",\"id\":\"b9\"},\"end\":12850,\"start\":12497}]", "bib_title": "[{\"end\":10155,\"start\":10095},{\"end\":10670,\"start\":10609},{\"end\":11351,\"start\":11284}]", "bib_author": "[{\"end\":9069,\"start\":9056},{\"end\":9081,\"start\":9069},{\"end\":9091,\"start\":9081},{\"end\":9103,\"start\":9091},{\"end\":9318,\"start\":9311},{\"end\":9328,\"start\":9318},{\"end\":9334,\"start\":9328},{\"end\":9342,\"start\":9334},{\"end\":9348,\"start\":9342},{\"end\":9354,\"start\":9348},{\"end\":9365,\"start\":9354},{\"end\":9372,\"start\":9365},{\"end\":9382,\"start\":9372},{\"end\":9394,\"start\":9382},{\"end\":9795,\"start\":9788},{\"end\":9801,\"start\":9795},{\"end\":9811,\"start\":9801},{\"end\":9822,\"start\":9811},{\"end\":9833,\"start\":9822},{\"end\":9845,\"start\":9833},{\"end\":10169,\"start\":10157},{\"end\":10178,\"start\":10169},{\"end\":10189,\"start\":10178},{\"end\":10196,\"start\":10189},{\"end\":10683,\"start\":10672},{\"end\":10692,\"start\":10683},{\"end\":11009,\"start\":10999},{\"end\":11020,\"start\":11009},{\"end\":11030,\"start\":11020},{\"end\":11044,\"start\":11030},{\"end\":11056,\"start\":11044},{\"end\":11366,\"start\":11353},{\"end\":11374,\"start\":11366},{\"end\":11383,\"start\":11374},{\"end\":11993,\"start\":11985},{\"end\":12008,\"start\":11993},{\"end\":12219,\"start\":12212},{\"end\":12228,\"start\":12219},{\"end\":12238,\"start\":12228},{\"end\":12245,\"start\":12238},{\"end\":12253,\"start\":12245},{\"end\":12263,\"start\":12253},{\"end\":12269,\"start\":12263},{\"end\":12278,\"start\":12269},{\"end\":12286,\"start\":12278},{\"end\":12507,\"start\":12497},{\"end\":12514,\"start\":12507},{\"end\":12526,\"start\":12514},{\"end\":12533,\"start\":12526},{\"end\":12539,\"start\":12533},{\"end\":12545,\"start\":12539},{\"end\":12551,\"start\":12545},{\"end\":12559,\"start\":12551},{\"end\":12572,\"start\":12559}]", "bib_venue": "[{\"end\":9054,\"start\":8975},{\"end\":9464,\"start\":9410},{\"end\":9786,\"start\":9675},{\"end\":10292,\"start\":10196},{\"end\":10736,\"start\":10692},{\"end\":10997,\"start\":10909},{\"end\":11470,\"start\":11383},{\"end\":11983,\"start\":11922},{\"end\":12210,\"start\":12162},{\"end\":12647,\"start\":12588},{\"end\":10375,\"start\":10294},{\"end\":11564,\"start\":11472}]"}}}, "year": 2023, "month": 12, "day": 17}