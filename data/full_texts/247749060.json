{"id": 247749060, "updated": "2023-10-05 16:03:38.251", "metadata": {"title": "Navigable Proximity Graph-Driven Native Hybrid Queries with Structured and Unstructured Constraints", "authors": "[{\"first\":\"Mengzhao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Lingwei\",\"last\":\"Lv\",\"middle\":[]},{\"first\":\"Xiaoliang\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Yuxiang\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Qiang\",\"last\":\"Yue\",\"middle\":[]},{\"first\":\"Jiongkang\",\"last\":\"Ni\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "As research interest surges, vector similarity search is applied in multiple fields, including data mining, computer vision, and information retrieval. {Given a set of objects (e.g., a set of images) and a query object, we can easily transform each object into a feature vector and apply the vector similarity search to retrieve the most similar objects. However, the original vector similarity search cannot well support \\textit{hybrid queries}, where users not only input unstructured query constraint (i.e., the feature vector of query object) but also structured query constraint (i.e., the desired attributes of interest). Hybrid query processing aims at identifying these objects with similar feature vectors to query object and satisfying the given attribute constraints. Recent efforts have attempted to answer a hybrid query by performing attribute filtering and vector similarity search separately and then merging the results later, which limits efficiency and accuracy because they are not purpose-built for hybrid queries.} In this paper, we propose a native hybrid query (NHQ) framework based on proximity graph (PG), which provides the specialized \\textit{composite index and joint pruning} modules for hybrid queries. We easily deploy existing various PGs on this framework to process hybrid queries efficiently. Moreover, we present two novel navigable PGs (NPGs) with optimized edge selection and routing strategies, which obtain better overall performance than existing PGs. After that, we deploy the proposed NPGs in NHQ to form two hybrid query methods, which significantly outperform the state-of-the-art competitors on all experimental datasets (10$\\times$ faster under the same \\textit{Recall}), including eight public and one in-house real-world datasets. Our code and datasets have been released at \\url{https://github.com/AshenOn3/NHQ}.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2203.13601", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2203-13601", "doi": "10.48550/arxiv.2203.13601"}}, "content": {"source": {"pdf_hash": "4a9dd560cfe60562182b6d52989774ae36a60ec7", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2203.13601v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "35aa0e0649da12777d3b483cf43788777fd6982d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4a9dd560cfe60562182b6d52989774ae36a60ec7.txt", "contents": "\nNavigable Proximity Graph-Driven Native Hybrid Queries with Structured and Unstructured Constraints\n\n\nMengzhao Wang \nLingwei Lv \nXiaoliang Xu \nMember, IEEEYuxiang Wang \nQiang Yue \nJiongkang Ni \nNavigable Proximity Graph-Driven Native Hybrid Queries with Structured and Unstructured Constraints\n1\nAs research interest surges, vector similarity search is applied in multiple fields, including data mining, computer vision, and information retrieval. Given a set of objects (e.g., a set of images) and a query object, we can easily transform each object into a feature vector and apply the vector similarity search to retrieve the most similar objects. However, the original vector similarity search cannot well support hybrid queries, where users not only input unstructured query constraint (i.e., the feature vector of query object) but also structured query constraint (i.e., the desired attributes of interest). Hybrid query processing aims at identifying these objects with similar feature vectors to query object and satisfying the given attribute constraints. Recent efforts have attempted to answer a hybrid query by performing attribute filtering and vector similarity search separately and then merging the results later, which limits efficiency and accuracy because they are not purpose-built for hybrid queries. In this paper, we propose a native hybrid query (NHQ) framework based on proximity graph (PG), which provides the specialized composite index and joint pruning modules for hybrid queries. We easily deploy existing various PGs on this framework to process hybrid queries efficiently. Moreover, we present two novel navigable PGs (NPGs) with optimized edge selection and routing strategies, which obtain better overall performance than existing PGs. After that, we deploy the proposed NPGs in NHQ to form two hybrid query methods, which significantly outperform the state-of-the-art competitors on all experimental datasets (10\u00d7 faster under the same Recall), including eight public and one in-house real-world datasets. Our code and datasets have been released at https://github.com/AshenOn3/NHQ. Index Terms-Proximity graph, vector similarity search, hybrid query processing, feature vector and attribute ! arXiv:2203.13601v1 [cs.DB]\n\nINTRODUCTION\n\nW ITH smart devices and applications on the rise, a tremendous amount of unstructured data (such as video, text, image) is being produced [1], [2]. International Data Corporation (IDC) projects that by 2025, 80% of the digital universe will be unstructured [3], but thanks to the rapid development of deep neural networks (DNNs), we can describe unstructured data accurately via feature vectors. Then we can process unstructured data through vector similarity search [4] to support a multitude of downstream applications, e.g., image retrieval [5], paper retrieval [6], and recommendation systems [4]. As Fig. 1(a) shows, in a paperretrieval system based on vector similarity search, a user aims to find the most semantically similar papers to her input query text. Specifically, each paper's unstructured text can be embedded in a high-dimensional space to establish a feature vector; then the query text is also transformed into a feature vector in the same space. Next, the system performs a vector similarity search using a vector index to obtain papers with semantically similar content. In this case, the feature vector of query text can be viewed as an unstructured query constraint that all returned papers should satisfy (measured by vector similarity). Many effective vector similarity search methods have been proposed to achieve better trade-offs between query efficiency and \u2022 M. Wang [7], [8], [9], [10], which we often see in AI scenarios. Even though vector similarity search is widely studied, it does not support many important real-world scenarios where users not only expect the unstructured query constraint to be satisfied but also some structured query constraints can be matched as well [4], [11]. For example, in many applications, e.g., visual products search and movie recommendation, it is very common that a user can come up with a hybrid query by inputting unstructured and structured constraints simultaneously [4], [12], [13], [14]. Similar to [4], [12], the unstructured query constraint is provided as a feature vector of the query object (e.g., a paper) and the structured query constraint is given as a set of attributes of interest (e.g., topic, venue, and publish year of a paper). We call such a query a hybrid query that identifies objects with similar feature vectors to the query object and satisfying the given attribute query constraints [4], [11], [12], [13], [15], [16], [17]. For example, in Fig. 1(b), a user wants to find some of the latest top-tier conference papers related to her research interest. She can form a hybrid query by providing a detailed descriptive text of her research interest as the unstructured constraint and paper's two attributes (e.g., SIGMOD and 2021) as the structured constraints. Unfortunately, the traditional vector similarity search shown in Fig.  1(a) only obtains papers with semantically similar content, and it is difficult to ensure matching attribute constraints. In response, many have tried to expand attribute filtering on top of vector similarity search to answer a hybrid query.\n\nVearch (a library provided by the famous e-commerce company Jingdong in China) [18] first obtains candidates A running example of different paper-retrieval schemes, including (a) vector similarity search and (c-d) hybrid query. Vector similarity search retrieves results with semantically similar content, but it cannot ensure the structured query constraints. A hybrid query processing obtains results that satisfy the structured and unstructured query constraints.\n\nthat satisfy the unstructured constraint through vector similarity search, and then performs attribute filtering to return the final results [13]. This strategy extends easily to other dedicated vector similarity search libraries, e.g., SPTAG [19], NGT [20], Faiss [21]. In contrast, Alibaba AnalyticDB-V (ADBV) [12] adds multiple query plans based on product quantization (PQ) 1 [22], including performing attribute filtering first and then vector similarity search to return the final results. This method has good scalability and is suitable for most use cases [4]. Milvus [23] pre-partitions the dataset into several subsets based on frequently used attributes, so that it can quickly select the subsets whose attribute values overlap with the query's attribute constraints, then it only need to perform the vector similarity search on these subsets rather than the entire dataset, thereby improving the hybrid query processing's performance [4].\n\nIn a nutshell, the basic idea behind each of these hybrid query solutions is to execute attribute filtering and vector similarity search separately in a different order (we illustrate the details in Sec. 2.2). Regardless of the execution order of attribute filtering and vector similarity search, the entire query pipeline can be abstracted into the logic shown in Fig. 1(c), where two subquery systems (for attribute filtering and vector similarity search) are established independently.\n\nAlthough they can answer a hybrid query, each subquery system was not originally tailored for hybrid queries, which limits overall efficiency and effectiveness. We next analyze their limitations, and then propose our solution.\n\n\nExisting Methods' Limitations\n\nL1: Two indexes must be maintained simultaneously. Fig.  1(c) shows that existing solutions require maintaining both the attribute index and vector index simultaneously for two separate subquery systems. Not only does this increase memory overhead, it also introduces additional logic and a post-processing step (i.e., merging) to ensure two indexes' consistency and query results' correctness [12]. With the advent of frequently updated data, updating different indexes synchronously is now an onerous task.\n\nL2: Unnecessary computational overhead caused by two separate pruning strategies. For existing solutions, pruning is performed based on attribute index and vector index separately, which increases computational overhead. For example, suppose that the vector index in Fig. 1(c) is implemented as a proximity graph (PG, we will give the formal definition of PG in Sec. 3.1) [24], where each vertex in a PG corresponds to an object's feature vector. Then, some vertices close to the query object's feature vector but inconsistent with the attribute constraints causing the search to follow 1. PQ is a quantization-based vector similarity search method. the wrong paths and return incorrect answers to a hybrid query. Because we eventually prune these incorrect answers in the attribute-filtering phase, and the computational cost spent in the vector similarity search phase for returning them is unnecessary and inefficient, we should avoid it.\n\nL3: Query results depend on the merging of the candidates obtained from both subquery systems. Given a hybrid query, we usually expect to return the top-k results that are similar to the query object's feature vector and match the attribute constraints. As shown in Fig. 1(c), a post-processing step, namely merging, is required to combine the top-k candidates C 1 and C 2 obtained from both subquery systems to generate the final results R = C 1 \u2229 C 2 . However, usually we have |R| < k. This is because existing solutions perform vector similarity search and attribute filtering separately, so that each subquery system can only guarantee it will return candidates satisfying one type of query constraint. To ensure |R| = k, we need to enlarge C 1 and C 2 , which seriously increases both subquery systems' latency [15].\n\nL4: Existing solutions are not friendly to PGs. Recent solutions, e.g., ADBV [12] and Milvus [4], adopt a \"first attribute filtering, then vector similarity search\" strategy to answer a hybrid query; they only deploy PQ [22] for vector similarity search. Because many studies have shown PG is 10\u00d7 faster than PQ for vector similarity search [25], [26], it is necessary to exploit a way to integrate PG into hybrid query methods. A straightforward way is to replace PQ-based vector index with another PG-based one, unfortunately, it is problematic for the following reasons: (1) In the \"first attribute filtering, then vector similarity search\" strategy, we expect to find similar vectors from a dynamic space of the vectors filtered by attribute constraints. To be more precise, different user input attribute constraints cause different filtered vectors (this is a dynamic process in runtime regarding user input). It is not worth building a vector index for these filtered vectors dynamically in runtime because of the efficiency issue. (2) Worse still, if we enumerate all possible conjunctions of attribute values and prebuild a vector index for each one offline (the same as [14]), it would suffer from exorbitant memory overhead, e.g., m n different indexes for n attributes where each attribute has m values.\n\nTo sum up, most the aforementioned limitations occur because existing solutions are committed to answer hybrid queries in a \"decomposition-assembly\" model, where a hybrid query is decomposed into two standalone subquery systems that process separately and then assemble the final results. This motivates us to present a novel generalized framework tailored for hybrid queries that works well with existing PGs, offering the well-designed composite index and joint pruning modules to support unstructured and struc-tured query constraints simultaneously, rather than maintaining two independent indexes and performing prune operations separately.\n\n\nOur Solution and Contributions\n\nTo the best of our knowledge, we are the first to present a Native Hybrid Query framework (NHQ) for answering hybrid queries with unstructured and structured constraints in a fused way, rather than the \"decomposition-assembly\" model adopted by existing solutions. As Fig. 1(d) shows, essentially the framework first embeds feature vectors and attributes in a well-designed composite index (for L1); then it prunes the search space collectively on the composite index by considering both the given unstructured and structured query constraints (for L2); and finally, it obtains the final top-k results directly rather than resorting to a merging operation (for L3). It is worth noting that our NHQ is a generalized framework, where the composite index is built based on PG, therefore existing PGs deploy easily into NHQ (for L4) and it also supports custom-optimized PGs (e.g., we present two navigable PGs, discussed in Sec. 4).\n\nSpecifically, our solution includes three aspects. (1) NHQ framework (Sec. 3). The framework contains two modules: composite index and joint pruning. The former is built based on a PG via the presented fusion distance that can measure the similarity of two objects by considering both their feature vectors and attributes simultaneously. The latter concurrently prunes unpromising objects with dissimilar feature vectors and mismatched attributes by evaluating the fusion distance when searching on the composite index, thereby returning hybrid query results in one step. We emphasize that our NHQ is a generalized framework, and thus we can deploy current PGs (such as HNSW [9]) on it to serve as a composite index with a minor modification (Sec. 6.2). (2) Navigable PGs (NPGs) (Sec. 4). In NHQ, PG plays a key role and a PG's edge selection and routing strategies have an important impact on the hybrid query processing's performance. Although we can implement different existing PGs on NHQ, the inherent limitations of existing PGs still leave us room for optimization. Hence, we present two NPGs with novel edge selection and routing strategies, which offer the state-of-the-art performance of PG. (3) NPGbased hybrid query methods (Sec. 5). We obtain two hybrid query methods by integrating the proposed NPGs into our NHQ framework. Specifically, we build two NPG-based composite indexes with our fusion distance measure. Meanwhile, we implement an optimized joint pruning on a NPGbased composite index. We experimentally prove that our NPG-based hybrid query methods significantly outperform the state-of-the-art competitors (e.g., Vearch, ADBV, and Milvus) on all experimental datasets.\n\nThus, our main contributions are as follows:\n\n\u2022 We present NHQ, a generalized framework tailored for answering hybrid queries with unstructured and structured constraints in a fused way. Our NHQ is friendly to existing popular PGs, and we can offer them the powerful ability to handle hybrid queries by deploying them in NHQ with a lightweight modification. \u2022 We propose two NPGs by designing novel edge selection and routing strategies, which yield better performance re- The attribute vector space of all objects in S \u03bd(e) \u03bd(e) \u2208 X is the feature vector of object e (e) (e) \u2208 Y is the attribute vector of object e q A query object \u03b4(, )\n\nThe Euclidean distance between feature vectors G = (V, E)\n\nA PG G with a vertex set V and an edge set E garding efficiency, accuracy, and memory usage compared to existing PGs. \u2022 We deploy two NPGs in our NHQ framework, thus obtaining two new hybrid query methods that significantly outperform the state-of-the-art competitors on all datasets (10\u00d7 faster under the same Recall). \u2022 We verify our approaches' superiority of effectiveness and efficiency by various metrics on nine real-world datasets, compared with six popular PGs and six hybrid query methods, respectively. The remainder of this paper is organized as follows. Preliminaries are presented in Sec. 2. Sec. 3 outlines the NHQ framework. Sec. 4 presents two proposed NPGs with the optimized edge selection and routing strategies. The NPG-based hybrid query methods are discussed in Sec. 5. Sec. 6 shows the experimental evaluation. Sec. 7 reviews the related work, and Sec. 8 concludes this paper.\n\n\nPRELIMINARIES\n\nWe first define the hybrid query in Sec. 2.1 and then briefly overview different types of solutions to hybrid queries in Sec. 2.2. For ease of understanding, we summarize frequently used notations in Tab. 1.\n\n\nProblem Definition\n\nSimilar to [27], [28], we use the term object set to refer to the data that we deal with in this paper. Definition 1. Object Set. An object set is defined as a set S = {e 0 , . . . , e n\u22121 } of size n. For each object e \u2208 S, (1) its features are represented as a high-dimensional vector, denoted by \u03bd(e), (2) e has a set of attributes denoted by {a 0 , . . . , a m\u22121 } and e.a i indicates the value of attribute a i of e. Moreover, we define the feature vector space of all objects in S as X = {\u03bd(e)|e \u2208 S}. Example 1. An object set could refer to different types of data, e.g., a set of images or papers. When we specify each object e as a paper, it carries two types of information: one is the implicit semantics behind the text, which is usually represented as a feature vector \u03bd(e) through deep neural network (DNN), e.g., BERT [29]; and another is the explicit attributes, e.g., {publish year, venue, topic}, such as e.venue =\"SIGMOD\".\n\nGiven an object set S and a user-input query object q, many approaches have been studied to retrieve the most similar objects to q from S by considering feature vectors' distance [9], [30], [31]. In the following, we first introduce vector similarity search, then formally define the hybrid query that we focus in this paper.\n\nGiven an object e \u2208 S and its feature vector \u03bd(e) \u2208 X , we have \u03bd(e) = [\u03bd(e) 0 , \u03bd(e) 1 , . . . , \u03bd(e) d\u22121 ], where \u03bd(e) i denotes the value of \u03bd(e) on the i-th dimension. In particular, we are interested in the high-dimensional case where d ranges from hundreds to thousands. For any two objects e, o \u2208 S with feature vectors \u03bd(e), \u03bd(o) \u2208 X , we can measure their vector similarity through various methods, e.g., Euclidean distance [8] and Cosine similarity [9]. Among these methods, Euclidean distance is the most commonly used in the literature [26], which is given in Eq. 1.\n\u03b4(\u03bd(e), \u03bd(o)) = d\u22121 i=0 (\u03bd(e) i \u2212 \u03bd(o) i ) 2(1)\nWe define the exact vector similarity search as follows [7], [32], [33], [34]: Definition 2. Exact Vector Similarity Search. Given an object set S, a query object q with the feature vector \u03bd(q), and an integer k, the exact vector similarity search aims at obtaining the exact top-k objects from S whose feature vectors are closest to \u03bd(q).\n\nIn Def. 2, the exact top-k objects (denoted by G) hold that G = arg min G\u2286S,|G|=k e\u2208G \u03b4(\u03bd(e), \u03bd(q)) .\n\n(\n\nApplying exact vector similarity search on a large S is impractical because of the high computational cost [24]. So, an approximate vector similarity search is of realistic significance, as it balances a trade-off between accuracy and efficiency using a vector index [24] and can be defined as follows [7], [32], [33], [34]:\n\nDefinition 3. Approximate Vector Similarity Search. Given an object set S, a query object q with the feature vector \u03bd(q), and parameters (>0) and k, the approximate vector similarity search aims at obtaining the approximate top-k objects D from S, such that \u2200i = 0, 1, \u00b7 \u00b7 \u00b7 , k\u22121, \u03b4(\u03bd(e i ), \u03bd(q)) \u2264 (1+ )\u00b7\u03b4(\u03bd(o), \u03bd(q)), where e i \u2208 D, and o \u2208 S is the exact top-1 object whose feature vector is closest to \u03bd(q).\n\nGiven the exact top-k objects G from Def. 2 and the approximate top-k objects D from Def. 3, we can evaluate the approximate vector similarity search's accuracy by recall rate Recall@k:\nRecall@k = |D \u2229 G| k .(3)\nA larger Recall@k indicates that more accurate results are returned for approximate vector similarity search. Hereafter, we refer to approximate vector similarity search simply as vector similarity search unless otherwise specified.\n\nTo efficiently return the query results, vector similarity search preprocess S by building an vector index I(S) based on the feature vector space X of S. According to how the index I(S) is built, we divide vector similarity search methods into four common genres: quantization-based [22], [35]; tree-based [36], [37]; hashing-based [38], [39]; and proximity graph (PG)-based groups [8], [9]. Many works in the literature [8], [26] have demonstrated that the PG-based methods achieve better speedup vs recall rate trade-off.\n\nRecently, a rich spectrum of emerging applications-e.g., finding similar products in e-commerce [4], [12]-not only expect that the query results and given query object will have similar feature vectors, but also have the same attributes [4], [12], [13], [15]. For queries in this context, the query object's feature vector and attributes of interests can be viewed as the unstructured and structured query constraints, respectively [12]. We aim to identify similar objects that satisfy both the given feature vector and attribute constraints. We define such a hybrid query as follows:\n\nDefinition 4. Hybrid Query. Given an object set S and a query object q with the feature vector \u03bd(q) and a set of attributes {a 0 , . . . , a m\u22121 } of size m, a hybrid query returns approximate top-k objects from S, denoted by D. The relationship between the feature vectors of objects in D and \u03bd(q) satisfies Def. 3 and objects' attributes are the same as those of query object q-that is, for e \u2208 D, \u2200i = 0, 1, \u00b7 \u00b7 \u00b7 , m \u2212 1, e.a i = q.a i .\n\nAlthough the hybrid query can be viewed as an extended vector similarity search with attribute constraints [4], it is a non-trivial problem to solve, because all existing solutions suffer from effectiveness and efficiency issues (L1-L4 in Sec. 1.1), which is what inspired our research. We next briefly introduce essential differences in the implementation strategies of hybrid query processing between existing solutions and ours, to highlight our solution's novelty.\n\n\nImplementation Strategies\n\nAs Sec. 1.1 mentions, existing solutions to hybrid queries are designed in a \"decomposition-assembly\" model, where a hybrid query is decomposed to two standalone subqueries for processing separately and final results assembling-that is, the vector similarity search is based on a vector index and attribute filtering is based on an attribute index. There are two implementation strategies regarding the execution order of these two subqueries, i.e., Strategies A and B in Fig.  2. Current work mainly falls in these categories and cannot achieve the best performance. By contrast, our solution works by using a new fusion strategy (Strategy C).\n\nStrategy A: \"first attribute filtering, then vector similarity search.\" In general, attribute filtering shows higher unit efficiency than vector similarity search, e.g., in our evaluation, the average overhead time is about 0.02\u00b5s and 0.50\u00b5s for checking whether two attributes are equal and computing the distance of a pair of feature vectors, respectively. Therefore, when attribute filtering runs first, vector similarity search can be performed on the obtained candidate subset rather than the entire object set, which will boost query efficiency to some extent. As a side effect, this strategy suffers from memory and efficiency issues regarding vector index construction for searching similar vectors, as we analyzed in Sec 1.1 (L4). To address this problem, existing efforts [4], [12] encode the feature vectors by PQ [22] for approximating Euclidean distance (Eq. 1). Yet, they yield remarkably low query accuracy, especially compared to vector similarity search on top of the PG-based vector index [26].\n\nStrategy B: \"first vector similarity search, then attribute filtering.\" This strategy can be classified further into two different categories: (1) One is to perform attribute filtering after completing vector similarity search; that is, we only need to check the attributes of the returned approximate top-k objects. (2) The other is to perform attribute filtering after each step of vector similarity search; that is, we need to check the attributes for each object explored during the vector similarity search (i.e., the entire search space). In our evaluation (see Sec. 6.2), (1) is more efficient than (2) in most cases, so that (1) is adopted more widely in most existing work [13], [18]. One possible explanation is that, because current vector indexes are built with feature vectors rather than attributes, prematurely filtering the object that mismatches the attributes in each search step may impair vector similarity search's performance [24]. Nevertheless, a hybrid query working off (1) usually requires vector similarity search to return more than k candidates for subsequent attribute filtering (e.g., it may need to process 300 candidates for obtaining top-10 nearest objects), which limits vector similarity search's efficiency (Sec.\n\n\n1.1, L3).\n\nIn addition, Strategies A and B must maintain the attribute index and vector index simultaneously (Fig. 2), and separately searching on the two indexes would increase computational overhead (we discussed these in Sec. 1.1, L1-2). To overcome these limitations, we design and implement the following novel solution.\n\nStrategy C: \"joint attribute filtering and vector similarity search\". This strategy carries out vector similarity search and attribute filtering concurrently on a composite index ( Fig. 2) that contains both the feature vectors' and attributes' information, which obviously differs from the vector index or attribute index in Strategies A and B. For Strategy C, given a query object and a composite index, we jointly prune unpromising objects with dissimilar vectors and mismatched attributes during query processing, so that we return query results in one step without intermediate candidates. To the best of our knowledge, this is the first work to present a hybrid query solution in a fused way. With this essential difference, compared to existing solutions we achieve 10\u00d7 improvement in effectiveness and efficiency.\n\n\nNATIVE HYBRID QUERY FRAMEWORK\n\nRather than answering a hybrid query by two separate subqueries on different indexes, we propose a native hybrid query (NHQ) framework, with a well-designed composite index to support unstructured and structured query constraints simultaneously. We first show the intuition of our solution (Sec. 3.1), and then describe the NHQ framework working off Strategy C in detail (Sec. 3.2).\n\n\nIntuition\n\nTo build a purpose-built index for hybrid queries, it is natural to assemble both unstructured and structured information in this index (i.e., a composite index). Research suggests that the graph has inherent advantages in embedding complex information [40], [41], [42]. For example, the multimodal knowledge graph embeds various modal data (e.g., image, text) into a graph, thereby benefiting a multitude of downstream tasks [43], [44]. Driven by this, we explore how to integrate various types of information into a graph as our composite index. In the literature of vector similarity search, the mainstream vector index is implemented based on PG (see Def. 5), which has been proven to offer state-of-the-art performance compared to other index types [7], [45], [46]. However, existing PGs only contain feature vectors' neighborhood relationship while excluding attributes, so they cannot be used directly for building the composite index.\n\n\nDefinition 5. Proximity graph (PG).\n\nGiven an object set S, we define the PG of S w.r.t. an distance threshold \u0398 as a graph G = (V, E) with the vertex set V and edge set E. (1) For each vertex u \u2208 V , it corresponds to an object e \u2208 S. (2) For any two vertices u i and u j from V , we have an edge\nu i u j \u2208 E, iff \u03b4(\u03bd(u i ), \u03bd(u j )) \u2264 \u0398, where \u03bd(u i ) and \u03bd(u j )\nis the feature vectors of objects u i and u j , respectively.\n\nFor a vertex u i , we denote by N G (u i ) the set of all neighbors of u i in G, that is, {u i | u j \u2208 V, u i u j \u2208 E}; we omit the subscript G when the graph is clear from the context. In Def. 5, \u0398 controls the vertex u i 's neighbors. The higher the value of \u0398, the more neighbors u i has. Specifically, a larger \u0398 yields more neighbors of u i , thus increasing the search space expanded from u i , in turn, reducing search efficiency [9]. Meanwhile, a smaller \u0398 may break a PG's connectivity, which impacts search accuracy [8]. In general, some specific PGs' differences mainly fall into the implementation of (2) in Def. 5 (e.g., DPG [26] and NSG [8]).\n\nInstead of building an ordinary PG only based on objects' feature vectors, our insight is to build a new PG based on both the feature vectors and attributes to serve as the composite index, so that we can answer hybrid queries directly with structured and unstructured constraints. For example, consider an ordinary PG G = (V, E) built for an object set S. For any vertex u \u2208 V , it only links neighbors (i.e., N (u)) with similar feature vectors. In contrast, for our new PG, we not only expect the feature vector of each object in N (u) to be similar to u's feature vector, but also to have the matched attributes. On this basis, we can jointly prune the unpromising vertices with dissimilar feature vectors and mismatched attributes by evaluating the fusion distance (defined in Sec. 3.2). Overview. To build a composite index, the key challenge is to measure the distance between objects-or rather, the fusion distance of feature vectors and attributes. We can evaluate the distance of feature vectors by Eq. 1, while for the distance of attributes, we must quantify a set of attributes for each object and then define a distance metric function on it [47]. Given the distance of feature vectors and distance of attributes, fusing them into a unified distance is the basic premise of calculating the objects' distance. Next, we discuss how to compute the fusion distance between two objects, and show the procedure of a composite index construction based on this fusion distance. Then we present a joint pruning strategy based on the built composite index. Fusion distance. Given an object set S with the feature vector space X shown in Fig. 3 (bottom left), we first apply ordinal encoding [48] to encode the attribute values for each object e \u2208 S as an attribute vector\n\n\nNHQ Framework\n(e) = [ (e) 0 , \u00b7 \u00b7 \u00b7 , (e) m\u22121 ],\nwhere (e) i is the encoded value of attribute value e.a i . Then, we have the attribute vector space Y = { (e)|e \u2208 S}.\n\nFor e i , e j \u2208 S, the distance between \u03bd(e i ) \u2208 X and \u03bd(e j ) \u2208 X can be evaluated by \u03b4(\u03bd(e i ), \u03bd(e j )) (Eq. 1). The smaller \u03b4(\u03bd(e i ), \u03bd(e j )) is, the more similar \u03bd(e i ) and \u03bd(e j ) are. We measure the distance between (e i ) \u2208 Y and (e j ) \u2208 Y as follows:\n\u03c7( (e i ), (e j )) = m\u22121 k=0 \u03c6( (e i ) k , (e j ) k ) ,(4)where \u03c6( (e i ) k , (e j ) k ) satisfies \u03c6( (e i ) k , (e j ) k ) = 0, (e i ) k = (e j ) k 1, (e i ) k = (e j ) k .(5)\nIn Eq. 4-5, m is the number of dimensions of (e i ) and (e i ) k denotes the values of (e i ) on the k-th dimension. The smaller the \u03c7( (e i ), (e j )), the higher the similarity between (e i ) and (e j ).\n\nGiven \u03b4(\u03bd(e i ), \u03bd(e j )) and \u03c7( (e i ), (e j )), we deal with the fusion distance \u0393 (e i , e j ) of objects e i and e j as follows:\n\n\u0393 (e i , e j ) = \u03c9 \u03bd \u00b7 \u03b4(\u03bd(e i ), \u03bd(e j )) + \u03c9 \u00b7 \u03c7( (e i ), (e j )) , (6) where \u03c9 \u03bd , \u03c9 are the weights of feature vectors' and attribute vectors' distances, respectively; and the smaller \u0393 (e i , e j ) is, the more e i and e j are similar in both the feature vectors and attributes.\n\nEq. 6 shows a simple and practically feasible way to fuse two completely different distances. It is especially friendly for building a composite index on top of existing PGs, because we easily extend an existing PG to a composite index simply by replacing the distance measure from Eq. 1 to Eq. 6. For example, if we set \u03c9 \u03bd = 1 and \u03c9 = 0, then we have \u0393 (e i , e j ) = \u03b4(\u03bd(e i ), \u03bd(e j )), which is exactly the case of building an original PG based on the feature vector distance. Meanwhile, if we set \u03c9 \u03bd = 0 and \u03c9 = 1-that is, \u0393 (e i , e j ) = \u03c7( (e i ), (e j ))-we then get a PG based on the distance of attribute vectors. So, we can obtain an optimal compromise between feature vector distance and attribute vector distance by adjusting \u03c9 \u03bd and \u03c9 . Optimal weight configuration. In our experimental study, \u03c9 \u03bd = 1 and \u03c9 = \u03b4(\u03bd(e i ), \u03bd(e j ))/m (m is the dimension of an attribute vector) offer the best hybrid querying Algorithm 1: Building Composite Index (S)\nInput: Object set S Output: Composite Index G = (V, E) 1 V \u2190 S, E \u2190 \u2205 2 forall u i \u2208 V do 3 forall u j \u2208 V \\ {u i } do 4 if \u0393 (u i , u j ) \u2264 \u0398 then 5 E = E \u222a {u i u j } 6 return G = (V, E) Algorithm 2: Joint Pruning (G, q, B) Input: Composite index G, query object q, seed set B Output: result set R 1 candidate set C \u2190 B; result set R \u2190 B 2 while R is updated do 3 u i \u2190 arg min ui\u2208C \u0393 (q, u i ); C = C \\ {u i } 4 N (u i ) \u2190 the neighbors of u i ; C = C \u222a N (u i ) 5 forall u j \u2208 N (u i ) do 6 u r \u2190 arg max ur\u2208R \u0393 (q, u r ) 7 if \u0393 (q, u j ) < \u0393 (q, u r ) then 8 R = R \\ {u r }; R = R \u222a {u j } 9 return R\nperformance and are recommended for most datasets and algorithms (see Sec. 6.5). In other words, this configuration of \u03c9 \u03bd and \u03c9 is dataset independent-that is, they only relate to the feature vector distance \u03b4(\u03bd(e i ), \u03bd(e j )) of two specific objects and the attribute vector dimension m-so we do not need to configure them specifically for different datasets. The basic idea behind this configuration is that we obtain a fusion distance by fine-tuning the feature vector distance via the attribute distance. More precisely, for two objects e i and e j , we first obtain their feature vector distance \u03b4(\u03bd(e i ), \u03bd(e j )), then the attribute distance \u03c7( (e i ), (e j )) serves as a fine-tuning effect for the fusion distance. For example, if e i and e j have exactly matched attributes, that is, \u03c7( (e i ), (e j )) = 0, we make no changes on the original feature vector distance (i.e., \u0393 (e i , e j ) = \u03b4(\u03bd(e i ), \u03bd(e j ))). While if the attributes of e i and e j do not match at all-that is, \u03c7( (e i ), (e j )) = m-we impose a penalty on the original feature vector distance and achieve a fusion distance of\n\u0393 (e i , e j ) = 2 \u00b7 \u03b4(\u03bd(e i ), \u03bd(e j )). Finally, we get that \u0393 (e i , e j ) satisfies \u03b4(\u03bd(e i ), \u03bd(e j )) \u2264 \u0393 (e i , e j ) \u2264 2 \u00b7 \u03b4(\u03bd(e i ), \u03bd(e j )) for different attribute distances.\nComposite index. The original PGs construct the neighborhood relationship between two objects based on their feature vector distance (Eq. 1), so that we say only the objects' unstructured information is preserved in the PG's topological structure. By contrast, we take into consideration both the distances of feature vectors and attribute vectors by Eq. 6. So, we preserve both the unstructured and structured information in the neighborhood relationship between objects, thereby generating a new PG and we can take it as the composite index (as Fig. 3 (bottom right) shows).\n\nSpecifically, given an object set S, we build a composite index G = (V, E) on S through Alg. 1. We first initialize the vertex set V by S to ensure that each object e \u2208 S corresponds to a vertex u \u2208 V in G, as well as an empty edge set E (line 1); and then we connect two objects u i , u j \u2208 V with an edge u i u j \u2208 E, iff \u0393 (u i , u j ) \u2264 \u0398 , \u0398 is a predefined fusion distance threshold (lines 2-5). In our experimental study, \u0398 has an optimal value to achieve a robust query performance. We found that \u0398 is strongly related to the degree of vertex, and the upper bound on each vertex's degree is always 20 for an optimal \u0398 on most datasets, e.g., the optimal \u0398 is around 230 on SIFT1M dataset 5 . We can determine this value via a grid search [49].\n\nJoint pruning. Different from the separate pruning for unstructured and structured query constraints in the existing work [4], [12], [13] (Strategies A and B in Fig. 2), we simultaneously prune the unpromising objects with dissimilar feature vectors and mismatched attributes (measured by the fusion distance, Eq. 6) based on the aforementioned composite index (top right in Fig. 3).\n\nGiven a composite index G = (V, E) built for an object set S, a query object q, and a seed set B \u2286 V (usually these are selected randomly from V [24]), we obtain the approximate top-k objects via joint pruning using the following steps (Alg. 2): (1) Initialization. We use a visited vertex set C to record the vertices for further search expansion and use a result set R of size k to record current query results, both of which are initialized by B (line 1). (2) Search expansion. We take out the vertex u i with the smallest \u0393 (q, u i ) from C as the next visited vertex for search expansion (line 3), and then expand C by C \u222a N (u i ) (lines 3-4). (3) Query results update. We update R by the better vertices in N (u i ) (lines 5-8). To be more precise, for any vertex u j \u2208 N (u i ) and a vertex u r \u2208 R that has the farthest distance to q, we replace u r in R by u j , if u j is closer to q than u r in terms of our fusion distance, i.e., \u0393 (q, u j ) < \u0393 (q, u r ). This is because u j is more similar to q in both the feature vectors and attributes, compared with u r . Finally, we repeat (2) and (3) until R cannot be further updated, then we terminate the query and return R as the approximate top-k objects. Fig. 3 (top right), we show the joint pruning of returning the nearest object (i.e., top-1) to the query object q. First, we initialize the seed vertex set with a randomly selected vertex, i.e., B = {u 0 }, then we have C = {u 0 }, R = {u 0 }. Second, we expand the search space with u 0 's neighbors and update C = {u 1 , u 2 , u 3 , u 4 }. Third, we replace u 0 in R by u 4 because \u0393 (q, u 4 ) = 0.5 is smaller than \u0393 (q, u 0 ) = 1.0. We then keep expanding the search space by updating C with the neighbors of u 4 , u 5 , and finally we have R = {u 5 }. Because each vertex u j \u2208 N (u 5 ) satisfies \u0393 (q, u j ) > \u0393 (q, u 5 ), we return u 5 as the approximate top-1 nearest object. In the process, we prune the search space from vertices u 1 , u 2 , and u 3 .\n\n\nExample 2. In\n\n\nRemarks.\n\n(1) It is worth emphasizing that joint pruning can obtain the hybrid query results in one step, thereby avoiding the post-processing operation (merging in Fig.  1(c)) of solutions following the \"decomposition-assembly\" model. (2) Although the fusion distance weights' optimal configuration (\u03c9 \u03bd and \u03c9 in Eq. 6) is dataset independent, users still can set them flexibly according to their query preference, thereby getting personalized query results (e.g., only considering the attributes by setting \u03c9 \u03bd =0 and \u03c9 = 1).\n\n\nNAVIGABLE PROXIMITY GRAPH\n\nNHQ is a generalized framework works well with existing PGs. We easily can deploy a specific PG in NHQ to form the composite index by changing its original distance measure to our fusion distance (Eq. 6), so that we can offer this PG the ability to handle hybrid queries effectively. However, current PGs' limitations would have a side-effect on NHQ's overall performance with the PG-based composite index. This motivates us to present two new Navigable PGs (NPGs) in this section, by optimizing the edge selection and routing strategies widely used for building a PG and searching on a PG, respectively. We discuss the details of how to deploy our NPGs in NHQ in Sec. 5.\n\n\nEdge Selection\n\nEdge selection is a key step in building a PG [24]. Given an object set S, we obtain the neighbors of each object e in S based on an edge selection strategy. Different strategies produce noteworthy index structure discrepancies for a PG, thereby impacting search performance on the PG [8], [26].\n\nIntuition. In general, existing PGs focus on two factors during edge selection: distance between two vertices (D1) and distribution of all vertices (D2) [24]. Early PGs such as NSW [50] and KGraph [51] only consider D1 when selecting edges-that is, each vertex is connected with a certain number of its nearest neighbors. [52] argued that only considering D1 would lead to redundant computations and impair search efficiency, as illustrated in Example 3.\n\nExample 3. Fig. 4(a) illustrates a situation where four nearest neighbors are connected to u i . We divide u i 's neighborhood into four parts (P1-P4) by a black solid line, and the neighbors located in each part will guide the search to approach the query object q along this orientation. Suppose we are going to expand the search space from u i . Because most of u i 's neighbors are in the same area (such as u 0 -u 2 in P1), we cannot use these neighbors to guide a search toward q that is located in different areas (e.g., P3). Instead, we need extra computation to find a new route to q's area, which adds computational overhead and reduces query efficiency.\n\nRecent PGs add D2 to edge selection, which diversifies each vertex's neighbors under the premise of ensuring a similarity of neighbors [26], thereby significantly improving the search performance. [24] concluded that state-of-the-art PGs connect neighbors in more directions through the edge selection strategy of Relative Neighborhood Graph (RNG), which we define next. Definition 6. RNG [53]. A RNG G = (V, E) w.r.t a given object set S holds that Def. 5 (PG's definition), and for any two vertices u i , u j from V , we have an edge u i u j \u2208 E, iff \u03b4(\u03bd(u i ), \u03bd(u j ))<\u03b4(\u03bd(u i ), \u03bd(u k )), or \u03b4(\u03bd(u i ), \u03bd(u j ))<\u03b4(\u03bd(u k ), \u03bd(u j )), where u k \u2208 V is an arbitrary vertex that satisfies u k = u i = u j . Fig. 4(b) shows, the yellow lune is formed by two circles' intersection. For example, the small lune is formed by two circles with vertices u i and u 0 as the centers and distance \u03b4(\u03bd(u i ), \u03bd(u 0 )) as the radius. We add an edge between u i and u 0 , iff the lune formed by u i and u 0 does not include any other vertices, which is equivalent to the condition of RNG's edge selection in Def. 6 (refer to [54] for proof). Under this rule, RNG's edge selection prevents u i 's neighbors from congregating in the same area, so that there is more opportunity to link neighbors in different areas (such as u 5 in P4) [26]. However, RNG's edge selection is still problematic, e.g., the vertices in P2 and P3 cannot be connected to u i in Fig. 4(b) because they violate the above rule. For instance, the lune formed by u 3 and u i already includes a vertex u 2 , so we cannot add an edge between u 3 and u i . As a result, for the case that query object q in the area of P2 or P3, we still need more computations for finding a new route to q's area.\n\n\nExample 4. As\n\nOur edge selection. Based on the above observations, we design a new edge selection strategy considering D1 and D2, which is committed to link one nearest neighbor to u i in each area of u i (Fig. 4(c)). We first give the definition of the landing zone formed by u i and u i 's one neighbor u j \u2208 N (u i ), which depicts the area that only the vertices in it could be added to N (u i ). We then give our edge selection strategy working off the landing zone.\n\nDefinition 7. Landing zone. Given a vertex u i and u i 's one neighbor u j \u2208 N (u i ), the landing zone L(u i , u j ) formed by u i and u j is an area defined by\nH(u i , u j ) \\ B(u i , \u03b4(\u03bd(u i ), \u03bd(u j ))),\nwhere H(u i , u j ) is the half space containing u i , it is divided by the perpendicular bisectionplane U(u i , u j ) of the line connecting u i and u j [55], and B(u i , \u03b4(\u03bd(u i ), \u03bd(u j ))) is the hypersphere with u i as the center and \u03b4(\u03bd(u i ), \u03bd(u j )) as the radius [8]. Fig. 4(c) shows, in a two dimensional space, U(u i , u 0 ) is a perpendicular bisector of the line connecting u i and u 0 (i.e., the red line), H(u i , u 0 ) is located on the upper side of U(u i , u 0 ), and B(u i , \u03b4(\u03bd(u i ), \u03bd(u 0 ))) is the area enclosed by the green circle. Therefore, the landing zone L(u i , u 0 ) formed by u i and u 0 is the green shaded region (i.e., H(u i , u 0 ) \\ B(u i , \u03b4(\u03bd(u i ), \u03bd(u 0 )))).\n\n\nExample 5. As\n\nBy defining a landing zone L(u i , u j ) for each neighbor u j \u2208 N (u i ), we easily locate the areas without any neighbor of u i by the intersection of the landing zones of u i 's all neighbors (i.e., uj \u2208N (ui) L(u i , u j )). Then, we add a nearest vertex in this intersection area to N (u i ), which facilitates the diversity of neighbors regarding areas. We now introduce the core steps of our edge selection strategy for building a PG G = (V, E); more specifically, for \u2200u i \u2208 V , we aim to obtain its neighbors N (u i ) as follows.\n\n(1) Candidates initialization. We acquire a candidate neighbor set C(u i ) (a subset of (V \\ {u i })) of size l (by randomly sampling [51] or an additional index [50]); it holds that l \u2265 k, where k is the upper bound of the number of u i 's neighbors (i.e., |N (u i )| \u2264 k).\n\n(2) Neighbors initialization. We sort the vertices in C(u i ) in ascending order w.r.t their distances to u i . We then initialize N (u i ) with u i 's nearest candidate neighbor u t from C(u i ) and add u t to N (u i ), as well as removing u t from C(u i ). (3) Neighbors update. We remove the nearest vertex u p of u i from C(u i ), and add u p to N (u i ), iff u p is in the new intersection area (i.e., uj \u2208N (ui) L(u i , u j )) of the landing zones formed by u i and all its neighbors. We repeat this process until C(u i ) = \u2205.\n\n\nRemarks.\n\nSince each u p added to N (u i ) is located in a new area, our strategy ensures the diversity of u i 's neighbors regarding areas where C(u i ) is located. Example 6. In Fig. 4(c), suppose we have N (u i ) = {u 0 } and C(u i ) = {u 3 , u 4 , \u00b7 \u00b7 \u00b7 , u 9 }, we can add u 3 to N (u i ) because u 3 is in the landing zone L(u i , u 0 ) (i.e., the green shaded region); then we can add u 5 to N (u i ) because u 5 is in the intersection of L(u i , u 0 ) and L(u i , u 3 ) (i.e., L(u i , u 0 ) \u2229 L(u i , u 3 )). We repeat our edge selection until each new area contains one neighbor of u i (e.g., u 7 in P3 and u 5 in P4). Compared to RNG's edge selection, the neighbors acquired by ours are more diverse. Hence, we can route to the query object q based on u i 's neighbor that is located in the same area of q, thereby improving the query efficiency.\n\nComplexity. For u i \u2208 V , the time complexity of sorting C(u i ) is O(l \u00b7 log(l)). For each u p \u2208 C(u i ), we must check |N (u i )| (\u2264 k) times to see if u p is in the landing zone L(u i , u j ) for each u j \u2208 N (u i ). Thus, the check times of getting the final N (u i ) via our edge selection is l \u00b7 k ( |V |). So, the time complexity of performing our edge selection on V is O(l \u00b7 (k + log(l)) \u00b7 |V |).\n\n\nRouting\n\nRouting is a key step of searching on a PG [56]. Given a PG and a query object, the process of finding the query results is implemented using a proper routing strategy that determines a routing path from the start vertex to the result vertex [56], e.g., the path indicated by the black arrows in Fig. 5. Obviously, the routing strategy directly affects the efficiency and accuracy of searching on the PG [57].\n\n\nIntuition.\n\nMost existing PGs adopt a greedy search as their routing strategy, which leads to some redundant computations (see Example 7) [58], [59]. Some machine learning optimizations [59] are used to mitigate this problem, which achieve better Speedup vs Recall trade-off at the expense of more index processing time and memory [24]. [56] specifies two search stages on which routing has different requirements, i.e., the stage far from the query object (S1) and the stage closer to the query object (S2). In S1, [56] claims that we should quickly locate the query object's neighborhood (for efficiency requirement), while in S2, we should focus on comprehensively visiting vertices nearest to the query object (for accuracy requirement). Therefore, [56] designs a tailored routing strategy for each stage, to form a two-stage routing strategy, called TOGG. However, TOGG attaches a tree-based index (e.g., KD-tree is used) to organize the neighbors of each vertex (see Example 7), which increases the index construction time and memory overhead.\n\nOur routing. As a simple and effective solution, we design a random TOGG. In S1, for each visited vertex u i , rather than obtaining partial neighbors of u i for distance calculation by an additional tree (as TOGG did), we randomly select u i 's k/h (1 \u2264 h \u2264 k, where k is the upper bound of the vertex degree) neighbors from N (u i ) for distance calculation and quickly approach the query object's neighborhood. While in S2, we evaluate the distance of all neighbors in N (u i ) to the query object to ensure query accuracy, which is the same as the greedy search. Different from TOGG, our routing can achieve the comparable accuracy but does not introduce additional index management overhead. Example 7. Fig. 5 shows the routing process (indicated by black arrows) from the start vertex u 0 to the result vertex u 9 working off different routing strategies. When we use greedy search as the routing strategy (Fig 5(a)), neighbors of each visiting vertex are explored fully; e.g., for u 0 , the distances from all vertices in N (u 0 )={u 1 , u 2 , u 3 } to the query object are computed. As a result, the greedy search has 12 distance calculations. In Fig. 5(b), TOGG organizes each vertex's neighbors (N (u 0 )) in a tree-based index. In S1 (u 0 \u2192u 3 \u2192u 8 ), TOGG selects the next hop based on each visited vertex's neighbor index. In S2 (u 8 \u2192u 9 ), TOGG uses a similar routing to the greedy search. Finally, TOGG only needs 9 distance calculations. Instead of an additional tree-based index for each vertex's neighbors, our routing randomly selects k/h =2 neighbors (when we set k=4 and h=2) for computing the distance to the query object in S1 and performs the greedy search in S2. Similar to TOGG, our strategy requires only 8 distance calculations in Fig. 5(c). It is worth noting that our routing avoids some unnecessary calculations and improves query efficiency without extra index processing and memory overhead.\n\nComplexity. According to [8], [9], [56], the greedy search's time complexity on a PG is about O(k \u00b7 log(|V |)), where k |V | is the maximum number of neighbors of each vertex and log(|V |) is approximately the length of the routing path on average. In our routing, we set the length of the routing path in S1 as l 1 and the length of the routing path in S2 as l 2 on average; then we have l 1 +l 2 =log(|V |). Therefore, our routing's time complexity is O(( k/h ) \u00b7 l 1 + k \u00b7 l 2 ).\n\n\nNPG with Our Edge Selection and Routing\n\nWe present two NPGs, i.e., NPG nsw and NPG kgraph, based on two mainstream PGs, NSW [50] and KGraph [51], by using our edge selection and routing strategies. More precisely, we construct two NPGs through our edge selection and conduct the search on them through our routing. NPG nsw. NPG nsw is constructed by inserting an object incrementally. Specifically, a newly inserted object e i \u2208 S (corresponding to a vertex u i ) is regarded as a query object, so we conduct a greedy search [8] to obtain l vertices closest to u i as u i 's candidate neighbors C(u i ) from the NPG nsw built on previously inserted objects [50]. Then we apply our edge selection to form u i 's neighbors N (u i ). We repeat these operations for each object in S, yielding a NPG nsw. NPG kgraph. This is built by iteratively updating neighbors. For each e i \u2208 S (corresponding to a vertex u i ), we randomly generate its l initial candidate neighbors C(u i ). To make the vertices in C(u i ) are closer to u i , we refine C(u i ) through the neighbors of u i 's neighbors, as the neighbors of a vertex's neighbors are likely to be neighbors of the vertex [51]. Then we obtain N (u i ) by using our edge selection in the final C(u i ). Specifically, we execute the following process until the graph quality (see Def. 8) [60] reaches a preset threshold (in our experiment, 0.8 is enough to achieve a good performance): for any u j \u2208 C(u i ) and u k \u2208 C(u j ),\nwe add u k to C(u i ), if \u03b4(u i , u k ) < \u03b4(u i , u t ),\nwhere u t is the farthest vertex to u i in C(u i ). After the iteration completes, for each u i \u2208 V , its neighbors N (u i ) are produced from C(u i ) via our edge selection. Definition 8. Graph quality [24]. Given a PG G = (V, E), we define the graph quality Q G of G as the mean ratio of the number of u's neighbors (i.e., N (u)) in M(u) over |M(u)| for all u \u2208 V (Eq. 7), where M(u) is the k nearest vertices of u in (V \\ {u}).\nQ G = 1 |V | u\u2208V |N (u) \u2229 M(u)| k (7)\nGiven the two constructed NPGs, we can apply our routing strategy on them directly to return query results efficiently, starting from a randomly acquired seed vertex.\n\nRemarks. In addition to NSW and KGraph, our edge selection and routing also apply to many other PGs, e.g., SPTAG [19]. So, we easily obtain a new user customized PG by improving the original PG with our edge selection and routing (just like we built NPG nsw and NPG kgraph). Our experiment shows that our proposed NPGs yield significant performance improvement, comparing to the original PGs.\n\n\nNPG-BASED HYBRID QUERY METHODS\n\nWe integrate the proposed NPGs (i.e., NPG nsw and NPG kgraph) into our NHQ framework, thereby yielding two practical hybrid query methods; namely, NHQ-NPG nsw and NHQ-NPG kgraph. According to the evaluation (Sec. 6.4), our methods far exceed the state-of-the-art competitors (e.g., Vearch, ADBV, and Milvus).\n\n\nNPG-Based Composite Index\n\nWe modify the build process of NPGs by changing its original Euclidean distance measure to our fusion distance (Eq. 6). Then we deploy NPG nsw (NPG kgraph) in NHQ to serve as a composite index, so generating a hybrid query method called NHQ-NPG nsw (NHQ-NPG kgraph).\n\n\nRemarks.\n\nWe emphasize that our NHQ is practically flexible, so it is friendly to existing PGs as well as customoptimized PGs, by simply replacing their distance measures by our fusion distance.\n\n\nJoint Pruning Optimization\n\nDriven by our routing strategy, we optimize the joint pruning of NHQ by performing a two-stage search (stages S1 and S2 mentioned in Sec. 4.2) on a composite index. Specifically, in S1, our search only visits k/h neighbors of each vertex in the search path for efficiency; thus we modify line 5 of Alg. 2 (joint pruning) to randomly visit each vertex's k/h neighbors, and execute the loop in lines 2-8 until it falls into \n\n\nEXPERIMENTS\n\nIn this section, we demonstrate our methods' effectiveness and efficiency from the following aspects. First, we deploy several mainstream PGs into our NHQ framework to verify the universality of NHQ in Sec. 6.2. We then compare the proposed NPGs with six existing PGs in Sec. 6.3 to demonstrate the superiority our NPGs. In Sec. 6.4, we deploy our NPGs in NHQ to form two NPG-based hybrid query methods and we verify their state-of-the-art performance.\n\nIn addition, we study the parameter sensitivity of fusion distance weights (\u03c9 \u03bd and \u03c9 in Eq. 6) in Sec. 6.5.\n\n\nExperimental Setting\n\nDatasets. We used eight publicly available real-world datasets and one in-house dataset 9 ; they cover various modes, such as video, image, audio, and text. We summarize their main characteristics in Tab. 2. Among them, the first eight public datasets are composed of high-dimensional feature vectors extracted from different unstructured information, which do not originally contain structured attributes; so, we generate attributes for each object in the eight public datasets following the same method in [4], [14]. For example, we add attributes such as date, location, size to each image on SIFT1M to form an object set having both feature vectors and attributes. Paper 9 is an in-house dataset, each object denotes an individual academic paper that consists of a feature vector extracted from the textual Compared methods. To verify the proposed NPGs' effectiveness with optimized edge selection and routing, we evaluate them alongside six existing state-of-the-art PGs.\n\n\u2022 HNSW [9] is a hierarchical PG used widely in various fields; it is optimized by hardware or learning [59], [61]. \u2022 NSW [50] is the precursor of HNSW, and is a single-layer PG constructed by incrementally inserting data. \u2022 KGraph [51] is an approximate KNNG. It only considers the distance factor in edge selection. \u2022 DPG [26] maximizes the angle between neighbors on the basis of KGraph to alleviate redundant calculation. \u2022 NSG [8] ensures the routing path's monotonicity by approximating a Monotonic Search Network (MSNET) [8]. \u2022 NSSG [7] is similar to DPG; it adjusts the angle between neighbors to adapt to different data characteristics. \u2022 NPG nsw and NPG kgraph are two proposed NPGs in this paper that add our edge selection and routing atop NSW [50] and KGraph [51], respectively (Sec. 4.3).\n\nWe compare our hybrid query methods with six existing ones that have been used in many high-tech companies.\n\n\u2022 ADBV [12] is a cost-based method proposed by Alibaba.\n\nIt optimizes PQ [22] for vector similarity search. \u2022 Milvus [4], [23] divides the object set through frequently used attributes, and deploys ADBV [12] on each subset. \u2022 Vearch [13], [18] is developed by Jingdong, which implements the hybrid query working off Strategy B. \u2022 NGT [20] is a vector similarity search library released by Yahoo Japan, which answers a hybrid query to conduct attribute filtering atop the candidates recalled by NGT. \u2022 Faiss [21] is a library developed by Facebook, which answer a hybrid query based on IVFPQ (it uses k-means to cluster PQ codes into groups [12]) and Strategy A. \u2022 SPTAG [19] is a PG-based vector similarity search library from Microsoft and answers hybrid queries on Strategy B. \u2022 NHQ-NPG nsw and NHQ-NPG kgraph are our hybrid query methods based on the NHQ integrating two NPGs.\n\n\nMetrics.\n\nFor index build performance, we record the index build time, peak memory overhead, index size, and graph quality (Def. 8). We evaluate the search efficiency, accuracy and peak memory overhead to demonstrate search performance. In terms of vector similarity search, the search efficiency can be measured by queries per second (QPS) and Speedup; QPS is the ratio of the number of queries (#q) to the search time (t), i.e., #q/t; Speedup is defined as |S|/N DC, where |S| is the object set's size and is also the total number of distance calculations of the linear scan for a query, and NDC is the number of distance calculations of searching on a PG. We use the Recall rate to evaluate the search accuracy, which is measured by Eq. 3. As for hybrid query, QPS is more suitable for evaluating search efficiency, because different methods' calculation cost is distinguishing. Unlike vector similarity search, attribute constraints are added to the Recall rate formula in Eq. 3 for hybrid query processing, i.e., the elements in D and G have exactly the same attributes as the query object. In addition, selectivity is used to evaluate each method's scalability. It is defined as 1 \u2212 |P|/|S| in [12], where |P| is the number of objects that match the given attributes, and |S| is the total number of objects. Implementation setup. Most methods' codes are publicly accessible online, otherwise we implement the corresponding methods according to their papers. Given that all the compared approaches have parallel versions and SIMD, prefetching instructions' optimizations in their index construction codes, we build all the indexes in parallel with 64 threads and turn on these time-saving optimizations. However, considering that not all algorithms support the parallelization of a single query, we mainly use a single thread to perform search, which is a mainstream setting in related work [7], [8]. In addition, we implement a parallel version of our hybrid query method for comparison with two existing methods that parallelize a query (Sec. 6.4).\n\nAll codes are written in C++, and are compiled by g++ 6.5. All experiments are conducted on a Linux server with an Intel(R) Xeon(R) Gold 6248R CPU at 3.00GHz, and a 755G memory. We report the average results of all indicators by performing three repeated trials.\n\n\nParameters.\n\nBecause doing parameters' adjustment in the entire dataset may cause overfitting [8], we randomly sample a certain percentage of data from the entire dataset to form a validation dataset and search for the optimal values of all the adjustable parameters of each PG on each validation dataset. In our experiment, all the optimal configurations are given in our open source library 9 .\n\n\nValidation of NHQ framework.\n\nTo verify the capabilities and universality of our NHQ (Strategy C), we implement the hybrid queries working off the \"decomposition-assembly\" model with several PGs (including HNSW, NSG, and our two NPGs) based on Strategy B. We also deploy IVFPQ [22] in Strategy A following [4], [12] because Strategy A does not support PG (L4 in Sec. 1.1). Meanwhile, we deploy HNSW and NSG on NHQ to form NHQ-HNSW and NHQ-NSG, respectively.\n\nAnalysis. We first evaluate two different implementations of Strategy B (i.e., \"(1)\" and \"(2)\" discussed in Sec. 2.2) under the same condition, which implements \"(1)\" and \"(2)\" on HNSW and NSG. As Fig. 6 shows, \"(1)\" obtains a better QPS vs Recall trade-off than \"(2)\". A possible explanation is that current PGs are built with feature vectors rather than attributes, prematurely filtering those objects that mismatch the attributes may impair the vector similarity search's performance. To the best of our knowledge, existing hybrid query methods based on Strategy B deploy \"(2)\" rather than \"(1)\". Therefore, we follow this implementation of Strategy B (i.e., \"(2)\") when comparing to other strategies. As Fig. 7 shows, NHQ (Strategy C) beats others on different PGs, and maintains stable QPS on datasets with various LIDs (such as (d) and (f)); however, the hybrid queries based on Strategy B drops significantly on datasets with a higher LID. Because of IVFPQ's limitations, the hybrid queries based on Strategy A are difficult to reach high accuracy.\n\n\nPerformance of NPG\n\nIndex building. As Tab. 3 shows, different PGs have different index build time, memory overhead, index size on different datasets; while graph quality mainly is determined by PG. Specifically, NSW, KGraph, and our two NPGs have higher index building efficiency; and KGraph and NPGs show lower memory overhead; the smallest index falls on NPG kgraph; and KGraph and DPG achieve higher graph quality. In general, the higher the dataset's LID, the longer it takes to build a PG on it, and the larger the memory overhead during index building. Overall, compared to stateof-the-art PGs (such as HNSW and NSG), the index building of our NPGs is more efficient, and takes up less memory. Additionally, NPGs significantly reduce the index size compared to their precursors (i.e., NSW and KGraph). Analysis. The characteristics of the dataset will affect build performance. For example, the build time and memory overhead on GIST1M are higher than that on SIFT1M, this phenomenon is universal. NSW and KGraph only consider D1 (Sec. 4.1) when selecting edges, so their build time is smaller. An additional consideration of D2 is that it will demand extra time (e.g., NSSG). NPGs still have a low build time although they also take into account D2, which demonstrates that our edge selection is more efficient. In addition, our NPGs have a smaller index than the original PGs (e.g., KGraph) because our edge selection cuts  off redundant neighbors in the same direction. Meanwhile, the diversity of neighbors makes NPGs link to not only the nearest neighbors, but also some other directions' ones farther away, which will improve query performance. Search performance. As Tab. 4 and Fig. 8 show, NPGs obtain state-of-the-art accuracy vs efficiency trade-off with lower memory overhead on most datasets. As the dataset's LID increases, the search efficiency of each PG decreases; for example, from SIFT1M to GIST1M ((a) and (b) in Fig. 8),\n\nQPS decreases by about one order of magnitude. In general, the fewer the number of distance calculations (which means a bigger Speedup), the larger the QPS; whereas the differences in some PGs' routing implementation lead to noncompliance with this phenomenon (e.g., NSSG). On the same dataset, for different Recall requirements, such as Recall@1 ((i)) and Recall@10 ((a)), each PG's search performance ranking is almost the same, especially NPG kgraph, which shows robust scalability. Compared with HNSW and NSG, our NPGs occupy less memory during the search.\n\nConsidering the graph quality in Tab. 3, we can see that the optimal search performance does not require the highest graph quality, but a proper value (0.2 \u223c 0.7 in different datasets) as explained in Example 3.\n\nAnalysis. It is difficult to organize the neighborhood relationship of a dataset with high LID via a PG (graph quality is low), which degrades the search performance (the \"curse of dimensionality\" [26]). For searching on a PG, a large proportion of the search time is used for calculating the distance between vectors [45], so there exists a positive proportional relationship for Speedup and QPS. HNSW divides neighbors with different distances into different subgraphs via a hierarchical structure to accelerate the search, which increases memory overhead. NPGs diversify the neighbors' distribution on a single-layer graph, and achieves better search performance on the basis of inheriting the original PG's (i.e., NSW and KGraph) low memory overhead.\n\n\nEvaluation of Hybrid Query Methods\n\nIndex building. In Tab. 5, our approaches take the least time to complete index building on almost all datasets. As Tab. 5 and Tab. 6 show, although PQ-based methods (ADBV, Milvus, and Faiss) generally have smaller memory overhead and index size, they show the limited query performance in Fig. 9. If only PG-based methods are considered, ours are lower than other methods on most datasets (e.g., GIST1M).\n\nIn general, the higher the dataset's LID, the longer it takes to complete index building, and the larger the memory overhead of building the index and the index size.\n\nAnalysis. Clustering on large-scale high-dimensional vectors results in low index building efficiency for PQ-based schemes (such as ADBV); however, these methods' memory overhead and index size are tiny when they comress into raw vectors. Milvus divides datasets according to attributes [4], and indexes are built independently on each subset, which reduces memory overhead. Milvus's and ADBV's index size far exceeds Faiss's, because they must generate more indexes for the cost model [4], [12]. PG-based methods rely on raw feature vectors, which lead to a larger index size; nevertheless, our methods show the smallest index size among them, which is because our edge selection eliminates numerous redundant neighbors (Example 6). Considering the significant advantages of our methods in hybrid query performance, more memory overhead during index building and a larger index size are worthwhile. Additionally, our methods reach the state-of-the-art hybrid query performance with the highest index building efficiency, which is the priority of most practical applications [8].\n\nHybrid query performance. As Fig. 9 shows, our methods are significantly better than existing methods w.r.t QPS vs Recall trade-off across all datasets. For example, when Recall@10 > 0.99, the QPS of our methods is 2 to 3 orders of magnitude higher than others. Meanwhile, our methods' search performance is robustly scalable, which gives them a distinct advantage on harder datasets, since other methods (e.g., SPTAG) perform worse on these datasets. In addition, there is no Recall data for Milvus and ADBV between (about) 0.8 and 0.99, as their PQ-based plans are can achieve only a limited Recall; and to reach a high Recall (Recall@10 =1), they execute a linear scan. NHQ-NPG kgraph easily obtain a high Recall under high QPS, so that its QPS is too high to show in figure when Recall@10 < (about) 0.9.\n\nAnalysis. Existing methods are based on Strategies A and B (Fig. 2), which implement attribute filtering and vector similarity search separately, thereby limiting their ability to answer a hybrid query. In contrast, ours perform attribute filtering and vector similarity search concurrently by embedding feature vectors and attributes into a composite index. By jointly pruning, our methods avoid redundant computation and improve the query efficiency.\n\nEffect of different PGs under NHQ. We implement HNSW [9], NSG [8], and our NPG kgraph on NHQ, thus obtaining NHQ-HNSW, NHQ-NSG, and NHQ-NPG kgraph. We compare their performance on SIFT1M (with low LID) and GloVe (with high LID). As Fig. 10(a) and (b) illustrate, NHQ-NPG kgraph outperforms the others by a large margin, and this superiority becomes even more significant on the harder  Msong Audio Enron   ADBV  1,614  1,812  1,838  2,895  1,598  1,627  478  611  857  5,117  1,287  5,042  1,356  3,538  220  1,867  Milvus  2,131  2,526  2,645  3,389  2,116  2,113  607  933  621  2,795  761  2,112  954  1,639  215  1,534  Vearch  108  519  150  406  21  47  1  43  3,052  16,104  3,365  13,348 4,890  8,133  235  2,378  NGT  27  1,124  806  2,745  31  73  1  20  1,427  7,870  1,513  5,598  2,417  3,701  111  1,072  Faiss  1,591  1,721  1,838  2,791  1,598  1,627  478  611  790  5,117  1,287  5,042  1,356  3,538  220  1,867  SPTAG  456  2,690  609  1,555  614  1,574  27  276  2,821  12,503  2,491  8,611  4,293  9,031  376  2,082  NHQ-NPG nsw  17  121  83  406  40  207  2  23  2,575  11,920  2,441  9,310  3,   dataset. We may attribute this to NPGs' edge selection and joint pruning's optimization for NHQ-NPG kgraph, which further improve performance.\n\nDifferent recall requirements. In Fig. 9(i) and (a), as the number of results increases, the QPS of different methods degenerates. Note that the interval between our methods and others enlarges, which shows our methods' superiority when more targets need to be recalled. Analysis. For existing methods, to improve the Recall, we must obtain numerous candidates (intermediate results that satisfy one query constraint). For example, when k = 10 in Recall@k, the number of candidates must reach 10\u00d7 \u223c 1000\u00d7 larger than k, which undermines the query efficiency. In contrast, our methods directly return the final results without intermediate candidates.\n\n\nDimension of attributes (m).\n\nAttributes' dimension corresponds to the number of attributes in a set of attributes. As shown in Fig. 10(c) and (d), as the dimension increases, each method's query performance decreases by varying degrees. Our methods still retain the advantage in a higher attribute dimension by a large margin.\n\n\nSelectivity.\n\nA larger selectivity means fewer objects match the attributes constraint, which increases the difficulty in answering a hybrid query. In Fig. 10(e) and (f), Recall@10 decreases under a high selectivity for most methods. However, NHQ-NPG nsw appears to excel when there is large selectivity. One explanation is that, when the construction strategy inserts objects incrementally, it may link better with vertices possessing the same attributes but are farther away for feature vectors. This improves NHQ-NPG nsw's ability to search objects with matched attributes, especially such objects are few (that is, high selectivity). NHQ-NPG kgraph reaches the highest Recall with the least search time for different selectivity. In addition, Faiss is not shown in Fig.  10(e) because its Recall@10 is only about 0.7.\n\n\nNumber of threads.\n\nWe implement a parallel version of hybrid queries on NHQ-NPG kgraph, which divides an object set into multiple subsets and builds a NHQ-NPG kgraph on each subset, so that we can search in parallel on these NHQ-NPG kgraphs. As shown in Fig. 10(g), more threads in the higher Recall area (Recall@10 > 0.98) will improve query performance, while a single thread is more suitable for hybrid queries in the lower Recall area (Recall@10 < 0.95). We explain that to reach a higher Recall rate, we need to visit more vertices, where parallel search is more advantageous; but for a lower Recall rate, with a small number of vertices visited, the percentage of time consumed to frequently create and start a thread will increase (for the total query time), and search with multi-threads will degrade query performance. Nevertheless, in Fig. 10(h), NHQ-NPG kgraph under a parallel implementation shows better hybrid query performance than Milvus and ADBV.\n\n\nParameter Sensitivity\n\nFor NHQ, \u03c9 \u03bd and \u03c9 in Eq. 6 are a pair of parameters that regulate the weights of \u03b4(\u03bd(e i ), \u03bd(e j )) (abbr. as \u03b4) and \u03c7( (e i ), (e j )) (abbr. as \u03c7) in \u0393 (e i , e j ) (abbr. as \u0393 ), which impacts the query performance. In the following, based on experimental observations, we evaluate the different settings' effects on performance. Because of space constraints, we only show the execution results of NHQ-NPG kgraph on SIFT1M, considering that other datasets and methods working off NHQ show similar phenomena.\n\nIn general, we treat feature vectors and attributes equally; that is, \u03b4 and \u03c7 have the same contribution to \u0393 . We set \u03c9 \u03bd = \u03c7/(\u03b4 + \u03c7) and \u03c9 = \u03b4/(\u03b4 + \u03c7), so \u0393 = 2 \u00b7 \u03b4 \u00b7 \u03c7/(\u03b4 + \u03c7) is the harmonic mean of \u03b4 and \u03c7. As Fig. 10(i) (curve i) shows, this setting does not perform well for hybrid queries. We found that \u03c7 = 0 is a common occurrence (two objects possessing the same attributes), and then \u0393 = 0 no matter what value \u03b4 is, which contradicts the same contribution of the two. To resolve this, we adjust \u03c7 as (e-f) Performance under different selectivity, refer to Fig. 9 for the legend. (g-h) Performance under different number of threads. (i-j) Performance under different \u03c9\u03bd and \u03c9 . In (i), curve i: \u03c9\u03bd = \u03c7/(\u03b4 + \u03c7), \u03c9 = \u03b4/(\u03b4 + \u03c7); curve ii modifies \u03c7 by Eq. 8 following curve i; curve iii: \u03c9\u03bd = \u03b4 \u22121 max , \u03c9 = \u03c7 \u22121 max ; curve iv: \u03c9\u03bd = 1, \u03c9 = \u03b4/\u03c7max; and curve v: \u03c9 /\u03c9\u03bd = 10, 000.\n\u03c7 = 1, for \u03c7 = 0 \u03c7 \u00b7 c, for \u03c7 = 0 ,(8)\nwhere c is a given constant, and it holds that c > 1. This alleviates the defect caused by \u03c7 = 0, and it significantly improves the hybrid query performance (curve ii, c = 100).\n\nNevertheless, a suitable c is difficult to obtain, so we turn to the other two options to deal with the fact that \u03b4 and \u03c7 are in different value spaces. One option is to map both \u03b4 and \u03c7 to [0, 1] by setting \u03c9 \u03bd = \u03b4 \u22121 max and \u03c9 = \u03c7 \u22121 max , where \u03b4 max and \u03c7 max are the max value of \u03b4 and \u03c7 on a given object set S. We further promote query performance under this setting (curve iii). However, it needs to compute \u03b4 max of S in advance, which is not easy in a real-world scenario because S is dynamic. The other option is to map \u03c7 to \u03b4's value space, i.e., \u03c9 \u03bd = 1 and \u03c9 = \u03b4/\u03c7 max , where \u03c7 max = m, m is the dimension of attributes, and it can be obtained easily. As curve iv depicts, this approach achieves the best performance in all our settings. Additionally, we also study a naive setting method, where \u03c9 \u03bd and \u03c9 are fixed as constants. We traverse multiple possible \u03c9 /\u03c9 \u03bd values by grid search. Fig. 10(j) illustrates that different \u03c9 /\u03c9 \u03bd leads to a huge difference in hybrid query performance, and the optimal value falls to 10,000 (curve v in Fig. 10(i)), which makes it close to the median of all \u03b4 on S. Thus, we chose the settings of curve iv for all other experiments.\n\n\nUse Case Study\n\nWe deployed NHQ-NPG kgraph into an academic expert finding system [6] to provide hybrid query processing services for academic papers with attributes on the Paper dataset. An important component in such system is the semantically similar papers retrieval that supports attribute filtering. Given a user input hybrid query with a descriptive text as the unstructured constraint and paper's some attributes as the structured constraints, expert finding aims at extracting the top-k experts from top-m papers that have the semantically similar vectors to the given query text as well as satisfying the given attribute constraints. Because we extract the experts from the returned papers, the results of hybrid queries would directly affect the retrieved experts. As comparison, we also implemented two existing methods for retrieving papers (i.e., Vearch and HNSW).\n\nAnalysis. Tab. 7 shows the top-5 experts who have published papers on CVPR for a hybrid query having the abstract of [62] as the unstructured constraint and CVPR as an \n\n\nRELATED WORK\n\nTo provide a better sense of how our approach compares to other efforts in vector similarity search and hybrid query processing, here we briefly discuss related work by topic.\n\nVector similarity search. Vector similarity search-or approximate nearest neighbor search (ANNS)-is a task to obtain results similar to a given query through a well-designed vector index [4], [24]. According to the different indexes, we can divide existing methods into four categories: tree-based [36], [37], [63]; hashing-based [38], [39], [64]; quantizationbased [22], [35], [65]; and PG-based [7], [9], [24], [61], [66]. Recently, PGs have yielded state-of-the-art Speedup vs Recall trade-off [8], [26], which has sparked substantial interest. So far, researchers have proposed dozens of PGs using different optimizations. However, these works are full-fledged solutions for content retrieval of unstructured data, and they cannot deal effectively with attribute filtering [12].\n\nHybrid query processing. Hybrid queries entails advanced query processing with structured and unstructured constraints [4], which requires not only feature vectors' similarity, but also attributes' consistency [11], [17]. Increasingly, this technique is indispensable to many applications [12].\n\nExisting hybrid query methods [4], [12], [13], [14] perform vector similarity search and attribute filtering separately along Strategies A and B in Fig. 2, which leads to some limitations (L1-L4 in Sec. 1). By contrast, our methods nimbly and adeptly address issues without these limitations.\n\n\nCONCLUSION\n\nIn this paper, we tackle hybrid queries with structured and unstructured query constraints. We design a NHQ framework that is tailored for hybrid queries, which is friendly to most current PGs. We present two NPGs with optimized edge selection and routing strategies that obtain better performance compared to existing PGs. We deploy our proposed NPGs in NHQ to form two NPG-based hybrid query methods that are 10\u00d7 faster than the state-of-the-art competitors. Interesting future and ongoing work includes applying our proposed framework to multimodal search to cope with more complex query requirements.\n\nFig. 1 .\n1Fig. 1. A running example of different paper-retrieval schemes, including (a) vector similarity search and (c-d) hybrid query. Vector similarity search retrieves results with semantically similar content, but it cannot ensure the structured query constraints. A hybrid query processing obtains results that satisfy the structured and unstructured query constraints.\n\nFig. 3 .\n3NHQ framework working off Strategy C.\n\nFig. 3\n3overviews our NHQ framework, which consists of two modules: composite index and joint pruning.\n\nFig. 4 .\n4Neighbors of u i obtained by different edge selection strategies (we assume u i has a maximum of four neighbors).\n\nFig. 5 .\n5Different routing strategies on PG. The white vertices do not need to, but the blue ones must calculate the distance from the query.\n\nFig. 7 .\n7Hybrid Querying performance comparison of different strategies.\n\nFig. 8 .\n8QPS and Speedup vs Recall. Here, (a)-(d) are the QPS vs Recall@10 of different PG-based algorithms; (e)-(h) are the Speedup vs Recall@10; (i) is the QPS vs Recall@1 (the top right is better).\n\nFig. 9 .\n9(a-h) are the queries per second (QPS) vs Recall@10 of different hybrid query methods; (i) is the QPS vs Recall@1. The attributes' dimension of all datasets is three.\n\nFig. 10 .\n10(a-b) Effect of hybrid query with different PGs on NHQ. (c-d) Performance under different attribute dimensions, refer toFig. 9for the legend.\n\n\n, L. Lv, X. Xu, Y. Wang, Q. Yue, and J. Ni are with the Schoolof Computer Science and Technology, Hangzhou Dianzi University, \nHangzhou 310018, China. \nE-mail: {mzwang, llw, xxl, lsswyx, yq, hananoyuuki}@hdu.edu.cn \n\nCorresponding author: Xiaoliang Xu \n\naccuracy \n\nTABLE 1\n1Frequently Used NotationsNotations \nDescriptions \nS \nAn object set \nX \nThe feature vector space of all objects in S \nY \n\n\nTABLE 2\n2Statistics of real-world datasets.Dataset \nDimension # Base \n# Query LID [7], [26] Type \n\nUQ-V 2 \n256 \n1,000,000 10,000 \n7.2 \nVideo + Attributes \nMsong 3 \n420 \n992,272 \n200 \n9.5 \nAudio + Attributes \nAudio 4 \n192 \n53,387 \n200 \n5.6 \nAudio + Attributes \nSIFT1M 5 128 \n1,000,000 10,000 \n9.3 \nImage + Attributes \nGIST1M 5 960 \n1,000,000 1,000 \n18.9 \nImage + Attributes \nCrawl 6 \n300 \n1,989,995 10,000 \n15.7 \nText + Attributes \nGloVe 7 \n100 \n1,183,514 10,000 \n20.0 \nText + Attributes \nEnron 8 \n1,369 \n94,987 \n200 \n11.7 \nText + Attributes \nPaper 9 \n200 \n2,029,997 10,000 \n-\nText + Attributes \n\nthe local optimum (i.e., it reaches the vicinity of the result \nvertices). In S2, we set C = C \u222a R, and continue to perform \nlines 2-8 (without modifying line 5) to visit all neighbors of \nthe vertices in the search path comprehensively. Finally, R \nis returned as the query results. \n\nRemarks. (1) In our optimized joint pruning, different stages \nhave specific requirements when searching on a composite \nindex. In S1, it quickly reaches a small area of objects that \nare similar to the given query object's feature vector and \nattributes; while in S2, it accurately obtains top-k objects \nwith similar feature vectors and matched attributes. (2) \nGiven the NHQ-NPG kgraph and NHQ-NPG nsw built in \nSec. 5.1, we can apply the optimized joint pruning directly \non them to return hybrid query results efficiently. \n\n\n\nTABLE 3\n3Index build time, peak memory overhead, index size, and graph quality of different PG-based vector similarity search (the bold values are the best).Algorithm \nBuild Time (s) \nMemory Overhead (GB) \nIndex Size (MB) \nGraph Quality \nSIFT1M GIST1M GloVe Crawl SIFT1M GIST1M GloVe Crawl SIFT1M GIST1M GloVe Crawl SIFT1M GIST1M GloVe Crawl \n\nHNSW \n108 \n519 \n150 \n406 \n3.00 \n15.74 \n3.30 \n13.07 \n198 \n149 \n280 \n492 \n0.875 \n0.643 \n0.642 \n0.672 \nNSW \n34 \n294 \n27 \n224 \n2.88 \n15.73 \n3.57 \n12.85 \n160 \n149 \n371 \n416 \n0.831 \n0.515 \n0.481 \n0.598 \nKGraph \n31 \n228 \n101 \n71 \n4.61 \n8.41 \n7.34 \n8.46 \n504 \n465 \n686 \n774 \n0.998 \n0.992 \n0.942 \n0.929 \nDPG \n90 \n519 \n150 \n406 \n5.51 \n9.04 \n6.88 \n12.72 \n640 \n741 \n861 \n1,383 \n0.995 \n0.988 \n0.875 \n0.971 \nNSG \n195 \n1,906 \n656 \n1,392 \n4.80 \n17.03 \n12.07 \n24.39 \n100 \n58 \n68 \n107 \n0.547 \n0.363 \n0.479 \n0.280 \nNSSG \n329 \n1,163 \n1,618 \n2,982 \n9.53 \n16.63 \n8.80 \n11.71 \n80 \n101 \n74 \n111 \n0.578 \n0.393 \n0.405 \n0.348 \nNPG nsw \n27 \n285 \n105 \n231 \n2.87 \n15.72 \n3.29 \n12.83 \n155 \n145 \n275 \n408 \n0.621 \n0.418 \n0.460 \n0.446 \nNPG kgraph 37 \n172 \n215 \n320 \n4.75 \n7.16 \n9.16 \n15.58 \n95 \n55 \n59 \n91 \n0.541 \n0.398 \n0.431 \n0.273 \n\n\n\nTABLE 4\n4Peak memory overhead during search on PGs.Algorithm \nMemory Overhead (MB) \nSIFT1M \nGIST1M \nGloVe \nCrawl \n\nHNSW \n1,733 \n11,526 \n1,838 \n9,125 \nNSG \n1,609 \n7,564 \n1,958 \n14,799 \nNPG nsw \n1,691 \n11,522 \n1,834 \n9,041 \nNPG kgraph \n1,285 \n7,585 \n1,477 \n5,506 \n\n\n\nTABLE 5\n5Index build time and peak memory overhead of different hybrid query methods on eight real-world datasets (the bold items are the best). SIFT1M GIST1M GloVe Crawl UQ-V Msong Audio Enron SIFT1M GIST1M GloVe Crawl UQ-VAlgorithm \nBuild Time (s) \nMemory Overhead (MB) \n\n\nTABLE 6\n6Index size of different hybrid query methods. SIFT1M GIST1M GloVe Crawl UQ-V Msong Audio EnronAlgorithm \nIndex Size (MB) \nADBV \n113 \n343 \n130 \n292 \n148 \n200 \n11 \n75 \nMilvus \n126 \n438 \n144 \n347 \n174 \n245 \n15 \n105 \nVearch \n691 \n3,903 \n736 \n2,832 1,141 1,904 50 \n526 \nNGT \n672 \n3,939 \n665 \n2,688 1,171 1,803 49 \n530 \nFaiss \n40 \n54 \n47 \n83 \n42 \n45 \n3 \n11 \nSPTAG \n656 \n3,830 \n650 \n2,611 1,144 1,756 48 \n512 \nNHQ-NPG nsw \n648 \n3,745 \n550 \n3,050 1,098 1,786 52 \n529 \nNHQ-NPG kgraph 561 \n3,709 \n491 \n2,346 1,041 1,678 42 \n500 \n\n0.6 \n0.7 \n0.8 \n0.9 \n1.0 \n\n10 1 \n\n10 2 \n\n10 3 \n\n10 4 \n\nQPS (1/s) \n\nNHQ-NPG_kgraph \nNHQ-NPG_nsw \nVearch \nADBV \nMilvus \nFaiss \nSPTAG \nNGT \n\n0.6 \n0.7 \n0.8 \n0.9 \n1.0 \n\n10 1 \n\n10 2 \n\n10 3 \n\n10 4 \n\nQPS (1/s) \n\n\n\nTABLE 7\n7Experts output by the academic expert finding engine under different query methods (the bold items are correct).attribute constraint under a given query time. Our method obtains the best results in compared methods. HNSW only returns the experts who are relevant to the query text without considering the CVPR constraint. Although Vearch adds the CVPR constraints, the experts are insufficient after attribute filtering. However, the query latency will surge when increasing the number of candidate experts.HNSW \nVearch \nNHQ-NPG kgraph \n\nJohn Collomosse \n\n\u221a \n\nJohn Collomosse \n\n\u221a \n\nJohn Collomosse \n\n\u221a \n\nMayu Iwata \u00d7 \nRahul Duggal \n\n\u221a \n\nRahul Duggal \n\n\u221a \n\nYanhao Zhang \u00d7 \n-\nHosnieh Sattar \n\n\u221a \n\nTakuma Yamaguchi \u00d7 \n-\nThanh-Toan Do \n\n\u221a \n\nRahul Duggal \n\n\u221a \n\n-\nGregory Zelinsky \n\n\u221a \n\n\nACKNOWLEDGMENTS\nIn Tab. 2, LID indicates local intrinsic dimensionality, and a larger LID value implies a \"harder. affiliation, venue, and topic). dataset [26https://github.com/AshenOn3/NHQ content and structured attributes (e.g., affiliation, venue, and topic). In Tab. 2, LID indicates local intrinsic dimensionality, and a larger LID value implies a \"harder\" dataset [26].\n\nReturn of the lernaean hydra: Experimental evaluation of data series approximate similarity search. K Echihabi, K Zoumpatianos, T Palpanas, H Benbrahim, PVLDB. 133K. Echihabi, K. Zoumpatianos, T. Palpanas, and H. Benbrahim, \"Return of the lernaean hydra: Experimental evaluation of data series approximate similarity search,\" PVLDB, vol. 13, no. 3, pp. 403-420, 2019.\n\nMining competitors from large unstructured datasets. G Valkanas, T Lappas, D Gunopulos, TKDE. 299G. Valkanas, T. Lappas, and D. Gunopulos, \"Mining competitors from large unstructured datasets,\" TKDE, vol. 29, no. 9, pp. 1971- 1984, 2017.\n\n80 percent of your data will be unstructured in five years. T King, T. King, \"80 percent of your data will be unstructured in five years,\" https://solutionsreview.com/data-management/80- percent-of-your-data-will-be-unstructured-in-five-years/, 2019.\n\nMilvus: A purpose-built vector data management system. J Wang, X Yi, R Guo, H Jin, P Xu, S Li, X Wang, X Guo, C Li, X Xu, K Yu, Y Yuan, Y Zou, J Long, Y Cai, Z Li, Z Zhang, Y Mo, J Gu, R Jiang, Y Wei, C Xie, in SIGMOD, 2021J. Wang, X. Yi, R. Guo, H. Jin, P. Xu, S. Li, X. Wang, X. Guo, C. Li, X. Xu, K. Yu, Y. Yuan, Y. Zou, J. Long, Y. Cai, Z. Li, Z. Zhang, Y. Mo, J. Gu, R. Jiang, Y. Wei, and C. Xie, \"Milvus: A purpose-built vector data management system,\" in SIGMOD, 2021, pp. 2614-2627.\n\nScalable knn graph construction for visual descriptors. J Wang, J Wang, G Zeng, Z Tu, R Gan, S Li, CVPR. J. Wang, J. Wang, G. Zeng, Z. Tu, R. Gan, and S. Li, \"Scalable k- nn graph construction for visual descriptors,\" in CVPR, 2012, pp. 1106-1113.\n\nAcademic expert finding via (k, P)-core based embedding over heterogeneous graphs. Anonymous, accepted by ICDE 2022 (Early NotificationAnonymous, \"Academic expert finding via (k, P)-core based em- bedding over heterogeneous graphs,\" 2021, accepted by ICDE 2022 (Early Notification).\n\nHigh dimensional similarity search with satellite system graph: Efficiency, scalability, and unindexed query compatibility. C Fu, C Wang, D Cai, TPAMIC. Fu, C. Wang, and D. Cai, \"High dimensional similarity search with satellite system graph: Efficiency, scalability, and unindexed query compatibility,\" TPAMI, 2021.\n\nFast approximate nearest neighbor search with the navigating spreading-out graph. C Fu, C Xiang, C Wang, D Cai, PVLDB. 125C. Fu, C. Xiang, C. Wang, and D. Cai, \"Fast approximate near- est neighbor search with the navigating spreading-out graph,\" PVLDB, vol. 12, no. 5, pp. 461-474, 2019.\n\nEfficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. Y A Malkov, D A Yashunin, TPAMI. 424Y. A. Malkov and D. A. Yashunin, \"Efficient and robust approx- imate nearest neighbor search using hierarchical navigable small world graphs,\" TPAMI, vol. 42, no. 4, pp. 824-836, 2020.\n\nGraph-based nearest neighbor search: From practice to theory. L Prokhorenkova, A Shekhovtsov, ICML. L. Prokhorenkova and A. Shekhovtsov, \"Graph-based nearest neighbor search: From practice to theory,\" in ICML, 2020, pp. 7803- 7813.\n\nSimilarity query processing for high-dimensional data. J Qin, W Wang, C Xiao, Y Zhang, PVLDB. 1312J. Qin, W. Wang, C. Xiao, and Y. Zhang, \"Similarity query pro- cessing for high-dimensional data,\" PVLDB, vol. 13, no. 12, pp. 3437-3440, 2020.\n\nAnalyticdb-v: A hybrid analytical engine towards query fusion for structured and unstructured data. C Wei, B Wu, S Wang, R Lou, C Zhan, F Li, Y Cai, PVLDB. 1312C. Wei, B. Wu, S. Wang, R. Lou, C. Zhan, F. Li, and Y. Cai, \"Analyticdb-v: A hybrid analytical engine towards query fusion for structured and unstructured data,\" PVLDB, vol. 13, no. 12, pp. 3152-3165, 2020.\n\nThe design and implementation of a real time visual search system on JD e-commerce platform. J Li, H Liu, C Gui, J Chen, Z Ni, N Wang, Y Chen, Proceedings of the 19th International Middleware Conference. the 19th International Middleware ConferenceJ. Li, H. Liu, C. Gui, J. Chen, Z. Ni, N. Wang, and Y. Chen, \"The design and implementation of a real time visual search system on JD e-commerce platform,\" in Proceedings of the 19th International Middleware Conference, 2018, pp. 9-16.\n\nMultiattribute approximate nearest neighbor search based on navigable small world graph. X Xu, C Li, Y Wang, Y Xia, Concurrency and Computation: Practice and Experience. 3224X. Xu, C. Li, Y. Wang, and Y. Xia, \"Multiattribute approximate nearest neighbor search based on navigable small world graph,\" Concurrency and Computation: Practice and Experience, vol. 32, no. 24, 2020.\n\nPase: Postgresql ultra-highdimensional approximate nearest neighbor search extension. W Yang, T Li, G Fang, H Wei, SIGMOD. W. Yang, T. Li, G. Fang, and H. Wei, \"Pase: Postgresql ultra-high- dimensional approximate nearest neighbor search extension,\" in SIGMOD, 2020, p. 2241-2253.\n\nComposing text and image for image retrieval -an empirical odyssey. N Vo, L Jiang, C Sun, K Murphy, L Li, L Fei-Fei, J Hays, CVPR. N. Vo, L. Jiang, C. Sun, K. Murphy, L. Li, L. Fei-Fei, and J. Hays, \"Composing text and image for image retrieval -an empirical odyssey,\" in CVPR, 2019, pp. 6439-6448.\n\nHighdimensional similarity query processing for data science. J Qin, W Wang, C Xiao, Y Zhang, Y Wang, SIGKDD. J. Qin, W. Wang, C. Xiao, Y. Zhang, and Y. Wang, \"High- dimensional similarity query processing for data science,\" in SIGKDD, 2021, pp. 4062-4063.\n\nA distributed system for embedding-based retrieval. Jingdong, 2020Jingdong, \"A distributed system for embedding-based retrieval,\" https://github.com/vearch/vearch, 2020.\n\nSptag: A library for fast approximate nearest neighbor search. Microsoft, Microsoft, \"Sptag: A library for fast approximate nearest neighbor search,\" https://github.com/microsoft/SPTAG, 2020.\n\nNearest neighbor search with neighborhood graph and tree for high-dimensional data. Yahoo, Yahoo, \"Nearest neighbor search with neighbor- hood graph and tree for high-dimensional data,\" https://github.com/yahoojapan/NGT, 2016.\n\nA library for efficient similarity search and clustering of dense vectors. Facebook, Facebook, \"A library for efficient similarity search and clustering of dense vectors,\" https://github.com/facebookresearch/faiss, 2018.\n\nProduct quantization for nearest neighbor search. H J\u00e9gou, M Douze, C Schmid, TPAMI. 331H. J\u00e9gou, M. Douze, and C. Schmid, \"Product quantization for nearest neighbor search,\" TPAMI, vol. 33, no. 1, pp. 117-128, 2011.\n\nThe world's most advanced open-source vector database. Milvus, Milvus, \"The world's most advanced open-source vector database,\" https://github.com/milvus-io/milvus, 2019.\n\nA comprehensive survey and experimental comparison of graph-based approximate nearest neighbor search. M Wang, X Xu, Q Yue, Y Wang, PVLDB. 1411M. Wang, X. Xu, Q. Yue, and Y. Wang, \"A comprehensive survey and experimental comparison of graph-based approximate nearest neighbor search,\" PVLDB, vol. 14, no. 11, pp. 1964-1978, 2021.\n\nBenchmarking nearest neighbors. E Bernhardsson, M Aum\u00fcller, A Faithfull, E. Bernhardsson, M. Aum\u00fcller, and A. Faithfull, \"Bench- marking nearest neighbors,\" https://github.com/erikbern/ann- benchmarks, 2018.\n\nApproximate nearest neighbor search on high dimensional data -experiments, analyses, and improvement. W Li, Y Zhang, Y Sun, W Wang, M Li, W Zhang, X Lin, TKDE. 328W. Li, Y. Zhang, Y. Sun, W. Wang, M. Li, W. Zhang, and X. Lin, \"Approximate nearest neighbor search on high dimensional data -experiments, analyses, and improvement,\" TKDE, vol. 32, no. 8, pp. 1475-1488, 2020.\n\nFast approximate similarity search based on degree-reduced neighborhood graphs. K Aoyama, K Saito, H Sawada, N Ueda, SIGKDD. K. Aoyama, K. Saito, H. Sawada, and N. Ueda, \"Fast approximate similarity search based on degree-reduced neighborhood graphs,\" in SIGKDD, 2011, pp. 1055-1063.\n\nRank-based similarity search: Reducing the dimensional dependence. M E Houle, M Nett, TPAMI. 371M. E. Houle and M. Nett, \"Rank-based similarity search: Reducing the dimensional dependence,\" TPAMI, vol. 37, no. 1, pp. 136-150, 2014.\n\nBert: Pretraining of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, abs/1810.04805CoRR. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre- training of deep bidirectional transformers for language under- standing,\" CoRR, vol. abs/1810.04805, 2018.\n\nHigh-dimensional vector similarity search: From time series to deep network embeddings. K Echihabi, SIGMOD. K. Echihabi, \"High-dimensional vector similarity search: From time series to deep network embeddings,\" in SIGMOD, 2020, pp. 2829-2832.\n\nHighdimensional similarity search for scalable data science. K Echihabi, K Zoumpatianos, T Palpanas, ICDE. K. Echihabi, K. Zoumpatianos, and T. Palpanas, \"High- dimensional similarity search for scalable data science,\" in ICDE, 2021, pp. 2369-2372.\n\nSimilarity search in high dimensions via hashing. A Gionis, P Indyk, R Motwani, VLDB. A. Gionis, P. Indyk, and R. Motwani, \"Similarity search in high dimensions via hashing,\" in VLDB, 1999, pp. 518-529.\n\nOn nonmetric similarity search problems in complex domains. T Skopal, B Bustos, ACM Computing Surveys (CSUR). 434T. Skopal and B. Bustos, \"On nonmetric similarity search problems in complex domains,\" ACM Computing Surveys (CSUR), vol. 43, no. 4, pp. 1-50, 2011.\n\nPerformance analysis of graph-based methods for exact and approximate similarity search in metric spaces. L C Shimomura, M R Vieira, D S Kaster, International Conference on Similarity Search and Applications. L. C. Shimomura, M. R. Vieira, and D. S. Kaster, \"Performance analysis of graph-based methods for exact and approximate simi- larity search in metric spaces,\" in International Conference on Simi- larity Search and Applications, 2018, pp. 18-32.\n\nAccelerating large-scale inference with anisotropic vector quantization. R Guo, P Sun, E Lindgren, Q Geng, D Simcha, F Chern, S Kumar, ICML. R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and S. Kumar, \"Accelerating large-scale inference with anisotropic vector quantization,\" in ICML, 2020, pp. 3887-3896.\n\nOptimised kd-trees for fast image descriptor matching. C Silpa-Anan, R I Hartley, CVPR. C. Silpa-Anan and R. I. Hartley, \"Optimised kd-trees for fast image descriptor matching,\" in CVPR, 2008.\n\nHd-index: Pushing the scalability-accuracy boundary for approximate knn search in high-dimensional spaces. A Arora, S Sinha, P Kumar, A Bhattacharya, PVLDB. 118A. Arora, S. Sinha, P. Kumar, and A. Bhattacharya, \"Hd-index: Pushing the scalability-accuracy boundary for approximate knn search in high-dimensional spaces,\" PVLDB, vol. 11, no. 8, pp. 906-919, 2018.\n\nidec: Indexable distance estimating codes for approximate nearest neighbor search. L Gong, H Wang, M Ogihara, J Xu, PVLDB. 139L. Gong, H. Wang, M. Ogihara, and J. Xu, \"idec: Indexable dis- tance estimating codes for approximate nearest neighbor search,\" PVLDB, vol. 13, no. 9, pp. 1483-1497, 2020.\n\nQueryaware locality-sensitive hashing for approximate nearest neighbor search. Q Huang, J Feng, Y Zhang, Q Fang, W Ng, PVLDB. 91Q. Huang, J. Feng, Y. Zhang, Q. Fang, and W. Ng, \"Query- aware locality-sensitive hashing for approximate nearest neighbor search,\" PVLDB, vol. 9, no. 1, pp. 1-12, 2015.\n\nRelational graph neural network with hierarchical attention for knowledge graph completion. Z Zhang, F Zhuang, H Zhu, Z Shi, H Xiong, Q He, AAAI. Z. Zhang, F. Zhuang, H. Zhu, Z. Shi, H. Xiong, and Q. He, \"Relational graph neural network with hierarchical attention for knowledge graph completion,\" in AAAI, 2020, pp. 9612-9619.\n\nLabel informed attributed network embedding. X Huang, J Li, X Hu, WSDM. X. Huang, J. Li, and X. Hu, \"Label informed attributed network embedding,\" in WSDM, 2017, pp. 731-739.\n\nA survey of knowledge graph embedding and their applications. S Choudhary, T Luthra, A Mittal, R Singh, abs/2107.07842CoRR. S. Choudhary, T. Luthra, A. Mittal, and R. Singh, \"A survey of knowledge graph embedding and their applications,\" CoRR, vol. abs/2107.07842, 2021.\n\nBuilding a largescale multimodal knowledge base for visual question answering. Y Zhu, C Zhang, C R\u00e9, L Fei-Fei, abs/1507.05670CoRR. Y. Zhu, C. Zhang, C. R\u00e9, and L. Fei-Fei, \"Building a large- scale multimodal knowledge base for visual question answering,\" CoRR, vol. abs/1507.05670, 2015.\n\nMultimodal knowledge graph for deep learning papers and code. A V Kannan, D Fradkin, I Akrotirianakis, T Kulahcioglu, A Canedo, A Roy, S Yu, A V Malawade, M A A Faruque, CIKM. A. V. Kannan, D. Fradkin, I. Akrotirianakis, T. Kulahcioglu, A. Canedo, A. Roy, S. Yu, A. V. Malawade, and M. A. A. Faruque, \"Multimodal knowledge graph for deep learning papers and code,\" in CIKM, 2020, pp. 3417-3420.\n\nSONG: approximate nearest neighbor search on GPU. W Zhao, S Tan, P Li, ICDE. W. Zhao, S. Tan, and P. Li, \"SONG: approximate nearest neighbor search on GPU,\" in ICDE, 2020, pp. 1033-1044.\n\nApproximate k-nn graph construction: A generic online approach. W.-L Zhao, H Wang, C.-W Ngo, IEEE Transactions on Multimedia. W.-L. Zhao, H. Wang, and C.-W. Ngo, \"Approximate k-nn graph construction: A generic online approach,\" IEEE Transactions on Multimedia, 2021.\n\nLatent semantics encoding for label distribution learning. S Xu, L Shang, F Shen, IJCAI. S. Xu, L. Shang, and F. Shen, \"Latent semantics encoding for label distribution learning,\" in IJCAI, 2019, pp. 3982-3988.\n\nOrdinal measures for iris recognition. Z Sun, T Tan, TPAMI. 3112Z. Sun and T. Tan, \"Ordinal measures for iris recognition,\" TPAMI, vol. 31, no. 12, pp. 2211-2226, 2008.\n\nAn improved grid search algorithm of svr parameters optimization. Q Huang, J Mao, Y Liu, ICCT. Q. Huang, J. Mao, and Y. Liu, \"An improved grid search algorithm of svr parameters optimization,\" in ICCT, 2012, pp. 1022-1026.\n\nApproximate nearest neighbor algorithm based on navigable small world graphs. Y Malkov, A Ponomarenko, A Logvinov, V Krylov, Information Systems. 45Y. Malkov, A. Ponomarenko, A. Logvinov, and V. Krylov, \"Approx- imate nearest neighbor algorithm based on navigable small world graphs,\" Information Systems, vol. 45, pp. 61-68, 2014.\n\nEfficient k-nearest neighbor graph construction for generic similarity measures. W Dong, M Charikar, K Li, WWWW. Dong, M. Charikar, and K. Li, \"Efficient k-nearest neighbor graph construction for generic similarity measures,\" in WWW, 2011, pp. 577-586.\n\nGraph based nearest neighbor search: Promises and failures. P C Lin, W L Zhao, abs/1904.02077CoRR. P. C. Lin and W. L. Zhao, \"Graph based nearest neighbor search: Promises and failures,\" CoRR, vol. abs/1904.02077, 2019.\n\nThe relative neighbourhood graph of a finite planar set. G T Toussaint, Pattern recognition. 124G. T. Toussaint, \"The relative neighbourhood graph of a finite planar set,\" Pattern recognition, vol. 12, no. 4, pp. 261-268, 1980.\n\nProximity graphs: E, \u03b4, \u03b4, \u03c7 and \u03c9. P Bose, V Dujmovi\u0107, F Hurtado, J Iacono, S Langerman, H Meijer, V Sacrist\u00e1n, M Saumell, D R Wood, International Journal of Computational Geometry & Applications. 2205P. Bose, V. Dujmovi\u0107, F. Hurtado, J. Iacono, S. Langerman, H. Mei- jer, V. Sacrist\u00e1n, M. Saumell, and D. R. Wood, \"Proximity graphs: E, \u03b4, \u03b4, \u03c7 and \u03c9,\" International Journal of Computational Geometry & Applications, vol. 22, no. 05, pp. 439-469, 2012.\n\nConstruction of voronoi diagram on the upper half-plane. K Onishi, N Takayama, Transactions on Fundamentals of Electronics. 794Communications and Computer SciencesK. Onishi and N. Takayama, \"Construction of voronoi diagram on the upper half-plane,\" Transactions on Fundamentals of Electronics, Communications and Computer Sciences, vol. 79, no. 4, pp. 533-539, 1996.\n\nTwo-stage routing with optimized guided search and greedy algorithm on proximity graph. X Xu, M Wang, Y Wang, D Ma, 107305Knowledge-Based SystemsX. Xu, M. Wang, Y. Wang, and D. Ma, \"Two-stage routing with optimized guided search and greedy algorithm on proximity graph,\" Knowledge-Based Systems, p. 107305, 2021.\n\nHierarchical clustering-based graphs for large scale approximate nearest neighbor search. J A V Mu\u00f1oz, M A Gon\u00e7alves, Z Dias, R Da Silva, Torres, Pattern Recognition. 96106970J. A. V. Mu\u00f1oz, M. A. Gon\u00e7alves, Z. Dias, and R. da Silva Torres, \"Hierarchical clustering-based graphs for large scale approximate nearest neighbor search,\" Pattern Recognition, vol. 96, p. 106970, 2019.\n\nLearning to route in similarity graphs. D Baranchuk, D Persiyanov, A Sinitsin, A Babenko, ICML. 97D. Baranchuk, D. Persiyanov, A. Sinitsin, and A. Babenko, \"Learn- ing to route in similarity graphs,\" in ICML, vol. 97, 2019, pp. 475- 484.\n\nImproving approximate nearest neighbor search through learned adaptive early termination. C Li, M Zhang, D G Andersen, Y He, SIGMOD. C. Li, M. Zhang, D. G. Andersen, and Y. He, \"Improving ap- proximate nearest neighbor search through learned adaptive early termination,\" in SIGMOD, 2020, pp. 2539-2554.\n\nBeing prepared in a sparse world: The case of KNN graph construction. A Boutet, A Kermarrec, N Mittal, F Ta\u00efani, ICDE. A. Boutet, A. Kermarrec, N. Mittal, and F. Ta\u00efani, \"Being prepared in a sparse world: The case of KNN graph construction,\" in ICDE, 2016, pp. 241-252.\n\nHM-ANN: efficient billion-point nearest neighbor search on heterogeneous memory. J Ren, M Zhang, D Li, NeurIPSJ. Ren, M. Zhang, and D. Li, \"HM-ANN: efficient billion-point nearest neighbor search on heterogeneous memory,\" in NeurIPS, 2020.\n\nLivesketch: Query perturbations for guided sketch-based visual search. J P Collomosse, T Bui, H Jin, CVPR. J. P. Collomosse, T. Bui, and H. Jin, \"Livesketch: Query perturba- tions for guided sketch-based visual search,\" in CVPR, 2019, pp. 2879-2887.\n\nScalable nearest neighbor algorithms for high dimensional data. M Muja, D G Lowe, TPAMI. 3611M. Muja and D. G. Lowe, \"Scalable nearest neighbor algorithms for high dimensional data,\" TPAMI, vol. 36, no. 11, pp. 2227-2240, 2014.\n\nI/O efficient approximate nearest neighbour search based on learned functions. M Li, Y Zhang, Y Sun, W Wang, I W Tsang, X Lin, ICDE. M. Li, Y. Zhang, Y. Sun, W. Wang, I. W. Tsang, and X. Lin, \"I/O efficient approximate nearest neighbour search based on learned functions,\" in ICDE, 2020, pp. 289-300.\n\nCache locality is not enough: High-performance nearest neighbor search with product quantization fast scan. F Andr\u00e9, A Kermarrec, N L Scouarnec, PVLDB. 94F. Andr\u00e9, A. Kermarrec, and N. L. Scouarnec, \"Cache locality is not enough: High-performance nearest neighbor search with product quantization fast scan,\" PVLDB, vol. 9, no. 4, pp. 288-299, 2015.\n\nDiskann: Fast accurate billion-point nearest neighbor search on a single node. S Subramanya, F Devvrit, H V Simhadri, R Krishnawamy, R Kadekodi, NeurIPS. 32S. Jayaram Subramanya, F. Devvrit, H. V. Simhadri, R. Krish- nawamy, and R. Kadekodi, \"Diskann: Fast accurate billion-point nearest neighbor search on a single node,\" in NeurIPS, vol. 32, 2019.\n", "annotations": {"author": "[{\"end\":117,\"start\":103},{\"end\":129,\"start\":118},{\"end\":143,\"start\":130},{\"end\":169,\"start\":144},{\"end\":180,\"start\":170},{\"end\":194,\"start\":181}]", "publisher": null, "author_last_name": "[{\"end\":116,\"start\":112},{\"end\":128,\"start\":126},{\"end\":142,\"start\":140},{\"end\":168,\"start\":164},{\"end\":179,\"start\":176},{\"end\":193,\"start\":191}]", "author_first_name": "[{\"end\":111,\"start\":103},{\"end\":125,\"start\":118},{\"end\":139,\"start\":130},{\"end\":163,\"start\":156},{\"end\":175,\"start\":170},{\"end\":190,\"start\":181}]", "author_affiliation": null, "title": "[{\"end\":100,\"start\":1},{\"end\":294,\"start\":195}]", "venue": null, "abstract": "[{\"end\":2256,\"start\":297}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2413,\"start\":2410},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2418,\"start\":2415},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2532,\"start\":2529},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2742,\"start\":2739},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2819,\"start\":2816},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2840,\"start\":2837},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2872,\"start\":2869},{\"end\":3669,\"start\":3665},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3673,\"start\":3670},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3678,\"start\":3675},{\"end\":3683,\"start\":3680},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3689,\"start\":3685},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3986,\"start\":3983},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3992,\"start\":3988},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4217,\"start\":4214},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4223,\"start\":4219},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4229,\"start\":4225},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4235,\"start\":4231},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4251,\"start\":4248},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4257,\"start\":4253},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4657,\"start\":4654},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4663,\"start\":4659},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4669,\"start\":4665},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4675,\"start\":4671},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4681,\"start\":4677},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4687,\"start\":4683},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4693,\"start\":4689},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5427,\"start\":5423},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5957,\"start\":5953},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6059,\"start\":6055},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6069,\"start\":6065},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6081,\"start\":6077},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6128,\"start\":6124},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6191,\"start\":6190},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6196,\"start\":6192},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6379,\"start\":6376},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6392,\"start\":6388},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6761,\"start\":6758},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7912,\"start\":7908},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8400,\"start\":8396},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8612,\"start\":8611},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9788,\"start\":9784},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9872,\"start\":9868},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9887,\"start\":9884},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10015,\"start\":10011},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10136,\"start\":10132},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10142,\"start\":10138},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10975,\"start\":10971},{\"end\":13396,\"start\":13393},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15911,\"start\":15910},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16275,\"start\":16271},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16281,\"start\":16277},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17096,\"start\":17092},{\"end\":17384,\"start\":17381},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17390,\"start\":17386},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17396,\"start\":17392},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17965,\"start\":17962},{\"end\":17991,\"start\":17988},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18081,\"start\":18077},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18215,\"start\":18212},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":18221,\"start\":18217},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18227,\"start\":18223},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18233,\"start\":18229},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18714,\"start\":18710},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18874,\"start\":18870},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18908,\"start\":18905},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":18914,\"start\":18910},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18920,\"start\":18916},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18926,\"start\":18922},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20077,\"start\":20073},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20083,\"start\":20079},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20100,\"start\":20096},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20106,\"start\":20102},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20126,\"start\":20122},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20132,\"start\":20128},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20175,\"start\":20172},{\"end\":20180,\"start\":20177},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20214,\"start\":20211},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20220,\"start\":20216},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20414,\"start\":20411},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20420,\"start\":20416},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20555,\"start\":20552},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20561,\"start\":20557},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20567,\"start\":20563},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20573,\"start\":20569},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20751,\"start\":20747},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21454,\"start\":21451},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23273,\"start\":23270},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23279,\"start\":23275},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23317,\"start\":23313},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23499,\"start\":23495},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23822,\"start\":23819},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24111,\"start\":24108},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24188,\"start\":24184},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24194,\"start\":24190},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24454,\"start\":24450},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":26589,\"start\":26585},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26595,\"start\":26591},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":26601,\"start\":26597},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":26762,\"start\":26758},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":26768,\"start\":26764},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27089,\"start\":27086},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":27095,\"start\":27091},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27101,\"start\":27097},{\"end\":28146,\"start\":28143},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28235,\"start\":28232},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28348,\"start\":28344},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28360,\"start\":28357},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":29524,\"start\":29520},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":30063,\"start\":30059},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31167,\"start\":31164},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":35575,\"start\":35571},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35703,\"start\":35700},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35709,\"start\":35705},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":35715,\"start\":35711},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":39256,\"start\":39252},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":39494,\"start\":39491},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":39500,\"start\":39496},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":39660,\"start\":39656},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":39688,\"start\":39684},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":39704,\"start\":39700},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":39829,\"start\":39825},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":40764,\"start\":40760},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":40826,\"start\":40822},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":41018,\"start\":41014},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":41742,\"start\":41738},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":41950,\"start\":41946},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":43219,\"start\":43215},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":43337,\"start\":43334},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":44459,\"start\":44455},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":44487,\"start\":44483},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":46454,\"start\":46450},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":46653,\"start\":46649},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":46815,\"start\":46811},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":46961,\"start\":46957},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":46967,\"start\":46963},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":47009,\"start\":47005},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":47154,\"start\":47150},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":47160,\"start\":47156},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":47339,\"start\":47335},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":47576,\"start\":47572},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":49825,\"start\":49822},{\"end\":49830,\"start\":49827},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":49836,\"start\":49832},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":50411,\"start\":50407},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":50427,\"start\":50423},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":50811,\"start\":50808},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":50944,\"start\":50940},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":51458,\"start\":51454},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":51622,\"start\":51618},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":52021,\"start\":52017},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":52568,\"start\":52564},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":55246,\"start\":55243},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":55252,\"start\":55248},{\"end\":55723,\"start\":55720},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":55820,\"start\":55816},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":55826,\"start\":55822},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":55838,\"start\":55834},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":55948,\"start\":55944},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":56040,\"start\":56036},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":56147,\"start\":56144},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":56243,\"start\":56240},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":56255,\"start\":56252},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":56472,\"start\":56468},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":56488,\"start\":56484},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":56636,\"start\":56632},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":56702,\"start\":56698},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":56745,\"start\":56742},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":56751,\"start\":56747},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":56832,\"start\":56828},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":56862,\"start\":56858},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":56868,\"start\":56864},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":56963,\"start\":56959},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":57136,\"start\":57132},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":57269,\"start\":57265},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":57299,\"start\":57295},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":58711,\"start\":58707},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":59406,\"start\":59403},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":59411,\"start\":59408},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":59926,\"start\":59923},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":60509,\"start\":60505},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":60537,\"start\":60534},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":60543,\"start\":60539},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":64670,\"start\":64666},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":64791,\"start\":64787},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":66127,\"start\":66124},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":66326,\"start\":66323},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":66332,\"start\":66328},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":66915,\"start\":66912},{\"end\":68237,\"start\":68234},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":68246,\"start\":68243},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":75133,\"start\":75130},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":76049,\"start\":76045},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":76480,\"start\":76477},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":76486,\"start\":76482},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":76592,\"start\":76588},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":76598,\"start\":76594},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":76604,\"start\":76600},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":76624,\"start\":76620},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":76630,\"start\":76626},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":76636,\"start\":76632},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":76660,\"start\":76656},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":76666,\"start\":76662},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":76672,\"start\":76668},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":76690,\"start\":76687},{\"end\":76695,\"start\":76692},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":76701,\"start\":76697},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":76707,\"start\":76703},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":76713,\"start\":76709},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":76790,\"start\":76787},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":76796,\"start\":76792},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":77071,\"start\":77067},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":77196,\"start\":77193},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":77288,\"start\":77284},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":77294,\"start\":77290},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":77367,\"start\":77363},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":77403,\"start\":77400},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":77409,\"start\":77405},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":77415,\"start\":77411},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":77421,\"start\":77417}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":78658,\"start\":78282},{\"attributes\":{\"id\":\"fig_1\"},\"end\":78707,\"start\":78659},{\"attributes\":{\"id\":\"fig_2\"},\"end\":78811,\"start\":78708},{\"attributes\":{\"id\":\"fig_3\"},\"end\":78936,\"start\":78812},{\"attributes\":{\"id\":\"fig_4\"},\"end\":79080,\"start\":78937},{\"attributes\":{\"id\":\"fig_5\"},\"end\":79155,\"start\":79081},{\"attributes\":{\"id\":\"fig_6\"},\"end\":79358,\"start\":79156},{\"attributes\":{\"id\":\"fig_7\"},\"end\":79536,\"start\":79359},{\"attributes\":{\"id\":\"fig_8\"},\"end\":79691,\"start\":79537},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":79957,\"start\":79692},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":80088,\"start\":79958},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":81502,\"start\":80089},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":82650,\"start\":81503},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":82915,\"start\":82651},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":83190,\"start\":82916},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":83924,\"start\":83191},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":84716,\"start\":83925}]", "paragraph": "[{\"end\":5342,\"start\":2272},{\"end\":5810,\"start\":5344},{\"end\":6762,\"start\":5812},{\"end\":7252,\"start\":6764},{\"end\":7480,\"start\":7254},{\"end\":8022,\"start\":7514},{\"end\":8965,\"start\":8024},{\"end\":9789,\"start\":8967},{\"end\":11106,\"start\":9791},{\"end\":11753,\"start\":11108},{\"end\":12716,\"start\":11788},{\"end\":14411,\"start\":12718},{\"end\":14457,\"start\":14413},{\"end\":15051,\"start\":14459},{\"end\":15110,\"start\":15053},{\"end\":16012,\"start\":15112},{\"end\":16237,\"start\":16030},{\"end\":17200,\"start\":16260},{\"end\":17527,\"start\":17202},{\"end\":18107,\"start\":17529},{\"end\":18495,\"start\":18156},{\"end\":18598,\"start\":18497},{\"end\":18601,\"start\":18600},{\"end\":18927,\"start\":18603},{\"end\":19342,\"start\":18929},{\"end\":19529,\"start\":19344},{\"end\":19788,\"start\":19556},{\"end\":20313,\"start\":19790},{\"end\":20899,\"start\":20315},{\"end\":21342,\"start\":20901},{\"end\":21812,\"start\":21344},{\"end\":22486,\"start\":21842},{\"end\":23500,\"start\":22488},{\"end\":24751,\"start\":23502},{\"end\":25079,\"start\":24765},{\"end\":25902,\"start\":25081},{\"end\":26318,\"start\":25936},{\"end\":27274,\"start\":26332},{\"end\":27574,\"start\":27314},{\"end\":27704,\"start\":27643},{\"end\":28362,\"start\":27706},{\"end\":30139,\"start\":28364},{\"end\":30309,\"start\":30191},{\"end\":30575,\"start\":30311},{\"end\":30958,\"start\":30753},{\"end\":31092,\"start\":30960},{\"end\":31377,\"start\":31094},{\"end\":32344,\"start\":31379},{\"end\":34060,\"start\":32951},{\"end\":34823,\"start\":34247},{\"end\":35576,\"start\":34825},{\"end\":35961,\"start\":35578},{\"end\":37940,\"start\":35963},{\"end\":38486,\"start\":37969},{\"end\":39187,\"start\":38516},{\"end\":39501,\"start\":39206},{\"end\":39957,\"start\":39503},{\"end\":40623,\"start\":39959},{\"end\":42376,\"start\":40625},{\"end\":42851,\"start\":42394},{\"end\":43014,\"start\":42853},{\"end\":43763,\"start\":43061},{\"end\":44319,\"start\":43781},{\"end\":44595,\"start\":44321},{\"end\":45129,\"start\":44597},{\"end\":45988,\"start\":45142},{\"end\":46395,\"start\":45990},{\"end\":46816,\"start\":46407},{\"end\":47868,\"start\":46831},{\"end\":49795,\"start\":47870},{\"end\":50279,\"start\":49797},{\"end\":51756,\"start\":50323},{\"end\":52244,\"start\":51814},{\"end\":52449,\"start\":52283},{\"end\":52843,\"start\":52451},{\"end\":53186,\"start\":52878},{\"end\":53482,\"start\":53216},{\"end\":53679,\"start\":53495},{\"end\":54132,\"start\":53710},{\"end\":54600,\"start\":54148},{\"end\":54710,\"start\":54602},{\"end\":55711,\"start\":54735},{\"end\":56514,\"start\":55713},{\"end\":56623,\"start\":56516},{\"end\":56680,\"start\":56625},{\"end\":57504,\"start\":56682},{\"end\":59562,\"start\":57517},{\"end\":59826,\"start\":59564},{\"end\":60225,\"start\":59842},{\"end\":60685,\"start\":60258},{\"end\":61742,\"start\":60687},{\"end\":63692,\"start\":61765},{\"end\":64254,\"start\":63694},{\"end\":64467,\"start\":64256},{\"end\":65223,\"start\":64469},{\"end\":65667,\"start\":65262},{\"end\":65835,\"start\":65669},{\"end\":66916,\"start\":65837},{\"end\":67725,\"start\":66918},{\"end\":68179,\"start\":67727},{\"end\":69441,\"start\":68181},{\"end\":70093,\"start\":69443},{\"end\":70423,\"start\":70126},{\"end\":71247,\"start\":70440},{\"end\":72214,\"start\":71270},{\"end\":72752,\"start\":72240},{\"end\":73641,\"start\":72754},{\"end\":73858,\"start\":73681},{\"end\":75045,\"start\":73860},{\"end\":75926,\"start\":75064},{\"end\":76096,\"start\":75928},{\"end\":76288,\"start\":76113},{\"end\":77072,\"start\":76290},{\"end\":77368,\"start\":77074},{\"end\":77662,\"start\":77370},{\"end\":78281,\"start\":77677}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18155,\"start\":18108},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19555,\"start\":19530},{\"attributes\":{\"id\":\"formula_3\"},\"end\":27642,\"start\":27575},{\"attributes\":{\"id\":\"formula_4\"},\"end\":30190,\"start\":30156},{\"attributes\":{\"id\":\"formula_5\"},\"end\":30634,\"start\":30576},{\"attributes\":{\"id\":\"formula_6\"},\"end\":30752,\"start\":30634},{\"attributes\":{\"id\":\"formula_7\"},\"end\":32950,\"start\":32345},{\"attributes\":{\"id\":\"formula_8\"},\"end\":34246,\"start\":34061},{\"attributes\":{\"id\":\"formula_9\"},\"end\":43060,\"start\":43015},{\"attributes\":{\"id\":\"formula_10\"},\"end\":51813,\"start\":51757},{\"attributes\":{\"id\":\"formula_11\"},\"end\":52282,\"start\":52245},{\"attributes\":{\"id\":\"formula_12\"},\"end\":73680,\"start\":73642}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":69296,\"start\":68567}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2270,\"start\":2258},{\"attributes\":{\"n\":\"1.1\"},\"end\":7512,\"start\":7483},{\"attributes\":{\"n\":\"1.2\"},\"end\":11786,\"start\":11756},{\"attributes\":{\"n\":\"2\"},\"end\":16028,\"start\":16015},{\"attributes\":{\"n\":\"2.1\"},\"end\":16258,\"start\":16240},{\"attributes\":{\"n\":\"2.2\"},\"end\":21840,\"start\":21815},{\"end\":24763,\"start\":24754},{\"attributes\":{\"n\":\"3\"},\"end\":25934,\"start\":25905},{\"attributes\":{\"n\":\"3.1\"},\"end\":26330,\"start\":26321},{\"end\":27312,\"start\":27277},{\"attributes\":{\"n\":\"3.2\"},\"end\":30155,\"start\":30142},{\"end\":37956,\"start\":37943},{\"end\":37967,\"start\":37959},{\"attributes\":{\"n\":\"4\"},\"end\":38514,\"start\":38489},{\"attributes\":{\"n\":\"4.1\"},\"end\":39204,\"start\":39190},{\"end\":42392,\"start\":42379},{\"end\":43779,\"start\":43766},{\"end\":45140,\"start\":45132},{\"attributes\":{\"n\":\"4.2\"},\"end\":46405,\"start\":46398},{\"end\":46829,\"start\":46819},{\"attributes\":{\"n\":\"4.3\"},\"end\":50321,\"start\":50282},{\"attributes\":{\"n\":\"5\"},\"end\":52876,\"start\":52846},{\"attributes\":{\"n\":\"5.1\"},\"end\":53214,\"start\":53189},{\"end\":53493,\"start\":53485},{\"attributes\":{\"n\":\"5.2\"},\"end\":53708,\"start\":53682},{\"attributes\":{\"n\":\"6\"},\"end\":54146,\"start\":54135},{\"attributes\":{\"n\":\"6.1\"},\"end\":54733,\"start\":54713},{\"end\":57515,\"start\":57507},{\"end\":59840,\"start\":59829},{\"attributes\":{\"n\":\"6.2\"},\"end\":60256,\"start\":60228},{\"attributes\":{\"n\":\"6.3\"},\"end\":61763,\"start\":61745},{\"attributes\":{\"n\":\"6.4\"},\"end\":65260,\"start\":65226},{\"end\":70124,\"start\":70096},{\"end\":70438,\"start\":70426},{\"end\":71268,\"start\":71250},{\"attributes\":{\"n\":\"6.5\"},\"end\":72238,\"start\":72217},{\"attributes\":{\"n\":\"6.6\"},\"end\":75062,\"start\":75048},{\"attributes\":{\"n\":\"7\"},\"end\":76111,\"start\":76099},{\"attributes\":{\"n\":\"8\"},\"end\":77675,\"start\":77665},{\"end\":78291,\"start\":78283},{\"end\":78668,\"start\":78660},{\"end\":78715,\"start\":78709},{\"end\":78821,\"start\":78813},{\"end\":78946,\"start\":78938},{\"end\":79090,\"start\":79082},{\"end\":79165,\"start\":79157},{\"end\":79368,\"start\":79360},{\"end\":79547,\"start\":79538},{\"end\":79966,\"start\":79959},{\"end\":80097,\"start\":80090},{\"end\":81511,\"start\":81504},{\"end\":82659,\"start\":82652},{\"end\":82924,\"start\":82917},{\"end\":83199,\"start\":83192},{\"end\":83933,\"start\":83926}]", "table": "[{\"end\":79957,\"start\":79756},{\"end\":80088,\"start\":79993},{\"end\":81502,\"start\":80133},{\"end\":82650,\"start\":81661},{\"end\":82915,\"start\":82703},{\"end\":83190,\"start\":83141},{\"end\":83924,\"start\":83295},{\"end\":84716,\"start\":84442}]", "figure_caption": "[{\"end\":78658,\"start\":78293},{\"end\":78707,\"start\":78670},{\"end\":78811,\"start\":78717},{\"end\":78936,\"start\":78823},{\"end\":79080,\"start\":78948},{\"end\":79155,\"start\":79092},{\"end\":79358,\"start\":79167},{\"end\":79536,\"start\":79370},{\"end\":79691,\"start\":79550},{\"end\":79756,\"start\":79694},{\"end\":79993,\"start\":79968},{\"end\":80133,\"start\":80099},{\"end\":81661,\"start\":81513},{\"end\":82703,\"start\":82661},{\"end\":83141,\"start\":82926},{\"end\":83295,\"start\":83201},{\"end\":84442,\"start\":83935}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2886,\"start\":2877},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4720,\"start\":4711},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5102,\"start\":5095},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7138,\"start\":7129},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7572,\"start\":7565},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8300,\"start\":8291},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9242,\"start\":9233},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12064,\"start\":12055},{\"end\":22321,\"start\":22314},{\"end\":24871,\"start\":24863},{\"end\":25268,\"start\":25262},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30011,\"start\":30005},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":34800,\"start\":34794},{\"end\":35746,\"start\":35739},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":35960,\"start\":35953},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37185,\"start\":37179},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38134,\"start\":38124},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":39979,\"start\":39970},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":41342,\"start\":41333},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":42072,\"start\":42066},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":42595,\"start\":42585},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":43348,\"start\":43339},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":45321,\"start\":45312},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":46709,\"start\":46703},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":48584,\"start\":48578},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":48791,\"start\":48782},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":49034,\"start\":49025},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":49639,\"start\":49630},{\"end\":52243,\"start\":52234},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":60599,\"start\":60590},{\"end\":60890,\"start\":60884},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":61401,\"start\":61395},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":63443,\"start\":63437},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":63691,\"start\":63684},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":65558,\"start\":65552},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":66953,\"start\":66947},{\"end\":67794,\"start\":67786},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":68431,\"start\":68413},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":69483,\"start\":69477},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":70234,\"start\":70224},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":70587,\"start\":70577},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":71203,\"start\":71195},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":71515,\"start\":71505},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":72106,\"start\":72096},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":72989,\"start\":72969},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":73329,\"start\":73323},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":74775,\"start\":74765},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":74923,\"start\":74916},{\"end\":77524,\"start\":77518}]", "bib_author_first_name": "[{\"end\":85195,\"start\":85194},{\"end\":85207,\"start\":85206},{\"end\":85223,\"start\":85222},{\"end\":85235,\"start\":85234},{\"end\":85517,\"start\":85516},{\"end\":85529,\"start\":85528},{\"end\":85539,\"start\":85538},{\"end\":85763,\"start\":85762},{\"end\":86010,\"start\":86009},{\"end\":86018,\"start\":86017},{\"end\":86024,\"start\":86023},{\"end\":86031,\"start\":86030},{\"end\":86038,\"start\":86037},{\"end\":86044,\"start\":86043},{\"end\":86050,\"start\":86049},{\"end\":86058,\"start\":86057},{\"end\":86065,\"start\":86064},{\"end\":86071,\"start\":86070},{\"end\":86077,\"start\":86076},{\"end\":86083,\"start\":86082},{\"end\":86091,\"start\":86090},{\"end\":86098,\"start\":86097},{\"end\":86106,\"start\":86105},{\"end\":86113,\"start\":86112},{\"end\":86119,\"start\":86118},{\"end\":86128,\"start\":86127},{\"end\":86134,\"start\":86133},{\"end\":86140,\"start\":86139},{\"end\":86149,\"start\":86148},{\"end\":86156,\"start\":86155},{\"end\":86503,\"start\":86502},{\"end\":86511,\"start\":86510},{\"end\":86519,\"start\":86518},{\"end\":86527,\"start\":86526},{\"end\":86533,\"start\":86532},{\"end\":86540,\"start\":86539},{\"end\":87104,\"start\":87103},{\"end\":87110,\"start\":87109},{\"end\":87118,\"start\":87117},{\"end\":87380,\"start\":87379},{\"end\":87386,\"start\":87385},{\"end\":87395,\"start\":87394},{\"end\":87403,\"start\":87402},{\"end\":87693,\"start\":87692},{\"end\":87695,\"start\":87694},{\"end\":87705,\"start\":87704},{\"end\":87707,\"start\":87706},{\"end\":87977,\"start\":87976},{\"end\":87994,\"start\":87993},{\"end\":88203,\"start\":88202},{\"end\":88210,\"start\":88209},{\"end\":88218,\"start\":88217},{\"end\":88226,\"start\":88225},{\"end\":88491,\"start\":88490},{\"end\":88498,\"start\":88497},{\"end\":88504,\"start\":88503},{\"end\":88512,\"start\":88511},{\"end\":88519,\"start\":88518},{\"end\":88527,\"start\":88526},{\"end\":88533,\"start\":88532},{\"end\":88852,\"start\":88851},{\"end\":88858,\"start\":88857},{\"end\":88865,\"start\":88864},{\"end\":88872,\"start\":88871},{\"end\":88880,\"start\":88879},{\"end\":88886,\"start\":88885},{\"end\":88894,\"start\":88893},{\"end\":89333,\"start\":89332},{\"end\":89339,\"start\":89338},{\"end\":89345,\"start\":89344},{\"end\":89353,\"start\":89352},{\"end\":89708,\"start\":89707},{\"end\":89716,\"start\":89715},{\"end\":89722,\"start\":89721},{\"end\":89730,\"start\":89729},{\"end\":89972,\"start\":89971},{\"end\":89978,\"start\":89977},{\"end\":89987,\"start\":89986},{\"end\":89994,\"start\":89993},{\"end\":90004,\"start\":90003},{\"end\":90010,\"start\":90009},{\"end\":90021,\"start\":90020},{\"end\":90266,\"start\":90265},{\"end\":90273,\"start\":90272},{\"end\":90281,\"start\":90280},{\"end\":90289,\"start\":90288},{\"end\":90298,\"start\":90297},{\"end\":91326,\"start\":91325},{\"end\":91335,\"start\":91334},{\"end\":91344,\"start\":91343},{\"end\":91769,\"start\":91768},{\"end\":91777,\"start\":91776},{\"end\":91783,\"start\":91782},{\"end\":91790,\"start\":91789},{\"end\":92029,\"start\":92028},{\"end\":92045,\"start\":92044},{\"end\":92057,\"start\":92056},{\"end\":92308,\"start\":92307},{\"end\":92314,\"start\":92313},{\"end\":92323,\"start\":92322},{\"end\":92330,\"start\":92329},{\"end\":92338,\"start\":92337},{\"end\":92344,\"start\":92343},{\"end\":92353,\"start\":92352},{\"end\":92660,\"start\":92659},{\"end\":92670,\"start\":92669},{\"end\":92679,\"start\":92678},{\"end\":92689,\"start\":92688},{\"end\":92932,\"start\":92931},{\"end\":92934,\"start\":92933},{\"end\":92943,\"start\":92942},{\"end\":93179,\"start\":93178},{\"end\":93192,\"start\":93188},{\"end\":93201,\"start\":93200},{\"end\":93208,\"start\":93207},{\"end\":93500,\"start\":93499},{\"end\":93717,\"start\":93716},{\"end\":93729,\"start\":93728},{\"end\":93745,\"start\":93744},{\"end\":93956,\"start\":93955},{\"end\":93966,\"start\":93965},{\"end\":93975,\"start\":93974},{\"end\":94170,\"start\":94169},{\"end\":94180,\"start\":94179},{\"end\":94479,\"start\":94478},{\"end\":94481,\"start\":94480},{\"end\":94494,\"start\":94493},{\"end\":94496,\"start\":94495},{\"end\":94506,\"start\":94505},{\"end\":94508,\"start\":94507},{\"end\":94901,\"start\":94900},{\"end\":94908,\"start\":94907},{\"end\":94915,\"start\":94914},{\"end\":94927,\"start\":94926},{\"end\":94935,\"start\":94934},{\"end\":94945,\"start\":94944},{\"end\":94954,\"start\":94953},{\"end\":95203,\"start\":95202},{\"end\":95217,\"start\":95216},{\"end\":95219,\"start\":95218},{\"end\":95449,\"start\":95448},{\"end\":95458,\"start\":95457},{\"end\":95467,\"start\":95466},{\"end\":95476,\"start\":95475},{\"end\":95788,\"start\":95787},{\"end\":95796,\"start\":95795},{\"end\":95804,\"start\":95803},{\"end\":95815,\"start\":95814},{\"end\":96083,\"start\":96082},{\"end\":96092,\"start\":96091},{\"end\":96100,\"start\":96099},{\"end\":96109,\"start\":96108},{\"end\":96117,\"start\":96116},{\"end\":96395,\"start\":96394},{\"end\":96404,\"start\":96403},{\"end\":96414,\"start\":96413},{\"end\":96421,\"start\":96420},{\"end\":96428,\"start\":96427},{\"end\":96437,\"start\":96436},{\"end\":96677,\"start\":96676},{\"end\":96686,\"start\":96685},{\"end\":96692,\"start\":96691},{\"end\":96870,\"start\":96869},{\"end\":96883,\"start\":96882},{\"end\":96893,\"start\":96892},{\"end\":96903,\"start\":96902},{\"end\":97159,\"start\":97158},{\"end\":97166,\"start\":97165},{\"end\":97175,\"start\":97174},{\"end\":97181,\"start\":97180},{\"end\":97432,\"start\":97431},{\"end\":97434,\"start\":97433},{\"end\":97444,\"start\":97443},{\"end\":97455,\"start\":97454},{\"end\":97473,\"start\":97472},{\"end\":97488,\"start\":97487},{\"end\":97498,\"start\":97497},{\"end\":97505,\"start\":97504},{\"end\":97511,\"start\":97510},{\"end\":97513,\"start\":97512},{\"end\":97525,\"start\":97524},{\"end\":97529,\"start\":97526},{\"end\":97816,\"start\":97815},{\"end\":97824,\"start\":97823},{\"end\":97831,\"start\":97830},{\"end\":98021,\"start\":98017},{\"end\":98029,\"start\":98028},{\"end\":98040,\"start\":98036},{\"end\":98281,\"start\":98280},{\"end\":98287,\"start\":98286},{\"end\":98296,\"start\":98295},{\"end\":98473,\"start\":98472},{\"end\":98480,\"start\":98479},{\"end\":98670,\"start\":98669},{\"end\":98679,\"start\":98678},{\"end\":98686,\"start\":98685},{\"end\":98906,\"start\":98905},{\"end\":98916,\"start\":98915},{\"end\":98931,\"start\":98930},{\"end\":98943,\"start\":98942},{\"end\":99242,\"start\":99241},{\"end\":99250,\"start\":99249},{\"end\":99262,\"start\":99261},{\"end\":99475,\"start\":99474},{\"end\":99477,\"start\":99476},{\"end\":99484,\"start\":99483},{\"end\":99486,\"start\":99485},{\"end\":99693,\"start\":99692},{\"end\":99695,\"start\":99694},{\"end\":99901,\"start\":99900},{\"end\":99909,\"start\":99908},{\"end\":99921,\"start\":99920},{\"end\":99932,\"start\":99931},{\"end\":99942,\"start\":99941},{\"end\":99955,\"start\":99954},{\"end\":99965,\"start\":99964},{\"end\":99978,\"start\":99977},{\"end\":99989,\"start\":99988},{\"end\":99991,\"start\":99990},{\"end\":100377,\"start\":100376},{\"end\":100387,\"start\":100386},{\"end\":100776,\"start\":100775},{\"end\":100782,\"start\":100781},{\"end\":100790,\"start\":100789},{\"end\":100798,\"start\":100797},{\"end\":101092,\"start\":101091},{\"end\":101096,\"start\":101093},{\"end\":101105,\"start\":101104},{\"end\":101107,\"start\":101106},{\"end\":101120,\"start\":101119},{\"end\":101128,\"start\":101127},{\"end\":101423,\"start\":101422},{\"end\":101436,\"start\":101435},{\"end\":101450,\"start\":101449},{\"end\":101462,\"start\":101461},{\"end\":101712,\"start\":101711},{\"end\":101718,\"start\":101717},{\"end\":101727,\"start\":101726},{\"end\":101729,\"start\":101728},{\"end\":101741,\"start\":101740},{\"end\":101996,\"start\":101995},{\"end\":102006,\"start\":102005},{\"end\":102019,\"start\":102018},{\"end\":102029,\"start\":102028},{\"end\":102278,\"start\":102277},{\"end\":102285,\"start\":102284},{\"end\":102294,\"start\":102293},{\"end\":102509,\"start\":102508},{\"end\":102511,\"start\":102510},{\"end\":102525,\"start\":102524},{\"end\":102532,\"start\":102531},{\"end\":102753,\"start\":102752},{\"end\":102761,\"start\":102760},{\"end\":102763,\"start\":102762},{\"end\":102997,\"start\":102996},{\"end\":103003,\"start\":103002},{\"end\":103012,\"start\":103011},{\"end\":103019,\"start\":103018},{\"end\":103027,\"start\":103026},{\"end\":103029,\"start\":103028},{\"end\":103038,\"start\":103037},{\"end\":103328,\"start\":103327},{\"end\":103337,\"start\":103336},{\"end\":103350,\"start\":103349},{\"end\":103352,\"start\":103351},{\"end\":103650,\"start\":103649},{\"end\":103664,\"start\":103663},{\"end\":103675,\"start\":103674},{\"end\":103677,\"start\":103676},{\"end\":103689,\"start\":103688},{\"end\":103704,\"start\":103703}]", "bib_author_last_name": "[{\"end\":85204,\"start\":85196},{\"end\":85220,\"start\":85208},{\"end\":85232,\"start\":85224},{\"end\":85245,\"start\":85236},{\"end\":85526,\"start\":85518},{\"end\":85536,\"start\":85530},{\"end\":85549,\"start\":85540},{\"end\":85768,\"start\":85764},{\"end\":86015,\"start\":86011},{\"end\":86021,\"start\":86019},{\"end\":86028,\"start\":86025},{\"end\":86035,\"start\":86032},{\"end\":86041,\"start\":86039},{\"end\":86047,\"start\":86045},{\"end\":86055,\"start\":86051},{\"end\":86062,\"start\":86059},{\"end\":86068,\"start\":86066},{\"end\":86074,\"start\":86072},{\"end\":86080,\"start\":86078},{\"end\":86088,\"start\":86084},{\"end\":86095,\"start\":86092},{\"end\":86103,\"start\":86099},{\"end\":86110,\"start\":86107},{\"end\":86116,\"start\":86114},{\"end\":86125,\"start\":86120},{\"end\":86131,\"start\":86129},{\"end\":86137,\"start\":86135},{\"end\":86146,\"start\":86141},{\"end\":86153,\"start\":86150},{\"end\":86160,\"start\":86157},{\"end\":86508,\"start\":86504},{\"end\":86516,\"start\":86512},{\"end\":86524,\"start\":86520},{\"end\":86530,\"start\":86528},{\"end\":86537,\"start\":86534},{\"end\":86543,\"start\":86541},{\"end\":86787,\"start\":86778},{\"end\":87107,\"start\":87105},{\"end\":87115,\"start\":87111},{\"end\":87122,\"start\":87119},{\"end\":87383,\"start\":87381},{\"end\":87392,\"start\":87387},{\"end\":87400,\"start\":87396},{\"end\":87407,\"start\":87404},{\"end\":87702,\"start\":87696},{\"end\":87716,\"start\":87708},{\"end\":87991,\"start\":87978},{\"end\":88006,\"start\":87995},{\"end\":88207,\"start\":88204},{\"end\":88215,\"start\":88211},{\"end\":88223,\"start\":88219},{\"end\":88232,\"start\":88227},{\"end\":88495,\"start\":88492},{\"end\":88501,\"start\":88499},{\"end\":88509,\"start\":88505},{\"end\":88516,\"start\":88513},{\"end\":88524,\"start\":88520},{\"end\":88530,\"start\":88528},{\"end\":88537,\"start\":88534},{\"end\":88855,\"start\":88853},{\"end\":88862,\"start\":88859},{\"end\":88869,\"start\":88866},{\"end\":88877,\"start\":88873},{\"end\":88883,\"start\":88881},{\"end\":88891,\"start\":88887},{\"end\":88899,\"start\":88895},{\"end\":89336,\"start\":89334},{\"end\":89342,\"start\":89340},{\"end\":89350,\"start\":89346},{\"end\":89357,\"start\":89354},{\"end\":89713,\"start\":89709},{\"end\":89719,\"start\":89717},{\"end\":89727,\"start\":89723},{\"end\":89734,\"start\":89731},{\"end\":89975,\"start\":89973},{\"end\":89984,\"start\":89979},{\"end\":89991,\"start\":89988},{\"end\":90001,\"start\":89995},{\"end\":90007,\"start\":90005},{\"end\":90018,\"start\":90011},{\"end\":90026,\"start\":90022},{\"end\":90270,\"start\":90267},{\"end\":90278,\"start\":90274},{\"end\":90286,\"start\":90282},{\"end\":90295,\"start\":90290},{\"end\":90303,\"start\":90299},{\"end\":90521,\"start\":90513},{\"end\":90704,\"start\":90695},{\"end\":90914,\"start\":90909},{\"end\":91136,\"start\":91128},{\"end\":91332,\"start\":91327},{\"end\":91341,\"start\":91336},{\"end\":91351,\"start\":91345},{\"end\":91554,\"start\":91548},{\"end\":91774,\"start\":91770},{\"end\":91780,\"start\":91778},{\"end\":91787,\"start\":91784},{\"end\":91795,\"start\":91791},{\"end\":92042,\"start\":92030},{\"end\":92054,\"start\":92046},{\"end\":92067,\"start\":92058},{\"end\":92311,\"start\":92309},{\"end\":92320,\"start\":92315},{\"end\":92327,\"start\":92324},{\"end\":92335,\"start\":92331},{\"end\":92341,\"start\":92339},{\"end\":92350,\"start\":92345},{\"end\":92357,\"start\":92354},{\"end\":92667,\"start\":92661},{\"end\":92676,\"start\":92671},{\"end\":92686,\"start\":92680},{\"end\":92694,\"start\":92690},{\"end\":92940,\"start\":92935},{\"end\":92948,\"start\":92944},{\"end\":93186,\"start\":93180},{\"end\":93198,\"start\":93193},{\"end\":93205,\"start\":93202},{\"end\":93218,\"start\":93209},{\"end\":93509,\"start\":93501},{\"end\":93726,\"start\":93718},{\"end\":93742,\"start\":93730},{\"end\":93754,\"start\":93746},{\"end\":93963,\"start\":93957},{\"end\":93972,\"start\":93967},{\"end\":93983,\"start\":93976},{\"end\":94177,\"start\":94171},{\"end\":94187,\"start\":94181},{\"end\":94491,\"start\":94482},{\"end\":94503,\"start\":94497},{\"end\":94515,\"start\":94509},{\"end\":94905,\"start\":94902},{\"end\":94912,\"start\":94909},{\"end\":94924,\"start\":94916},{\"end\":94932,\"start\":94928},{\"end\":94942,\"start\":94936},{\"end\":94951,\"start\":94946},{\"end\":94960,\"start\":94955},{\"end\":95214,\"start\":95204},{\"end\":95227,\"start\":95220},{\"end\":95455,\"start\":95450},{\"end\":95464,\"start\":95459},{\"end\":95473,\"start\":95468},{\"end\":95489,\"start\":95477},{\"end\":95793,\"start\":95789},{\"end\":95801,\"start\":95797},{\"end\":95812,\"start\":95805},{\"end\":95818,\"start\":95816},{\"end\":96089,\"start\":96084},{\"end\":96097,\"start\":96093},{\"end\":96106,\"start\":96101},{\"end\":96114,\"start\":96110},{\"end\":96120,\"start\":96118},{\"end\":96401,\"start\":96396},{\"end\":96411,\"start\":96405},{\"end\":96418,\"start\":96415},{\"end\":96425,\"start\":96422},{\"end\":96434,\"start\":96429},{\"end\":96440,\"start\":96438},{\"end\":96683,\"start\":96678},{\"end\":96689,\"start\":96687},{\"end\":96695,\"start\":96693},{\"end\":96880,\"start\":96871},{\"end\":96890,\"start\":96884},{\"end\":96900,\"start\":96894},{\"end\":96909,\"start\":96904},{\"end\":97163,\"start\":97160},{\"end\":97172,\"start\":97167},{\"end\":97178,\"start\":97176},{\"end\":97189,\"start\":97182},{\"end\":97441,\"start\":97435},{\"end\":97452,\"start\":97445},{\"end\":97470,\"start\":97456},{\"end\":97485,\"start\":97474},{\"end\":97495,\"start\":97489},{\"end\":97502,\"start\":97499},{\"end\":97508,\"start\":97506},{\"end\":97522,\"start\":97514},{\"end\":97537,\"start\":97530},{\"end\":97821,\"start\":97817},{\"end\":97828,\"start\":97825},{\"end\":97834,\"start\":97832},{\"end\":98026,\"start\":98022},{\"end\":98034,\"start\":98030},{\"end\":98044,\"start\":98041},{\"end\":98284,\"start\":98282},{\"end\":98293,\"start\":98288},{\"end\":98301,\"start\":98297},{\"end\":98477,\"start\":98474},{\"end\":98484,\"start\":98481},{\"end\":98676,\"start\":98671},{\"end\":98683,\"start\":98680},{\"end\":98690,\"start\":98687},{\"end\":98913,\"start\":98907},{\"end\":98928,\"start\":98917},{\"end\":98940,\"start\":98932},{\"end\":98950,\"start\":98944},{\"end\":99247,\"start\":99243},{\"end\":99259,\"start\":99251},{\"end\":99265,\"start\":99263},{\"end\":99481,\"start\":99478},{\"end\":99491,\"start\":99487},{\"end\":99705,\"start\":99696},{\"end\":99906,\"start\":99902},{\"end\":99918,\"start\":99910},{\"end\":99929,\"start\":99922},{\"end\":99939,\"start\":99933},{\"end\":99952,\"start\":99943},{\"end\":99962,\"start\":99956},{\"end\":99975,\"start\":99966},{\"end\":99986,\"start\":99979},{\"end\":99996,\"start\":99992},{\"end\":100384,\"start\":100378},{\"end\":100396,\"start\":100388},{\"end\":100779,\"start\":100777},{\"end\":100787,\"start\":100783},{\"end\":100795,\"start\":100791},{\"end\":100801,\"start\":100799},{\"end\":101102,\"start\":101097},{\"end\":101117,\"start\":101108},{\"end\":101125,\"start\":101121},{\"end\":101137,\"start\":101129},{\"end\":101145,\"start\":101139},{\"end\":101433,\"start\":101424},{\"end\":101447,\"start\":101437},{\"end\":101459,\"start\":101451},{\"end\":101470,\"start\":101463},{\"end\":101715,\"start\":101713},{\"end\":101724,\"start\":101719},{\"end\":101738,\"start\":101730},{\"end\":101744,\"start\":101742},{\"end\":102003,\"start\":101997},{\"end\":102016,\"start\":102007},{\"end\":102026,\"start\":102020},{\"end\":102036,\"start\":102030},{\"end\":102282,\"start\":102279},{\"end\":102291,\"start\":102286},{\"end\":102297,\"start\":102295},{\"end\":102522,\"start\":102512},{\"end\":102529,\"start\":102526},{\"end\":102536,\"start\":102533},{\"end\":102758,\"start\":102754},{\"end\":102768,\"start\":102764},{\"end\":103000,\"start\":102998},{\"end\":103009,\"start\":103004},{\"end\":103016,\"start\":103013},{\"end\":103024,\"start\":103020},{\"end\":103035,\"start\":103030},{\"end\":103042,\"start\":103039},{\"end\":103334,\"start\":103329},{\"end\":103347,\"start\":103338},{\"end\":103362,\"start\":103353},{\"end\":103661,\"start\":103651},{\"end\":103672,\"start\":103665},{\"end\":103686,\"start\":103678},{\"end\":103701,\"start\":103690},{\"end\":103713,\"start\":103705}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":85092,\"start\":84733},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":208193691},\"end\":85461,\"start\":85094},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":5981518},\"end\":85700,\"start\":85463},{\"attributes\":{\"id\":\"b3\"},\"end\":85952,\"start\":85702},{\"attributes\":{\"id\":\"b4\"},\"end\":86444,\"start\":85954},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5108428},\"end\":86693,\"start\":86446},{\"attributes\":{\"id\":\"b6\"},\"end\":86977,\"start\":86695},{\"attributes\":{\"id\":\"b7\"},\"end\":87295,\"start\":86979},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":44175941},\"end\":87584,\"start\":87297},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":8915893},\"end\":87912,\"start\":87586},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":195767092},\"end\":88145,\"start\":87914},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":221352167},\"end\":88388,\"start\":88147},{\"attributes\":{\"id\":\"b12\"},\"end\":88756,\"start\":88390},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":53713854},\"end\":89241,\"start\":88758},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":225290177},\"end\":89619,\"start\":89243},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":218981628},\"end\":89901,\"start\":89621},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":56173957},\"end\":90201,\"start\":89903},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":236980130},\"end\":90459,\"start\":90203},{\"attributes\":{\"id\":\"b18\"},\"end\":90630,\"start\":90461},{\"attributes\":{\"id\":\"b19\"},\"end\":90823,\"start\":90632},{\"attributes\":{\"id\":\"b20\"},\"end\":91051,\"start\":90825},{\"attributes\":{\"id\":\"b21\"},\"end\":91273,\"start\":91053},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":5850884},\"end\":91491,\"start\":91275},{\"attributes\":{\"id\":\"b23\"},\"end\":91663,\"start\":91493},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":231728434},\"end\":91994,\"start\":91665},{\"attributes\":{\"id\":\"b25\"},\"end\":92203,\"start\":91996},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1364239},\"end\":92577,\"start\":92205},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":17060132},\"end\":92862,\"start\":92579},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5573265},\"end\":93095,\"start\":92864},{\"attributes\":{\"doi\":\"abs/1810.04805\",\"id\":\"b29\",\"matched_paper_id\":52967399},\"end\":93409,\"start\":93097},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":218982503},\"end\":93653,\"start\":93411},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":235616917},\"end\":93903,\"start\":93655},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1578969},\"end\":94107,\"start\":93905},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1897758},\"end\":94370,\"start\":94109},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":52956075},\"end\":94825,\"start\":94372},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":218614141},\"end\":95145,\"start\":94827},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":10191182},\"end\":95339,\"start\":95147},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":5045956},\"end\":95702,\"start\":95341},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":219557181},\"end\":96001,\"start\":95704},{\"attributes\":{\"id\":\"b39\"},\"end\":96300,\"start\":96003},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":214198198},\"end\":96629,\"start\":96302},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":13675759},\"end\":96805,\"start\":96631},{\"attributes\":{\"doi\":\"abs/2107.07842\",\"id\":\"b42\",\"matched_paper_id\":236034048},\"end\":97077,\"start\":96807},{\"attributes\":{\"doi\":\"abs/1507.05670\",\"id\":\"b43\"},\"end\":97367,\"start\":97079},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":224291855},\"end\":97763,\"start\":97369},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":218906516},\"end\":97951,\"start\":97765},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":4712016},\"end\":98219,\"start\":97953},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":199466155},\"end\":98431,\"start\":98221},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":6474794},\"end\":98601,\"start\":98433},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":22092340},\"end\":98825,\"start\":98603},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":9896397},\"end\":99158,\"start\":98827},{\"attributes\":{\"id\":\"b51\"},\"end\":99412,\"start\":99160},{\"attributes\":{\"doi\":\"abs/1904.02077\",\"id\":\"b52\",\"matched_paper_id\":174799457},\"end\":99633,\"start\":99414},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":2830642},\"end\":99862,\"start\":99635},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":14566493},\"end\":100317,\"start\":99864},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":118710278},\"end\":100685,\"start\":100319},{\"attributes\":{\"id\":\"b56\"},\"end\":100999,\"start\":100687},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":199113077},\"end\":101380,\"start\":101001},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":166228603},\"end\":101619,\"start\":101382},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":218914310},\"end\":101923,\"start\":101621},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":17362696},\"end\":102194,\"start\":101925},{\"attributes\":{\"id\":\"b61\"},\"end\":102435,\"start\":102196},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":119301186},\"end\":102686,\"start\":102437},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":206765442},\"end\":102915,\"start\":102688},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":218907102},\"end\":103217,\"start\":102917},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":5966664},\"end\":103568,\"start\":103219},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":209392043},\"end\":103919,\"start\":103570}]", "bib_title": "[{\"end\":85192,\"start\":85094},{\"end\":85514,\"start\":85463},{\"end\":86500,\"start\":86446},{\"end\":87377,\"start\":87297},{\"end\":87690,\"start\":87586},{\"end\":87974,\"start\":87914},{\"end\":88200,\"start\":88147},{\"end\":88488,\"start\":88390},{\"end\":88849,\"start\":88758},{\"end\":89330,\"start\":89243},{\"end\":89705,\"start\":89621},{\"end\":89969,\"start\":89903},{\"end\":90263,\"start\":90203},{\"end\":91323,\"start\":91275},{\"end\":91766,\"start\":91665},{\"end\":92305,\"start\":92205},{\"end\":92657,\"start\":92579},{\"end\":92929,\"start\":92864},{\"end\":93176,\"start\":93097},{\"end\":93497,\"start\":93411},{\"end\":93714,\"start\":93655},{\"end\":93953,\"start\":93905},{\"end\":94167,\"start\":94109},{\"end\":94476,\"start\":94372},{\"end\":94898,\"start\":94827},{\"end\":95200,\"start\":95147},{\"end\":95446,\"start\":95341},{\"end\":95785,\"start\":95704},{\"end\":96080,\"start\":96003},{\"end\":96392,\"start\":96302},{\"end\":96674,\"start\":96631},{\"end\":96867,\"start\":96807},{\"end\":97156,\"start\":97079},{\"end\":97429,\"start\":97369},{\"end\":97813,\"start\":97765},{\"end\":98015,\"start\":97953},{\"end\":98278,\"start\":98221},{\"end\":98470,\"start\":98433},{\"end\":98667,\"start\":98603},{\"end\":98903,\"start\":98827},{\"end\":99472,\"start\":99414},{\"end\":99690,\"start\":99635},{\"end\":99898,\"start\":99864},{\"end\":100374,\"start\":100319},{\"end\":101089,\"start\":101001},{\"end\":101420,\"start\":101382},{\"end\":101709,\"start\":101621},{\"end\":101993,\"start\":101925},{\"end\":102506,\"start\":102437},{\"end\":102750,\"start\":102688},{\"end\":102994,\"start\":102917},{\"end\":103325,\"start\":103219},{\"end\":103647,\"start\":103570}]", "bib_author": "[{\"end\":85206,\"start\":85194},{\"end\":85222,\"start\":85206},{\"end\":85234,\"start\":85222},{\"end\":85247,\"start\":85234},{\"end\":85528,\"start\":85516},{\"end\":85538,\"start\":85528},{\"end\":85551,\"start\":85538},{\"end\":85770,\"start\":85762},{\"end\":86017,\"start\":86009},{\"end\":86023,\"start\":86017},{\"end\":86030,\"start\":86023},{\"end\":86037,\"start\":86030},{\"end\":86043,\"start\":86037},{\"end\":86049,\"start\":86043},{\"end\":86057,\"start\":86049},{\"end\":86064,\"start\":86057},{\"end\":86070,\"start\":86064},{\"end\":86076,\"start\":86070},{\"end\":86082,\"start\":86076},{\"end\":86090,\"start\":86082},{\"end\":86097,\"start\":86090},{\"end\":86105,\"start\":86097},{\"end\":86112,\"start\":86105},{\"end\":86118,\"start\":86112},{\"end\":86127,\"start\":86118},{\"end\":86133,\"start\":86127},{\"end\":86139,\"start\":86133},{\"end\":86148,\"start\":86139},{\"end\":86155,\"start\":86148},{\"end\":86162,\"start\":86155},{\"end\":86510,\"start\":86502},{\"end\":86518,\"start\":86510},{\"end\":86526,\"start\":86518},{\"end\":86532,\"start\":86526},{\"end\":86539,\"start\":86532},{\"end\":86545,\"start\":86539},{\"end\":86789,\"start\":86778},{\"end\":87109,\"start\":87103},{\"end\":87117,\"start\":87109},{\"end\":87124,\"start\":87117},{\"end\":87385,\"start\":87379},{\"end\":87394,\"start\":87385},{\"end\":87402,\"start\":87394},{\"end\":87409,\"start\":87402},{\"end\":87704,\"start\":87692},{\"end\":87718,\"start\":87704},{\"end\":87993,\"start\":87976},{\"end\":88008,\"start\":87993},{\"end\":88209,\"start\":88202},{\"end\":88217,\"start\":88209},{\"end\":88225,\"start\":88217},{\"end\":88234,\"start\":88225},{\"end\":88497,\"start\":88490},{\"end\":88503,\"start\":88497},{\"end\":88511,\"start\":88503},{\"end\":88518,\"start\":88511},{\"end\":88526,\"start\":88518},{\"end\":88532,\"start\":88526},{\"end\":88539,\"start\":88532},{\"end\":88857,\"start\":88851},{\"end\":88864,\"start\":88857},{\"end\":88871,\"start\":88864},{\"end\":88879,\"start\":88871},{\"end\":88885,\"start\":88879},{\"end\":88893,\"start\":88885},{\"end\":88901,\"start\":88893},{\"end\":89338,\"start\":89332},{\"end\":89344,\"start\":89338},{\"end\":89352,\"start\":89344},{\"end\":89359,\"start\":89352},{\"end\":89715,\"start\":89707},{\"end\":89721,\"start\":89715},{\"end\":89729,\"start\":89721},{\"end\":89736,\"start\":89729},{\"end\":89977,\"start\":89971},{\"end\":89986,\"start\":89977},{\"end\":89993,\"start\":89986},{\"end\":90003,\"start\":89993},{\"end\":90009,\"start\":90003},{\"end\":90020,\"start\":90009},{\"end\":90028,\"start\":90020},{\"end\":90272,\"start\":90265},{\"end\":90280,\"start\":90272},{\"end\":90288,\"start\":90280},{\"end\":90297,\"start\":90288},{\"end\":90305,\"start\":90297},{\"end\":90523,\"start\":90513},{\"end\":90706,\"start\":90695},{\"end\":90916,\"start\":90909},{\"end\":91138,\"start\":91128},{\"end\":91334,\"start\":91325},{\"end\":91343,\"start\":91334},{\"end\":91353,\"start\":91343},{\"end\":91556,\"start\":91548},{\"end\":91776,\"start\":91768},{\"end\":91782,\"start\":91776},{\"end\":91789,\"start\":91782},{\"end\":91797,\"start\":91789},{\"end\":92044,\"start\":92028},{\"end\":92056,\"start\":92044},{\"end\":92069,\"start\":92056},{\"end\":92313,\"start\":92307},{\"end\":92322,\"start\":92313},{\"end\":92329,\"start\":92322},{\"end\":92337,\"start\":92329},{\"end\":92343,\"start\":92337},{\"end\":92352,\"start\":92343},{\"end\":92359,\"start\":92352},{\"end\":92669,\"start\":92659},{\"end\":92678,\"start\":92669},{\"end\":92688,\"start\":92678},{\"end\":92696,\"start\":92688},{\"end\":92942,\"start\":92931},{\"end\":92950,\"start\":92942},{\"end\":93188,\"start\":93178},{\"end\":93200,\"start\":93188},{\"end\":93207,\"start\":93200},{\"end\":93220,\"start\":93207},{\"end\":93511,\"start\":93499},{\"end\":93728,\"start\":93716},{\"end\":93744,\"start\":93728},{\"end\":93756,\"start\":93744},{\"end\":93965,\"start\":93955},{\"end\":93974,\"start\":93965},{\"end\":93985,\"start\":93974},{\"end\":94179,\"start\":94169},{\"end\":94189,\"start\":94179},{\"end\":94493,\"start\":94478},{\"end\":94505,\"start\":94493},{\"end\":94517,\"start\":94505},{\"end\":94907,\"start\":94900},{\"end\":94914,\"start\":94907},{\"end\":94926,\"start\":94914},{\"end\":94934,\"start\":94926},{\"end\":94944,\"start\":94934},{\"end\":94953,\"start\":94944},{\"end\":94962,\"start\":94953},{\"end\":95216,\"start\":95202},{\"end\":95229,\"start\":95216},{\"end\":95457,\"start\":95448},{\"end\":95466,\"start\":95457},{\"end\":95475,\"start\":95466},{\"end\":95491,\"start\":95475},{\"end\":95795,\"start\":95787},{\"end\":95803,\"start\":95795},{\"end\":95814,\"start\":95803},{\"end\":95820,\"start\":95814},{\"end\":96091,\"start\":96082},{\"end\":96099,\"start\":96091},{\"end\":96108,\"start\":96099},{\"end\":96116,\"start\":96108},{\"end\":96122,\"start\":96116},{\"end\":96403,\"start\":96394},{\"end\":96413,\"start\":96403},{\"end\":96420,\"start\":96413},{\"end\":96427,\"start\":96420},{\"end\":96436,\"start\":96427},{\"end\":96442,\"start\":96436},{\"end\":96685,\"start\":96676},{\"end\":96691,\"start\":96685},{\"end\":96697,\"start\":96691},{\"end\":96882,\"start\":96869},{\"end\":96892,\"start\":96882},{\"end\":96902,\"start\":96892},{\"end\":96911,\"start\":96902},{\"end\":97165,\"start\":97158},{\"end\":97174,\"start\":97165},{\"end\":97180,\"start\":97174},{\"end\":97191,\"start\":97180},{\"end\":97443,\"start\":97431},{\"end\":97454,\"start\":97443},{\"end\":97472,\"start\":97454},{\"end\":97487,\"start\":97472},{\"end\":97497,\"start\":97487},{\"end\":97504,\"start\":97497},{\"end\":97510,\"start\":97504},{\"end\":97524,\"start\":97510},{\"end\":97539,\"start\":97524},{\"end\":97823,\"start\":97815},{\"end\":97830,\"start\":97823},{\"end\":97836,\"start\":97830},{\"end\":98028,\"start\":98017},{\"end\":98036,\"start\":98028},{\"end\":98046,\"start\":98036},{\"end\":98286,\"start\":98280},{\"end\":98295,\"start\":98286},{\"end\":98303,\"start\":98295},{\"end\":98479,\"start\":98472},{\"end\":98486,\"start\":98479},{\"end\":98678,\"start\":98669},{\"end\":98685,\"start\":98678},{\"end\":98692,\"start\":98685},{\"end\":98915,\"start\":98905},{\"end\":98930,\"start\":98915},{\"end\":98942,\"start\":98930},{\"end\":98952,\"start\":98942},{\"end\":99249,\"start\":99241},{\"end\":99261,\"start\":99249},{\"end\":99267,\"start\":99261},{\"end\":99483,\"start\":99474},{\"end\":99493,\"start\":99483},{\"end\":99707,\"start\":99692},{\"end\":99908,\"start\":99900},{\"end\":99920,\"start\":99908},{\"end\":99931,\"start\":99920},{\"end\":99941,\"start\":99931},{\"end\":99954,\"start\":99941},{\"end\":99964,\"start\":99954},{\"end\":99977,\"start\":99964},{\"end\":99988,\"start\":99977},{\"end\":99998,\"start\":99988},{\"end\":100386,\"start\":100376},{\"end\":100398,\"start\":100386},{\"end\":100781,\"start\":100775},{\"end\":100789,\"start\":100781},{\"end\":100797,\"start\":100789},{\"end\":100803,\"start\":100797},{\"end\":101104,\"start\":101091},{\"end\":101119,\"start\":101104},{\"end\":101127,\"start\":101119},{\"end\":101139,\"start\":101127},{\"end\":101147,\"start\":101139},{\"end\":101435,\"start\":101422},{\"end\":101449,\"start\":101435},{\"end\":101461,\"start\":101449},{\"end\":101472,\"start\":101461},{\"end\":101717,\"start\":101711},{\"end\":101726,\"start\":101717},{\"end\":101740,\"start\":101726},{\"end\":101746,\"start\":101740},{\"end\":102005,\"start\":101995},{\"end\":102018,\"start\":102005},{\"end\":102028,\"start\":102018},{\"end\":102038,\"start\":102028},{\"end\":102284,\"start\":102277},{\"end\":102293,\"start\":102284},{\"end\":102299,\"start\":102293},{\"end\":102524,\"start\":102508},{\"end\":102531,\"start\":102524},{\"end\":102538,\"start\":102531},{\"end\":102760,\"start\":102752},{\"end\":102770,\"start\":102760},{\"end\":103002,\"start\":102996},{\"end\":103011,\"start\":103002},{\"end\":103018,\"start\":103011},{\"end\":103026,\"start\":103018},{\"end\":103037,\"start\":103026},{\"end\":103044,\"start\":103037},{\"end\":103336,\"start\":103327},{\"end\":103349,\"start\":103336},{\"end\":103364,\"start\":103349},{\"end\":103663,\"start\":103649},{\"end\":103674,\"start\":103663},{\"end\":103688,\"start\":103674},{\"end\":103703,\"start\":103688},{\"end\":103715,\"start\":103703}]", "bib_venue": "[{\"end\":84830,\"start\":84733},{\"end\":85252,\"start\":85247},{\"end\":85555,\"start\":85551},{\"end\":85760,\"start\":85702},{\"end\":86007,\"start\":85954},{\"end\":86549,\"start\":86545},{\"end\":86776,\"start\":86695},{\"end\":87101,\"start\":86979},{\"end\":87414,\"start\":87409},{\"end\":87723,\"start\":87718},{\"end\":88012,\"start\":88008},{\"end\":88239,\"start\":88234},{\"end\":88544,\"start\":88539},{\"end\":88960,\"start\":88901},{\"end\":89411,\"start\":89359},{\"end\":89742,\"start\":89736},{\"end\":90032,\"start\":90028},{\"end\":90311,\"start\":90305},{\"end\":90511,\"start\":90461},{\"end\":90693,\"start\":90632},{\"end\":90907,\"start\":90825},{\"end\":91126,\"start\":91053},{\"end\":91358,\"start\":91353},{\"end\":91546,\"start\":91493},{\"end\":91802,\"start\":91797},{\"end\":92026,\"start\":91996},{\"end\":92363,\"start\":92359},{\"end\":92702,\"start\":92696},{\"end\":92955,\"start\":92950},{\"end\":93238,\"start\":93234},{\"end\":93517,\"start\":93511},{\"end\":93760,\"start\":93756},{\"end\":93989,\"start\":93985},{\"end\":94217,\"start\":94189},{\"end\":94579,\"start\":94517},{\"end\":94966,\"start\":94962},{\"end\":95233,\"start\":95229},{\"end\":95496,\"start\":95491},{\"end\":95825,\"start\":95820},{\"end\":96127,\"start\":96122},{\"end\":96446,\"start\":96442},{\"end\":96701,\"start\":96697},{\"end\":96929,\"start\":96925},{\"end\":97209,\"start\":97205},{\"end\":97543,\"start\":97539},{\"end\":97840,\"start\":97836},{\"end\":98077,\"start\":98046},{\"end\":98308,\"start\":98303},{\"end\":98491,\"start\":98486},{\"end\":98696,\"start\":98692},{\"end\":98971,\"start\":98952},{\"end\":99239,\"start\":99160},{\"end\":99511,\"start\":99507},{\"end\":99726,\"start\":99707},{\"end\":100060,\"start\":99998},{\"end\":100441,\"start\":100398},{\"end\":100773,\"start\":100687},{\"end\":101166,\"start\":101147},{\"end\":101476,\"start\":101472},{\"end\":101752,\"start\":101746},{\"end\":102042,\"start\":102038},{\"end\":102275,\"start\":102196},{\"end\":102542,\"start\":102538},{\"end\":102775,\"start\":102770},{\"end\":103048,\"start\":103044},{\"end\":103369,\"start\":103364},{\"end\":103722,\"start\":103715},{\"end\":89006,\"start\":88962}]"}}}, "year": 2023, "month": 12, "day": 17}