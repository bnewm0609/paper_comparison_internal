{"id": 53297058, "updated": "2023-10-01 12:56:22.345", "metadata": {"title": "Efficient Neural Network Robustness Certification with General Activation Functions", "authors": "[{\"first\":\"Huan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Tsui-Wei\",\"last\":\"Weng\",\"middle\":[]},{\"first\":\"Pin-Yu\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Cho-Jui\",\"last\":\"Hsieh\",\"middle\":[]},{\"first\":\"Luca\",\"last\":\"Daniel\",\"middle\":[]}]", "venue": "NeurIPS", "journal": "4944-4953", "publication_date": {"year": 2018, "month": 11, "day": 2}, "abstract": "Finding minimum distortion of adversarial examples and thus certifying robustness in neural network classifiers for given data points is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a non-trivial certified lower bound of minimum adversarial distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for general activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions for given input data points. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by adaptively selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1811.00866", "mag": "2963424284", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/ZhangWCHD18", "doi": null}}, "content": {"source": {"pdf_hash": "7defe11eddc1ffcb8483aade1f84f4125d4d8cd4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1811.00866v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "df80dcd1eeb42dcbb55f9e388c39d72d67cd2742", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7defe11eddc1ffcb8483aade1f84f4125d4d8cd4.txt", "contents": "\nEfficient Neural Network Robustness Certification with General Activation Functions\n\n\nHuan Zhang \nUniversity of California\n90095Los Angeles, Los AngelesCA\n\nIBM Research\n10598Yorktown HeightsNY\n\nTsui-Wei Weng twweng@mit.edu \nMassachusetts Institute of Technology\n02139CambridgeMA\n\nPin-Yu Chen pin-yu.chen@ibm.com \nIBM Research\n10598Yorktown HeightsNY\n\nCho-Jui Hsieh chohsieh@cs.ucla.edu \nUniversity of California\n90095Los Angeles, Los AngelesCA\n\nLuca Daniel \nMassachusetts Institute of Technology\n02139CambridgeMA\n\nEfficient Neural Network Robustness Certification with General Activation Functions\n\nFinding minimum distortion of adversarial examples and thus certifying robustness in neural network classifiers for given data points is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a nontrivial certified lower bound of minimum adversarial distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for general activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions for given input data points. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by adaptively selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan.\n\nIntroduction\n\nWhile neural networks (NNs) have achieved remarkable performance and accomplished unprecedented breakthroughs in many machine learning tasks, recent studies have highlighted their lack of robustness against adversarial perturbations [1,2]. For example, in image learning tasks such as object classification [3,4,5,6] or content captioning [7], visually indistinguishable adversarial examples can be easily crafted from natural images to alter a NN's prediction result. Beyond the white-box attack setting where the target model is entirely transparent, visually imperceptible adversarial perturbations can also be generated in the black-box setting by only using the prediction results of the target model [8,9,10,11]. In addition, real-life adversarial examples have been made possible through the lens of realizing physical perturbations [12,13,14]. As NNs are becoming a core technique deployed in a wide range of applications, including safety-critical tasks, certifying robustness of a NN against adversarial perturbations has become an important research topic in machine learning.\n\nGiven a NN (possibly with a deep and complicated network architecture), we are interested in certifying the (local) robustness of an arbitrary natural example x 0 by ensuring all its neighborhood has the same inference outcome (e.g., consistent top-1 prediction). In this paper, the neighborhood of x 0 is characterized by an p ball centered at x 0 , for any p \u2265 1. Geometrically speaking, the minimum distance of a misclassified nearby example to x 0 is the least adversary strength (a.k.a. minimum adversarial distortion) required to alter the target model's prediction, which is also the largest possible robustness certificate for x 0 . Unfortunately, finding the minimum distortion of adversarial examples in NNs with Rectified Linear Unit (ReLU) activations, which is one of the most widely used activation functions, is known to be an NP-complete problem [15,16]. This makes formal verification techniques such as Reluplex [15] computationally demanding even for small-sized NNs and suffer from scalability issues.\n\nAlthough certifying the largest possible robustness is challenging for ReLU networks, the piecewise linear nature of ReLUs can be exploited to efficiently compute a non-trivial certified lower bound of the minimum distortion [17,18,19,20]. Beyond ReLU, one fundamental problem that remains largely unexplored is how to generalize the robustness certification technique to other popular activation functions that are not piece-wise linear, such as tanh and sigmoid, and how to motivate and certify the design of other activation functions towards improved robustness. In this paper, we tackle the preceding problem by proposing an efficient robustness certification framework for NNs with general activation functions. Our main contributions in this paper are summarized as follows:\n\n\u2022 We propose a generic analysis framework CROWN for certifying NNs using linear or quadratic upper and lower bounds for general activation functions that are not necessarily piece-wise linear. \u2022 Unlike previous work [20], CROWN allows flexible selections of upper/lower bounds for activation functions, enabling an adaptive scheme that helps to reduce approximation errors. Our experiments show that CROWN achieves up to 26% improvements in certified lower bounds compared to [20]. \u2022 Our algorithm is efficient and can scale to large NNs with various activation functions. For a NN with over 10,000 neurons, we can give a certified lower bound in about 1 minute on 1 CPU core.\n\n\nBackground and Related Work\n\nFor ReLU networks, finding the minimum adversarial distortion for a given input data point x 0 can be cast as a mixed integer linear programming (MILP) problem [21,22,23]. Reluplex [15,24] uses a satisfiable modulo theory (SMT) to encode ReLU activations into linear constraints. Similarly, Planet [25] uses satisfiability (SAT) solvers. However, due to the NP-completeness for solving such a problem [15], these methods can only find minimum distortion for very small networks. It can take Reluplex several hours to find the minimum distortion of an example for a ReLU network with 5 inputs, 5 outputs and 300 neurons [15].\n\nOn the other hand, a computationally feasible alternative of robustness certificate is to provide a non-trivial and certified lower bound of minimum distortion. Some analytical lower bounds based on operator norms on the weight matrices [3] or the Jacobian matrix in NNs [17] do not exploit special property of ReLU and thus can be very loose [20]. The bounds in [26,27] are based on the local Lipschitz constant. [26] assumes a continuous differentiable NN and hence excludes ReLU networks; a closed form lower-bound is also hard to derive for networks beyond 2 layers. [27] applies to ReLU networks and uses Extreme Value Theory to provide an estimated lower bound (CLEVER score). Although the CLEVER score is capable of reflecting the level of robustness in different NNs and is scalable to large networks, it is not a certified lower bound. On the other hand, [18] use the idea of a convex outer adversarial polytope in ReLU networks to compute a certified lower bound by relaxing the MILP certification problem to linear programing (LP). [19] apply semidefinite programming for robustness certification in ReLU networks but their approach is limited to NNs with one hidden layer. [20] exploit the ReLU property to bound the activation function (or the local Lipschitz constant) and provide efficient algorithms (Fast-Lin and Fast-Lip) for computing a certified lower bound, achieving state-of-the-art performance. A recent work [28] uses abstract transformations to zonotopes for proving robustness property for ReLU networks. Nonetheless, there are still some applications demand non-ReLU activations, e.g. RNN and LSTM, thus a general framework that can efficiently compute non-trivial and certified lower bounds for NNs with general activation functions is of great importance. We aim at filling this gap and propose CROWN that can perform efficient robustness certification to NNs with general activation functions. Table 1 summarizes the differences of other approaches and CROWN. Note that a recent work [29] based on solving Lagrangian dual can also handle general activation functions but it trades off the quality of robustness bound with scalability.  [3] \u00d7 Reluplex [15], Planet [25] \u00d7 \u00d7 Hein & Andriushchenko [26] \u00d7 differentiable * Raghunathan et al. [19] \u00d7 \u00d7 \u00d7 Kolter and Wong [18] \u00d7 Fast-lin / Fast-lip [20] \u00d7 CROWN (ours) (general) * Continuously differentiable activation function required (soft-plus is demonstrated in [26]) Some recent works (such as robust optimization based adversarial training [30] or region-based classification [31]) empirically exhibit strong robustness against several adversarial attacks, which is beyond the scope of provable robustness certification. In addition, Sinha et al. [16] provide distributional robustness certification based on Wasserstein distance between data distributions, which is different from the local p ball robustness model considered in this paper.\n\n\nCROWN: A general framework for certifying neural networks\n\nOverview of our results. In this section, we present a general framework CROWN for efficiently computing a certified lower bound of minimum adversarial distortion given any input data point x 0 with general activation functions in larger NNs. We first provide principles in Section 3.1 to derive output bounds of NNs when the inputs are perturbed within an p ball and each neuron has different (adaptive) linear approximation bounds on its activation function. In Section 3.2, we demonstrate how to provide robustness certification for four widely-used activation functions (ReLU, tanh, sigmoid and arctan) using CROWN. In particular, we show that the state-of-the-art Fast-Lin algorithm is a special case under the CROWN framework and that the adaptive selections of approximation bounds allow CROWN to achieve a tighter (larger) certified lower bound (see Section 4). In Section 3.3, we further highlight the flexibility of CROWN to incorporate quadratic approximations on the activation functions in addition to the linear approximations described in Section 3.1.\n\n\nGeneral framework\n\nNotations. For an m-layer neural network with an input vector x \u2208 R n0 , let the number of neurons in each layer be n k , \u2200k \u2208 [m], where [i] denotes set {1, 2, \u00b7 \u00b7 \u00b7 , i}. Let the k-th layer weight matrix be W (k) \u2208 R n k \u00d7n k\u22121 and bias vector be b (k) \u2208 R n k , and let \u03a6 k : R n0 \u2192 R n k be the operator mapping from input to layer k.\nWe have \u03a6 k (x) = \u03c3(W (k) \u03a6 k\u22121 (x) + b (k) ), \u2200k \u2208 [m \u2212 1],\nwhere \u03c3(\u00b7) is the coordinate-wise activation function. While our methodology is applicable to any activation function of interest, we emphasize on four most widely-used activation functions, namely ReLU: \u03c3(y) = max(y, 0), hyperbolic tangent: \u03c3(y) = tanh(y), sigmoid: \u03c3(y) = 1/(1 + e \u2212y ) and arctan: \u03c3(y) = arctan(y). Note that the input \u03a6 0 (x) = x, and the vector output of the NN is\nf (x) = \u03a6 m (x) = W (m) \u03a6 m\u22121 (x)+b (m) . The j-th output element is denoted as f j (x) = [\u03a6 m (x)] j .\nInput perturbation and pre-activation bounds. Let x 0 \u2208 R n0 be a given data point, and let the perturbed input vector x be within an -bounded p -ball centered at x 0 , i.e.,\nx \u2208 B p (x 0 , ), where B p (x 0 , ) := {x | x \u2212 x 0 p \u2264 }.\nFor the r-th neuron in k-th layer, let its pre-activation input be y\n(k) r , where y (k) r = W (k) r,: \u03a6 k\u22121 (x) + b (k) r and W (k)\nr,: denotes the r-th row of matrix W (k) . When x 0 is perturbed within an -bounded p -ball, let l       \nf L j : R n0 \u2192 R and f U j : R n0 \u2192 R such that \u2200j \u2208 [n m ], \u2200x \u2208 B p (x 0 , ), the inequality f L j (x) \u2264 f j (x) \u2264 f U j (x) holds true, where f U j (x) = \u039b (0) j,: x + m k=1 \u039b (k) j,: (b (k) + \u2206 (k) :,j ), f L j (x) = \u2126 (0) j,: x + m k=1 \u2126 (k) j,: (b (k) + \u0398 (k) :,j ), (1) \u039b (k\u22121) j,: = e j if k = m + 1; (\u039b (k) j,: W (k) ) \u03bb (k\u22121) j,: if k \u2208 [m]\n.\n\u2126 (k\u22121) j,: = e j if k = m + 1; (\u2126 (k) j,: W (k) ) \u03c9 (k\u22121) j,: if k \u2208 [m].\nand \u2200i \u2208 [n k ], we define four matrices \u03bb (k) , \u03c9 (k) , \u2206 (k) , \u0398 (k) \u2208 R nm\u00d7n k :\n\u03bb (k) j,i = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b1 (k) U,i if k = 0, \u039b (k+1) j,: W (k+1) :,i \u2265 0; \u03b1 (k) L,i if k = 0, \u039b (k+1) j,: W (k+1) :,i < 0; 1 if k = 0. \u03c9 (k) j,i = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b1 (k) L,i if k = 0, \u2126 (k+1) j,: W (k+1) :,i \u2265 0; \u03b1 (k) U,i if k = 0, \u2126 (k+1) j,: W (k+1) :,i < 0; 1 if k = 0. \u2206 (k) i,j = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b2 (k) U,i if k = m, \u039b (k+1) j,: W (k+1) :,i \u2265 0; \u03b2 (k) L,i if k = m, \u039b (k+1) j,: W (k+1) :,i < 0; 0 if k = m. \u0398 (k) i,j = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b2 (k) L,i if k = m, \u2126 (k+1) j,: W (k+1) :,i \u2265 0; \u03b2 (k) U,i if k = m, \u2126 (k+1) j,: W (k+1) :,i < 0; 0\nif k = m. and is the Hadamard product and e j \u2208 R nm is a standard unit vector at jth coordinate .  L,r , respectively) and then repeat the procedure to \"back-propagate\" to the input layer. This allows us to obtain f U j (x) and f L j (x) in (1). The formal proof of Theorem 3.2 is in Appendix A. Note that for a neuron r in layer k, the slopes of its linear upper and lower bounds \u03b1\n(k) U,r , \u03b1 (k)\nL,r can be different. This implies:\n\n1. Fast-Lin [20] is a special case of our framework as they require the slopes \u03b1\n(k) U,r , \u03b1 (k)\nL,r to be the same; and it only applies to ReLU networks (cf. Sec. 3.2). In Fast-Lin, \u039b (0) and \u2126 (0) are identical. 2. Our CROWN framework allows adaptive selections on the linear approximation when computing certified lower bounds of minimum adversarial distortion, which is the main contributor to improve the certified lower bound as demonstrated in the experiments in Section 4.\n\nGlobal bounds. More importantly, since the input x \u2208 B p (x 0 , ), we can take the maximum, i.e.\nmax x\u2208Bp(x0, ) f U j (x), and minimum, i.e. min x\u2208Bp(x0, ) f L j (x)\n, as a pair of global upper and lower bound of f j (x) -which in fact has closed-form solutions because f U j (x) and f L j (x) are two linear functions and x \u2208 B p (x 0 , ) is a convex norm constraint. This result is formally presented below and its proof is given in Appendix B. Corollary 3.3 (Closed-form global bounds). Given a data point x 0 \u2208 R n0 , p ball parameters p \u2265 1 and > 0. For an m-layer neural network function f : R n0 \u2192 R nm , there exists two fixed values \u03b3 L j and \u03b3 U j such that \u2200x \u2208 B p (x 0 , ) and \u2200j \u2208 [\nn m ], 1/q = 1 \u2212 1/p, the inequality \u03b3 L j \u2264 f j (x) \u2264 \u03b3 U j holds true, where \u03b3 U j = \u039b (0) j,: q + \u039b (0) j,: x0 + m k=1 \u039b (k) j,: (b (k) + \u2206 (k) :,j ), \u03b3 L j = \u2212 \u2126 (0) j,: q + \u2126 (0) j,: x0 + m k=1 \u2126 (k) j,: (b (k) + \u0398 (k)\n:,j ).\n\n(2) \nU,r (y) = \u03b1 (k) U,r (y + \u03b2 (k) U,r ) Upper bound h (k) U,r r \u2208 S + k r \u2208 S \u2212 k r \u2208 S \u00b1 k for activation function \u03b1 (k) U,r \u03b2 (k) U,r \u03b1 (k) U,r \u03b2 (k) U,r \u03b1 (k) U,r \u03b2 (k) U,r ReLU 1 0 0 0 a \u2212l (k) r (a \u2265 u (k) r u (k) r \u2212l (k) r , e.g. a = u (k) r u (k) r \u2212l (k) r ) Sigmoid, tanh \u03c3 (d) \u03c3(d) \u03b1 (k) U,r \u2212 d * \u03c3(u (k) r )\u2212\u03c3(l (k) r ) u (k) r \u2212l (k) r \u03c3(l (k) r ) \u03b1 (k) U,r \u2212 l (k) r \u03c3 (d) \u03c3(l (k) r ) \u03b1 (k) U,r \u2212 l (k) r (denoted as \u03c3(y)) (l (k) r \u2264 d \u2264 u (k) r ) ( \u03c3(d)\u2212\u03c3(l (k) r ) d\u2212l (k) r \u2212 \u03c3 (d) = 0, d \u2265 0 ) * If \u03b1 (k)\nU,r is close to 0, we suggest to calculate the intercept directly, \u03b1\n(k) U,r \u00b7 \u03b2 (k) U,r = \u03c3(d) \u2212 \u03b1 (k)\nU,r d, to avoid numerical issues in implementation. Same for other similar cases. \nAlternatively, if the solution d \u2265 u (k) r , then we can set \u03b1 (k) U,r = \u03c3(u (k) r )\u2212\u03c3(l (k) r ) u (k) r \u2212l (k) r .L,r (y) = \u03b1 (k) L,r (y + \u03b2 (k) L,r ) Lower bound h (k) L,r r \u2208 S + k r \u2208 S \u2212 k r \u2208 S \u00b1 k for activation function \u03b1 (k) L,r \u03b2 (k) L,r \u03b1 (k) L,r \u03b2 (k) L,r \u03b1 (k) L,r \u03b2 (k) L,r ReLU 1 0 0 0 a 0 (0 \u2264 a \u2264 1, e.g. a = u (k) r u (k) r \u2212l (k) r , 0, 1) Sigmoid, tanh \u03c3(u (k) r )\u2212\u03c3(l (k) r ) u (k) r \u2212l (k) r \u03c3(l (k) r ) \u03b1 (k) L,r \u2212 l (k) r \u03c3 (d) \u03c3(d) \u03b1 (k) L,r \u2212 d \u03c3 (d) \u03c3(u (k) r ) \u03b1 (k) L,r \u2212 u (k) r (denoted as \u03c3(y)) (l (k) r \u2264 d \u2264 u (k) r ) ( \u03c3(d)\u2212\u03c3(u (k) r ) d\u2212u (k) r \u2212 \u03c3 (d) = 0, d \u2264 0 ) \u2020 \u2020 Alternatively, if the solution d \u2264 l (k) r , then we can set \u03b1 (k) L,r = \u03c3(u (k) r )\u2212\u03c3(l (k) r ) u (k) r \u2212l (k) r .\nCertified lower bound of minimum distortion. Given an input example x 0 and an m-layer NN, let c be the predicted class of x 0 and t = c be the targeted attack class. We aim to use the uniform bounds established in Corollary 3.3 to obtain the largest possible lower bound\u02dc t and\u02dc of targeted and untargeted attacks respectively, which can be formulated as follows:\n\nWe note that although there is a linear term in (2), other terms such as \u039b (k) , \u2206 (k) and \u2126 (k) , \u0398 (k) also implicitly depend on . This is because the parameters \u03b1 (k)\nU,i , \u03b2 (k) U,i , \u03b1 (k) L,i , \u03b2 (k) L,i depend on l (k) i , u (k)\ni , which may vary with ; thus the values in \u039b (k) , \u2206 (k) , \u2126 (k) , \u0398 (k) depend on . It is therefore difficult to obtain an explicit expression of \u03b3 L c ( ) \u2212 \u03b3 U t ( ) in terms of . Fortunately, we can still perform a binary search to obtain\u02dc t with Corollary 3.3. More precisely, we first initialize at some fixed positive value and apply Corollary 3.3 repeatedly to obtain l (k) r and u (k) r from k = 1 to m and r \u2208 [n k ]. We then check if the condition \u03b3 L c \u2212 \u03b3 U t > 0 is satisfied. If so, we increase ; otherwise, we decrease ; and we repeat the procedure until a given tolerance level is met. 2 Time Complexity. With Corollary 3.3, we can compute analytic output bounds efficiently without resorting to any optimization solvers for general p distortion, and the time complexity for an m-layer ReLU network is polynomial time in contrast to Reluplex or Mixed-Integer Optimization-based approach [22,23] \n\n\nCase studies: CROWN for ReLU, tanh, sigmoid and arctan activations\n\nIn Section 3.1 we showed that as long as one can identify two linear functions h U (y), h L (y) to bound a general activation function \u03c3(y) for each neuron, we can use Corollary 3.3 with a binary search to obtain certified lower bounds of minimum distortion. In this section, we illustrate how to find parameters \u03b1\n(a) r \u2208 S + k (b) r \u2208 S \u2212 k (c) r \u2208 S \u00b1 k(k) U,r , \u03b1 (k) L,r and \u03b2 (k) U,r , \u03b2 (k) L,r of h U (y), h L (y)\nfor four most widely used activation functions: ReLU, tanh, sigmoid and arctan. Other activations, including but not limited to leaky ReLU, ELU and softplus, can be easily incorporated into our CROWN framework following a similar procedure.\n\nSegmenting activation functions. Based on the signs of l\n(k) r and u (k) r , we define a partition {S + k , S \u00b1 k , S \u2212 k } of set [n k ]\nsuch that every neuron in k-th layer belongs to exactly one of the three sets. The formal definition of \nS + k , S \u00b1 k and S \u2212 k is S + k = {r \u2208 [n k ] | 0 \u2264 l (k) r \u2264 u (k) r }, S \u00b1 k = {r \u2208 [n k ] | l (k) r < 0 < u (k) r }, and S \u2212 k = {r \u2208 [n k ] | l (k) r \u2264 u (k) r \u2264 0}.(k) r , u (k)\nr rather than their signs), and we can easily incorporate this into our framework to provide the corresponding explicit output bounds for the new partition sets. In the case study, we consider S + k , S \u00b1 k and S \u2212 k for the four activations, as this partition reflects the curvature of tanh, sigmoid and arctan functions and activation states of ReLU.\n\nBounding tanh/sigmoid/arctan. For tanh activation, \u03c3(y) = 1\u2212e \u22122y 1+e \u22122y ; for sigmoid activation, \u03c3(y) = 1 1+e \u2212y ; for arctan activation, \u03c3(y) = arctan(y). All functions are convex on one side (y < 0) and concave on the other side (y > 0), thus the same rules can be used to find h Bounding ReLU. For ReLU activation, \u03c3(y) = max(0, y). If r \u2208 S + k , we have \u03c3(y) = y and so we can set h\n(k) U,r = h (k) L,r = y; if r \u2208 S \u2212 k , we have \u03c3(y) = 0, and thus we can set h (k) U,r = h (k) L,r = 0; if r \u2208 S \u00b1 k , we can set h (k) U,r = u (k) r u (k) r \u2212l (k) r (y \u2212 l (k) r ) and h (k) L,r = ay, 0 \u2264 a \u2264 1. Setting a = u (k) r u (k) r \u2212l (k) r\nleads to the linear lower bound used in Fast-Lin [20]. Thus, Fast-Lin is a special case under our framework. We propose to adaptively choose a, where we set a = 1 when u (k) r \u2265 |l L,r as valid upper/lower bounds of \u03c3(y), but ideally, we would like them to be close to \u03c3(y) in order to achieve a tighter lower bound of minimum distortion.\n\n\nExtension to quadratic bounds\n\nIn addition to the linear bounds on activation functions, the proposed CROWN framework can also incorporate quadratic bounds by adding a quadratic term to h (k)\nU,r and h (k) L,r : h (k) U,r (y) = \u03b7 (k) U,r y 2 + \u03b1 (k) U,r (y + \u03b2 (k) U,r ), h (k) L,r (y) = \u03b7 (k) L,r y 2 + \u03b1 (k) L,r (y + \u03b2 (k) L,r ), where \u03b7 (k) U,r , \u03b7 (k)\nL,r \u2208 R. Following the procedure of unwrapping the activation functions at the layer m \u2212 1, we show in Appendix D that the output upper bound and lower bound with quadratic approximations are:\nf U j (x) = \u03a6 m\u22122 (x) Q (m\u22121) U \u03a6 m\u22122 (x) + 2p (m\u22121) U \u03a6 m\u22122 (x) + s (m\u22121) U ,(3)f L j (x) = \u03a6 m\u22122 (x) Q (m\u22121) L \u03a6 m\u22122 (x) + 2p (m\u22121) L \u03a6 m\u22122 (x) + s (m\u22121) L ,(4)\nwhere Q\n(m\u22121) U = W (m\u22121) D (m\u22121) U W (m\u22121) , Q (m\u22121) L = W (m\u22121) D (m\u22121) L W (m\u22121) , p (m\u22121) U , p (m\u22121) L , s (m\u22121) U\n, and s (m\u22121) L are defined in Appendix D due to page limit. When m = 2, \u03a6 m\u22122 (x) = x and we can directly optimize over x \u2208 B p (x 0 , ); otherwise, we can use the post activation bounds of layer m \u2212 2 as the constraints. D\n(m\u22121) U in (3) is a diagonal matrix with i-th entry being W (m) j,i \u03b7 (m\u22121) U,i , if W (m) j,i \u2265 0 or W (m) j,i \u03b7 (m\u22121) L,i , if W (m) j,i < 0. Thus, in general Q (m\u22121) U is indefinite,\nresulting in a non-convex optimization when finding the global bounds as in Corollary 3.3. Fortunately, by properly choosing the quadratic bounds, we can make the problem max x\u2208Bp(x0, ) f U j (x) into a convex Quadratic Programming problem; for example, we can let \u03b7 have non-negative diagonals and hence the problem min x\u2208Bp(x0, ) f L j (x) is convex. We can solve this convex program with projected gradient descent (PGD) for x \u2208 B p (x 0 , ) and Armijo line search. Empirically, we find that PGD usually converges within a few iterations.\n\n\nExperiments\n\nMethods. For ReLU networks, CROWN-Ada is CROWN with adaptive linear bounds (Sec. 3.2), CROWN-Quad is CROWN with quadratic bounds (Sec. 3.3). Fast-Lin and Fast-Lip are state-of-the-art fast certified lower bound proposed in [20]. Reluplex can solve the exact minimum adversarial distortion but is only computationally feasible for very small networks. LP-Full is based on the LP formulation in [18] and we solve LPs for each neuron exactly to achieve the best possible bound. For networks with other activation functions, CROWN-general is our proposed method.\n\nModel and Dataset. We evaluate CROWN and other baselines on multi-layer perceptron (MLP) models trained on MNIST and CIFAR-10 datasets. We denote a feed-forward network with m layers and n neurons per layer as m \u00d7 [n]. For models with ReLU activation, we use pretrained models provided by [20] and also evaluate the same set of 100 random test images and random attack targets as in [20] (according to their released code) to make our results comparable. For training NN models with other activation functions, we search for best learning rate and weight decay parameters to achieve a similar level of accuracy as ReLU models.\n\nImplementation and Setup. We implement our algorithm using Python (numpy with numba). Most computations in our method are matrix operations that can be automatically parallelized by the BLAS library; however, we set the number of BLAS threads to 1 for a fair comparison to other methods. Experiments were conducted on an Intel Skylake server CPU running at 2.0 GHz on Google Cloud. Our code is available at https://github.com/huanzhang12/CROWN-Robustness-Certification   Results on Small Networks. Figure 2 shows the certified lower bound for 2 and \u221e distortions found by different algorithms on small networks, where Reluplex is feasible and we can observe the gap between different certified lower bounds and the true minimum adversarial distortion. Reluplex and LP-Full are orders of magnitudes slower than other methods (note the logarithmic scale on right y-axis), and CROWN-Quad (for 2-layer) and CROWN-Ada achieve the largest lower bounds. Improvements of CROWN-Ada over Fast-Lin are more significant in larger NNs, as we show below.\n\nResults on Large ReLU Networks. Table 4 demonstrates the lower bounds found by different algorithms for all common p norms. CROWN-Ada significantly outperforms Fast-Lin and Fast-Lip, while the computation time increased by less than 2X over Fast-Lin, and is comparable with Fast-Lip. See Appendix for results on more networks.\n\nResults on Different Activations. Table 7 compares the certified lower bound computed by CROWNgeneral for four activation functions and different p norm on large networks. CROWN-general is able to certify non-trivial lower bounds for all four activation functions efficiently. Comparing to CROWN-Ada on ReLU networks, certifying general activations that are not piece-wise linear only incurs about 20% additional computational overhead.\n\n\nConclusion\n\nWe have presented a general framework CROWN to efficiently compute a certified lower bound of minimum distortion in neural networks for any given data point x 0 . CROWN features adaptive bounds for improved robustness certification and applies to general activation functions. Moreover, experiments show that (1) CROWN outperforms state-of-the-art baselines on ReLU networks and (2) CROWN can efficiently certify non-trivial lower bounds for large networks with over 10K neurons and with different activation functions.\n\n\nA Proof of Theorem 3.2\n\nGiven an m-layer neural network function f : R n0 \u2192 R nm with pre-activation bounds l (k) and u (k) for x \u2208 B p (x 0 , ) and \u2200k \u2208 [m \u2212 1], let the pre-activation inputs for the i-th neuron at layer m \u2212 1 be y\n(m\u22121) i := W (m\u22121) i,: \u03a6 m\u22122 (x) + b (m\u22121) i\n. The j-th output of the neural network is the following:\nf j (x) = nm\u22121 i=1 W (m) j,i [\u03a6 m\u22121 (x)] i + b (m) j ,(5)= nm\u22121 i=1 W (m) j,i \u03c3(y (m\u22121) i ) + b (m) j , = W (m) j,i \u22650 W (m) j,i \u03c3(y (m\u22121) i ) F1 + W (m) j,i <0 W (m) j,i \u03c3(y (m\u22121) i ) F2 +b (m) j .(6)\nAssume the activation function \u03c3(y) is bounded by two linear functions h\n(m\u22121) U,i , h (m\u22121) L,i in Defini- tion 3.1, we have h (m\u22121) L,i (y (m\u22121) i ) \u2264 \u03c3(y (m\u22121) i ) \u2264 h (m\u22121) U,i (y (m\u22121) i ).\nThus, if the associated weight W (m) j,i to the i-th neuron is non-negative (the terms in\nF 1 bracket), we have W (m) j,i \u00b7 h (m\u22121) L,i (y (m\u22121) i ) \u2264 W (m) j,i \u03c3(y (m\u22121) i ) \u2264 W (m) j,i \u00b7 h (m\u22121) U,i (y (m\u22121) i );(7)\notherwise (the terms in F 2 bracket), we have j,i \u2265 0 terms in (6), the upper bound is the righthand-side (RHS) in (7); and for the W (m) j,i < 0 terms in (6), the upper bound is the RHS in (8). Thus, we obtain:\nW (m) j,i \u00b7 h (m\u22121) U,i (y (m\u22121) i ) \u2264 W (m) j,i \u03c3(y (m\u22121) i ) \u2264 W (m) j,i \u00b7 h (m\u22121) L,i (y (m\u22121) i ).(8f U,m\u22121 j (x) = W (m) j,i \u22650 W (m) j,i \u00b7 h (m\u22121) U,i (y (m\u22121) i ) + W (m) j,i <0 W (m) j,i \u00b7 h (m\u22121) L,i (y (m\u22121) i ) + b (m) j ,(9)= W (m) j,i \u22650 W (m) j,i \u03b1 (m\u22121) U,i (y (m\u22121) i + \u03b2 (m\u22121) U,i ) + W (m) j,i <0 W (m) j,i \u03b1 (m\u22121) L,i (y (m\u22121) i + \u03b2 (m\u22121) L,i ) + b (m) j ,(10)= nm\u22121 i=1 W (m) j,i \u03bb (m\u22121) j,i (y (m\u22121) i + \u2206 (m\u22121) i,j ) + b (m) j ,(11)= nm\u22121 i=1 \u039b (m\u22121) j,i ( nm\u22122 r=1 W (m\u22121) i,r [\u03a6 m\u22122 (x)] r + b (m\u22121) i + \u2206 (m\u22121) i,j ) + b (m) j ,(12)= nm\u22121 i=1 \u039b (m\u22121) j,i ( nm\u22122 r=1 W (m\u22121) i,r [\u03a6 m\u22122 (x)] r ) + nm\u22121 i=1 \u039b (m\u22121) j,i (b (m\u22121) i + \u2206 (m\u22121) i,j ) + b (m) j , (13) = nm\u22122 r=1 nm\u22121 i=1 \u039b (m\u22121) j,i W (m\u22121) i,r [\u03a6 m\u22122 (x)] r + nm\u22121 i=1 \u039b (m\u22121) j,i (b (m\u22121) i + \u2206 (m\u22121) i,j ) + b (m) j ,(14)= nm\u22122 r=1W (m\u22121) j,r [\u03a6 m\u22122 (x)] r +b (m\u22121) j .(15)\nFrom (9) (15):\n\u03bb (m\u22121) j,i = \u03b1 (m\u22121) U,i if W (m) j,i \u2265 0 ( \u21d0\u21d2 \u039b (m) j,: W (m) :,i \u2265 0); \u03b1 (m\u22121) L,i if W (m) j,i < 0 ( \u21d0\u21d2 \u039b (m) j,: W (m) :,i < 0); (16) \u2206 (m\u22121) i,j = \u03b2 (m\u22121) U,i if W (m) j,i \u2265 0 ( \u21d0\u21d2 \u039b (m) j,: W (m) :,i \u2265 0); \u03b2 (m\u22121) L,i if W (m) j,i < 0 ( \u21d0\u21d2 \u039b (m)W (m\u22121) j,r = nm\u22121 i=1 \u039b (m\u22121) j,i W (m\u22121) i,r = \u039b (m\u22121) j,: W (m\u22121) :,r , b (m\u22121) j = nm\u22121 i=1 \u039b (m\u22121) j,i (b (m\u22121) i + \u2206 (m\u22121) i,j ) + b (m) j = \u039b (m\u22121) j,: (b (m\u22121) + \u2206 (m\u22121) :,j ) + b (m) j .\nNotice that after defining the new equivalent weightW \n(b (m\u22122) + \u2206 (m\u22122) :,j ) +b (m\u22121) j \u03bb (m\u22122) j,i = \u03b1 (m\u22122) U,i ifW (m\u22121) j,i \u2265 0 ( \u21d0\u21d2 \u039b (m\u22121) j,: W (m\u22121) :,i \u2265 0); \u03b1 (m\u22122) L,i ifW (m\u22121) j,i < 0 ( \u21d0\u21d2 \u039b (m\u22121) j,: W (m\u22121) :,i < 0); \u2206 (m\u22122) i,j = \u03b2 (m\u22122) U,i ifW (m\u22121) j,i \u2265 0 ( \u21d0\u21d2 \u039b (m\u22121) j,: W (m\u22121) :,i \u2265 0); \u03b2 (m\u22122) L,i ifW (m\u22121) j,i < 0 ( \u21d0\u21d2 \u039b (m\u22121) j,: W (m\u22121) :,i < 0).\n\nand repeat again iteratively until obtain the final upper bound\nf U,1 j (x), where f j (x) \u2264 f U,m\u22121 j (x) \u2264 f U,m\u22122 j (x) \u2264 . . . \u2264 f U,1 j (x)\n. We let f j (x) denote the final upper bound f U,1 j (x), and we have\nf U j (x) = \u039b (0) j,: x + m k=1 \u039b (k) j,: (b (k) + \u2206 (k) :,j )\nand ( is the Hadamard product)\n\u039b (k\u22121) j,: = e j if k = m + 1; (\u039b (k) j,: W (k) ) \u03bb (k\u22121) j,: if k \u2208 [m].\nand \u2200i \u2208 [n k ],\n\u03bb (k) j,i = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b1 (k) U,i if k \u2208 [m \u2212 1], \u039b (k+1) j,: W (k+1) :,i \u2265 0; \u03b1 (k) L,i if k \u2208 [m \u2212 1], \u039b (k+1) j,: W (k+1) :,i < 0; 1 if k = 0. \u2206 (k) i,j = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b2 (k) U,i if k \u2208 [m \u2212 1], \u039b (k+1) j,: W (k+1) :,i \u2265 0; \u03b2 (k) L,i if k \u2208 [m \u2212 1], \u039b (k+1) j,: W (k+1) :,i < 0; 0 if k = m.\nLower bound. The above derivations of upper bound can be applied similarly to deriving lower bounds of f j (x), and the only difference is now we need to use the LHS of (7) and (8) (rather than RHS when deriving upper bound) to bound the two terms in (6). Thus, following the same procedure in deriving the upper bounds, we can iteratively unwrap the activation functions and obtain a final lower bound f L,1\nj (x), where f j (x) \u2265 f L,m\u22121 j (x) \u2265 f L,m\u22122 j (x) \u2265 . . . \u2265 f L,1 j (x). Let f L j (x) = f L,1 j (x),\nwe have:\nf L j (x) = \u2126 (0) j,: x + m k=1 \u2126 (k) j,: (b (k) + \u0398 (k) :,j ) \u2126 (k\u22121) j,: = e j if k = m + 1; (\u2126 (k) j,: W (k) ) \u03c9 (k\u22121) j,: if k \u2208 [m]. and \u2200i \u2208 [n k ], \u03c9 (k) j,i = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b1 (k) L,i if k \u2208 [m \u2212 1], \u2126 (k+1) j,: W (k+1) :,i \u2265 0; \u03b1 (k) U,i if k \u2208 [m \u2212 1], \u2126 (k+1) j,: W (k+1) :,i < 0; 1 if k = 0. \u0398 (k) i,j = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b2 (k) L,i if k \u2208 [m \u2212 1], \u2126 (k+1) j,: W (k+1) :,i \u2265 0; \u03b2 (k) U,i if k \u2208 [m \u2212 1], \u2126 (k+1) j,: W (k+1) :,i < 0; 0 if k = m.\nIndeed, \u03bb Global upper bound. Our goal is to find a global upper and lower bound for the m-th layer network output f j (x), \u2200x \u2208 B p (x 0 , ). By Theorem 3.2, for \nx \u2208 B p (x 0 , ), we have f L j (x) \u2264 f j (x) \u2264 f U j (x) and f U j (x) = \u039b (0) j,: x + m k=1 \u039b (k) j,: (b (k) + \u2206 (k) :,j ). Thus define \u03b3 U j := max x\u2208Bp(x0, ) f U j (x), and we have f j (x) \u2264 f U j (x) \u2264 max x\u2208Bp(x0, ) f U j (x) = \u03b3 U j , since \u2200x \u2208 B p (x 0 , ). In particular, max x\u2208Bp(x0, ) f U j (x) = max x\u2208Bp(x0, ) \u039b (0) j,: x + m k=1 \u039b (k) j,: (b (k) + \u2206\n= \u039b \n\nFrom (18) to (19), let y := x\u2212x0 , and thus y \u2208 B p (0, 1). From (19) to (20), apply Definition B.1 and use the fact that q norm is dual of p norm for p, q \u2208 [1, \u221e].\n\nGlobal lower bound. Similarly, let \u03b3 L j := min x\u2208Bp(x0, ) f L j (x), we have\nf j (x) \u2265 f L j (x) \u2265 min x\u2208Bp(x0, ) f L j (x) = \u03b3 L j . Since f L j (x) = \u2126 (0) j,: x + m k=1 \u2126 (k) j,: (b (k) + \u0398 (k)\n:,j ), we can derive \u03b3 L j (similar to the derivation of \u03b3 U j ) below: . To compute f U j (x) with quadratic approximations, we can still apply (7) and (8) \nmin x\u2208Bp(x0, ) f L j (x) = min x\u2208Bp(x0, ) \u2126 (0) j,: x + m k=1 \u2126 (k) j,: (b (k) + \u0398L,r (y) = u (k) r u (k) r \u2212l (k) r y (dotted). Therefore, f U j (x) = W (m) j,i \u22650 W (m) j,i \u00b7 h (m\u22121) U,i (y (m\u22121) i ) + W (m) j,i <0 W (m) j,i \u00b7 h (m\u22121) L,i (y (m\u22121) i ) + b (m) j ,(21)= nm\u22121 i=1 W (m) j,i \u03c4 (m\u22121) j,i y (m\u22121)2 i + \u03bb (m\u22121) j,i (y (m\u22121) i + \u2206 (m\u22121) i,j ) + b (m) j ,(22)= y (m\u22121) diag(q (m\u22121) U,j )y (m\u22121) + \u039b (m\u22121) j,: y (m\u22121) + W (m) j,: \u2206 (m\u22121) :,j ,(23)= \u03a6 m\u22122 (x) Q (m\u22121) U \u03a6 m\u22122 (x) + 2p (m\u22121) U \u03a6 m\u22122 (x) + s (m\u22121) U .(24)\nFrom (21) to (22), we replace h\n(m\u22121) U,i (y (m\u22121) i ) and h (m\u22121) L,i (y (m\u22121) i\n) by their definitions and let (\u03c4  \n(m\u22121) j,i , \u03bb (m\u22121) j,i , \u2206 (m\u22121) i,j ) = (\u03b7 (m\u22121) U,i , \u03b1 (m\u22121) U,i , \u03b2 (m\u22121) U,i ) if W (m) j,i \u2265 0; (\u03b7 (m\u22121) L,i , \u03b1 (m\u22121) L,i , \u03b2 (m\u22121) L,i ) if W (m) j,i < 0.L j (x): f L j (x) = W (m) j,i <0 W (m) j,i \u00b7 h (m\u22121) U,i (y (m\u22121) i ) + W (m) j,i \u22650 W (m) j,i \u00b7 h (m\u22121) L,i (y (m\u22121) i ) + b (m) j , = \u03a6 m\u22122 (x) Q (m\u22121) L \u03a6 m\u22122 (x) + 2p (m\u22121) L \u03a6 m\u22122 (x) + s (m\u22121) L , where Q (m\u22121) L = W (m\u22121) diag(q (m\u22121) L,j )W (m\u22121) , q (m\u22121) L,j = W (m) j,: \u03bd (m\u22121) j,i ; (25) p (m\u22121) U = b (m\u22121) q (m\u22121) U,j + \u039b (m\u22121) j,: , p (m\u22121) L = b (m\u22121) q (m\u22121) L,j + \u2126 (m\u22121)\nand Table 6: Comparison of our proposed certified lower bounds for ReLU with adaptive lower bounds (CROWN-Ada), Fast-Lin and Fast-Lip and Op-nrom. LP-full and Reluplex cannot finish within a reasonable amount of time for all the networks reported here. We also include Op-norm, where we directly compute the operator norm (for example, for p = 2 it is the spectral norm) for each layer and use their products as a global Lipschitz constant and then compute the robustness lower bound. CLEVER is an estimated robustness lower bound, and attacking algorithms (including CW [6] and EAD [32]) provide upper bounds of the minimum adversarial distortion. For each norm, we consider the robustness against three targeted attack classes: the runner-up class (with the second largest probability), a random class and the least likely class. It is clear that CROWN-Ada notably improves the lower bound comparing to Fast-Lin, especially for larger and deeper networks, the improvements can be up to 28%.  \n(\u03bd (m\u22121) j,i , \u03c9 (m\u22121) j,i , \u0398 (m\u22121) i,j ) = (\u03b7 (m\u22121) L,i , \u03b1 (m\u22121) L,i , \u03b2 (m\u22121) L,i ) if W (m) j,i \u2265 0; (\u03b7 (m\u22121) U,i , \u03b1 (m\u22121) U,i , \u03b2 (m\u22121) U,i ) if W (m) j,i < 0.(28)\n\nE Additional Experimental Results\n\n\nE.1 Results on CROWN-Ada\n\n\nR be the pre-activation lower bound and upper bound of y\n\n\nwe first define the linear upper bounds and lower bounds of activation functions in Definition 3.1, which are the key to derive explicit output bounds for an m-layer neural network with general activation functions. The formal statement of the explicit output bounds is shown in Theorem 3.2.\n\n\nDefinition 3.1 (Linear bounds on activation function). For the r-th neuron in k-th layer with preactivation bounds l(k) r ,u (k) r and the activation function \u03c3(y), define two linear functions h (k) U,r , h (k) L,r : R \u2192 R, h (k) U,r (y) = \u03b1 (k) U,r (y + \u03b2 (k) U,r ), h (k) L,r (y) = \u03b1 (k) L,r (y + \u03b2 (k) L,r ), such that h (k) L,r (y) \u2264 \u03c3(y) \u2264 h (k) U,r (y), y \u2208 [l (k) r , u (k) r ], \u2200k \u2208 [m \u2212 1], r \u2208 [n k ] and \u03b1 (k) U,r , \u03b1 (k)L,r \u2208 R + , \u03b2 output bounds of neural network f ). Given an m-layer neural network function f : R n0 \u2192 R nm , there exists two explicit functions\n\n\u039b\nTheorem 3.2 illustrates how a NN function f j (x) can be bounded by two linear functions f U j (x) and f L j (x) when the activation function of each neuron is bounded by two linear functions h r in Definition 3.1. The central idea is to unwrap the activation functions layer by layer by considering the signs of the associated (equivalent) weights of each neuron and apply the two linear bounds h r . As we demonstrate in the proof, when we replace the activation functions with the corresponding linear upper bounds and lower bounds at the layer m \u2212 1, we can then define equivalent weights and biases based on the parameters of h (k) , \u2206 (k) , \u2126 (k) , \u0398 (k) are related to the terms \u03b1\n\n\nwhere SMT and MIO solvers are exponential-time. For an m layer network with n neurons per layer and n outputs, time complexity of CROWN is O(m 2 n 3 ). Forming \u039b (0) and \u2126 (0) for the m-th layer involves multiplications of layer weights in a similar cost of forward propagation in O(mn 3 ) time. Also, the bounds for all previous k \u2208 [m \u2212 1] layers need to be computed beforehand in O(kn 3 ) time; thus the total time complexity is O(m 2 n 3 ).\n\nFigure 1 :\n1\u03c3(y) = tanh. Green lines are the upper bounds h\n\n\nuseful to bound a given activation function. We note there are multiple ways of segmenting the activation functions and defining the partitioned sets (e.g. based on the values of l\n\n\n) as right end-point. For r \u2208 S + k , since \u03c3(y) is concave, we can let h (k) U,r be any tangent line of \u03c3(y) at point d \u2208 [l r pass the two end-points. Similarly, \u03c3(y) is concave for r \u2208 S + k , thus we can let h (k) L,r be any tangent line of \u03c3(y) at point d \u2208 [l r pass the two end-points. Lastly, for r \u2208 S \u00b1 k , we can let h(k) U,r be the tangent line that passes the left end-point and (d, \u03c3(d)) where d \u2265 0 and h (k) U,r be the tangent line that passes the right end-point and (d, \u03c3(d)) where d \u2264 0. The value of d for transcendental functions can be found using a binary search. The plots of upper and lower bounds for tanh and sigmoid are in Figure 1 and 3 (in Appendix). Plots for arctan are similar and so omitted.\n\nr\n|. In this way, the area between the lower bound h(k) L,r = ay and \u03c3(y) (which reflects the gap between the lower bound and the ReLU function) is always minimized. As shown in our experiments, the adaptive selection of h (k) L,r based on the value of u (k) r and l (k) r helps to achieve a tighter certified lower bound. Figure 4 (in Appendix) illustrates the idea discussed here. Summary. We summarized the above analysis on choosing valid linear functions h\n\n\nnegative and zero diagonals for the maximization problem -this is equivalent to applying a linear upper bound and a quadratic lower bound to bound the activation function. Similarly, for D\n\nFigure 2 :\n2a) MNIST 2 \u00d7 [20], 2 (b) MNIST 2 \u00d7 [20], \u221e (c) MNIST 3 \u00d7 [20], 2 (d) MNIST 3 \u00d7 [20], \u221e Certified lower bounds and minimum distortion comparisons for 2 and \u221e distortions. Left y-axis is distortion and right y-axis (black line) is computation time (seconds, logarithmic scale). On the top of figures are the avg. CLEVER score and the upper bound found by C&W attack [6]. From left to right in (a)-(d): CROWN-Ada, (CROWN-Quad), Fast-Lin, Fast-Lip, LP-Full and (Reluplex).\n\n)\nUpper bound. Let f U,m\u22121 j (x) be an upper bound of f j (x). To compute f U,are the key equations. Precisely, for the W (m)\n\n\n: = e j (the standard unit vector with the only non-zero jth element equal to 1), and thus we can rewrite the conditions of W i . From(12) to(13), we collect the constant terms that are not related to x. From(13) to(14), we swap the summation order of i and r, and the coefficients in front of [\u03a6 m\u22122 (x)] r can be combined into a new equivalent weight W\n\n\nin(15) and f j (x) in (5) are in the same form. Thus, we can repeat the above procedure again to obtain an upper bound of f U,\n\n\n.1 (Dual norm). Let \u00b7 be a norm on R n . The associated dual norm, denoted as \u00b7 * , is defined as a * = {sup y a y | y \u2264 1}.\n\n\n: (b (k) + \u2206 (k) :,j )\n\n\n: (b (k) + \u2206 (k):,j ).\n\nCFigure 3 :\n3Illustration of linear upper and lower bounds on sigmoid activation function. The linear upper and lower bounds for \u03c3(y) = sigmoid D f U j (x) and f L j (x) by Quadratic approximation Upper bound. Let f U j (x) be an upper bound of f j (x)\n\n\nexcept that h (k) U,r (y) and h (k) L,r (y) are replaced by the following quadratic functions: h (k) U,r (y) = \u03b7 (k) U,r y 2 + \u03b1 (k) U,r (y + \u03b2 (k) U,r ), h (k) L,r (y) = \u03b7 (k) L,r y 2 + \u03b1 (k) L,r (y + \u03b2 (k) L,r ).\n\nFigure 4 :\n4The linear upper and lower bounds for \u03c3(y) = ReLU. For the cases (a) and (b), the linear upper bound and lower bound are exactly the function \u03c3(y) in the region (grey-shaded). For (c), we plot three out of many choices of lower bound, and they are h\n\n.\nwrite in the matrix form. From (23) to (24), we substitute y (m\u22121) by its definition: y (m\u22121) = W (m\u22121) \u03a6 (m\u22122) (x) + b (m\u22121) and then collect the quadratic terms, linear terms and constant terms of \u03a6 (m\u22122)Lower bound. Similar to the above derivation, we can simply swap h\n\nTable 1 :\n1Comparison of methods for providing adversarial robustness certification in NNs.MethodNon-trivial bound Multi-layer Scalability Beyond ReLU Szegedy et. al.\n\nTable 2 :\n2Linear upper bound parameters of various activation functions: h(k) \n\n\n\nTable 3 :\n3Linear lower bound parameters of various activation functions: h(k) \n\n\n\nTable 4 :\n4Comparison of certified lower bounds on large ReLU networks. Bounds are the average over 100 images (skipped misclassified images) with random attack targets. Percentage improvements are calculated against Fast-Lin as Fast-Lip is worse than Fast-Lin. Fast-Lin Fast-Lip CROWN-Ada CROWN-Ada vs Fast-Lin Fast-Lin Fast-Lip CROWN-Ada MNIST 4 \u00d7 [1024]Network \nCertified Bounds \nImprovement (%) \nAverage Computation Time (sec) \np norm 1 \n\n1.57649 0.72800 \n1.88217 \n+19% \n1.80 \n2.04 \n3.54 \n\n2 \n\n0.18891 0.06487 \n0.22811 \n+21% \n1.78 \n1.96 \n3.79 \n\n\u221e \n\n0.00823 0.00264 \n0.00997 \n+21% \n1.53 \n2.17 \n3.57 \n\nCIFAR-10 \n7 \u00d7 [1024] \n\n1 \n\n0.86468 0.09239 \n1.09067 \n+26% \n13.21 \n19.76 \n22.43 \n\n2 \n\n0.05937 0.00407 \n0.07496 \n+26% \n12.57 \n18.71 \n21.82 \n\n\u221e \n\n0.00134 0.00008 \n0.00169 \n+26% \n8.98 \n20.34 \n16.66 \n\n\n\nTable 5 :\n5Comparison of certified lower bounds by CROWN-Ada on ReLU networks and CROWNgeneral on networks with tanh, sigmoid and arctan activations. CIFAR models with sigmoid activations achieve much worse accuracy than other networks and are thus excluded.Network \nCertified Bounds by CROWN-Ada and CROWN-general \nAverage Computation Time (sec) \np norm \nReLU \ntanh \nsigmoid \narctan \nReLU tanh sigmoid arctan \n\nMNIST \n3 \u00d7 [1024] \n\n1 \n\n3.00231 2.48407 2.94239 \n2.33246 \n1.25 \n1.61 \n1.68 \n1.70 \n\n2 \n\n0.50841 0.27287 0.44471 \n0.30345 \n1.26 \n1.76 \n1.61 \n1.75 \n\n\u221e \n\n0.02576 0.01182 0.02122 \n0.01363 \n1.37 \n1.78 \n1.76 \n1.77 \n\nCIFAR-10 \n6 \u00d7 [2048] \n\n1 \n\n0.91201 0.44059 \n-\n0.46198 \n71.62 89.77 \n-\n83.80 \n\n2 \n\n0.05245 0.02538 \n-\n0.02515 \n71.51 84.22 \n-\n83.12 \n\n\u221e \n\n0.00114 0.00055 \n-\n0.00055 \n49.28 59.72 \n-\n58.04 \n\n\n\n\nFast-Lin Fast-Lip Op norm CROWN-Ada Fast-Lin CLEVER CW/EAD Fast-Lin Fast-Lip CROWN-Ada MNIST 2 \u00d7 [1024]E.2 Results on CROWN-generalNetworks \nLower bounds and upper bounds (Avg.) \nTime per Image (Avg.) \n\nConfig \np \nTarget \n\nLower Bounds (certified) \nimprovements \nUncertified \nLower Bounds \n[20] \n[3] \nOur algorithm \nover \n[27] \nAttacks \n[20] \nOur bound \n\u221e \n\nrunner-up 0.02256 0.01802 0.00159 \n0.02467 \n+9.4% \n0.0447 \n0.0856 \n163 ms 179 ms \n128 ms \nrand \n0.03083 0.02512 0.00263 \n0.03353 \n+8.8% \n0.0708 \n0.1291 \n176 ms 213 ms \n166 ms \nleast \n0.03854 0.03128 0.00369 \n0.04221 \n+9.5% \n0.0925 \n0.1731 \n176 ms 251 ms \n143 ms \n\n2 \n\nrunner-up 0.46034 0.42027 0.24327 \n0.50110 \n+8.9% \n0.8104 \n1.1874 \n154 ms 184 ms \n110 ms \nrand \n0.63299 0.59033 0.40201 \n0.68506 \n+8.2% \n1.2841 \n1.8779 \n141 ms 212 ms \n133 ms \nleast \n0.79263 0.73133 0.56509 \n0.86377 \n+9.0% \n1.6716 \n2.4556 \n152 ms 291 ms \n116 ms \n\n1 \n\nrunner-up 2.78786 3.46500 0.20601 \n3.01633 \n+8.2% \n4.5970 \n9.5295 \n159 ms 989 ms \n136 ms \nrand \n3.88241 5.10000 0.35957 \n4.17760 \n+7.6% \n7.4186 \n17.259 \n168 ms \n1.15 s \n157 ms \nleast \n4.90809 6.36600 0.48774 \n5.33261 \n+8.6% \n9.9847 \n23.933 \n179 ms \n1.37 s \n144 ms \n\nMNIST \n3 \u00d7 [1024] \n\n\u221e \n\nrunner-up 0.01830 0.01021 0.00004 \n0.02114 \n+15.5% \n0.0509 \n0.1037 \n805 ms \n1.28 s \n1.33 s \nrand \n0.02216 0.01236 0.00007 \n0.02576 \n+16.2% \n0.0717 \n0.1484 \n782 ms 859 ms \n1.37 s \nleast \n0.02432 0.01384 0.00009 \n0.02835 \n+16.6% \n0.0825 \n0.1777 \n792 ms 684 ms \n1.37 s \n\n2 \n\nrunner-up 0.35867 0.22120 0.06626 \n0.41295 \n+15.1% \n0.8402 \n1.3513 \n732 ms \n1.06 s \n1.26 s \nrand \n0.43892 0.26980 0.10233 \n0.50841 \n+15.8% \n1.2441 \n2.0387 \n711 ms 696 ms \n1.26 s \nleast \n0.48361 0.30147 0.13256 \n0.56167 \n+16.1% \n1.4401 \n2.4916 \n723 ms 655 ms \n1.25 s \n\n1 \n\nrunner-up 2.08887 1.80150 0.00734 \n2.39443 \n+14.6% \n4.8370 \n10.159 \n685 ms \n2.36 s \n1.15 s \nrand \n2.59898 2.25950 0.01133 \n3.00231 \n+15.5% \n7.2177 \n17.796 \n743 ms \n2.69 s \n1.25 s \nleast \n2.87560 2.50000 0.01499 \n3.33231 \n+15.9% \n8.3523 \n22.395 \n729 ms \n3.08 s \n1.31 s \n\nMNIST \n4 \u00d7 [1024] \n\n\u221e \n\nrunner-up 0.00715 0.00219 0.00001 \n0.00861 \n+20.4% \n0.0485 \n0.08635 \n1.54 s \n3.42 s \n3.23 s \nrand \n0.00823 0.00264 0.00001 \n0.00997 \n+21.1% \n0.0793 \n0.1303 \n1.53 s \n2.17 s \n3.57 s \nleast \n0.00899 0.00304 0.00001 \n0.01096 \n+21.9% \n0.1028 \n0.1680 \n1.74 s \n2.00 s \n3.87 s \n\n2 \n\nrunner-up 0.16338 0.05244 0.11015 \n0.19594 \n+19.9% \n0.8689 \n1.2422 \n1.79 s \n2.58 s \n3.52 s \nrand \n0.18891 0.06487 0.17734 \n0.22811 \n+20.8% \n1.4231 \n1.8921 \n1.78 s \n1.96 s \n3.79 s \nleast \n0.20671 0.07440 0.23710 \n0.25119 \n+21.5% \n1.8864 \n2.4451 \n1.98 s \n2.01 s \n4.01 s \n\n1 \n\nrunner-up 1.33794 0.58480 0.00114 \n1.58151 \n+18.2% \n5.2685 \n10.079 \n1.87 s \n1.93 s \n3.34 s \nrand \n1.57649 0.72800 0.00183 \n1.88217 \n+19.4% \n8.9764 \n17.200 \n1.80 s \n2.04 s \n3.54 s \nleast \n1.73874 0.82800 0.00244 \n2.09157 \n+20.3% \n11.867 \n23.910 \n1.94 s \n2.40 s \n3.72 s \n\nCIFAR \n5 \u00d7 [2048] \n\n\u221e \n\nrunner-up 0.00137 0.00020 0.00000 \n0.00167 \n+21.9% \n0.0062 \n0.00950 \n18.2 s \n38.2 s \n33.1 s \nrand \n0.00170 0.00030 0.00000 \n0.00212 \n+24.7% \n0.0147 \n0.02351 \n19.6 s \n48.2 s \n36.7 s \nleast \n0.00188 0.00036 0.00000 \n0.00236 \n+25.5% \n0.0208 \n0.03416 \n20.4 s \n50.5 s \n38.6 s \n\n2 \n\nrunner-up 0.06122 0.00948 0.00156 \n0.07466 \n+22.0% \n0.2712 \n0.3778 \n24.2 s \n39.4 s \n41.0 s \nrand \n0.07654 0.01417 0.00333 \n0.09527 \n+24.5% \n0.6399 \n0.9497 \n26.0 s \n31.2 s \n42.5 s \nleast \n0.08456 0.01778 0.00489 \n0.10588 \n+25.2% \n0.9169 \n1.4379 \n25.0 s \n33.2 s \n44.4 s \n\n1 \n\nrunner-up 0.93836 0.22632 0.00000 \n1.13799 \n+21.3% \n4.0755 \n7.6529 \n24.7 s \n45.1 s \n40.5 s \nrand \n1.18928 0.31984 0.00000 \n1.47393 \n+23.9% \n9.7145 \n21.643 \n25.7 s \n36.2 s \n44.0 s \nleast \n1.31904 0.38887 0.00001 \n1.64452 \n+24.7% \n12.793 \n34.497 \n26.0 s \n31.7 s \n44.9 s \n\nCIFAR \n6 \u00d7 [2048] \n\n\u221e \n\nrunner-up 0.00075 0.00005 0.00000 \n0.00094 \n+25.3% \n0.0054 \n0.00770 \n27.6 s \n64.7 s \n47.3 s \nrand \n0.00090 0.00007 0.00000 \n0.00114 \n+26.7% \n0.0131 \n0.01866 \n28.1 s \n72.3 s \n49.3 s \nleast \n0.00095 0.00008 0.00000 \n0.00122 \n+28.4% \n0.0199 \n0.02868 \n28.1 s \n76.3 s \n49.4 s \n\n2 \n\nrunner-up 0.03462 0.00228 0.00476 \n0.04314 \n+24.6% \n0.2394 \n0.2979 \n37.0 s \n60.7 s \n65.8 s \nrand \n0.04129 0.00331 0.01079 \n0.05245 \n+27.0% \n0.5860 \n0.7635 \n40.0 s \n56.8 s \n71.5 s \nleast \n0.04387 0.00385 0.01574 \n0.05615 \n+28.0% \n0.8756 \n1.2111 \n40.0 s \n56.3 s \n72.5 s \n\n1 \n\nrunner-up 0.59636 0.05647 0.00000 \n0.73727 \n+23.6% \n3.3569 \n6.0112 \n37.2 s \n65.6 s \n66.8 s \nrand \n0.72178 0.08212 0.00000 \n0.91201 \n+26.4% \n8.2507 \n17.160 \n39.5 s \n53.5 s \n71.6 s \nleast \n0.77179 0.09397 0.00000 \n0.98331 \n+27.4% \n12.603 \n28.958 \n40.7 s \n42.1 s \n72.5 s \n\nCIFAR \n7 \u00d7 [1024] \n\n\u221e \n\nrunner-up 0.00119 0.00006 0.00000 \n0.00148 \n+24.4% \n0.0062 \n0.0102 \n8.98 s \n20.1 s \n16.2 s \nrand \n0.00134 0.00008 0.00000 \n0.00169 \n+26.1% \n0.0112 \n0.0218 \n8.98 s \n20.3 s \n16.7 s \nleast \n0.00141 0.00010 0.00000 \n0.00179 \n+27.0% \n0.0148 \n0.0333 \n8.81 s \n22.1 s \n17.4 s \n\n2 \n\nrunner-up 0.05279 0.00308 0.00020 \n0.06569 \n+24.4% \n0.2661 \n0.3943 \n12.7 s \n20.9 s \n20.7 s \nrand \n0.05937 0.00407 0.00029 \n0.07496 \n+26.3% \n0.5145 \n0.9730 \n12.6 s \n18.7 s \n21.8 s \nleast \n0.06249 0.00474 0.00038 \n0.07943 \n+27.1% \n0.6253 \n1.3709 \n12.9 s \n20.7 s \n22.2 s \n\n1 \n\nrunner-up 0.76648 0.07028 0.00000 \n0.95204 \n+24.2% \n4.815 \n7.9987 \n12.8 s \n21.0 s \n21.9 s \nrand \n0.86468 0.09239 0.00000 \n1.09067 \n+26.1% \n8.630 \n22.180 \n13.2 s \n19.8 s \n22.4 s \nleast \n0.91127 0.10639 0.00000 \n1.15687 \n+27.0% \n11.44 \n31.529 \n13.3 s \n17.6 s \n22.9 s \n\n\nTable 7 :\n7Comparison of certified lower bounds by CROWN-Ada on ReLU networks and CROWNgeneral on networks with tanh, sigmoid and arctan activations. CIFAR models with sigmoid activations achieve much worse accuracy than other networks and are thus excluded. For each norm, we consider the robustness against three targeted attack classes: the runner-up class (with the second largest probability), a random class and the least likely class. Certified Bounds by CROWN-general Average Computation Time (sec)Network \np norm \ntarget \ntanh \nsigmoid \narctan \ntanh \nsigmoid \narctan \n\nMNIST \n3 \u00d7 [1024] \n\n\u221e \n\nrunner-up 0.0164 0.0225 \n0.0169 \n0.3374 \n0.3213 \n0.3148 \nrandom \n0.0230 0.0325 \n0.0240 \n0.3185 \n0.3388 \n0.3128 \nleast \n0.0306 0.0424 \n0.0314 \n0.3129 \n0.3586 \n0.3156 \n\n2 \n\nrunner-up 0.3546 0.4515 \n0.3616 \n0.3139 \n0.3110 \n0.3005 \nrandom \n0.5023 0.6552 \n0.5178 \n0.3044 \n0.3183 \n0.2931 \nleast \n0.6696 0.8576 \n0.6769 \n0.3869 \n0.3495 \n0.2676 \n\n1 \n\nrunner-up 2.4600 2.7953 \n2.4299 \n0.2940 \n0.3339 \n0.3053 \nrandom \n3.5550 4.0854 \n3.5995 \n0.3277 \n0.3306 \n0.3109 \nleast \n4.8215 5.4528 \n4.7548 \n0.3201 \n0.3915 \n0.3254 \n\nMNIST \n4 \u00d7 [1024] \n\n\u221e \n\nrunner-up 0.0091 0.0162 \n0.0107 \n1.6794 \n1.7902 \n1.7099 \nrandom \n0.0118 0.0212 \n0.0136 \n1.7783 \n1.7597 \n1.7667 \nleast \n0.0147 0.0243 \n0.0165 \n1.8908 \n1.8483 \n1.7930 \n\n2 \n\nrunner-up 0.2086 0.3389 \n0.2348 \n1.6416 \n1.7606 \n1.8267 \nrandom \n0.2729 0.4447 \n0.3034 \n1.7589 \n1.7518 \n1.6945 \nleast \n0.3399 0.5064 \n0.3690 \n1.8206 \n1.7929 \n1.8264 \n\n1 \n\nrunner-up 1.8296 2.2397 \n1.7481 \n1.5506 \n1.6052 \n1.6704 \nrandom \n2.4841 2.9424 \n2.3325 \n1.6149 \n1.7015 \n1.6847 \nleast \n3.1261 3.3486 \n2.8881 \n1.7762 \n1.7902 \n1.8345 \n\nMNIST \n5 \u00d7 [1024] \n\n\u221e \n\nrunner-up 0.0060 0.0150 \n0.0062 \n3.9916 \n4.4614 \n3.7635 \nrandom \n0.0073 0.0202 \n0.0077 \n3.5068 \n4.4069 \n3.7387 \nleast \n0.0084 0.0230 \n0.0091 \n3.9076 \n4.6283 \n3.9730 \n\n2 \n\nrunner-up 0.1369 0.3153 \n0.1426 \n4.1634 \n4.3311 \n4.1039 \nrandom \n0.1660 0.4254 \n0.1774 \n4.1468 \n4.1797 \n4.0898 \nleast \n0.1909 0.4849 \n0.2096 \n4.5045 \n4.4773 \n4.5497 \n\n1 \n\nrunner-up 1.1242 2.0616 \n1.2388 \n4.4911 \n3.9944 \n4.4436 \nrandom \n1.3952 2.8082 \n1.5842 \n4.4543 \n4.0839 \n4.2609 \nleast \n1.6231 3.2201 \n1.9026 \n4.4674 \n4.5508 \n4.5154 \n\nCIFAR-10 \n5 \u00d7 [2048] \n\n\u221e \n\nrunner-up 0.0005 \n-\n0.0006 \n37.3918 \n-\n37.1383 \nrandom \n0.0008 \n-\n0.0009 \n38.0841 \n-\n37.9199 \nleast \n0.0010 \n-\n0.0011 \n39.1638 \n-\n39.4041 \n\n2 \n\nrunner-up 0.0219 \n-\n0.0256 \n47.4896 \n-\n48.3390 \nrandom \n0.0368 \n-\n0.0406 \n54.0104 \n-\n52.7471 \nleast \n0.0460 \n-\n0.0497 \n55.8924 \n-\n56.3877 \n\n1 \n\nrunner-up 0.3744 \n-\n0.4491 \n46.4041 \n-\n47.1640 \nrandom \n0.6384 \n-\n0.7264 \n54.2138 \n-\n51.6295 \nleast \n0.8051 \n-\n0.8955 \n56.2512 \n-\n55.6069 \n\nCIFAR-10 \n6 \u00d7 [2048] \n\n\u221e \n\nrunner-up 0.0004 \n-\n0.0003 \n59.5020 \n-\n58.2473 \nrandom \n0.0006 \n-\n0.0006 \n59.7220 \n-\n58.0388 \nleast \n0.0006 \n-\n0.0007 \n60.8031 \n-\n60.9790 \n\n2 \n\nrunner-up 0.0177 \n-\n0.0163 \n78.8801 \n-\n72.1884 \nrandom \n0.0254 \n-\n0.0251 \n84.2228 \n-\n83.1202 \nleast \n0.0294 \n-\n0.0306 \n86.2997 \n-\n86.9320 \n\n1 \n\nrunner-up 0.3043 \n-\n0.2925 \n78.7486 \n-\n70.2496 \nrandom \n0.4406 \n-\n0.4620 \n89.7717 \n-\n83.7972 \nleast \n0.5129 \n-\n0.5665 \n87.2094 \n-\n86.6502 \n\nCIFAR-10 \n7 \u00d7 [1024] \n\n\u221e \n\nrunner-up 0.0006 \n-\n0.0005 \n20.8612 \n-\n20.5169 \nrandom \n0.0008 \n-\n0.0007 \n21.4550 \n-\n21.2134 \nleast \n0.0008 \n-\n0.0008 \n21.3406 \n-\n21.1804 \n\n2 \n\nrunner-up 0.0260 \n-\n0.0225 \n27.9442 \n-\n27.0240 \nrandom \n0.0344 \n-\n0.0317 \n30.3782 \n-\n29.8086 \nleast \n0.0376 \n-\n0.0371 \n30.7492 \n-\n30.7321 \n\n1 \n\nrunner-up 0.3826 \n-\n0.3648 \n28.1898 \n-\n27.1238 \nrandom \n0.5087 \n-\n0.5244 \n29.6373 \n-\n30.5106 \nleast \n0.5595 \n-\n0.6171 \n31.3457 \n-\n30.6481 \n\nt = max s.t. \u03b3 L c ( ) \u2212 \u03b3 U t ( ) > 0 and\u02dc = min t =c\u02dc t .\nThe bound can be further improved by considering g(x) := fc(x) \u2212 ft(x) and replacing the last layer's weights by W (m) c,: \u2212 W (m) t,:. This is also used by[20].\nAcknowledgementThis work was supported in part by NSF IIS-1719097, Intel faculty award, Google Cloud Credits for Research Program and GPUs donated by NVIDIA. Tsui-Wei Weng and Luca Daniel are partially supported by MIT-IBM Watson AI Lab and MIT-Skoltech program.\nThe robustness of deep networks: A geometrical perspective. A Fawzi, S.-M Moosavi-Dezfooli, P Frossard, IEEE Signal Processing Magazine. 346A. Fawzi, S.-M. Moosavi-Dezfooli, and P. Frossard, \"The robustness of deep networks: A geometrical perspective,\" IEEE Signal Processing Magazine, vol. 34, no. 6, pp. 50-62, 2017.\n\nWild patterns: Ten years after the rise of adversarial machine learning. B Biggio, F Roli, arXiv:1712.03141arXiv preprintB. Biggio and F. Roli, \"Wild patterns: Ten years after the rise of adversarial machine learning,\" arXiv preprint arXiv:1712.03141, 2017.\n\nIntriguing properties of neural networks. C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, R Fergus, arXiv:1312.6199arXiv preprintC. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, \"Intriguing properties of neural networks,\" arXiv preprint arXiv:1312.6199, 2013.\n\nExplaining and harnessing adversarial examples. I J Goodfellow, J Shlens, C Szegedy, ICLRI. J. Goodfellow, J. Shlens, and C. Szegedy, \"Explaining and harnessing adversarial examples,\" ICLR, 2015.\n\nDeepfool: a simple and accurate method to fool deep neural networks. S.-M Moosavi-Dezfooli, A Fawzi, P Frossard, IEEE Conference on Computer Vision and Pattern Recognition. S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, \"Deepfool: a simple and accurate method to fool deep neural networks,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2574-2582.\n\nTowards evaluating the robustness of neural networks. N Carlini, D Wagner, IEEE Symposium on Security and Privacy (SP. N. Carlini and D. Wagner, \"Towards evaluating the robustness of neural networks,\" in IEEE Symposium on Security and Privacy (SP), 2017, pp. 39-57.\n\nShow-and-fool: Crafting adversarial examples for neural image captioning. H Chen, H Zhang, P.-Y Chen, J Yi, C.-J Hsieh, arXiv:1712.02051arXiv preprintH. Chen, H. Zhang, P.-Y. Chen, J. Yi, and C.-J. Hsieh, \"Show-and-fool: Crafting adversarial examples for neural image captioning,\" arXiv preprint arXiv:1712.02051, 2017.\n\nPractical black-box attacks against machine learning. N Papernot, P Mcdaniel, I Goodfellow, S Jha, Z B Celik, A Swami, ACM Asia Conference on Computer and Communications Security. N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami, \"Practical black-box attacks against machine learning,\" in ACM Asia Conference on Computer and Communications Security, 2017, pp. 506-519.\n\nDelving into transferable adversarial examples and black-box attacks. Y Liu, X Chen, C Liu, D Song, Y. Liu, X. Chen, C. Liu, and D. Song, \"Delving into transferable adversarial examples and black-box attacks,\" ICLR, 2017.\n\nZOO: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. P.-Y Chen, H Zhang, Y Sharma, J Yi, C.-J Hsieh, ACM Workshop on Artificial Intelligence and Security. P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh, \"ZOO: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models,\" in ACM Workshop on Artificial Intelligence and Security, 2017, pp. 15-26.\n\nDecision-based adversarial attacks: Reliable attacks against black-box machine learning models. W Brendel, J Rauber, M Bethge, ICLRW. Brendel, J. Rauber, and M. Bethge, \"Decision-based adversarial attacks: Reliable attacks against black-box machine learning models,\" ICLR, 2018.\n\nAdversarial examples in the physical world. A Kurakin, I Goodfellow, S Bengio, arXiv:1607.02533arXiv preprintA. Kurakin, I. Goodfellow, and S. Bengio, \"Adversarial examples in the physical world,\" arXiv preprint arXiv:1607.02533, 2016.\n\nRobust physical-world attacks on machine learning models. I Evtimov, K Eykholt, E Fernandes, T Kohno, B Li, A Prakash, A Rahmati, D Song, arXiv:1707.08945arXiv preprintI. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A. Rahmati, and D. Song, \"Robust physical-world attacks on machine learning models,\" arXiv preprint arXiv:1707.08945, 2017.\n\nSynthesizing robust adversarial examples. A Athalye, I Sutskever, arXiv:1707.07397arXiv preprintA. Athalye and I. Sutskever, \"Synthesizing robust adversarial examples,\" arXiv preprint arXiv:1707.07397, 2017.\n\nReluplex: An efficient smt solver for verifying deep neural networks. G Katz, C Barrett, D L Dill, K Julian, M J Kochenderfer, International Conference on Computer Aided Verification. SpringerG. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer, \"Reluplex: An efficient smt solver for verifying deep neural networks,\" in International Conference on Computer Aided Verification. Springer, 2017, pp. 97-117.\n\nCertifiable distributional robustness with principled adversarial training. A Sinha, H Namkoong, J Duchi, ICLRA. Sinha, H. Namkoong, and J. Duchi, \"Certifiable distributional robustness with principled adversarial training,\" ICLR, 2018.\n\nLower bounds on the robustness to adversarial perturbations. J Peck, J Roels, B Goossens, Y Saeys, NIPS. J. Peck, J. Roels, B. Goossens, and Y. Saeys, \"Lower bounds on the robustness to adversarial perturbations,\" in NIPS, 2017.\n\nProvable defenses against adversarial examples via the convex outer adversarial polytope. J Z Kolter, E Wong, ICMLJ. Z. Kolter and E. Wong, \"Provable defenses against adversarial examples via the convex outer adversarial polytope,\" ICML, 2018.\n\nCertified defenses against adversarial examples. A Raghunathan, J Steinhardt, P Liang, ICLRA. Raghunathan, J. Steinhardt, and P. Liang, \"Certified defenses against adversarial examples,\" ICLR, 2018.\n\nTowards fast computation of certified robustness for relu networks. T.-W Weng, H Zhang, H Chen, Z Song, C.-J Hsieh, D Boning, I S Dhillon, L Daniel, ICMLT.-W. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, D. Boning, I. S. Dhillon, and L. Daniel, \"Towards fast computation of certified robustness for relu networks,\" ICML, 2018.\n\nAn approach to reachability analysis for feed-forward relu neural networks. A Lomuscio, L Maganti, arXiv:1706.07351arXiv preprintA. Lomuscio and L. Maganti, \"An approach to reachability analysis for feed-forward relu neural networks,\" arXiv preprint arXiv:1706.07351, 2017.\n\nMaximum resilience of artificial neural networks. C.-H Cheng, G N\u00fchrenberg, H Ruess, International Symposium on Automated Technology for Verification and Analysis. SpringerC.-H. Cheng, G. N\u00fchrenberg, and H. Ruess, \"Maximum resilience of artificial neural networks,\" in International Symposium on Automated Technology for Verification and Analysis. Springer, 2017, pp. 251-268.\n\nDeep neural networks as 0-1 mixed integer linear programs: A feasibility study. M Fischetti, J Jo, arXiv:1712.06174arXiv preprintM. Fischetti and J. Jo, \"Deep neural networks as 0-1 mixed integer linear programs: A feasibility study,\" arXiv preprint arXiv:1712.06174, 2017.\n\nProvably minimally-distorted adversarial examples. N Carlini, G Katz, C Barrett, D L Dill, arXiv:1709.10207arXiv preprintN. Carlini, G. Katz, C. Barrett, and D. L. Dill, \"Provably minimally-distorted adversarial examples,\" arXiv preprint arXiv:1709.10207, 2017.\n\nFormal verification of piece-wise linear feed-forward neural networks. R Ehlers, International Symposium on Automated Technology for Verification and Analysis. SpringerR. Ehlers, \"Formal verification of piece-wise linear feed-forward neural networks,\" in Interna- tional Symposium on Automated Technology for Verification and Analysis. Springer, 2017, pp. 269-286.\n\nFormal guarantees on the robustness of a classifier against adversarial manipulation. M Hein, M Andriushchenko, NIPS. M. Hein and M. Andriushchenko, \"Formal guarantees on the robustness of a classifier against adversarial manipulation,\" in NIPS, 2017.\n\nEvaluating the robustness of neural networks: An extreme value theory approach. T.-W Weng, H Zhang, P.-Y Chen, J Yi, D Su, Y Gao, C.-J Hsieh, L Daniel, ICLRT.-W. Weng, H. Zhang, P.-Y. Chen, J. Yi, D. Su, Y. Gao, C.-J. Hsieh, and L. Daniel, \"Evaluating the robustness of neural networks: An extreme value theory approach,\" ICLR, 2018.\n\nAi2: Safety and robustness certification of neural networks with abstract interpretation. T Gehr, M Mirman, D Drachsler-Cohen, P Tsankov, S Chaudhuri, M Vechev, IEEE Symposium on Security and Privacy (SP). 00T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev, \"Ai2: Safety and robustness certification of neural networks with abstract interpretation,\" in IEEE Symposium on Security and Privacy (SP), vol. 00, 2018, pp. 948-963.\n\nA dual approach to scalable verification of deep networks. K Dvijotham, R Stanforth, S Gowal, T Mann, P Kohli, UAIK. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and P. Kohli, \"A dual approach to scalable verification of deep networks,\" UAI, 2018.\n\nTowards deep learning models resistant to adversarial attacks. A Madry, A Makelov, L Schmidt, D Tsipras, A Vladu, ICLRA. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, \"Towards deep learning models resistant to adversarial attacks,\" ICLR, 2018.\n\nMitigating evasion attacks to deep neural networks via region-based classification. X Cao, N Z Gong, ACM Annual Computer Security Applications Conference. X. Cao and N. Z. Gong, \"Mitigating evasion attacks to deep neural networks via region-based classification,\" in ACM Annual Computer Security Applications Conference, 2017, pp. 278-287.\n\nEAD: elastic-net attacks to deep neural networks via adversarial examples. P.-Y Chen, Y Sharma, H Zhang, J Yi, C.-J Hsieh, AAAIP.-Y. Chen, Y. Sharma, H. Zhang, J. Yi, and C.-J. Hsieh, \"EAD: elastic-net attacks to deep neural networks via adversarial examples,\" AAAI, 2018.\n", "annotations": {"author": "[{\"end\":194,\"start\":87},{\"end\":280,\"start\":195},{\"end\":351,\"start\":281},{\"end\":445,\"start\":352},{\"end\":514,\"start\":446}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":92},{\"end\":208,\"start\":204},{\"end\":292,\"start\":288},{\"end\":365,\"start\":360},{\"end\":457,\"start\":451}]", "author_first_name": "[{\"end\":91,\"start\":87},{\"end\":203,\"start\":195},{\"end\":287,\"start\":281},{\"end\":359,\"start\":352},{\"end\":450,\"start\":446}]", "author_affiliation": "[{\"end\":155,\"start\":99},{\"end\":193,\"start\":157},{\"end\":279,\"start\":225},{\"end\":350,\"start\":314},{\"end\":444,\"start\":388},{\"end\":513,\"start\":459}]", "title": "[{\"end\":84,\"start\":1},{\"end\":598,\"start\":515}]", "venue": null, "abstract": "[{\"end\":2093,\"start\":600}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2345,\"start\":2342},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2347,\"start\":2345},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2419,\"start\":2416},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2421,\"start\":2419},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2423,\"start\":2421},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2425,\"start\":2423},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2451,\"start\":2448},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2818,\"start\":2815},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2820,\"start\":2818},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2823,\"start\":2820},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2826,\"start\":2823},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2953,\"start\":2949},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2956,\"start\":2953},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2959,\"start\":2956},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4064,\"start\":4060},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4067,\"start\":4064},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4132,\"start\":4128},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4450,\"start\":4446},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4453,\"start\":4450},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4456,\"start\":4453},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4459,\"start\":4456},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5224,\"start\":5220},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5484,\"start\":5480},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5876,\"start\":5872},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5879,\"start\":5876},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5882,\"start\":5879},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5897,\"start\":5893},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5900,\"start\":5897},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6014,\"start\":6010},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6117,\"start\":6113},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6335,\"start\":6331},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6578,\"start\":6575},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6613,\"start\":6609},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6685,\"start\":6681},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6705,\"start\":6701},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6708,\"start\":6705},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6756,\"start\":6752},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6913,\"start\":6909},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7206,\"start\":7202},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7385,\"start\":7381},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7527,\"start\":7523},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7775,\"start\":7771},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8357,\"start\":8353},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8508,\"start\":8505},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8524,\"start\":8520},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8537,\"start\":8533},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8568,\"start\":8564},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8611,\"start\":8607},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8638,\"start\":8634},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8665,\"start\":8661},{\"end\":8785,\"start\":8780},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8864,\"start\":8860},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8900,\"start\":8896},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9071,\"start\":9067},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13258,\"start\":13254},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16505,\"start\":16502},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17296,\"start\":17295},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17600,\"start\":17596},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17603,\"start\":17600},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19815,\"start\":19811},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22130,\"start\":22126},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22300,\"start\":22296},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22756,\"start\":22752},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22850,\"start\":22846},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26451,\"start\":26448},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26543,\"start\":26540},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26578,\"start\":26575},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29246,\"start\":29243},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30502,\"start\":30498},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30510,\"start\":30506},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30562,\"start\":30558},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30570,\"start\":30566},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31554,\"start\":31550},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32791,\"start\":32788},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":32804,\"start\":32800},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":37891,\"start\":37887},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":37965,\"start\":37961},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":37972,\"start\":37968},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":38116,\"start\":38112},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":50506,\"start\":50502}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":33503,\"start\":33445},{\"attributes\":{\"id\":\"fig_5\"},\"end\":33797,\"start\":33504},{\"attributes\":{\"id\":\"fig_6\"},\"end\":34377,\"start\":33798},{\"attributes\":{\"id\":\"fig_7\"},\"end\":35068,\"start\":34378},{\"attributes\":{\"id\":\"fig_9\"},\"end\":35515,\"start\":35069},{\"attributes\":{\"id\":\"fig_10\"},\"end\":35576,\"start\":35516},{\"attributes\":{\"id\":\"fig_11\"},\"end\":35759,\"start\":35577},{\"attributes\":{\"id\":\"fig_12\"},\"end\":36487,\"start\":35760},{\"attributes\":{\"id\":\"fig_13\"},\"end\":36950,\"start\":36488},{\"attributes\":{\"id\":\"fig_14\"},\"end\":37141,\"start\":36951},{\"attributes\":{\"id\":\"fig_15\"},\"end\":37623,\"start\":37142},{\"attributes\":{\"id\":\"fig_16\"},\"end\":37750,\"start\":37624},{\"attributes\":{\"id\":\"fig_19\"},\"end\":38107,\"start\":37751},{\"attributes\":{\"id\":\"fig_20\"},\"end\":38236,\"start\":38108},{\"attributes\":{\"id\":\"fig_21\"},\"end\":38363,\"start\":38237},{\"attributes\":{\"id\":\"fig_22\"},\"end\":38388,\"start\":38364},{\"attributes\":{\"id\":\"fig_23\"},\"end\":38413,\"start\":38389},{\"attributes\":{\"id\":\"fig_24\"},\"end\":38667,\"start\":38414},{\"attributes\":{\"id\":\"fig_25\"},\"end\":38884,\"start\":38668},{\"attributes\":{\"id\":\"fig_26\"},\"end\":39147,\"start\":38885},{\"attributes\":{\"id\":\"fig_27\"},\"end\":39423,\"start\":39148},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":39591,\"start\":39424},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39674,\"start\":39592},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":39757,\"start\":39675},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":40559,\"start\":39758},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":41370,\"start\":40560},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":46742,\"start\":41371},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":50285,\"start\":46743}]", "paragraph": "[{\"end\":3196,\"start\":2109},{\"end\":4219,\"start\":3198},{\"end\":5002,\"start\":4221},{\"end\":5680,\"start\":5004},{\"end\":6336,\"start\":5712},{\"end\":9261,\"start\":6338},{\"end\":10389,\"start\":9323},{\"end\":10749,\"start\":10411},{\"end\":11196,\"start\":10811},{\"end\":11475,\"start\":11301},{\"end\":11604,\"start\":11536},{\"end\":11774,\"start\":11669},{\"end\":12127,\"start\":12126},{\"end\":12286,\"start\":12203},{\"end\":13188,\"start\":12805},{\"end\":13240,\"start\":13205},{\"end\":13322,\"start\":13242},{\"end\":13722,\"start\":13339},{\"end\":13820,\"start\":13724},{\"end\":14420,\"start\":13890},{\"end\":14651,\"start\":14645},{\"end\":14657,\"start\":14653},{\"end\":15247,\"start\":15179},{\"end\":15365,\"start\":15283},{\"end\":16452,\"start\":16088},{\"end\":16623,\"start\":16454},{\"end\":17604,\"start\":16690},{\"end\":17989,\"start\":17675},{\"end\":18337,\"start\":18097},{\"end\":18395,\"start\":18339},{\"end\":18581,\"start\":18477},{\"end\":19118,\"start\":18766},{\"end\":19510,\"start\":19120},{\"end\":20100,\"start\":19762},{\"end\":20294,\"start\":20134},{\"end\":20651,\"start\":20459},{\"end\":20822,\"start\":20815},{\"end\":21159,\"start\":20935},{\"end\":21887,\"start\":21346},{\"end\":22461,\"start\":21903},{\"end\":23089,\"start\":22463},{\"end\":24131,\"start\":23091},{\"end\":24459,\"start\":24133},{\"end\":24897,\"start\":24461},{\"end\":25431,\"start\":24912},{\"end\":25666,\"start\":25458},{\"end\":25769,\"start\":25712},{\"end\":26044,\"start\":25972},{\"end\":26256,\"start\":26167},{\"end\":26596,\"start\":26385},{\"end\":27473,\"start\":27459},{\"end\":27976,\"start\":27922},{\"end\":28517,\"start\":28447},{\"end\":28611,\"start\":28581},{\"end\":28703,\"start\":28687},{\"end\":29400,\"start\":28992},{\"end\":29514,\"start\":29506},{\"end\":30121,\"start\":29958},{\"end\":30491,\"start\":30487},{\"end\":30658,\"start\":30493},{\"end\":30737,\"start\":30660},{\"end\":31015,\"start\":30858},{\"end\":31576,\"start\":31545},{\"end\":31662,\"start\":31627},{\"end\":33211,\"start\":32217}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10810,\"start\":10750},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11300,\"start\":11197},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11535,\"start\":11476},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11668,\"start\":11605},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12125,\"start\":11775},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12202,\"start\":12128},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12804,\"start\":12287},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13204,\"start\":13189},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13338,\"start\":13323},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13889,\"start\":13821},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14644,\"start\":14421},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15178,\"start\":14658},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15282,\"start\":15248},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15481,\"start\":15366},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16087,\"start\":15481},{\"attributes\":{\"id\":\"formula_15\"},\"end\":16689,\"start\":16624},{\"attributes\":{\"id\":\"formula_16\"},\"end\":18031,\"start\":17990},{\"attributes\":{\"id\":\"formula_17\"},\"end\":18096,\"start\":18031},{\"attributes\":{\"id\":\"formula_18\"},\"end\":18476,\"start\":18396},{\"attributes\":{\"id\":\"formula_19\"},\"end\":18752,\"start\":18582},{\"attributes\":{\"id\":\"formula_20\"},\"end\":18765,\"start\":18752},{\"attributes\":{\"id\":\"formula_21\"},\"end\":19761,\"start\":19511},{\"attributes\":{\"id\":\"formula_22\"},\"end\":20458,\"start\":20295},{\"attributes\":{\"id\":\"formula_23\"},\"end\":20733,\"start\":20652},{\"attributes\":{\"id\":\"formula_24\"},\"end\":20814,\"start\":20733},{\"attributes\":{\"id\":\"formula_25\"},\"end\":20934,\"start\":20823},{\"attributes\":{\"id\":\"formula_26\"},\"end\":21345,\"start\":21160},{\"attributes\":{\"id\":\"formula_27\"},\"end\":25711,\"start\":25667},{\"attributes\":{\"id\":\"formula_28\"},\"end\":25827,\"start\":25770},{\"attributes\":{\"id\":\"formula_29\"},\"end\":25971,\"start\":25827},{\"attributes\":{\"id\":\"formula_30\"},\"end\":26166,\"start\":26045},{\"attributes\":{\"id\":\"formula_31\"},\"end\":26384,\"start\":26257},{\"attributes\":{\"id\":\"formula_32\"},\"end\":26701,\"start\":26597},{\"attributes\":{\"id\":\"formula_33\"},\"end\":26833,\"start\":26701},{\"attributes\":{\"id\":\"formula_34\"},\"end\":26976,\"start\":26833},{\"attributes\":{\"id\":\"formula_35\"},\"end\":27051,\"start\":26976},{\"attributes\":{\"id\":\"formula_36\"},\"end\":27154,\"start\":27051},{\"attributes\":{\"id\":\"formula_37\"},\"end\":27406,\"start\":27154},{\"attributes\":{\"id\":\"formula_38\"},\"end\":27458,\"start\":27406},{\"attributes\":{\"id\":\"formula_39\"},\"end\":27726,\"start\":27474},{\"attributes\":{\"id\":\"formula_40\"},\"end\":27921,\"start\":27726},{\"attributes\":{\"id\":\"formula_41\"},\"end\":28300,\"start\":27977},{\"attributes\":{\"id\":\"formula_42\"},\"end\":28446,\"start\":28366},{\"attributes\":{\"id\":\"formula_43\"},\"end\":28580,\"start\":28518},{\"attributes\":{\"id\":\"formula_44\"},\"end\":28686,\"start\":28612},{\"attributes\":{\"id\":\"formula_45\"},\"end\":28991,\"start\":28704},{\"attributes\":{\"id\":\"formula_46\"},\"end\":29505,\"start\":29401},{\"attributes\":{\"id\":\"formula_47\"},\"end\":29957,\"start\":29515},{\"attributes\":{\"id\":\"formula_48\"},\"end\":30486,\"start\":30122},{\"attributes\":{\"id\":\"formula_51\"},\"end\":30857,\"start\":30738},{\"attributes\":{\"id\":\"formula_52\"},\"end\":31098,\"start\":31016},{\"attributes\":{\"id\":\"formula_53\"},\"end\":31285,\"start\":31098},{\"attributes\":{\"id\":\"formula_54\"},\"end\":31385,\"start\":31285},{\"attributes\":{\"id\":\"formula_55\"},\"end\":31472,\"start\":31385},{\"attributes\":{\"id\":\"formula_56\"},\"end\":31544,\"start\":31472},{\"attributes\":{\"id\":\"formula_57\"},\"end\":31626,\"start\":31577},{\"attributes\":{\"id\":\"formula_58\"},\"end\":31826,\"start\":31663},{\"attributes\":{\"id\":\"formula_59\"},\"end\":32216,\"start\":31826},{\"attributes\":{\"id\":\"formula_61\"},\"end\":33382,\"start\":33212}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":8270,\"start\":8263},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24172,\"start\":24165},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24502,\"start\":24495},{\"end\":32228,\"start\":32221}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2107,\"start\":2095},{\"attributes\":{\"n\":\"2\"},\"end\":5710,\"start\":5683},{\"attributes\":{\"n\":\"3\"},\"end\":9321,\"start\":9264},{\"attributes\":{\"n\":\"3.1\"},\"end\":10409,\"start\":10392},{\"attributes\":{\"n\":\"3.2\"},\"end\":17673,\"start\":17607},{\"attributes\":{\"n\":\"3.3\"},\"end\":20132,\"start\":20103},{\"attributes\":{\"n\":\"4\"},\"end\":21901,\"start\":21890},{\"attributes\":{\"n\":\"5\"},\"end\":24910,\"start\":24900},{\"end\":25456,\"start\":25434},{\"end\":28365,\"start\":28302},{\"end\":33417,\"start\":33384},{\"end\":33444,\"start\":33420},{\"end\":34380,\"start\":34379},{\"end\":35527,\"start\":35517},{\"end\":36490,\"start\":36489},{\"end\":37153,\"start\":37143},{\"end\":37626,\"start\":37625},{\"end\":38426,\"start\":38415},{\"end\":38896,\"start\":38886},{\"end\":39150,\"start\":39149},{\"end\":39434,\"start\":39425},{\"end\":39602,\"start\":39593},{\"end\":39685,\"start\":39676},{\"end\":39768,\"start\":39759},{\"end\":40570,\"start\":40561},{\"end\":46753,\"start\":46744}]", "table": "[{\"end\":39674,\"start\":39668},{\"end\":39757,\"start\":39751},{\"end\":40559,\"start\":40115},{\"end\":41370,\"start\":40819},{\"end\":46742,\"start\":41504},{\"end\":50285,\"start\":47250}]", "figure_caption": "[{\"end\":33503,\"start\":33447},{\"end\":33797,\"start\":33506},{\"end\":34377,\"start\":33800},{\"end\":35068,\"start\":34381},{\"end\":35515,\"start\":35071},{\"end\":35576,\"start\":35529},{\"end\":35759,\"start\":35579},{\"end\":36487,\"start\":35762},{\"end\":36950,\"start\":36491},{\"end\":37141,\"start\":36953},{\"end\":37623,\"start\":37155},{\"end\":37750,\"start\":37627},{\"end\":38107,\"start\":37753},{\"end\":38236,\"start\":38110},{\"end\":38363,\"start\":38239},{\"end\":38388,\"start\":38366},{\"end\":38413,\"start\":38391},{\"end\":38667,\"start\":38428},{\"end\":38884,\"start\":38670},{\"end\":39147,\"start\":38898},{\"end\":39423,\"start\":39151},{\"end\":39591,\"start\":39436},{\"end\":39668,\"start\":39604},{\"end\":39751,\"start\":39687},{\"end\":40115,\"start\":39770},{\"end\":40819,\"start\":40572},{\"end\":41504,\"start\":41373},{\"end\":47250,\"start\":46755}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":23597,\"start\":23589}]", "bib_author_first_name": "[{\"end\":50832,\"start\":50831},{\"end\":50844,\"start\":50840},{\"end\":50864,\"start\":50863},{\"end\":51165,\"start\":51164},{\"end\":51175,\"start\":51174},{\"end\":51393,\"start\":51392},{\"end\":51404,\"start\":51403},{\"end\":51415,\"start\":51414},{\"end\":51428,\"start\":51427},{\"end\":51437,\"start\":51436},{\"end\":51446,\"start\":51445},{\"end\":51460,\"start\":51459},{\"end\":51718,\"start\":51717},{\"end\":51720,\"start\":51719},{\"end\":51734,\"start\":51733},{\"end\":51744,\"start\":51743},{\"end\":51939,\"start\":51935},{\"end\":51959,\"start\":51958},{\"end\":51968,\"start\":51967},{\"end\":52301,\"start\":52300},{\"end\":52312,\"start\":52311},{\"end\":52588,\"start\":52587},{\"end\":52596,\"start\":52595},{\"end\":52608,\"start\":52604},{\"end\":52616,\"start\":52615},{\"end\":52625,\"start\":52621},{\"end\":52889,\"start\":52888},{\"end\":52901,\"start\":52900},{\"end\":52913,\"start\":52912},{\"end\":52927,\"start\":52926},{\"end\":52934,\"start\":52933},{\"end\":52936,\"start\":52935},{\"end\":52945,\"start\":52944},{\"end\":53301,\"start\":53300},{\"end\":53308,\"start\":53307},{\"end\":53316,\"start\":53315},{\"end\":53323,\"start\":53322},{\"end\":53572,\"start\":53568},{\"end\":53580,\"start\":53579},{\"end\":53589,\"start\":53588},{\"end\":53599,\"start\":53598},{\"end\":53608,\"start\":53604},{\"end\":54016,\"start\":54015},{\"end\":54027,\"start\":54026},{\"end\":54037,\"start\":54036},{\"end\":54244,\"start\":54243},{\"end\":54255,\"start\":54254},{\"end\":54269,\"start\":54268},{\"end\":54495,\"start\":54494},{\"end\":54506,\"start\":54505},{\"end\":54517,\"start\":54516},{\"end\":54530,\"start\":54529},{\"end\":54539,\"start\":54538},{\"end\":54545,\"start\":54544},{\"end\":54556,\"start\":54555},{\"end\":54567,\"start\":54566},{\"end\":54839,\"start\":54838},{\"end\":54850,\"start\":54849},{\"end\":55076,\"start\":55075},{\"end\":55084,\"start\":55083},{\"end\":55095,\"start\":55094},{\"end\":55097,\"start\":55096},{\"end\":55105,\"start\":55104},{\"end\":55115,\"start\":55114},{\"end\":55117,\"start\":55116},{\"end\":55503,\"start\":55502},{\"end\":55512,\"start\":55511},{\"end\":55524,\"start\":55523},{\"end\":55726,\"start\":55725},{\"end\":55734,\"start\":55733},{\"end\":55743,\"start\":55742},{\"end\":55755,\"start\":55754},{\"end\":55985,\"start\":55984},{\"end\":55987,\"start\":55986},{\"end\":55997,\"start\":55996},{\"end\":56189,\"start\":56188},{\"end\":56204,\"start\":56203},{\"end\":56218,\"start\":56217},{\"end\":56411,\"start\":56407},{\"end\":56419,\"start\":56418},{\"end\":56428,\"start\":56427},{\"end\":56436,\"start\":56435},{\"end\":56447,\"start\":56443},{\"end\":56456,\"start\":56455},{\"end\":56466,\"start\":56465},{\"end\":56468,\"start\":56467},{\"end\":56479,\"start\":56478},{\"end\":56746,\"start\":56745},{\"end\":56758,\"start\":56757},{\"end\":56998,\"start\":56994},{\"end\":57007,\"start\":57006},{\"end\":57021,\"start\":57020},{\"end\":57403,\"start\":57402},{\"end\":57416,\"start\":57415},{\"end\":57649,\"start\":57648},{\"end\":57660,\"start\":57659},{\"end\":57668,\"start\":57667},{\"end\":57679,\"start\":57678},{\"end\":57681,\"start\":57680},{\"end\":57932,\"start\":57931},{\"end\":58313,\"start\":58312},{\"end\":58321,\"start\":58320},{\"end\":58563,\"start\":58559},{\"end\":58571,\"start\":58570},{\"end\":58583,\"start\":58579},{\"end\":58591,\"start\":58590},{\"end\":58597,\"start\":58596},{\"end\":58603,\"start\":58602},{\"end\":58613,\"start\":58609},{\"end\":58622,\"start\":58621},{\"end\":58905,\"start\":58904},{\"end\":58913,\"start\":58912},{\"end\":58923,\"start\":58922},{\"end\":58942,\"start\":58941},{\"end\":58953,\"start\":58952},{\"end\":58966,\"start\":58965},{\"end\":59332,\"start\":59331},{\"end\":59345,\"start\":59344},{\"end\":59358,\"start\":59357},{\"end\":59367,\"start\":59366},{\"end\":59375,\"start\":59374},{\"end\":59584,\"start\":59583},{\"end\":59593,\"start\":59592},{\"end\":59604,\"start\":59603},{\"end\":59615,\"start\":59614},{\"end\":59626,\"start\":59625},{\"end\":59861,\"start\":59860},{\"end\":59868,\"start\":59867},{\"end\":59870,\"start\":59869},{\"end\":60196,\"start\":60192},{\"end\":60204,\"start\":60203},{\"end\":60214,\"start\":60213},{\"end\":60223,\"start\":60222},{\"end\":60232,\"start\":60228}]", "bib_author_last_name": "[{\"end\":50838,\"start\":50833},{\"end\":50861,\"start\":50845},{\"end\":50873,\"start\":50865},{\"end\":51172,\"start\":51166},{\"end\":51180,\"start\":51176},{\"end\":51401,\"start\":51394},{\"end\":51412,\"start\":51405},{\"end\":51425,\"start\":51416},{\"end\":51434,\"start\":51429},{\"end\":51443,\"start\":51438},{\"end\":51457,\"start\":51447},{\"end\":51467,\"start\":51461},{\"end\":51731,\"start\":51721},{\"end\":51741,\"start\":51735},{\"end\":51752,\"start\":51745},{\"end\":51956,\"start\":51940},{\"end\":51965,\"start\":51960},{\"end\":51977,\"start\":51969},{\"end\":52309,\"start\":52302},{\"end\":52319,\"start\":52313},{\"end\":52593,\"start\":52589},{\"end\":52602,\"start\":52597},{\"end\":52613,\"start\":52609},{\"end\":52619,\"start\":52617},{\"end\":52631,\"start\":52626},{\"end\":52898,\"start\":52890},{\"end\":52910,\"start\":52902},{\"end\":52924,\"start\":52914},{\"end\":52931,\"start\":52928},{\"end\":52942,\"start\":52937},{\"end\":52951,\"start\":52946},{\"end\":53305,\"start\":53302},{\"end\":53313,\"start\":53309},{\"end\":53320,\"start\":53317},{\"end\":53328,\"start\":53324},{\"end\":53577,\"start\":53573},{\"end\":53586,\"start\":53581},{\"end\":53596,\"start\":53590},{\"end\":53602,\"start\":53600},{\"end\":53614,\"start\":53609},{\"end\":54024,\"start\":54017},{\"end\":54034,\"start\":54028},{\"end\":54044,\"start\":54038},{\"end\":54252,\"start\":54245},{\"end\":54266,\"start\":54256},{\"end\":54276,\"start\":54270},{\"end\":54503,\"start\":54496},{\"end\":54514,\"start\":54507},{\"end\":54527,\"start\":54518},{\"end\":54536,\"start\":54531},{\"end\":54542,\"start\":54540},{\"end\":54553,\"start\":54546},{\"end\":54564,\"start\":54557},{\"end\":54572,\"start\":54568},{\"end\":54847,\"start\":54840},{\"end\":54860,\"start\":54851},{\"end\":55081,\"start\":55077},{\"end\":55092,\"start\":55085},{\"end\":55102,\"start\":55098},{\"end\":55112,\"start\":55106},{\"end\":55130,\"start\":55118},{\"end\":55509,\"start\":55504},{\"end\":55521,\"start\":55513},{\"end\":55530,\"start\":55525},{\"end\":55731,\"start\":55727},{\"end\":55740,\"start\":55735},{\"end\":55752,\"start\":55744},{\"end\":55761,\"start\":55756},{\"end\":55994,\"start\":55988},{\"end\":56002,\"start\":55998},{\"end\":56201,\"start\":56190},{\"end\":56215,\"start\":56205},{\"end\":56224,\"start\":56219},{\"end\":56416,\"start\":56412},{\"end\":56425,\"start\":56420},{\"end\":56433,\"start\":56429},{\"end\":56441,\"start\":56437},{\"end\":56453,\"start\":56448},{\"end\":56463,\"start\":56457},{\"end\":56476,\"start\":56469},{\"end\":56486,\"start\":56480},{\"end\":56755,\"start\":56747},{\"end\":56766,\"start\":56759},{\"end\":57004,\"start\":56999},{\"end\":57018,\"start\":57008},{\"end\":57027,\"start\":57022},{\"end\":57413,\"start\":57404},{\"end\":57419,\"start\":57417},{\"end\":57657,\"start\":57650},{\"end\":57665,\"start\":57661},{\"end\":57676,\"start\":57669},{\"end\":57686,\"start\":57682},{\"end\":57939,\"start\":57933},{\"end\":58318,\"start\":58314},{\"end\":58336,\"start\":58322},{\"end\":58568,\"start\":58564},{\"end\":58577,\"start\":58572},{\"end\":58588,\"start\":58584},{\"end\":58594,\"start\":58592},{\"end\":58600,\"start\":58598},{\"end\":58607,\"start\":58604},{\"end\":58619,\"start\":58614},{\"end\":58629,\"start\":58623},{\"end\":58910,\"start\":58906},{\"end\":58920,\"start\":58914},{\"end\":58939,\"start\":58924},{\"end\":58950,\"start\":58943},{\"end\":58963,\"start\":58954},{\"end\":58973,\"start\":58967},{\"end\":59342,\"start\":59333},{\"end\":59355,\"start\":59346},{\"end\":59364,\"start\":59359},{\"end\":59372,\"start\":59368},{\"end\":59381,\"start\":59376},{\"end\":59590,\"start\":59585},{\"end\":59601,\"start\":59594},{\"end\":59612,\"start\":59605},{\"end\":59623,\"start\":59616},{\"end\":59632,\"start\":59627},{\"end\":59865,\"start\":59862},{\"end\":59875,\"start\":59871},{\"end\":60201,\"start\":60197},{\"end\":60211,\"start\":60205},{\"end\":60220,\"start\":60215},{\"end\":60226,\"start\":60224},{\"end\":60238,\"start\":60233}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":5707980},\"end\":51089,\"start\":50771},{\"attributes\":{\"doi\":\"arXiv:1712.03141\",\"id\":\"b1\"},\"end\":51348,\"start\":51091},{\"attributes\":{\"doi\":\"arXiv:1312.6199\",\"id\":\"b2\"},\"end\":51667,\"start\":51350},{\"attributes\":{\"id\":\"b3\"},\"end\":51864,\"start\":51669},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":12387176},\"end\":52244,\"start\":51866},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2893830},\"end\":52511,\"start\":52246},{\"attributes\":{\"doi\":\"arXiv:1712.02051\",\"id\":\"b6\"},\"end\":52832,\"start\":52513},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1090603},\"end\":53228,\"start\":52834},{\"attributes\":{\"id\":\"b8\"},\"end\":53451,\"start\":53230},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2179389},\"end\":53917,\"start\":53453},{\"attributes\":{\"id\":\"b10\"},\"end\":54197,\"start\":53919},{\"attributes\":{\"doi\":\"arXiv:1607.02533\",\"id\":\"b11\"},\"end\":54434,\"start\":54199},{\"attributes\":{\"doi\":\"arXiv:1707.08945\",\"id\":\"b12\"},\"end\":54794,\"start\":54436},{\"attributes\":{\"doi\":\"arXiv:1707.07397\",\"id\":\"b13\"},\"end\":55003,\"start\":54796},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":516928},\"end\":55424,\"start\":55005},{\"attributes\":{\"id\":\"b15\"},\"end\":55662,\"start\":55426},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":19508971},\"end\":55892,\"start\":55664},{\"attributes\":{\"id\":\"b17\"},\"end\":56137,\"start\":55894},{\"attributes\":{\"id\":\"b18\"},\"end\":56337,\"start\":56139},{\"attributes\":{\"id\":\"b19\"},\"end\":56667,\"start\":56339},{\"attributes\":{\"doi\":\"arXiv:1706.07351\",\"id\":\"b20\"},\"end\":56942,\"start\":56669},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":26530174},\"end\":57320,\"start\":56944},{\"attributes\":{\"doi\":\"arXiv:1712.06174\",\"id\":\"b22\"},\"end\":57595,\"start\":57322},{\"attributes\":{\"doi\":\"arXiv:1709.10207\",\"id\":\"b23\"},\"end\":57858,\"start\":57597},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1931807},\"end\":58224,\"start\":57860},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10490694},\"end\":58477,\"start\":58226},{\"attributes\":{\"id\":\"b26\"},\"end\":58812,\"start\":58479},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":206579396},\"end\":59270,\"start\":58814},{\"attributes\":{\"id\":\"b28\"},\"end\":59518,\"start\":59272},{\"attributes\":{\"id\":\"b29\"},\"end\":59774,\"start\":59520},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1213397},\"end\":60115,\"start\":59776},{\"attributes\":{\"id\":\"b31\"},\"end\":60389,\"start\":60117}]", "bib_title": "[{\"end\":50829,\"start\":50771},{\"end\":51933,\"start\":51866},{\"end\":52298,\"start\":52246},{\"end\":52886,\"start\":52834},{\"end\":53566,\"start\":53453},{\"end\":55073,\"start\":55005},{\"end\":55723,\"start\":55664},{\"end\":56992,\"start\":56944},{\"end\":57929,\"start\":57860},{\"end\":58310,\"start\":58226},{\"end\":58902,\"start\":58814},{\"end\":59858,\"start\":59776}]", "bib_author": "[{\"end\":50840,\"start\":50831},{\"end\":50863,\"start\":50840},{\"end\":50875,\"start\":50863},{\"end\":51174,\"start\":51164},{\"end\":51182,\"start\":51174},{\"end\":51403,\"start\":51392},{\"end\":51414,\"start\":51403},{\"end\":51427,\"start\":51414},{\"end\":51436,\"start\":51427},{\"end\":51445,\"start\":51436},{\"end\":51459,\"start\":51445},{\"end\":51469,\"start\":51459},{\"end\":51733,\"start\":51717},{\"end\":51743,\"start\":51733},{\"end\":51754,\"start\":51743},{\"end\":51958,\"start\":51935},{\"end\":51967,\"start\":51958},{\"end\":51979,\"start\":51967},{\"end\":52311,\"start\":52300},{\"end\":52321,\"start\":52311},{\"end\":52595,\"start\":52587},{\"end\":52604,\"start\":52595},{\"end\":52615,\"start\":52604},{\"end\":52621,\"start\":52615},{\"end\":52633,\"start\":52621},{\"end\":52900,\"start\":52888},{\"end\":52912,\"start\":52900},{\"end\":52926,\"start\":52912},{\"end\":52933,\"start\":52926},{\"end\":52944,\"start\":52933},{\"end\":52953,\"start\":52944},{\"end\":53307,\"start\":53300},{\"end\":53315,\"start\":53307},{\"end\":53322,\"start\":53315},{\"end\":53330,\"start\":53322},{\"end\":53579,\"start\":53568},{\"end\":53588,\"start\":53579},{\"end\":53598,\"start\":53588},{\"end\":53604,\"start\":53598},{\"end\":53616,\"start\":53604},{\"end\":54026,\"start\":54015},{\"end\":54036,\"start\":54026},{\"end\":54046,\"start\":54036},{\"end\":54254,\"start\":54243},{\"end\":54268,\"start\":54254},{\"end\":54278,\"start\":54268},{\"end\":54505,\"start\":54494},{\"end\":54516,\"start\":54505},{\"end\":54529,\"start\":54516},{\"end\":54538,\"start\":54529},{\"end\":54544,\"start\":54538},{\"end\":54555,\"start\":54544},{\"end\":54566,\"start\":54555},{\"end\":54574,\"start\":54566},{\"end\":54849,\"start\":54838},{\"end\":54862,\"start\":54849},{\"end\":55083,\"start\":55075},{\"end\":55094,\"start\":55083},{\"end\":55104,\"start\":55094},{\"end\":55114,\"start\":55104},{\"end\":55132,\"start\":55114},{\"end\":55511,\"start\":55502},{\"end\":55523,\"start\":55511},{\"end\":55532,\"start\":55523},{\"end\":55733,\"start\":55725},{\"end\":55742,\"start\":55733},{\"end\":55754,\"start\":55742},{\"end\":55763,\"start\":55754},{\"end\":55996,\"start\":55984},{\"end\":56004,\"start\":55996},{\"end\":56203,\"start\":56188},{\"end\":56217,\"start\":56203},{\"end\":56226,\"start\":56217},{\"end\":56418,\"start\":56407},{\"end\":56427,\"start\":56418},{\"end\":56435,\"start\":56427},{\"end\":56443,\"start\":56435},{\"end\":56455,\"start\":56443},{\"end\":56465,\"start\":56455},{\"end\":56478,\"start\":56465},{\"end\":56488,\"start\":56478},{\"end\":56757,\"start\":56745},{\"end\":56768,\"start\":56757},{\"end\":57006,\"start\":56994},{\"end\":57020,\"start\":57006},{\"end\":57029,\"start\":57020},{\"end\":57415,\"start\":57402},{\"end\":57421,\"start\":57415},{\"end\":57659,\"start\":57648},{\"end\":57667,\"start\":57659},{\"end\":57678,\"start\":57667},{\"end\":57688,\"start\":57678},{\"end\":57941,\"start\":57931},{\"end\":58320,\"start\":58312},{\"end\":58338,\"start\":58320},{\"end\":58570,\"start\":58559},{\"end\":58579,\"start\":58570},{\"end\":58590,\"start\":58579},{\"end\":58596,\"start\":58590},{\"end\":58602,\"start\":58596},{\"end\":58609,\"start\":58602},{\"end\":58621,\"start\":58609},{\"end\":58631,\"start\":58621},{\"end\":58912,\"start\":58904},{\"end\":58922,\"start\":58912},{\"end\":58941,\"start\":58922},{\"end\":58952,\"start\":58941},{\"end\":58965,\"start\":58952},{\"end\":58975,\"start\":58965},{\"end\":59344,\"start\":59331},{\"end\":59357,\"start\":59344},{\"end\":59366,\"start\":59357},{\"end\":59374,\"start\":59366},{\"end\":59383,\"start\":59374},{\"end\":59592,\"start\":59583},{\"end\":59603,\"start\":59592},{\"end\":59614,\"start\":59603},{\"end\":59625,\"start\":59614},{\"end\":59634,\"start\":59625},{\"end\":59867,\"start\":59860},{\"end\":59877,\"start\":59867},{\"end\":60203,\"start\":60192},{\"end\":60213,\"start\":60203},{\"end\":60222,\"start\":60213},{\"end\":60228,\"start\":60222},{\"end\":60240,\"start\":60228}]", "bib_venue": "[{\"end\":50906,\"start\":50875},{\"end\":51162,\"start\":51091},{\"end\":51390,\"start\":51350},{\"end\":51715,\"start\":51669},{\"end\":52037,\"start\":51979},{\"end\":52363,\"start\":52321},{\"end\":52585,\"start\":52513},{\"end\":53012,\"start\":52953},{\"end\":53298,\"start\":53230},{\"end\":53668,\"start\":53616},{\"end\":54013,\"start\":53919},{\"end\":54241,\"start\":54199},{\"end\":54492,\"start\":54436},{\"end\":54836,\"start\":54796},{\"end\":55187,\"start\":55132},{\"end\":55500,\"start\":55426},{\"end\":55767,\"start\":55763},{\"end\":55982,\"start\":55894},{\"end\":56186,\"start\":56139},{\"end\":56405,\"start\":56339},{\"end\":56743,\"start\":56669},{\"end\":57106,\"start\":57029},{\"end\":57400,\"start\":57322},{\"end\":57646,\"start\":57597},{\"end\":58018,\"start\":57941},{\"end\":58342,\"start\":58338},{\"end\":58557,\"start\":58479},{\"end\":59018,\"start\":58975},{\"end\":59329,\"start\":59272},{\"end\":59581,\"start\":59520},{\"end\":59929,\"start\":59877},{\"end\":60190,\"start\":60117}]"}}}, "year": 2023, "month": 12, "day": 17}