{"id": 254366618, "updated": "2023-10-05 07:28:21.45", "metadata": {"title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training", "authors": "[{\"first\":\"Liang\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Nan\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Xiaolong\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Binxing\",\"last\":\"Jiao\",\"middle\":[]},{\"first\":\"Linjun\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Daxin\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Rangan\",\"last\":\"Majumder\",\"middle\":[]},{\"first\":\"Furu\",\"last\":\"Wei\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2212.03533", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2212-03533", "doi": "10.48550/arxiv.2212.03533"}}, "content": {"source": {"pdf_hash": "5a3c1afe73d8bcc8288d17cb17be2baec8a98464", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2212.03533v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4e3ff6cd1e6aac53f5033797ccb9ba1c9625f54f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5a3c1afe73d8bcc8288d17cb17be2baec8a98464.txt", "contents": "\nText Embeddings by Weakly-Supervised Contrastive Pre-training\n\n\nLiang Wang \nMicrosoft Corporation\n\n\nNan Yang \nMicrosoft Corporation\n\n\nXiaolong Huang \nMicrosoft Corporation\n\n\nBinxing Jiao \nMicrosoft Corporation\n\n\nLinjun Yang \nMicrosoft Corporation\n\n\nDaxin Jiang \nMicrosoft Corporation\n\n\nRangan Majumder \nMicrosoft Corporation\n\n\nFuru Wei \nMicrosoft Corporation\n\n\nText Embeddings by Weakly-Supervised Contrastive Pre-training\n\nThis paper presents E5 1 , a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40\u00d7 more parameters.\n\nIntroduction\n\nText embeddings are low-dimensional vector representations for arbitrary-length texts and play key roles in many NLP tasks such as large-scale retrieval. Compared to the high-dimensional and sparse representations like TF-IDF, text embeddings have the potential to overcome the lexical mismatch issue and facilitate efficient retrieval and matching between texts. It also offers a versatile interface easily consumable by downstream applications.\n\nWhile pre-trained language models such as BERT [17] and GPT [7] can produce transferrable text representations, they are not ideal for tasks such as retrieval and text matching where a singlevector embedding of texts is more desired due to its efficiency and versatility. To obtain better text embeddings, contrastive learning is often the go-to framework to enhance the sequence-level representations from text pairs. Along this line of research, some works are geared towards learning task-specific embeddings. For example, GTR [43] and Sentence-T5 [44] fine-tune pre-trained models with supervised datasets to learn embeddings customized for passage retrieval and semantic textual similarity, respectively. Other works learn unsupervised embeddings from automatically constructed text pairs. Typical methods to construct text pairs include Inverse Close Task (ICT) [9], random cropping [28] and neighboring text spans [41], etc. While such synthetic data are of unlimited quantity, they are often poor in quality and the resulted embeddings fail to match the performance of the classic BM25 baseline without further fine-tuning [40].\n\nIn this work, we learn a high-quality general-purpose text embedding termed E5, EmbEddings from bidirEctional Encoder rEpresentations. E5 aims to provide strong off-the-shelf text embeddings suitable for any tasks requiring single-vector representations in both zero-shot or fine-tuned settings. To achieve this goal, instead of relying on limited labeled data or low-quality synthetic text pairs, we contrastively train E5 embeddings from CCPairs, a curated web-scale text pair dataset containing heterogeneous training signals. We construct the CCPairs dataset by combining various semistructured data sources such as CommunityQA, Common Crawl and Scientific papers, and perform aggressive filtering with a consistency-based filter [15] to improve data quality. We choose a simple contrastive learning recipe using in-batch negatives with a large batch-size to train our model. Extensive experiments on both BEIR and MTEB benchmarks demonstrate the effectiveness of the proposed method. On the BEIR zero-shot retrieval benchmark [53], E5 is the first model to outperform the strong BM25 baseline without using any labeled data. When fine-tuned on labeled datasets, the performance can be further improved. Results on 56 datasets from the recently introduced MTEB benchmark [40] show that our E5 base is competitive against GTR xxl and Sentence-T5 xxl , which have 40\u00d7 more parameters.\n\n\nRelated Work\n\nThere have been long-lasting interests in transforming texts into low-dimensional dense embeddings. Early works include Latent Semantic Indexing (LSA) [16] and Latent Dirichlet Allocation (LDA) [3]. LSA utilizes the decomposition of a word-document co-occurrence matrix to generate document embeddings, while LDA adopts probabilistic graphical models to learn topic distributions. Arora et al. show that a simple weighted average of word vectors [38] can be a strong baseline for sentence embeddings.\n\nWith the development of pre-trained language models [17,35,48] and large-scale labeled datasets such as SNLI [6] and MS-MARCO [8], methods like Sentence-BERT [49], SimCSE [22], Sentence-T5 [44] and SGPT [39] directly fine-tune language models to output continuous embeddings. Most research focuses on short texts and thus uses the term \"sentence embeddings\". For long documents, it remains an open research question whether fixed-length embeddings can encode all the information. Contrastive loss popularized by SimCLR [10] turns out to be more effective than classificationbased losses [49,14] for embeddings. LaBSE [20], LASER [2] and CLIP [47] further extend to multilingual and multi-modal scenarios using parallel sentences and image-text pairs. Another direction is to design self-supervised pre-training tasks for text matching and retrieval. [9] proposes the well-known inverse cloze task (ICT), where a random sentence within a passage is chosen as a pseudo-query and the rest is treated as a positive sample. However, Contriever [28] shows that random cropping with data augmentation is more effective than ICT on a range of zero-shot information retrieval tasks. OpenAI text embeddings [41] use neighboring texts as positives and scale up the model size to 175B. Oguz et al. [45] performs domain-matched pre-training to improve in-domain results. SPAR [11] trains a dense retriever by treating BM25 as a teacher model. Although the aforementioned approaches can easily obtain abundant supervision signals, such synthetic data tend to be of low quality. Results on the BEIR benchmark [53] show they struggle to match the performance of BM25 if not further fine-tuned on labeled datasets.\n\nEvaluation and interpretation of text embeddings are also non-trivial. Most benchmarks measure the embedding quality through downstream task performances. For example, SentEval [13] uses linear probing and a collection of semantic textual similarity (STS) datasets, while the BEIR benchmark [53] focuses on zero-shot information retrieval scenarios. The recently introduced MTEB benchmark [40] combines 56 datasets spanning across 8 tasks and 112 languages. Experiments show no model can achieve state-of-the-art results on all embedding tasks yet. In this paper, we do not use the SentEval toolkit since its linear probing setup depends on the optimization hyperparameters.\n\nMost closely related to our work is a series of community efforts by sentence-transformers 2 to train embeddings with a collection of labeled and automatically collected datasets. In this paper, we show that it is possible to train high-quality embeddings using self-supervised pre-training only. In terms of benchmark results, our model can achieve superior performance when fine-tuned on less labeled data.\n\n\nCCPairs: A Large Collection of Text Pair Dataset\n\nThe quality and diversity of the data is crucial for training general-purpose text embeddings. In this work, we mine and assemble CCPairs, a large high-quality text pair dataset from web sources which provide diverse training signals transferring well to a wide range of tasks.  Figure 1: Overview of our data curation pipeline and model architecture.\n\nHarvesting semi-structured data sources Large-scale high-quality datasets like C4 [48] and CCMatrix [51] are vital for the success of language model pre-training and machine translation. For learning text embeddings, existing works either utilize small-scale human-annotated data such as NLI [22] and MS-MARCO [8] or adopt heuristics such as random cropping [28] to obtain large-scale but very noisy supervision signals.\n\nInstead, we curate a text pair dataset CCPairs (Colossal Clean text Pairs) by harvesting heterogeneous semi-structured data sources. Let (q, p) denote a text pair consisting of a query q and a passage p.\n\nHere we use \"passage\" to denote word sequences of arbitrary length, which can be a short sentence, a paragraph, or a long document. Our dataset includes (post, comment) pairs from Reddit 3 , (question, upvoted answer) pairs from Stackexchange 4 , (entity name + section title, passage) pairs from English Wikipedia, (title, abstract) and citation pairs from Scientific papers [36], and (title, passage) pairs from Common Crawl 5 web pages and various News sources.\n\nWe only include data sources that can be automatically mined, and some subsets are directly reused from existing datasets. Simple heuristic rules are applied to filter data from Reddit and Common Crawl. For example, we remove Reddit comments that are either too long (> 4096 characters) or receive score less than 1, and remove passages from web pages with high perplexity [60]. After preliminary filtering, we end up with \u223c 1.3 billion text pairs, most of which come from Reddit and Common Crawl. For more details and examples, please refer to Appendix A.\n\nConsistency-based filter To further improve data quality and make training costs manageable, we propose a consistency-based data filtering technique: a model is first trained on the 1.3B noisy text pairs, and then used to rank each pair against a pool of 1 million random passages. A text pair is kept only if it falls in the top-k ranked lists. In other words, the model's prediction should be consistent with the training labels. Here we set k = 2 based on manual inspection of data quality. After this step, we end up with \u223c 270M text pairs for contrastive pre-training.\n\nThe intuition for this technique comes from the memorization behaviors of neural networks [19]: when trained on noisy datasets, neural networks tend to memorize the clean labels first and then gradually overfit the noisy labels. Similar techniques [42,15,23] have been widely used for removing dataset noises. It is also possible to apply this filter iteratively, we will leave it for future work.\n\n\nMethod\n\nOur embeddings can be trained with only unlabeled text pairs from CCPairs with contrastive pretraining. A second-stage fine-tuning on small, high-quality labeled datasets can be performed to further boost the quality of the resulted embeddings. See Figure 1 for an overview.\n\n\nContrastive Pre-training with Unlabeled Data\n\nContrastive pre-training aims to distinguish the relevant text pairs from other irrelevant or negative pairs. Given a collection of text pairs\n{(q i , p i )} n i=1 , we assign a list of negative passages {p \u2212 ij } m j=1\nfor the i-th example. Then the InfoNCE contrastive loss [10] is as follows:\nmin L cont = \u2212 1 n i log e s \u03b8 (qi,pi) e s \u03b8 (qi,pi) + j e s \u03b8 (qi,p \u2212 ij )(1)\nwhere s \u03b8 (q, p) is a scoring function between query q and passage p parameterized by \u03b8. Following the popular biencoder architecture, we use a pre-trained Transformer encoder and average pooling over the output layer to get fixed-size text embeddings E q and E p . The score is the cosine similarity scaled by a temperature hyperparameter \u03c4 :\ns \u03b8 (p, q) = cos(E q , E p ) / \u03c4 (2)\nWhere \u03c4 is set to 0.01 in our experiments by default. We use a shared encoder for all input texts and break the symmetry by adding two prefix identifiers \"query:\" and \"passage:\" to q and d respectively.\n\nFor some data sources such as citation pairs, it is not obvious which side should be the query, we randomly choose one for simplicity. Such an asymmetric design turns out to be important for some retrieval tasks where there exist paraphrases of the query in the target corpus.\n\nAnother critical issue for contrastive training is how to select the negative samples. Here we choose to use the in-batch negatives [10], where the passages from other pairs in a batch serve as negative samples. We find that this simple strategy enables more stable training and outperforms methods such as MoCo [25] when the batch size is sufficiently large.\n\n\nFine-tuning with Labeled Data\n\nWhile contrastive pre-training on the CCPairs provides a solid foundation for general-purpose embeddings, further training on labeled data can inject human knowledge into the model to boost the performance. Although these datasets are small, existing works [43,44] have shown that supervised fine-tuning leads to consistent performance gains. In this paper, we choose to further train with a combination of 3 datasets: NLI 6 (Natural Language Inference), MS-MARCO passage ranking dataset [8], and NQ (Natural Questions) dataset [30,32]. Empirically, tasks like STS (Semantic Textual Similarity) and linear probing benefit from NLI data, while MS-MARCO and NQ datasets transfer well to retrieval tasks.\n\nBuilding on the practices of training state-of-the-art dense retrievers [50,58], we use mined hard negatives and knowledge distillation from a cross-encoder (CE) teacher model for the MS-MARCO and NQ datasets. For the NLI dataset, contradiction sentences are regarded as hard negatives. The loss function is a linear interpolation between contrastive loss L cont for hard labels and KL divergence D KL for distilling soft labels from the teacher model. min D KL (p ce , p stu ) + \u03b1L cont (3) Where p ce and p stu are the probabilities from the cross-encoder teacher model and our student model. \u03b1 is a hyperparameter to balance the two loss functions. L cont is the same as in Equation 1.\n\n\nApplications to Text Embedding Tasks\n\nAfter the above two steps, we obtain high-quality text embeddings transferring well to a wide range of tasks without fine-tuning the model parameters. Combined with techniques like approximate nearest neighbor search, embeddings provide a scalable and efficient solution for applications like web search. Here we briefly illustrate several use cases of our text embeddings.\n\nZero-shot Retrieval First, the passage embeddings for the target corpus are computed and indexed offline. Then for each query, we compute its query embedding and return the top-k ranked lists from the corpus based on cosine similarity.\n\nFew-shot Text Classification A linear classifier is trained on top of the frozen embeddings with a few labeled examples. Different tasks only need to train and save the parameters of the classification heads. It can be seen as a particular form of parameter-efficient learning [27].\n\nZero-shot Text Classification The input and label texts are converted to sentences based on manually written prompt templates. The predicted label is the one closest to the input text in the embedding space. Take the sentiment classification of movie reviews as an example, with the original input \"I enjoy watching it\", the label text is \"it is an example of terrible/great movie review\" and the input text becomes \"movie review: I enjoy watching it\".\n\nSemantic Textual Similarity Given two text embeddings, we use the cosine function to measure their semantic similarity. Since the absolute similarity scores do not enable an easy interpretation, the evaluation is usually based on rank correlation coefficients.\n\nText Clustering Standard clustering algorithms such as k-means can be applied straightforwardly. Texts belonging to the same category are expected to be close in the embedding space.\n\nFor tasks other than zero-shot text classification and retrieval, we use the query embeddings by default.\n\n\nExperiments\n\n\nPre-training and Fine-tuning Configurations\n\nPre-training We pre-train on our proposed text pair dataset for three model sizes: E5 small , E5 base and E5 large initialized from MiniLM [59], bert-base-uncased, and bert-large-uncased-whole-wordmasking respectively. The batch size is set to a large value of 32, 768 to increase the number of negatives. The learning rate is {3, 2, 1}\u00d710 \u22124 for the {small, base, large} models, with linear decay and the first 1, 000 steps for warmup. We pre-train for 20k steps in total with AdamW optimizer, which is approximately 2.5 epochs over the dataset. It takes {16, 32, 64} V100 GPUs and {1, 1, 2} days for the {small, base, large} models. To improve training efficiency and reduce GPU memory usage, we adopt mixed precision training and gradient checkpointing.\n\nFine-tuning is performed on the concatenation of 3 datasets: MS-MARCO passage ranking [8], NQ [32,30], and NLI [22] datasets. We reuse the mined hard negatives and re-ranker scores from SimLM [58] for the first two datasets. Models are fine-tuned for 3 epochs with batch size 256 on 8 GPUs. Learning rate is {3, 2, 1}\u00d710 \u22125 for the {small, base, large} models with 400 steps warmup. For each example, we use 7 hard negatives. Since the NLI dataset only has 1 hard negative for each example, 6 sentences are randomly sampled from the entire corpus.\n\nWe use E5-PT to denote models with contrastive pre-training only. More implementation details can be found in Appendix B.\n\n\nEvaluation Datasets\n\nBEIR Benchmark [53] is a collection of 19 information retrieval datasets, ranging across ad-hoc web search, question answering, fact verification and duplicate question retrieval, etc. We evaluate the 15 datasets that provide public downloads. The main metric is nDCG@10.\n\nMTEB Benchmark [40] is recently proposed for benchmarking massive text embedding tasks. Though MTEB is multilingual due to the inclusion of bitext mining datasets, most datasets are still only available in English. In this paper, we evaluate the English subsets, which have 56 datasets spanning across 6 categories: Classification (Class.), Clustering (Clust.), Pair Classification (PairClass.), Rerank, Retrieval (Retr.), STS, and Summarization (Summ.). The evaluation metrics are accuracy, v-measure, average precision, MAP, nDCG@10, and Spearman coefficients, respectively. Please refer to the MTEB paper for details.\n\n\nResults on BEIR benchmark\n\nResults with Unsupervised Methods In Table 1, we show model results that do not use any labeled data. When averaged over all 15 datasets, E5-PT base outperforms the classic BM25 algorithm by 1.2 points. To the best of our knowledge, this is the first reported result that an unsupervised model can beat BM25 on the BEIR benchmark. When scaling up to E5-PT large , we see further benefits from 42.9 to 44.2. In terms of pre-training tasks, Contriever adopts random cropping, while LaPraDor combines ICT and dropout-as-positive-instance from SimCSE. The methods can easily obtain large-scale training data, while our approach requires more effort in dataset curation. Such efforts pay off with better results. Recent studies [34,60,21] also show that improving data quality is a vital step for training large language models. Results with Supervised Fine-tuning In Table 2, we fine-tune our models on supervised datasets and then transfer them to the BEIR benchmark. Since our fine-tuning datasets include MS-MARCO and NQ, the corresponding numbers are in-domain results. For other datasets, these are zero-shot transfer results. Our E5 base model achieves an average nDCG@10 of 48.7, already surpassing existing methods with more parameters such as GTR large [43]. Most datasets benefit from supervised finetuning, but there are also a few exceptions such as FiQA, Scidocs, and Fever, etc. This is likely due to the lack of enough domain diversity for the fine-tuning datasets. \n\n\nResults on MTEB benchmark\n\nIn Table 3, E5 models not only substantially outperform existing ones with similar sizes, but also match the results of much larger models. The top-2 models on MTEB leaderboard 7 GTR xxl and Sentence-T5 xxl have 4.8B parameters, while our E5 large model is more than 10\u00d7 smaller with 300M parameters. We expect that our model will benefit from continual scaling up.\n\nSince the difference between BERT-FT base and E5 base is that BERT-FT base only has fine-tuning stage, their performance gap demonstrates the usefulness of contrastive pre-training on our proposed CCPairs dataset. For most task categories except Clustering, performance improves after supervised fine-tuning. Consistent with prior works [43,44], this once again demonstrates the importance of incorporating human knowledge for learning better text embeddings. It remains an open question whether state-of-the-art embeddings can be obtained in a purely self-supervised manner.  Table 4 shows the zero-shot text classification results on the dev set of the SST-2 dataset [52]. By formulating text classification as embedding matching between input and label texts, our model can be much better than the \"majority\" baseline in a zero-shot setting. We use the prompt template from Section 4.3.\n\n\nAnalysis\n\nIn this section, we conduct a series of analyses to examine various design choices. All the numbers in this section are from base-size models. For the BEIR benchmark, we choose 6 datasets with more stable results across different runs. Some negative results are also listed in Appendix C.\n\n\nImpacts of Batch Size\n\nSince we use in-batch negatives for contrastive pre-training, larger batch size will provide more negatives and therefore improve the quality of the learned text embeddings. In  Table 5, increasing batch size from 1K to 32K leads to consistent gains across all 6 datasets. It is also possible to train with smaller batch sizes by adding hard negatives [50]. However, the engineering efforts of mining hard negatives for large datasets (>100M) are non-trivial. Fine-tuning Datasets GTR models are fine-tuned with \"MS-MARCO + NQ\", while Sentence-T5 models use NLI instead. In Table 6, we can see that the \"MS-MARCO + NQ\" setting performs best on retrieval tasks, and the NLI data is beneficial for STS and linear probing classification. Similar observations are also made by Muennighoff et al. [40]. Combining all of them leads to the best overall scores on the MTEB benchmark. This also illustrates the importance of dataset diversity for learning text embeddings. Data Filtering One crucial step in our dataset curation pipeline is filtering out low-quality text pairs. In Table 7, when training with 1M pairs, using filtered data has a nearly 6 points advantage. When all the text pairs are used, the \"w/o filter\" setting has about 4\u00d7 more data but is still behind by 1.6 points. Though recent studies [29,47] show that deep learning models are quite robust to dataset noises, data filtering still has benefits in improving training efficiency and model quality.\n\nNegative Sampling We explore two alternative methods to enlarge the number of negatives: Prebatch negatives [33] reuse embeddings from previous batches as additional negatives, while MoCo [25] introduces a momentum encoder and uses a FIFO queue to store negatives. For both approaches, the negative size can be easily scaled up without incurring much GPU memory overhead. The downside is that most negatives are produced by an older version of model parameters. In Table 8, in-batch negatives still perform favorably. Empirically, we find that MoCo is more sensitive to certain hyperparameters such as temperature, better results are possible with more tuning.\n\n\nBM25 vs Dense Retrieval\n\nWith the rapid development of dense retrieval models, can we replace the long-standing BM25 algorithm from now on? The answer is likely \"not yet\". BM25 still holds obvious advantages in terms of simplicity, efficiency, and interpretability. For long-tail domains such as Trec-Covid [55] and retrieval tasks that involve long documents (Touche-2020) [4] or rely heavily on exact lexical match (Fever) [54], further research efforts are still necessary to improve current dense retrievers.\n\n\nConclusion\n\nIn this work, we train a general-purpose text embedding model E5 from weak supervision signals. We adopt a simple contrastive training framework with in-batch negatives and learn from a large-scale text pair dataset we harvest from heterogeneous data sources across the web. E5 offers strong off-the-shelf performance for a wide range of tasks requiring single-vector text representations such as retrieval, semantic textual similarity, and text matching. When further customized for downstream tasks, E5 achieves superior fine-tuned performance compared to existing embedding models with 40\u00d7 more parameters on the large, 56-task MTEB benchmark datasets. Table 9: Details for each data source after filtering. The \"Others\" category includes \"Sim-pleWiki\", \"GooAQ\", \"WikiHow\", \"Yahoo Answers\" from https://huggingface.co/datasets/ sentence-transformers/embedding-training-data. \n\n\nB Implementation Details\n\nWe list the hyperparameters in Table 11. Since some evaluation datasets have long texts, we freeze the position embeddings during both pre-training and fine-tuning and set the maximum text length to 512 for evaluation.\n\nFor the Quora duplicate retrieval task in the BEIR benchmark, we add prefix \"query: \" to all the questions. For other retrieval tasks, we use \"query: \" and \"passage: \" prefixes correspondingly.\n\nThe MS-MARCO results in Table 12 use document titles provided by RocketQA [50]. This evaluation setup is consistent with most state-of-the-art dense retrievers. However, the MS-MARCO data from the BEIR benchmark does not have titles, so the results are expected to be lower.  In-domain Evaluation We report results for in-domain datasets in  \n\n\nC Negative Results\n\nHere are some attempts that we eventually give up on:\n\nAdding BM25 hard negatives Similar to DPR [30], we add one BM25 hard negative for each positive pair during training. When using 15M data, this strategy improves the overall results by \u223c 0.5 points on the BEIR benchmark. However, running the BM25 algorithm over a 250M+ dataset is too time-consuming even with multi-node and multi-process parallelism.\n\nUsing RoBERTa instead of BERT for initialization Though RoBERTa shows consistent gains on many NLP tasks, we empirically find that RoBERTa performs worse than BERT initialization on most of the BEIR benchmark datasets.\n\nAuxiliary MLM objective We add a masked language modeling loss for 25% of the training text pairs. The numbers are on par with removing this auxiliary objective, but the training cost goes up. \n\nTable 1 :\n1Unsupervised methods on the BEIR benchmark (nDCG@10). For SimCSE, we report results with BERT base . cpt 300M[41] is only available through paid API and evaluation results on some datasets are missing in the original paper. The highest number for each dataset is in bold, and the second highest is underlined. \u2020: we report the LaPraDor[62] results without ensembling with BM25. * : reproduction with the released checkpoint.BM25 SimCSE LaPraDor \u2020 Contriever cpt 300M E5-PT small E5-PT base E5-PT largeMS MARCO \n22.8 \n9.4 \n16.9  *  \n20.6 \n19.9 \n25.4 \n26.0 \n26.2 \nTrec-Covid \n65.6 \n26.2 \n22.7 \n27.4 \n52.9 \n52.0 \n61.0 \n61.8 \nNFCorpus \n32.5 \n9.9 \n31.1 \n31.7 \n32.0 \n29.3 \n35.8 \n33.7 \nNQ \n32.9 \n11.7 \n18.1 \n25.4 \n-\n37.3 \n39.0 \n41.7 \nHotpotQA \n60.3 \n19.8 \n30.3 \n48.1 \n51.5 \n46.0 \n52.4 \n52.2 \nFiQA \n23.6 \n9.8 \n20.3 \n24.5 \n34.1 \n38.3 \n40.0 \n43.2 \nArguAna \n31.5 \n38.3 \n45.9 \n37.9 \n38.7 \n42.5 \n42.2 \n44.4 \nTouche-2020 \n36.7 \n8.9 \n9.4 \n19.3 \n21.0 \n19.9 \n16.9 \n19.8 \nCQADupStack \n29.9 \n13.2 \n22.0 \n28.4 \n-\n35.0 \n35.4 \n38.9 \nQuora \n78.9 \n78.0 \n78.7 \n83.5 \n68.1 \n85.8 \n85.7 \n86.1 \nDBPedia \n31.3 \n15.0 \n25.0 \n29.2 \n27.2 \n34.5 \n35.4 \n37.1 \nScidocs \n15.8 \n5.5 \n13.3 \n14.9 \n-\n19.9 \n21.1 \n21.8 \nFever \n75.3 \n21.1 \n36.8 \n68.2 \n57.1 \n62.5 \n63.4 \n68.6 \nClimate-Fever \n21.3 \n11.8 \n13.8 \n15.5 \n15.8 \n14.5 \n15.4 \n15.7 \nScifact \n66.5 \n25.7 \n55.5 \n64.9 \n65.4 \n68.5 \n73.7 \n72.3 \nAverage \n41.7 \n20.3 \n29.3 \n36.0 \n-\n40.8 \n42.9 \n44.2 \nBest on \n5 \n0 \n1 \n0 \n0 \n0 \n2 \n7 \n\n\n\nTable 2 :\n2Supervised fine-tuning results on the BEIR benchmark. Results for ANCE[61], ColBERT[31] and Contriever come from Izacard et al.[28]. The best result is in bold, and the second best is underlined.ANCE GTR base ColBERT Contriever cpt 300M GTR large E5 small E5 base E5 large \nMS MARCO \n38.8 \n42.0 \n40.1 \n40.7 \n-\n43.0 \n42.3 \n43.1 \n44.1 \nTrec-Covid \n65.4 \n53.9 \n67.7 \n59.6 \n67.9 \n55.7 \n76.8 \n79.6 \n78.3 \nNFCorpus \n23.7 \n30.8 \n30.5 \n32.8 \n33.2 \n32.9 \n33.9 \n36.6 \n36.1 \nNQ \n44.6 \n49.5 \n52.4 \n49.8 \n-\n54.7 \n58.7 \n60.0 \n62.9 \nHotpotQA \n45.6 \n53.5 \n59.3 \n63.8 \n59.4 \n57.9 \n56.3 \n62.2 \n63.3 \nFiQA \n29.5 \n34.9 \n31.7 \n32.9 \n38.4 \n42.4 \n34.8 \n36.4 \n38.6 \nArguAna \n41.5 \n51.1 \n23.3 \n44.6 \n47.0 \n52.5 \n46.7 \n51.4 \n49.4 \nTouche-2020 \n24.0 \n20.5 \n20.2 \n23.0 \n28.5 \n21.9 \n26.8 \n28.3 \n27.2 \nCQADupStack \n29.6 \n35.7 \n35.0 \n34.5 \n-\n38.4 \n36.1 \n38.9 \n39.4 \nQuora \n85.2 \n88.1 \n85.4 \n86.5 \n70.6 \n89.0 \n87.7 \n87.9 \n88.2 \nDBPedia \n28.1 \n34.7 \n39.2 \n41.3 \n36.2 \n39.1 \n38.6 \n41.0 \n42.4 \nScidocs \n12.2 \n14.9 \n14.5 \n16.5 \n-\n15.8 \n16.4 \n19.0 \n20.1 \nFever \n66.9 \n66.0 \n77.1 \n75.8 \n72.1 \n71.2 \n53.5 \n58.2 \n65.0 \nClimate-Fever \n19.8 \n24.1 \n18.4 \n23.7 \n18.5 \n26.2 \n15.8 \n15.4 \n22.4 \nScifact \n50.7 \n60.0 \n67.1 \n67.7 \n67.2 \n63.9 \n65.6 \n73.1 \n72.6 \nAverage \n40.5 \n44.0 \n44.4 \n46.6 \n-\n47.0 \n46.0 \n48.7 \n50.0 \nBest on \n0 \n0 \n1 \n1 \n1 \n4 \n0 \n3 \n5 \n\n\n\nTable 3 :\n3Results on the MTEB benchmark[40] (56 datasets in English subset). Here we only report averaged numbers on each task category for space reasons, please check out Appendix B for a detailed version. BERT-FT base uses the same fine-tuning data as E5 but initializes from BERT base .# of datasets \u2192 \nClass. Clust. PairClass. Rerank Retr. STS Summ. Avg \n12 \n11 \n3 \n4 \n15 \n10 \n1 \n56 \nUnsupervised models \nGlove \n57.3 \n27.7 \n70.9 \n43.3 \n21.6 61.9 \n28.9 \n42.0 \nBERT \n61.7 \n30.1 \n56.3 \n43.4 \n10.6 54.4 \n29.8 \n38.3 \nSimCSE-BERT-unsup \n62.5 \n29.0 \n70.3 \n46.5 \n20.3 74.3 \n31.2 \n45.5 \nE5-PT small \n67.0 \n41.7 \n78.2 \n53.1 \n40.8 68.8 \n25.2 \n54.2 \nE5-PT base \n67.9 \n43.4 \n79.2 \n53.5 \n42.9 69.5 \n24.3 \n55.5 \nE5-PT large \n69.0 \n44.3 \n80.3 \n54.4 \n44.2 69.9 \n24.8 \n56.4 \nSupervised models \nSimCSE-BERT-sup \n67.3 \n33.4 \n73.7 \n47.5 \n21.8 79.1 \n23.3 \n48.7 \nBERT-FT base \n68.7 \n33.9 \n82.6 \n50.5 \n41.5 79.2 \n29.0 \n55.2 \nContriever \n66.7 \n41.1 \n82.5 \n53.1 \n41.9 76.5 \n30.4 \n56.0 \nGTR large \n67.1 \n41.6 \n85.3 \n55.4 \n47.4 78.2 \n29.5 \n58.3 \nSentence-T5 large \n72.3 \n41.7 \n85.0 \n54.0 \n36.7 81.8 \n29.6 \n57.1 \nE5 small \n71.7 \n39.5 \n85.1 \n54.5 \n46.0 80.9 \n24.9 \n58.8 \nE5 base \n72.6 \n42.1 \n85.1 \n55.7 \n48.7 81.0 \n26.0 \n60.3 \nE5 large \n73.1 \n43.3 \n85.9 \n56.5 \n50.0 82.1 \n25.2 \n61.3 \nLarger models \nGTR xxl \n67.4 \n42.4 \n86.1 \n56.7 \n48.5 78.4 \n30.6 \n59.0 \nSentence-T5 xxl \n73.4 \n43.7 \n85.1 \n56.4 \n42.2 82.6 \n30.1 \n59.5 \n\n\n\nTable 4 :\n4Zero-shot text classification results. \"Majority\" always predicts the majority class label. Zero-shot BERT base uses the average pooling of the last layer as text embeddings.Zero-shot \nFull Fine-tune \nMajority BERT base E5 small E5 base E5 large \nBERT base BERT large \nSST-2 [52] \n50.9 \n58.9 \n79.7 \n81.3 \n85.3 \n93.5 \n94.9 \n\n\n\nTable 5 :\n5Impacts of different batch sizes for contrastive pre-training.batch size NFCorpus NQ FiQA Quora DBPedia Scifact Avg \n32k \n35.8 \n39.0 40.0 \n85.7 \n35.4 \n73.7 \n51.6 \n8k \n33.3 \n38.5 37.6 \n85.7 \n34.0 \n71.8 \n50.2 \n1k \n28.2 \n33.1 30.4 \n84.0 \n30.1 \n69.1 \n45.8 \n\n\n\nTable 6 :\n6Fine-tuning with different combinations of labeled data.Fine-tuned on \nRetrieval STS Classification Summ. MTEB Avg \nNo fine-tuning \n42.9 \n69.5 \n67.9 \n24.3 \n55.5 \nMS-MARCO + NQ \n50.3 \n78.3 \n68.3 \n25.8 \n58.9 \nNLI \n38.3 \n81.1 \n72.6 \n26.5 \n57.2 \nAll above \n48.7 \n81.0 \n73.1 \n26.0 \n60.3 \n\n\n\nTable 7 :\n7Data filtering. For the top 2 rows, we train with 1M random text pairs.# of pairs \nNFCorpus NQ FiQA Quora DBPedia Scifact Avg \n\n1M \nw/o filter \n23.0 \n15.1 18.5 \n83.1 \n18.2 \n51.4 \n34.9 \nw/ filter \n26.8 \n22.7 24.5 \n85.0 \n27.5 \n57.5 \n40.7 \n\nAll \nw/o filter \n34.5 \n35.4 39.1 \n85.7 \n32.9 \n72.5 \n50.0 \nw/ filter \n35.8 \n39.0 40.0 \n85.7 \n35.4 \n73.7 \n51.6 \n\n\n\nTable 8 :\n8Comparison of different negative sampling strategies.# negatives NFCorpus NQ FiQA Quora DBPedia Scifact Avg \nIn batch \n32k \n35.8 \n39.0 40.0 \n85.7 \n35.4 \n73.7 \n51.6 \n+ pre-batch \n64k \n29.4 \n27.2 29.4 \n84.6 \n25.0 \n64.3 \n43.3 \nMoCo \n130k \n29.7 \n36.1 32.0 \n81.6 \n29.9 \n63.6 \n45.5 \n\n\n\n\nConstructive Dual DP for Reservoir Optimization p: Dynamic programming (DP) is a well established technique for optimization of reservoir manage. . . LG Display reports Q1 operating loss as. . . p: April 25 (Reuters) -South Korea's LG Display Co Ltd reported its first quarterly operating loss. . .data source \ntype of text pairs \nrandom example \n# of pairs \n\nWikipedia \n(entity+section title, passage) \n\nq: Lexden History \np: The site on which Lexden now stands was crossed \nby the fortifications of iron age Colchester. . . \n\n24M \n\nReddit \n(post, upvoted comment) \n\nq: What makes a client good quality to you? \nI'm putting together my ideal client . . . \np: Respectful of schedules. And pays on time.. . . \n\n60M \n\nCommon Crawl (title, passage) \n\nq: Central Intake Unit | Broome County \np: Caseworkers from Central Intake assess the \nhousehold and risk of placement. If eligible. . . \n\n69M \n\nStackexchange \n(title, answer) \n(title+description, answer) \n\nq: Will killing Python made problems for Apache \np: Python and Apache aren't related, unless your \napp is making use of Python. . . . \n\n19M \n\nS2ORC \n\n(title, abstract) \n(title, citation title) \n(abstract, citation abstract) \n\nq: 90M \n\nNews \n(title, passage) \n(highlight, passage) \n\nq: 3M \n\nOthers \nmisc. \nmisc. \n6M \nAll above \n-\n-\n\u223c 270M \n\n\n\nTable 10 :\n10Model configurations.# layers hidden size # params \nE5 small \n12 \n384 \n33M \nE5 base \n12 \n768 \n110M \nE5 large \n24 \n1024 \n330M \n\n\n\nTable 11 :\n11Hyperparameters for contrastive pre-training and fine-tuning.pre-training \nfine-tuning \nE5-PT small E5-PT base E5-PT large \nE5 small \nE5 base \nE5 large \nlearning rate \n3 \u00d7 10 \u22124 2 \u00d7 10 \u22124 \n10 \u22124 \n3 \u00d7 10 \u22125 2 \u00d7 10 \u22125 10 \u22125 \nGPUs \n16 \n32 \n64 \n8 \n8 \n8 \nwarmup steps \n1000 \n1000 \n1000 \n400 \n400 \n400 \nbatch size \n32K \n32K \n32K \n256 \n256 \n256 \nmax steps \n20K \n20K \n20K \nn.a. \nn.a. \nn.a. \nmax length \n128 \n128 \n128 \n192 \n192 \n192 \nepochs \nn.a. \nn.a. \nn.a. \n3 \n3 \n3 \n\u03c4 \n0.01 \n0.01 \n0.01 \n0.01 \n0.01 \n0.01 \n\u03b1 \nn.a. \nn.a. \nn.a. \n0.2 \n0.2 \n0.2 \nweight decay \n0.01 \n0.01 \n0.01 \n0.01 \n0.01 \n0.01 \nhard negatives \n0 \n0 \n0 \n7 \n7 \n7 \n\n\n\nTable 12 .\n12These results can help illustrate the benefits brought by contrastive pre-training when abundant in-domain labeled data are available. For MS-MARCO passage ranking, MRR@10 and Recall@1k are reported. For the NQ dataset, Recall@20 and Recall@100 are the main metrics.\n\nTable 12 :\n12In-domain results. \"target pre-train\" refers to intermediate pre-training on the target corpus before supervised fine-tuning. For NQ, we use the passage retrieval setting from DPR[30].target pre-train? \nMS-MARCO \nNQ \nMRR@10 R@1k \nR@20 R@100 \nANCE [61] \n\n33.0 \n95.9 \n81.9 \n87.5 \nRocketQAv2 [50] \n\n38.8 \n98.1 \n83.7 \n89.0 \nSimLM [58] \n\n41.1 \n98.7 \n85.2 \n89.7 \nE5 small \n\n37.5 \n98.1 \n84.6 \n89.8 \nE5 base \n\n38.5 \n98.5 \n86.1 \n90.7 \nE5 large \n\n39.4 \n98.7 \n86.4 \n90.5 \n\n\n\nTable 13 :\n13Results for each dataset in the MTEB benchmark[40]. The numbers for the Retrieval category are not included here since the datasets are the same as the BEIR benchmark.unsupervised \nsupervised \nE5-PT small E5-PT base E5-PT large \nE5 small E5 base E5 large \nAmazonCounterfactualClassification \n71.7 \n73.6 \n70.4 \n76.2 \n79.7 \n77.7 \nAmazonPolarityClassification \n76.1 \n77.0 \n83.2 \n87.5 \n88.0 \n90.1 \nAmazonReviewsClassification \n35.0 \n35.8 \n37.4 \n42.6 \n42.7 \n43.0 \nBanking77Classification \n82.1 \n82.9 \n83.5 \n81.9 \n83.3 \n84.1 \nEmotionClassification \n42.2 \n44.2 \n43.5 \n46.9 \n49.4 \n48.1 \nImdbClassification \n67.9 \n67.3 \n77.7 \n75.6 \n76.0 \n82.1 \nMassiveIntentClassification \n70.2 \n71.1 \n70.8 \n72.2 \n72.3 \n73.2 \nMassiveScenarioClassification \n74.6 \n75.4 \n75.9 \n75.8 \n76.8 \n77.4 \nMTOPDomainClassification \n91.3 \n92.3 \n93.2 \n92.1 \n93.2 \n93.9 \nMTOPIntentClassification \n71.9 \n74.0 \n74.2 \n73.2 \n74.8 \n76.4 \nToxicConversationsClassification \n67.0 \n67.4 \n66.1 \n72.8 \n74.1 \n70.6 \nTweetSentimentExtractionClass. \n54.4 \n53.3 \n52.5 \n63.3 \n61.4 \n61.2 \nArxivClusteringP2P \n47.9 \n49.3 \n49.4 \n44.1 \n44.6 \n46.2 \nArxivClusteringS2S \n39.9 \n42.8 \n43.6 \n37.1 \n40.5 \n41.4 \nBiorxivClusteringP2P \n38.5 \n38.8 \n39.2 \n35.8 \n36.2 \n37.6 \nBiorxivClusteringS2S \n35.4 \n36.5 \n36.7 \n31.9 \n32.7 \n35.1 \nMedrxivClusteringP2P \n34.4 \n33.7 \n33.3 \n31.3 \n31.5 \n32.3 \nMedrxivClusteringS2S \n32.0 \n32.1 \n32.2 \n28.2 \n28.3 \n29.7 \nRedditClustering \n46.9 \n49.3 \n52.4 \n42.9 \n48.2 \n50.7 \nRedditClusteringP2P \n60.2 \n64.4 \n64.6 \n56.4 \n62.2 \n61.4 \nStackExchangeClustering \n57.7 \n60.2 \n63.3 \n59.1 \n63.9 \n65.0 \nStackExchangeClusteringP2P \n32.0 \n34.0 \n34.7 \n30.3 \n32.6 \n33.6 \nTwentyNewsgroupsClustering \n34.4 \n36.2 \n37.9 \n37.5 \n42.6 \n43.8 \nSprintDuplicateQuestions \n91.6 \n90.8 \n92.0 \n95.3 \n94.9 \n95.4 \nTwitterSemEval2015 \n60.0 \n62.8 \n64.7 \n74.2 \n74.4 \n76.1 \nTwitterURLCorpus \n83.2 \n84.0 \n84.1 \n85.8 \n86.0 \n86.3 \nAskUbuntuDupQuestions \n57.8 \n57.6 \n58.3 \n59.4 \n59.7 \n60.1 \nMindSmallReranking \n29.0 \n29.6 \n29.2 \n29.6 \n30.1 \n30.8 \nSciDocsRR \n81.1 \n82.6 \n84.3 \n79.8 \n82.9 \n83.9 \nStackOverflowDupQuestions \n44.4 \n44.2 \n45.8 \n49.1 \n50.1 \n51.3 \nBIOSSES \n69.2 \n71.9 \n69.7 \n84.2 \n85.1 \n84.7 \nSICK-R \n66.6 \n68.7 \n69.7 \n78.9 \n79.7 \n80.5 \nSTS12 \n60.7 \n57.9 \n54.7 \n75.2 \n74.2 \n75.9 \nSTS13 \n71.1 \n73.5 \n74.0 \n81.8 \n83.3 \n85.2 \nSTS14 \n64.2 \n64.0 \n65.3 \n78.5 \n78.5 \n80.5 \nSTS15 \n74.3 \n75.4 \n75.8 \n87.5 \n88.4 \n88.8 \nSTS16 \n76.6 \n79.8 \n80.1 \n84.6 \n84.2 \n85.3 \nSTS17 \n78.3 \n77.2 \n76.0 \n87.9 \n87.2 \n89.4 \nSTS22 \n59.2 \n56.2 \n62.8 \n63.8 \n62.9 \n63.0 \nSTSBenchmark \n67.7 \n70.5 \n70.9 \n86.4 \n86.2 \n87.2 \nSummEval \n25.2 \n24.3 \n24.8 \n24.9 \n26.0 \n25.2 \n\nE5: EmbEddings from bidirEctional Encoder rEpresentations Work in progress.\nhttps://github.com/UKPLab/sentence-transformers\nhttps://files.pushshift.io/reddit/ 4 https://archive.org/details/stackexchange 5 https://commoncrawl.org/\nThe version released by SimCSE.\nhttps://huggingface.co/spaces/mteb/leaderboard, as ofNovember 22, 2022   \nhttps://github.com/facebookresearch/cc_net\n\nA simple but tough-to-beat baseline for sentence embeddings. Sanjeev Arora, Yingyu Liang, Tengyu Ma, 5th International Conference on Learning Representations. Toulon, FranceConference Track Proceedings. OpenReview.netSanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence embeddings. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=SyK00v5xx.\n\nMassively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond. Mikel Artetxe, Holger Schwenk, 10.1162/tacl_a_00288Transactions of the Association for Computational Linguistics. 7Mikel Artetxe and Holger Schwenk. Massively multilingual sentence embeddings for zero- shot cross-lingual transfer and beyond. Transactions of the Association for Computational Linguistics, 7:597-610, 2019. doi: 10.1162/tacl_a_00288. URL https://aclanthology. org/Q19-1038.\n\nLatent dirichlet allocation. David M Blei, Andrew Y Ng, Michael I Jordan, Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic. Thomas G. Dietterich, Suzanna Becker, and Zoubin GhahramaniVancouver, British Columbia, CanadaMIT PressDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. In Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani, editors, Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada], pages 601-608. MIT Press, 2001. URL https://proceedings.neurips.cc/paper/2001/hash/ 296472c9542ad4d4788d543508116cbc-Abstract.html.\n\nOverview of touch\u00e9 2022: argument retrieval. Alexander Bondarenko, Maik Fr\u00f6be, Johannes Kiesel, Shahbaz Syed, Timon Gurcke, Meriem Beloucif, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, International Conference of the Cross-Language Evaluation Forum for European Languages. SpringerAlexander Bondarenko, Maik Fr\u00f6be, Johannes Kiesel, Shahbaz Syed, Timon Gurcke, Meriem Beloucif, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, et al. Overview of touch\u00e9 2022: argument retrieval. In International Conference of the Cross- Language Evaluation Forum for European Languages, pages 311-336. Springer, 2022.\n\nA full-text learning to rank dataset for medical information retrieval. Vera Boteva, Demian Gholipour, Artem Sokolov, Stefan Riezler, European Conference on Information Retrieval. SpringerVera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. A full-text learning to rank dataset for medical information retrieval. In European Conference on Information Retrieval, pages 716-722. Springer, 2016.\n\nA large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, 10.18653/v1/D15-1075Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal, 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https: //aclanthology.org/D15-1075.\n\n. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel MTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\n\nLanguage models are few-shot learners. Jeffrey Ziegler, Clemens Wu, Christopher Winter, Mark Hesse, Eric Chen, Mateusz Sigler, Scott Litwin, Benjamin Gray, Jack Chess, Christopher Clark, Sam Berner, Alec Mccandlish, Ilya Radford, Dario Sutskever, Amodei, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin2020Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learn- ers. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6- 12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\nMs marco: A human generated machine reading comprehension dataset. Tri Daniel Fernando Campos, Mir Nguyen, Xia Rosenberg, Jianfeng Song, Saurabh Gao, Rangan Tiwary, Li Majumder, Bhaskar Deng, Mitra, abs/1611.09268ArXiv. Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, and Bhaskar Mitra. Ms marco: A human generated machine reading comprehension dataset. ArXiv, abs/1611.09268, 2016.\n\nPre-training tasks for embedding-based large-scale retrieval. Wei-Cheng Chang, Felix X Yu, Yin-Wen Chang, Yiming Yang, Sanjiv Kumar, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. Pre-training tasks for embedding-based large-scale retrieval. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=rkg-mA4FDr.\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey E Hinton, PMLRProceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine Learning2020Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 1597-1607. PMLR, 2020. URL http: //proceedings.mlr.press/v119/chen20j.html.\n\nSalient phrase aware dense retrieval: Can a dense retriever imitate a sparse one. Xilun Chen, Kushal Lakhotia, Barlas Oguz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, Wen-Tau Yih, arXiv:2110.06918arXiv preprintXilun Chen, Kushal Lakhotia, Barlas Oguz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, and Wen-tau Yih. Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one? arXiv preprint arXiv:2110.06918, 2021.\n\nSpecter: Document-level representation learning using citation-informed transformers. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel S Weld, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsArman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld. Specter: Document-level representation learning using citation-informed transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2270-2282, 2020.\n\nSentEval: An evaluation toolkit for universal sentence representations. Alexis Conneau, Douwe Kiela, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)Miyazaki, JapanAlexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence representations. In Proceedings of the Eleventh International Conference on Language Re- sources and Evaluation (LREC 2018), Miyazaki, Japan, 2018. European Language Resources Association (ELRA). URL https://aclanthology.org/L18-1269.\n\nSupervised learning of universal sentence representations from natural language inference data. Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault, Antoine Bordes, 10.18653/v1/D17-1070Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault, and Antoine Bordes. Super- vised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670-680, Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1070. URL https://aclanthology.org/D17-1070.\n\nZhuyun Dai, Vincent Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, Ming-Wei Chang, abs/2209.11755Promptagator: Few-shot dense retrieval from 8 examples. Zhuyun Dai, Vincent Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. Promptagator: Few-shot dense retrieval from 8 examples. ArXiv, abs/2209.11755, 2022.\n\nIndexing by latent semantic analysis. Scott Deerwester, T Susan, George W Dumais, Furnas, K Thomas, Richard Landauer, Harshman, Journal of the American society for information science. 416Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman. Indexing by latent semantic analysis. Journal of the American society for information science, 41(6):391-407, 1990.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer- ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.\n\nClimate-fever: A dataset for verification of real-world climate claims. Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, Markus Leippold, arXiv:2012.00614arXiv preprintThomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, and Markus Leippold. Climate-fever: A dataset for verification of real-world climate claims. arXiv preprint arXiv:2012.00614, 2020.\n\nWhat neural networks memorize and why: Discovering the long tail via influence estimation. Vitaly Feldman, Chiyuan Zhang, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin2020Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/ paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html.\n\nLanguageagnostic bert sentence embedding. Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, Wei Wang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsLong Papers1Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Language- agnostic bert sentence embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 878-891, 2022.\n\nThe pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy, abs/2101.00027ArXiv. Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. ArXiv, abs/2101.00027, 2021.\n\nSimCSE: Simple contrastive learning of sentence embeddings. Tianyu Gao, Xingcheng Yao, Danqi Chen, 10.18653/v1/2021.emnlp-main.552Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingOnline and Punta Cana, Dominican RepublicAssociation for Computational LinguisticsTianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894-6910, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552. URL https://aclanthology.org/2021.emnlp-main.552.\n\nCo-teaching: Robust training of deep neural networks with extremely noisy labels. Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W Tsang, Masashi Sugiyama, ; Hanna, M Wallach, Hugo Larochelle, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman GarnettNeurIPS; Montr\u00e9al, CanadaSamy Bengio,Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W. Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kris- ten Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett, editors, Advances in Neu- ral Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 8536-8546, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ a19744e268754fb0148b017647355b7b-Abstract.html.\n\nDbpedia-entity v2: a test collection for entity search. Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisztian Balog, Svein Erik Bratsberg, Alexander Kotov, Jamie Callan, Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 40th International ACM SIGIR Conference on Research and Development in Information RetrievalFaegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisztian Balog, Svein Erik Bratsberg, Alexander Kotov, and Jamie Callan. Dbpedia-entity v2: a test collection for entity search. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1265-1268, 2017.\n\nMomentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross B Girshick, 10.1109/CVPR42600.2020.009752020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USAIEEE20202020Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 9726-9735. IEEE, 2020. doi: 10.1109/CVPR42600.2020.00975. URL https://doi.org/10.1109/ CVPR42600.2020.00975.\n\nCqadupstack: A benchmark data set for community question-answering research. Doris Hoogeveen, M Karin, Timothy Verspoor, Baldwin, Proceedings of the 20th Australasian document computing symposium. the 20th Australasian document computing symposiumDoris Hoogeveen, Karin M Verspoor, and Timothy Baldwin. Cqadupstack: A benchmark data set for community question-answering research. In Proceedings of the 20th Australasian document computing symposium, pages 1-8, 2015.\n\nParameter-efficient transfer learning for NLP. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, PMLRProceedings of the 36th International Conference on Machine Learning, ICML 2019. Kamalika Chaudhuri and Ruslan Salakhutdinovthe 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USA97Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2790-2799. PMLR, 2019. URL http://proceedings.mlr.press/v97/houlsby19a.html.\n\nTowards unsupervised dense information retrieval with contrastive learning. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave, abs/2112.09118ArXiv. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Towards unsupervised dense information retrieval with contrastive learning. ArXiv, abs/2112.09118, 2021.\n\nScaling up visual and vision-language representation learning with noisy text supervision. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yun-Hsuan Sung, Zhen Li, Tom Duerig, PMLRProceedings of the 38th International Conference on Machine Learning, ICML 2021. Marina Meila and Tong Zhangthe 38th International Conference on Machine Learning, ICML 2021139Virtual EventChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun- Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 4904-4916. PMLR, 2021. URL http://proceedings.mlr.press/v139/jia21b.html.\n\nDense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, doi: 10. 18653/v1/2020.emnlp-main.550Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsOnline, 2020Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online, 2020. Association for Computational Linguistics. doi: 10. 18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main. 550.\n\nEfficient and effective passage search via contextualized late interaction over BERT. Omar Khattab, Matei Zaharia, Colbert, 10.1145/3397271.3401075Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event. Jimmy Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liuthe 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual EventChinaACM2020Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contex- tualized late interaction over BERT. In Jimmy Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu, editors, Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Vir- tual Event, China, July 25-30, 2020, pages 39-48. ACM, 2020. doi: 10.1145/3397271.3401075. URL https://doi.org/10.1145/3397271.3401075.\n\nNatural questions: A benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, 10.1162/tacl_a_00276Transactions of the Association for Computational Linguistics. 7Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026.\n\nLearning dense representations of phrases at scale. Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, Danqi Chen, 10.18653/v1/2021.acl-long.518Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics1Online, 2021Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. Learning dense representations of phrases at scale. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6634-6647, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.518. URL https://aclanthology.org/2021. acl-long.518.\n\nDeduplicating training data makes language models better. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini, ACL. 2022Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In ACL, 2022.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta, abs/1907.11692A robustly optimized bert pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019.\n\nS2ORC: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel Weld, 10.18653/v1/2020.acl-main.447Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline, 2020. Association for Computational LinguisticsKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969-4983, Online, 2020. Associ- ation for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.447. URL https: //aclanthology.org/2020.acl-main.447.\n\nWww'18 open challenge: financial opinion mining and question answering. Macedo Maia, Siegfried Handschuh, Andr\u00e9 Freitas, Brian Davis, Ross Mcdermott, Manel Zarrouk, Alexandra Balahur, Companion proceedings of the the web. Macedo Maia, Siegfried Handschuh, Andr\u00e9 Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. Www'18 open challenge: financial opinion mining and question answering. In Companion proceedings of the the web conference 2018, pages 1941- 1942, 2018.\n\nEfficient estimation of word representations in vector space. Tomas Mikolov, Kai Chen, Gregory S Corrado, Jeffrey Dean, ICLR. Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In ICLR, 2013.\n\nSgpt: Gpt sentence embeddings for semantic search. Niklas Muennighoff, abs/2202.08904ArXiv. Niklas Muennighoff. Sgpt: Gpt sentence embeddings for semantic search. ArXiv, abs/2202.08904, 2022.\n\nNiklas Muennighoff, Nouamane Tazi, abs/2210.07316Loic Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. ArXiv. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. ArXiv, abs/2210.07316, 2022.\n\n. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas A Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David P. Schnurr, Felipe Petroski Such, Kenny Sai-Kin Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and Lilian WengText and code embeddings by contrastive pretraining. ArXiv, abs/2201.10005, 2022Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas A. Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David P. Schnurr, Felipe Petroski Such, Kenny Sai-Kin Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and Lilian Weng. Text and code embeddings by contrastive pre- training. ArXiv, abs/2201.10005, 2022.\n\nSELF: learning to filter noisy labels with selfensembling. 8th International Conference on Learning Representations. Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi-Phuong-Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas BroxAddis Ababa, Ethiopia2020Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi-Phuong-Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox. SELF: learning to filter noisy labels with self- ensembling. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview. net/forum?id=HkgsPhNYPS.\n\nLarge dual encoders are generalizable retrievers. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern&apos;andez &apos;abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, Yinfei Yang, abs/2112.07899ArXiv. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern'andez 'Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable retrievers. ArXiv, abs/2112.07899, 2021.\n\nSentence-t5: Scalable sentence encoders from pre-trained text-to-text models. Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, Yinfei Yang, Findings of the Association for Computational Linguistics: ACL 2022. Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1864-1874, 2022.\n\nDomainmatched pre-training tasks for dense retrieval. Barlas Oguz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis, Vladimir Karpukhin, Aleksandra Piktus, Xilun Chen, Sebastian Riedel, Scott Yih, Sonal Gupta, Yashar Mehdad, 10.18653/v1/2022.findings-naacl.114Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, WA, United StatesAssociation for Computational Linguistics2022Barlas Oguz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis, Vladimir Karpukhin, Aleksandra Piktus, Xilun Chen, Sebastian Riedel, Scott Yih, Sonal Gupta, and Yashar Mehdad. Domain- matched pre-training tasks for dense retrieval. In Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 1524-1534. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-naacl.114. URL https://doi.org/10.18653/v1/2022.findings-naacl.114.\n\nKilt: a benchmark for knowledge intensive language tasks. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vassilis Plachouras, Tim Rocktaschel, Sebastian Riedel, North American Chapter of the Association for Computational Linguistics. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vassilis Plachouras, Tim Rocktaschel, and Sebastian Riedel. Kilt: a benchmark for knowledge intensive language tasks. In North American Chapter of the Association for Computational Linguistics, 2020.\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, PMLRProceedings of the 38th International Conference on Machine Learning, ICML 2021. Marina Meila and Tong Zhangthe 38th International Conference on Machine Learning, ICML 2021139Virtual EventAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervi- sion. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 8748-8763. PMLR, 2021. URL http://proceedings.mlr.press/v139/radford21a.html.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1-67, 2020.\n\nSentence-BERT: Sentence embeddings using Siamese BERT-networks. Nils Reimers, Iryna Gurevych, 10.18653/v1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan- guage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/D19-1410.\n\nRocketQAv2: A joint training method for dense passage retrieval and passage re-ranking. Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, Ji-Rong Wen, 10.18653/v1/2021.emnlp-main.224Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsOnline and Punta CanaRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2825-2835, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.224. URL https://aclanthology.org/2021.emnlp-main.224.\n\nCCMatrix: Mining billions of high-quality parallel sentences on the web. Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, Angela Fan, 10.18653/v1/2021.acl-long.507Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics1Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. CCMatrix: Mining billions of high-quality parallel sentences on the web. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6490-6500, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.507. URL https://aclanthology.org/2021.acl-long.507.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, A Ng, Christopher Potts, Conference on Empirical Methods in Natural Language Processing. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Conference on Empirical Methods in Natural Language Processing, 2013.\n\nBeir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, Iryna Gurevych, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021Round 2Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\n\nFEVER: a large-scale dataset for fact extraction and VERification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, 10.18653/v1/N18-1074Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics1Long PapersJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809-819, New Orleans, Louisiana, 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https: //aclanthology.org/N18-1074.\n\nTrec-covid: constructing a pandemic information retrieval test collection. Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, R William, Kyle Hersh, Kirk Lo, Ian Roberts, Lucy Lu Soboroff, Wang, ACM SIGIR Forum. New York, NY, USAACM542021Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. Trec-covid: constructing a pandemic information retrieval test collection. In ACM SIGIR Forum, volume 54, pages 1-12. ACM New York, NY, USA, 2021.\n\nRetrieval of the best counterargument without prior topic knowledge. Henning Wachsmuth, Shahbaz Syed, Benno Stein, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers1Henning Wachsmuth, Shahbaz Syed, and Benno Stein. Retrieval of the best counterargument without prior topic knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 241-251, 2018.\n\nFact or fiction: Verifying scientific claims. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine Van Zuylen, Arman Cohan, Hannaneh Hajishirzi, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. Fact or fiction: Verifying scientific claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7534-7550, 2020.\n\nSimlm: Pre-training with representation bottleneck for dense passage retrieval. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, abs/2207.02578ArXiv. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Simlm: Pre-training with representation bottleneck for dense passage retrieval. ArXiv, abs/2207.02578, 2022.\n\nMinilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers. Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, Furu Wei, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2140-2151, 2021.\n\nCCNet: Extracting high quality monolingual datasets from web crawl data. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Armand Joulin, Edouard Grave, 979-10-95546-34-4Proceedings of the 12th Language Resources and Evaluation Conference. the 12th Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources AssociationGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 4003-4012, Marseille, France, 2020. European Language Resources Associ- ation. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020.lrec-1.494.\n\nApproximate nearest neighbor negative contrastive learning for dense text retrieval. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N Bennett, Junaid Ahmed, Arnold Overwijk, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview. net/forum?id=zeFrfgyZln.\n\nLaprador: Unsupervised pretrained dense retriever for zero-shot text retrieval. Canwen Xu, Daya Guo, Nan Duan, Julian Mcauley, Findings of the Association for Computational Linguistics: ACL 2022. Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Laprador: Unsupervised pretrained dense retriever for zero-shot text retrieval. In Findings of the Association for Computational Linguistics: ACL 2022, pages 3557-3569, 2022.\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380, 2018.\n\nA Dataset Details For Common Crawl, we download the 2022-33 snapshot and cc_net 8 is used for preprocessing including language identification, de-duplication, language model filtering, etc. Web pages from the MS-MARCO document ranking corpus are also included. For the data filtering step, we examine each pair of passages within a web page instead of just using the title as a query. For Wikipedia, we use the version released by. Petroni et al.46To avoid possible data contamination, we remove text pairs that occur in the evaluation datasets based on exact string matchA Dataset Details For Common Crawl, we download the 2022-33 snapshot and cc_net 8 is used for preprocessing including language identification, de-duplication, language model filtering, etc. Web pages from the MS-MARCO document ranking corpus are also included. For the data filtering step, we examine each pair of passages within a web page instead of just using the title as a query. For Wikipedia, we use the version released by Petroni et al. [46]. To avoid possible data contamination, we remove text pairs that occur in the evaluation datasets based on exact string match.\n\nFor the S2ORC data, we use a sample weight of 0.3 during training to avoid over-fitting the scientific domains. Reddit data is collected from theReddit data is collected from the year 2018 to August 2022. For the S2ORC data, we use a sample weight of 0.3 during training to avoid over-fitting the scientific domains.\n", "annotations": {"author": "[{\"end\":100,\"start\":65},{\"end\":134,\"start\":101},{\"end\":174,\"start\":135},{\"end\":212,\"start\":175},{\"end\":249,\"start\":213},{\"end\":286,\"start\":250},{\"end\":327,\"start\":287},{\"end\":361,\"start\":328}]", "publisher": null, "author_last_name": "[{\"end\":75,\"start\":71},{\"end\":109,\"start\":105},{\"end\":149,\"start\":144},{\"end\":187,\"start\":183},{\"end\":224,\"start\":220},{\"end\":261,\"start\":256},{\"end\":302,\"start\":294},{\"end\":336,\"start\":333}]", "author_first_name": "[{\"end\":70,\"start\":65},{\"end\":104,\"start\":101},{\"end\":143,\"start\":135},{\"end\":182,\"start\":175},{\"end\":219,\"start\":213},{\"end\":255,\"start\":250},{\"end\":293,\"start\":287},{\"end\":332,\"start\":328}]", "author_affiliation": "[{\"end\":99,\"start\":77},{\"end\":133,\"start\":111},{\"end\":173,\"start\":151},{\"end\":211,\"start\":189},{\"end\":248,\"start\":226},{\"end\":285,\"start\":263},{\"end\":326,\"start\":304},{\"end\":360,\"start\":338}]", "title": "[{\"end\":62,\"start\":1},{\"end\":423,\"start\":362}]", "venue": null, "abstract": "[{\"end\":1291,\"start\":425}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1806,\"start\":1802},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1818,\"start\":1815},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2289,\"start\":2285},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2310,\"start\":2306},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2626,\"start\":2623},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2648,\"start\":2644},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2680,\"start\":2676},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2890,\"start\":2886},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3631,\"start\":3627},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":3928,\"start\":3924},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4172,\"start\":4168},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4451,\"start\":4447},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4493,\"start\":4490},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4746,\"start\":4742},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4854,\"start\":4850},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4857,\"start\":4854},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4860,\"start\":4857},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4910,\"start\":4907},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4927,\"start\":4924},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4960,\"start\":4956},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4973,\"start\":4969},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4991,\"start\":4987},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5005,\"start\":5001},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5321,\"start\":5317},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5389,\"start\":5385},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5392,\"start\":5389},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5419,\"start\":5415},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5430,\"start\":5427},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":5444,\"start\":5440},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5651,\"start\":5648},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5841,\"start\":5837},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5999,\"start\":5995},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6088,\"start\":6084},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6165,\"start\":6161},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":6396,\"start\":6392},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6678,\"start\":6674},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":6792,\"start\":6788},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8073,\"start\":8069},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8091,\"start\":8087},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8283,\"start\":8279},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8300,\"start\":8297},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8349,\"start\":8345},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8994,\"start\":8990},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":9457,\"start\":9453},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10307,\"start\":10303},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10465,\"start\":10461},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10468,\"start\":10465},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10471,\"start\":10468},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11224,\"start\":11220},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12318,\"start\":12314},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12498,\"start\":12494},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12836,\"start\":12832},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12839,\"start\":12836},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13066,\"start\":13063},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13107,\"start\":13103},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13110,\"start\":13107},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":13354,\"start\":13350},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":13357,\"start\":13354},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14900,\"start\":14896},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":16113,\"start\":16109},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16817,\"start\":16814},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16826,\"start\":16822},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16829,\"start\":16826},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16843,\"start\":16839},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":16924,\"start\":16920},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":17441,\"start\":17437},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":17714,\"start\":17710},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19072,\"start\":19068},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":19075,\"start\":19072},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19078,\"start\":19075},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19607,\"start\":19603},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20560,\"start\":20556},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20563,\"start\":20560},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":20892,\"start\":20888},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":21791,\"start\":21787},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22231,\"start\":22227},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22742,\"start\":22738},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":22745,\"start\":22742},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23012,\"start\":23008},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23092,\"start\":23088},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":23874,\"start\":23870},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23940,\"start\":23937},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":23992,\"start\":23988},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":25490,\"start\":25486},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25878,\"start\":25874},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26724,\"start\":26720},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":26950,\"start\":26946},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":28135,\"start\":28131},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28148,\"start\":28144},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28192,\"start\":28188},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29414,\"start\":29410},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":34873,\"start\":34869},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":35217,\"start\":35213}]", "figure": "[{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":28048,\"start\":26599},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":29368,\"start\":28049},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30765,\"start\":29369},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31102,\"start\":30766},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":31369,\"start\":31103},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":31666,\"start\":31370},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":32028,\"start\":31667},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":32319,\"start\":32029},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":33617,\"start\":32320},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":33759,\"start\":33618},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":34394,\"start\":33760},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":34675,\"start\":34395},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":35152,\"start\":34676},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":37723,\"start\":35153}]", "paragraph": "[{\"end\":1753,\"start\":1307},{\"end\":2891,\"start\":1755},{\"end\":4279,\"start\":2893},{\"end\":4796,\"start\":4296},{\"end\":6495,\"start\":4798},{\"end\":7171,\"start\":6497},{\"end\":7581,\"start\":7173},{\"end\":7985,\"start\":7634},{\"end\":8407,\"start\":7987},{\"end\":8612,\"start\":8409},{\"end\":9078,\"start\":8614},{\"end\":9636,\"start\":9080},{\"end\":10211,\"start\":9638},{\"end\":10610,\"start\":10213},{\"end\":10895,\"start\":10621},{\"end\":11086,\"start\":10944},{\"end\":11239,\"start\":11164},{\"end\":11662,\"start\":11319},{\"end\":11902,\"start\":11700},{\"end\":12180,\"start\":11904},{\"end\":12541,\"start\":12182},{\"end\":13276,\"start\":12575},{\"end\":13966,\"start\":13278},{\"end\":14380,\"start\":14007},{\"end\":14617,\"start\":14382},{\"end\":14901,\"start\":14619},{\"end\":15355,\"start\":14903},{\"end\":15617,\"start\":15357},{\"end\":15801,\"start\":15619},{\"end\":15908,\"start\":15803},{\"end\":16726,\"start\":15970},{\"end\":17275,\"start\":16728},{\"end\":17398,\"start\":17277},{\"end\":17693,\"start\":17422},{\"end\":18315,\"start\":17695},{\"end\":19822,\"start\":18345},{\"end\":20217,\"start\":19852},{\"end\":21108,\"start\":20219},{\"end\":21409,\"start\":21121},{\"end\":22898,\"start\":21435},{\"end\":23560,\"start\":22900},{\"end\":24075,\"start\":23588},{\"end\":24968,\"start\":24090},{\"end\":25215,\"start\":24997},{\"end\":25410,\"start\":25217},{\"end\":25754,\"start\":25412},{\"end\":25830,\"start\":25777},{\"end\":26183,\"start\":25832},{\"end\":26403,\"start\":26185},{\"end\":26598,\"start\":26405}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11163,\"start\":11087},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11318,\"start\":11240},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11699,\"start\":11663}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18389,\"start\":18382},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19215,\"start\":19208},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19862,\"start\":19855},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":20803,\"start\":20796},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":21620,\"start\":21613},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":22016,\"start\":22009},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":22515,\"start\":22508},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":23372,\"start\":23365},{\"end\":24753,\"start\":24746},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25036,\"start\":25028},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25444,\"start\":25436}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1305,\"start\":1293},{\"attributes\":{\"n\":\"2\"},\"end\":4294,\"start\":4282},{\"attributes\":{\"n\":\"3\"},\"end\":7632,\"start\":7584},{\"attributes\":{\"n\":\"4\"},\"end\":10619,\"start\":10613},{\"attributes\":{\"n\":\"4.1\"},\"end\":10942,\"start\":10898},{\"attributes\":{\"n\":\"4.2\"},\"end\":12573,\"start\":12544},{\"attributes\":{\"n\":\"4.3\"},\"end\":14005,\"start\":13969},{\"attributes\":{\"n\":\"5\"},\"end\":15922,\"start\":15911},{\"attributes\":{\"n\":\"5.1\"},\"end\":15968,\"start\":15925},{\"attributes\":{\"n\":\"5.2\"},\"end\":17420,\"start\":17401},{\"attributes\":{\"n\":\"5.3\"},\"end\":18343,\"start\":18318},{\"attributes\":{\"n\":\"5.4\"},\"end\":19850,\"start\":19825},{\"attributes\":{\"n\":\"5.5\"},\"end\":21119,\"start\":21111},{\"end\":21433,\"start\":21412},{\"end\":23586,\"start\":23563},{\"attributes\":{\"n\":\"6\"},\"end\":24088,\"start\":24078},{\"end\":24995,\"start\":24971},{\"end\":25775,\"start\":25757},{\"end\":26609,\"start\":26600},{\"end\":28059,\"start\":28050},{\"end\":29379,\"start\":29370},{\"end\":30776,\"start\":30767},{\"end\":31113,\"start\":31104},{\"end\":31380,\"start\":31371},{\"end\":31677,\"start\":31668},{\"end\":32039,\"start\":32030},{\"end\":33629,\"start\":33619},{\"end\":33771,\"start\":33761},{\"end\":34406,\"start\":34396},{\"end\":34687,\"start\":34677},{\"end\":35164,\"start\":35154}]", "table": "[{\"end\":28048,\"start\":27112},{\"end\":29368,\"start\":28256},{\"end\":30765,\"start\":29660},{\"end\":31102,\"start\":30952},{\"end\":31369,\"start\":31177},{\"end\":31666,\"start\":31438},{\"end\":32028,\"start\":31750},{\"end\":32319,\"start\":32094},{\"end\":33617,\"start\":32620},{\"end\":33759,\"start\":33653},{\"end\":34394,\"start\":33835},{\"end\":35152,\"start\":34874},{\"end\":37723,\"start\":35334}]", "figure_caption": "[{\"end\":27112,\"start\":26611},{\"end\":28256,\"start\":28061},{\"end\":29660,\"start\":29381},{\"end\":30952,\"start\":30778},{\"end\":31177,\"start\":31115},{\"end\":31438,\"start\":31382},{\"end\":31750,\"start\":31679},{\"end\":32094,\"start\":32041},{\"end\":32620,\"start\":32322},{\"end\":33653,\"start\":33632},{\"end\":33835,\"start\":33774},{\"end\":34675,\"start\":34409},{\"end\":34874,\"start\":34690},{\"end\":35334,\"start\":35167}]", "figure_ref": "[{\"end\":7921,\"start\":7913},{\"end\":10878,\"start\":10870}]", "bib_author_first_name": "[{\"end\":38172,\"start\":38165},{\"end\":38186,\"start\":38180},{\"end\":38200,\"start\":38194},{\"end\":38729,\"start\":38724},{\"end\":38745,\"start\":38739},{\"end\":39148,\"start\":39143},{\"end\":39150,\"start\":39149},{\"end\":39163,\"start\":39157},{\"end\":39165,\"start\":39164},{\"end\":39177,\"start\":39170},{\"end\":39179,\"start\":39178},{\"end\":39933,\"start\":39924},{\"end\":39950,\"start\":39946},{\"end\":39966,\"start\":39958},{\"end\":39982,\"start\":39975},{\"end\":39994,\"start\":39989},{\"end\":40009,\"start\":40003},{\"end\":40029,\"start\":40020},{\"end\":40046,\"start\":40041},{\"end\":40061,\"start\":40056},{\"end\":40076,\"start\":40069},{\"end\":40600,\"start\":40596},{\"end\":40615,\"start\":40609},{\"end\":40632,\"start\":40627},{\"end\":40648,\"start\":40642},{\"end\":40998,\"start\":40997},{\"end\":41012,\"start\":41007},{\"end\":41032,\"start\":41021},{\"end\":41052,\"start\":41041},{\"end\":41054,\"start\":41053},{\"end\":41698,\"start\":41695},{\"end\":41700,\"start\":41699},{\"end\":41716,\"start\":41708},{\"end\":41727,\"start\":41723},{\"end\":41742,\"start\":41735},{\"end\":41757,\"start\":41752},{\"end\":41774,\"start\":41766},{\"end\":41791,\"start\":41785},{\"end\":42267,\"start\":42260},{\"end\":42284,\"start\":42277},{\"end\":42300,\"start\":42289},{\"end\":42313,\"start\":42309},{\"end\":42325,\"start\":42321},{\"end\":42339,\"start\":42332},{\"end\":42353,\"start\":42348},{\"end\":42370,\"start\":42362},{\"end\":42381,\"start\":42377},{\"end\":42400,\"start\":42389},{\"end\":42411,\"start\":42408},{\"end\":42424,\"start\":42420},{\"end\":42441,\"start\":42437},{\"end\":42456,\"start\":42451},{\"end\":43401,\"start\":43398},{\"end\":43429,\"start\":43426},{\"end\":43441,\"start\":43438},{\"end\":43461,\"start\":43453},{\"end\":43475,\"start\":43468},{\"end\":43487,\"start\":43481},{\"end\":43498,\"start\":43496},{\"end\":43516,\"start\":43509},{\"end\":43855,\"start\":43846},{\"end\":43868,\"start\":43863},{\"end\":43870,\"start\":43869},{\"end\":43882,\"start\":43875},{\"end\":43896,\"start\":43890},{\"end\":43909,\"start\":43903},{\"end\":44398,\"start\":44394},{\"end\":44410,\"start\":44405},{\"end\":44430,\"start\":44422},{\"end\":44448,\"start\":44440},{\"end\":44450,\"start\":44449},{\"end\":45073,\"start\":45068},{\"end\":45086,\"start\":45080},{\"end\":45103,\"start\":45097},{\"end\":45116,\"start\":45110},{\"end\":45131,\"start\":45124},{\"end\":45143,\"start\":45139},{\"end\":45163,\"start\":45157},{\"end\":45177,\"start\":45172},{\"end\":45192,\"start\":45185},{\"end\":45575,\"start\":45570},{\"end\":45589,\"start\":45583},{\"end\":45601,\"start\":45599},{\"end\":45615,\"start\":45611},{\"end\":45630,\"start\":45624},{\"end\":45632,\"start\":45631},{\"end\":46153,\"start\":46147},{\"end\":46168,\"start\":46163},{\"end\":46805,\"start\":46799},{\"end\":46820,\"start\":46815},{\"end\":46834,\"start\":46828},{\"end\":46848,\"start\":46844},{\"end\":46866,\"start\":46859},{\"end\":47541,\"start\":47535},{\"end\":47554,\"start\":47547},{\"end\":47563,\"start\":47561},{\"end\":47570,\"start\":47568},{\"end\":47583,\"start\":47577},{\"end\":47592,\"start\":47588},{\"end\":47602,\"start\":47597},{\"end\":47618,\"start\":47612},{\"end\":47629,\"start\":47624},{\"end\":47631,\"start\":47630},{\"end\":47646,\"start\":47638},{\"end\":47977,\"start\":47972},{\"end\":47991,\"start\":47990},{\"end\":48005,\"start\":47999},{\"end\":48007,\"start\":48006},{\"end\":48025,\"start\":48024},{\"end\":48041,\"start\":48034},{\"end\":48418,\"start\":48413},{\"end\":48435,\"start\":48427},{\"end\":48449,\"start\":48443},{\"end\":48463,\"start\":48455},{\"end\":49396,\"start\":49390},{\"end\":49415,\"start\":49409},{\"end\":49435,\"start\":49429},{\"end\":49456,\"start\":49444},{\"end\":49474,\"start\":49468},{\"end\":49823,\"start\":49817},{\"end\":49840,\"start\":49833},{\"end\":50618,\"start\":50608},{\"end\":50631,\"start\":50625},{\"end\":50644,\"start\":50638},{\"end\":50656,\"start\":50650},{\"end\":50673,\"start\":50670},{\"end\":51180,\"start\":51177},{\"end\":51192,\"start\":51186},{\"end\":51197,\"start\":51193},{\"end\":51211,\"start\":51208},{\"end\":51227,\"start\":51219},{\"end\":51243,\"start\":51237},{\"end\":51258,\"start\":51251},{\"end\":51272,\"start\":51267},{\"end\":51286,\"start\":51280},{\"end\":51296,\"start\":51291},{\"end\":51307,\"start\":51304},{\"end\":51324,\"start\":51319},{\"end\":51340,\"start\":51334},{\"end\":51706,\"start\":51700},{\"end\":51721,\"start\":51712},{\"end\":51732,\"start\":51727},{\"end\":52487,\"start\":52485},{\"end\":52501,\"start\":52493},{\"end\":52514,\"start\":52507},{\"end\":52523,\"start\":52519},{\"end\":52533,\"start\":52529},{\"end\":52544,\"start\":52538},{\"end\":52553,\"start\":52549},{\"end\":52555,\"start\":52554},{\"end\":52570,\"start\":52563},{\"end\":52582,\"start\":52581},{\"end\":52591,\"start\":52590},{\"end\":52605,\"start\":52601},{\"end\":53486,\"start\":53479},{\"end\":53500,\"start\":53495},{\"end\":53518,\"start\":53511},{\"end\":53535,\"start\":53526},{\"end\":53548,\"start\":53543},{\"end\":53553,\"start\":53549},{\"end\":53574,\"start\":53565},{\"end\":53587,\"start\":53582},{\"end\":54196,\"start\":54189},{\"end\":54206,\"start\":54201},{\"end\":54217,\"start\":54212},{\"end\":54229,\"start\":54222},{\"end\":54239,\"start\":54235},{\"end\":54241,\"start\":54240},{\"end\":54829,\"start\":54824},{\"end\":54842,\"start\":54841},{\"end\":54857,\"start\":54850},{\"end\":55266,\"start\":55262},{\"end\":55282,\"start\":55276},{\"end\":55301,\"start\":55292},{\"end\":55320,\"start\":55315},{\"end\":55337,\"start\":55330},{\"end\":55360,\"start\":55354},{\"end\":55375,\"start\":55371},{\"end\":55394,\"start\":55387},{\"end\":56219,\"start\":56212},{\"end\":56237,\"start\":56229},{\"end\":56250,\"start\":56245},{\"end\":56270,\"start\":56261},{\"end\":56284,\"start\":56279},{\"end\":56303,\"start\":56297},{\"end\":56319,\"start\":56312},{\"end\":56668,\"start\":56664},{\"end\":56680,\"start\":56674},{\"end\":56689,\"start\":56687},{\"end\":56702,\"start\":56695},{\"end\":56715,\"start\":56709},{\"end\":56728,\"start\":56724},{\"end\":56739,\"start\":56735},{\"end\":56741,\"start\":56740},{\"end\":56755,\"start\":56746},{\"end\":56766,\"start\":56762},{\"end\":56774,\"start\":56771},{\"end\":57550,\"start\":57542},{\"end\":57568,\"start\":57562},{\"end\":57580,\"start\":57575},{\"end\":57593,\"start\":57586},{\"end\":57607,\"start\":57601},{\"end\":57618,\"start\":57612},{\"end\":57632,\"start\":57627},{\"end\":57646,\"start\":57639},{\"end\":58445,\"start\":58441},{\"end\":58460,\"start\":58455},{\"end\":59436,\"start\":59433},{\"end\":59460,\"start\":59450},{\"end\":59477,\"start\":59471},{\"end\":59495,\"start\":59488},{\"end\":59510,\"start\":59505},{\"end\":59524,\"start\":59519},{\"end\":59542,\"start\":59534},{\"end\":59557,\"start\":59552},{\"end\":59575,\"start\":59570},{\"end\":59590,\"start\":59584},{\"end\":59604,\"start\":59596},{\"end\":59621,\"start\":59616},{\"end\":59636,\"start\":59629},{\"end\":59653,\"start\":59645},{\"end\":59667,\"start\":59661},{\"end\":59669,\"start\":59668},{\"end\":59680,\"start\":59675},{\"end\":59696,\"start\":59692},{\"end\":59705,\"start\":59701},{\"end\":60354,\"start\":60347},{\"end\":60366,\"start\":60360},{\"end\":60379,\"start\":60373},{\"end\":60391,\"start\":60386},{\"end\":61317,\"start\":61308},{\"end\":61329,\"start\":61323},{\"end\":61346,\"start\":61340},{\"end\":61363,\"start\":61356},{\"end\":61378,\"start\":61371},{\"end\":61389,\"start\":61384},{\"end\":61414,\"start\":61406},{\"end\":61632,\"start\":61626},{\"end\":61642,\"start\":61638},{\"end\":61653,\"start\":61648},{\"end\":61668,\"start\":61661},{\"end\":61679,\"start\":61673},{\"end\":61692,\"start\":61687},{\"end\":61703,\"start\":61699},{\"end\":61714,\"start\":61710},{\"end\":61726,\"start\":61722},{\"end\":61747,\"start\":61740},{\"end\":62106,\"start\":62102},{\"end\":62115,\"start\":62111},{\"end\":62118,\"start\":62116},{\"end\":62129,\"start\":62125},{\"end\":62145,\"start\":62139},{\"end\":62160,\"start\":62154},{\"end\":62863,\"start\":62857},{\"end\":62879,\"start\":62870},{\"end\":62896,\"start\":62891},{\"end\":62911,\"start\":62906},{\"end\":62923,\"start\":62919},{\"end\":62940,\"start\":62935},{\"end\":62959,\"start\":62950},{\"end\":63344,\"start\":63339},{\"end\":63357,\"start\":63354},{\"end\":63371,\"start\":63364},{\"end\":63373,\"start\":63372},{\"end\":63390,\"start\":63383},{\"end\":63601,\"start\":63595},{\"end\":63743,\"start\":63737},{\"end\":63765,\"start\":63757},{\"end\":64006,\"start\":64000},{\"end\":64023,\"start\":64020},{\"end\":64032,\"start\":64028},{\"end\":64043,\"start\":64039},{\"end\":64058,\"start\":64053},{\"end\":64066,\"start\":64059},{\"end\":64077,\"start\":64072},{\"end\":64092,\"start\":64086},{\"end\":64106,\"start\":64099},{\"end\":64108,\"start\":64107},{\"end\":64120,\"start\":64116},{\"end\":64125,\"start\":64121},{\"end\":64136,\"start\":64131},{\"end\":64154,\"start\":64146},{\"end\":64171,\"start\":64165},{\"end\":64184,\"start\":64179},{\"end\":65649,\"start\":65643},{\"end\":65658,\"start\":65654},{\"end\":65667,\"start\":65663},{\"end\":65678,\"start\":65672},{\"end\":65691,\"start\":65684},{\"end\":65724,\"start\":65722},{\"end\":65736,\"start\":65729},{\"end\":65745,\"start\":65743},{\"end\":65757,\"start\":65752},{\"end\":65759,\"start\":65758},{\"end\":65774,\"start\":65766},{\"end\":65788,\"start\":65782},{\"end\":66127,\"start\":66121},{\"end\":66139,\"start\":66132},{\"end\":66149,\"start\":66140},{\"end\":66162,\"start\":66158},{\"end\":66175,\"start\":66173},{\"end\":66185,\"start\":66180},{\"end\":66198,\"start\":66192},{\"end\":66210,\"start\":66204},{\"end\":66620,\"start\":66614},{\"end\":66633,\"start\":66627},{\"end\":66650,\"start\":66644},{\"end\":66665,\"start\":66658},{\"end\":66681,\"start\":66673},{\"end\":66703,\"start\":66693},{\"end\":66717,\"start\":66712},{\"end\":66733,\"start\":66724},{\"end\":66747,\"start\":66742},{\"end\":66758,\"start\":66753},{\"end\":66772,\"start\":66766},{\"end\":67534,\"start\":67529},{\"end\":67554,\"start\":67544},{\"end\":67569,\"start\":67563},{\"end\":67582,\"start\":67575},{\"end\":67595,\"start\":67590},{\"end\":67611,\"start\":67605},{\"end\":67614,\"start\":67612},{\"end\":67625,\"start\":67620},{\"end\":67640,\"start\":67634},{\"end\":67658,\"start\":67650},{\"end\":67674,\"start\":67671},{\"end\":67697,\"start\":67688},{\"end\":68176,\"start\":68172},{\"end\":68190,\"start\":68186},{\"end\":68195,\"start\":68191},{\"end\":68206,\"start\":68201},{\"end\":68222,\"start\":68216},{\"end\":68238,\"start\":68231},{\"end\":68252,\"start\":68244},{\"end\":68268,\"start\":68262},{\"end\":68283,\"start\":68277},{\"end\":68298,\"start\":68292},{\"end\":68312,\"start\":68308},{\"end\":68328,\"start\":68320},{\"end\":68342,\"start\":68338},{\"end\":69188,\"start\":69183},{\"end\":69201,\"start\":69197},{\"end\":69215,\"start\":69211},{\"end\":69234,\"start\":69225},{\"end\":69246,\"start\":69240},{\"end\":69262,\"start\":69255},{\"end\":69276,\"start\":69271},{\"end\":69286,\"start\":69283},{\"end\":69298,\"start\":69291},{\"end\":69674,\"start\":69670},{\"end\":69689,\"start\":69684},{\"end\":70639,\"start\":70632},{\"end\":70651,\"start\":70645},{\"end\":70660,\"start\":70656},{\"end\":70671,\"start\":70666},{\"end\":70675,\"start\":70672},{\"end\":70690,\"start\":70682},{\"end\":70699,\"start\":70696},{\"end\":70711,\"start\":70704},{\"end\":70725,\"start\":70718},{\"end\":71560,\"start\":71554},{\"end\":71579,\"start\":71570},{\"end\":71594,\"start\":71588},{\"end\":71610,\"start\":71603},{\"end\":71624,\"start\":71618},{\"end\":71639,\"start\":71633},{\"end\":72631,\"start\":72624},{\"end\":72644,\"start\":72640},{\"end\":72660,\"start\":72656},{\"end\":72670,\"start\":72665},{\"end\":72690,\"start\":72679},{\"end\":72692,\"start\":72691},{\"end\":72703,\"start\":72702},{\"end\":72719,\"start\":72708},{\"end\":73149,\"start\":73143},{\"end\":73162,\"start\":73158},{\"end\":73179,\"start\":73172},{\"end\":73196,\"start\":73188},{\"end\":73214,\"start\":73209},{\"end\":73696,\"start\":73691},{\"end\":73712,\"start\":73705},{\"end\":73730,\"start\":73722},{\"end\":73756,\"start\":73751},{\"end\":74683,\"start\":74678},{\"end\":74701,\"start\":74694},{\"end\":74714,\"start\":74708},{\"end\":74728,\"start\":74724},{\"end\":74746,\"start\":74745},{\"end\":74760,\"start\":74756},{\"end\":74772,\"start\":74768},{\"end\":74780,\"start\":74777},{\"end\":74794,\"start\":74790},{\"end\":74797,\"start\":74795},{\"end\":75220,\"start\":75213},{\"end\":75239,\"start\":75232},{\"end\":75251,\"start\":75246},{\"end\":75740,\"start\":75735},{\"end\":75758,\"start\":75749},{\"end\":75768,\"start\":75764},{\"end\":75777,\"start\":75773},{\"end\":75780,\"start\":75778},{\"end\":75796,\"start\":75787},{\"end\":75814,\"start\":75809},{\"end\":75830,\"start\":75822},{\"end\":76384,\"start\":76379},{\"end\":76394,\"start\":76391},{\"end\":76409,\"start\":76401},{\"end\":76424,\"start\":76417},{\"end\":76437,\"start\":76431},{\"end\":76449,\"start\":76444},{\"end\":76463,\"start\":76457},{\"end\":76478,\"start\":76474},{\"end\":76829,\"start\":76823},{\"end\":76842,\"start\":76836},{\"end\":76855,\"start\":76848},{\"end\":76865,\"start\":76863},{\"end\":76876,\"start\":76872},{\"end\":77305,\"start\":77296},{\"end\":77324,\"start\":77314},{\"end\":77340,\"start\":77334},{\"end\":77357,\"start\":77350},{\"end\":77378,\"start\":77369},{\"end\":77393,\"start\":77387},{\"end\":77409,\"start\":77402},{\"end\":78128,\"start\":78125},{\"end\":78143,\"start\":78136},{\"end\":78153,\"start\":78151},{\"end\":78167,\"start\":78158},{\"end\":78180,\"start\":78174},{\"end\":78190,\"start\":78186},{\"end\":78192,\"start\":78191},{\"end\":78208,\"start\":78202},{\"end\":78222,\"start\":78216},{\"end\":78791,\"start\":78785},{\"end\":78800,\"start\":78796},{\"end\":78809,\"start\":78806},{\"end\":78822,\"start\":78816},{\"end\":79209,\"start\":79203},{\"end\":79220,\"start\":79216},{\"end\":79233,\"start\":79225},{\"end\":79247,\"start\":79241},{\"end\":79263,\"start\":79256},{\"end\":79277,\"start\":79271},{\"end\":79306,\"start\":79293}]", "bib_author_last_name": "[{\"end\":38178,\"start\":38173},{\"end\":38192,\"start\":38187},{\"end\":38203,\"start\":38201},{\"end\":38737,\"start\":38730},{\"end\":38753,\"start\":38746},{\"end\":39155,\"start\":39151},{\"end\":39168,\"start\":39166},{\"end\":39186,\"start\":39180},{\"end\":39944,\"start\":39934},{\"end\":39956,\"start\":39951},{\"end\":39973,\"start\":39967},{\"end\":39987,\"start\":39983},{\"end\":40001,\"start\":39995},{\"end\":40018,\"start\":40010},{\"end\":40039,\"start\":40030},{\"end\":40054,\"start\":40047},{\"end\":40067,\"start\":40062},{\"end\":40086,\"start\":40077},{\"end\":40607,\"start\":40601},{\"end\":40625,\"start\":40616},{\"end\":40640,\"start\":40633},{\"end\":40656,\"start\":40649},{\"end\":41005,\"start\":40999},{\"end\":41019,\"start\":41013},{\"end\":41039,\"start\":41033},{\"end\":41060,\"start\":41055},{\"end\":41069,\"start\":41062},{\"end\":41706,\"start\":41701},{\"end\":41721,\"start\":41717},{\"end\":41733,\"start\":41728},{\"end\":41750,\"start\":41743},{\"end\":41764,\"start\":41758},{\"end\":41783,\"start\":41775},{\"end\":41803,\"start\":41792},{\"end\":42275,\"start\":42268},{\"end\":42287,\"start\":42285},{\"end\":42307,\"start\":42301},{\"end\":42319,\"start\":42314},{\"end\":42330,\"start\":42326},{\"end\":42346,\"start\":42340},{\"end\":42360,\"start\":42354},{\"end\":42375,\"start\":42371},{\"end\":42387,\"start\":42382},{\"end\":42406,\"start\":42401},{\"end\":42418,\"start\":42412},{\"end\":42435,\"start\":42425},{\"end\":42449,\"start\":42442},{\"end\":42466,\"start\":42457},{\"end\":42474,\"start\":42468},{\"end\":43424,\"start\":43402},{\"end\":43436,\"start\":43430},{\"end\":43451,\"start\":43442},{\"end\":43466,\"start\":43462},{\"end\":43479,\"start\":43476},{\"end\":43494,\"start\":43488},{\"end\":43507,\"start\":43499},{\"end\":43521,\"start\":43517},{\"end\":43528,\"start\":43523},{\"end\":43861,\"start\":43856},{\"end\":43873,\"start\":43871},{\"end\":43888,\"start\":43883},{\"end\":43901,\"start\":43897},{\"end\":43915,\"start\":43910},{\"end\":44403,\"start\":44399},{\"end\":44420,\"start\":44411},{\"end\":44438,\"start\":44431},{\"end\":44457,\"start\":44451},{\"end\":45078,\"start\":45074},{\"end\":45095,\"start\":45087},{\"end\":45108,\"start\":45104},{\"end\":45122,\"start\":45117},{\"end\":45137,\"start\":45132},{\"end\":45155,\"start\":45144},{\"end\":45170,\"start\":45164},{\"end\":45183,\"start\":45178},{\"end\":45196,\"start\":45193},{\"end\":45581,\"start\":45576},{\"end\":45597,\"start\":45590},{\"end\":45609,\"start\":45602},{\"end\":45622,\"start\":45616},{\"end\":45637,\"start\":45633},{\"end\":46161,\"start\":46154},{\"end\":46174,\"start\":46169},{\"end\":46813,\"start\":46806},{\"end\":46826,\"start\":46821},{\"end\":46842,\"start\":46835},{\"end\":46857,\"start\":46849},{\"end\":46873,\"start\":46867},{\"end\":47545,\"start\":47542},{\"end\":47559,\"start\":47555},{\"end\":47566,\"start\":47564},{\"end\":47575,\"start\":47571},{\"end\":47586,\"start\":47584},{\"end\":47595,\"start\":47593},{\"end\":47610,\"start\":47603},{\"end\":47622,\"start\":47619},{\"end\":47636,\"start\":47632},{\"end\":47652,\"start\":47647},{\"end\":47988,\"start\":47978},{\"end\":47997,\"start\":47992},{\"end\":48014,\"start\":48008},{\"end\":48022,\"start\":48016},{\"end\":48032,\"start\":48026},{\"end\":48050,\"start\":48042},{\"end\":48060,\"start\":48052},{\"end\":48425,\"start\":48419},{\"end\":48441,\"start\":48436},{\"end\":48453,\"start\":48450},{\"end\":48473,\"start\":48464},{\"end\":49407,\"start\":49397},{\"end\":49427,\"start\":49416},{\"end\":49442,\"start\":49436},{\"end\":49466,\"start\":49457},{\"end\":49483,\"start\":49475},{\"end\":49831,\"start\":49824},{\"end\":49846,\"start\":49841},{\"end\":50623,\"start\":50619},{\"end\":50636,\"start\":50632},{\"end\":50648,\"start\":50645},{\"end\":50668,\"start\":50657},{\"end\":50678,\"start\":50674},{\"end\":51184,\"start\":51181},{\"end\":51206,\"start\":51198},{\"end\":51217,\"start\":51212},{\"end\":51235,\"start\":51228},{\"end\":51249,\"start\":51244},{\"end\":51265,\"start\":51259},{\"end\":51278,\"start\":51273},{\"end\":51289,\"start\":51287},{\"end\":51302,\"start\":51297},{\"end\":51317,\"start\":51308},{\"end\":51332,\"start\":51325},{\"end\":51346,\"start\":51341},{\"end\":51710,\"start\":51707},{\"end\":51725,\"start\":51722},{\"end\":51737,\"start\":51733},{\"end\":52491,\"start\":52488},{\"end\":52505,\"start\":52502},{\"end\":52517,\"start\":52515},{\"end\":52527,\"start\":52524},{\"end\":52536,\"start\":52534},{\"end\":52547,\"start\":52545},{\"end\":52561,\"start\":52556},{\"end\":52579,\"start\":52571},{\"end\":52588,\"start\":52583},{\"end\":52599,\"start\":52592},{\"end\":52616,\"start\":52606},{\"end\":53493,\"start\":53487},{\"end\":53509,\"start\":53501},{\"end\":53524,\"start\":53519},{\"end\":53541,\"start\":53536},{\"end\":53563,\"start\":53554},{\"end\":53580,\"start\":53575},{\"end\":53594,\"start\":53588},{\"end\":54199,\"start\":54197},{\"end\":54210,\"start\":54207},{\"end\":54220,\"start\":54218},{\"end\":54233,\"start\":54230},{\"end\":54250,\"start\":54242},{\"end\":54839,\"start\":54830},{\"end\":54848,\"start\":54843},{\"end\":54866,\"start\":54858},{\"end\":54875,\"start\":54868},{\"end\":55274,\"start\":55267},{\"end\":55290,\"start\":55283},{\"end\":55313,\"start\":55302},{\"end\":55328,\"start\":55321},{\"end\":55352,\"start\":55338},{\"end\":55369,\"start\":55361},{\"end\":55385,\"start\":55376},{\"end\":55400,\"start\":55395},{\"end\":56227,\"start\":56220},{\"end\":56243,\"start\":56238},{\"end\":56259,\"start\":56251},{\"end\":56277,\"start\":56271},{\"end\":56295,\"start\":56285},{\"end\":56310,\"start\":56304},{\"end\":56325,\"start\":56320},{\"end\":56672,\"start\":56669},{\"end\":56685,\"start\":56681},{\"end\":56693,\"start\":56690},{\"end\":56707,\"start\":56703},{\"end\":56722,\"start\":56716},{\"end\":56733,\"start\":56729},{\"end\":56744,\"start\":56742},{\"end\":56760,\"start\":56756},{\"end\":56769,\"start\":56767},{\"end\":56781,\"start\":56775},{\"end\":57560,\"start\":57551},{\"end\":57573,\"start\":57569},{\"end\":57584,\"start\":57581},{\"end\":57599,\"start\":57594},{\"end\":57610,\"start\":57608},{\"end\":57625,\"start\":57619},{\"end\":57637,\"start\":57633},{\"end\":57650,\"start\":57647},{\"end\":58453,\"start\":58446},{\"end\":58468,\"start\":58461},{\"end\":58477,\"start\":58470},{\"end\":59448,\"start\":59437},{\"end\":59469,\"start\":59461},{\"end\":59486,\"start\":59478},{\"end\":59503,\"start\":59496},{\"end\":59517,\"start\":59511},{\"end\":59532,\"start\":59525},{\"end\":59550,\"start\":59543},{\"end\":59568,\"start\":59558},{\"end\":59582,\"start\":59576},{\"end\":59594,\"start\":59591},{\"end\":59614,\"start\":59605},{\"end\":59627,\"start\":59622},{\"end\":59643,\"start\":59637},{\"end\":59659,\"start\":59654},{\"end\":59673,\"start\":59670},{\"end\":59690,\"start\":59681},{\"end\":59699,\"start\":59697},{\"end\":59712,\"start\":59706},{\"end\":60358,\"start\":60355},{\"end\":60371,\"start\":60367},{\"end\":60384,\"start\":60380},{\"end\":60396,\"start\":60392},{\"end\":61321,\"start\":61318},{\"end\":61338,\"start\":61330},{\"end\":61354,\"start\":61347},{\"end\":61369,\"start\":61364},{\"end\":61382,\"start\":61379},{\"end\":61404,\"start\":61390},{\"end\":61422,\"start\":61415},{\"end\":61636,\"start\":61633},{\"end\":61646,\"start\":61643},{\"end\":61659,\"start\":61654},{\"end\":61671,\"start\":61669},{\"end\":61685,\"start\":61680},{\"end\":61697,\"start\":61693},{\"end\":61708,\"start\":61704},{\"end\":61720,\"start\":61715},{\"end\":61738,\"start\":61727},{\"end\":61756,\"start\":61748},{\"end\":61765,\"start\":61758},{\"end\":62109,\"start\":62107},{\"end\":62123,\"start\":62119},{\"end\":62137,\"start\":62130},{\"end\":62152,\"start\":62146},{\"end\":62165,\"start\":62161},{\"end\":62868,\"start\":62864},{\"end\":62889,\"start\":62880},{\"end\":62904,\"start\":62897},{\"end\":62917,\"start\":62912},{\"end\":62933,\"start\":62924},{\"end\":62948,\"start\":62941},{\"end\":62967,\"start\":62960},{\"end\":63352,\"start\":63345},{\"end\":63362,\"start\":63358},{\"end\":63381,\"start\":63374},{\"end\":63395,\"start\":63391},{\"end\":63613,\"start\":63602},{\"end\":63755,\"start\":63744},{\"end\":63770,\"start\":63766},{\"end\":64018,\"start\":64007},{\"end\":64026,\"start\":64024},{\"end\":64037,\"start\":64033},{\"end\":64051,\"start\":64044},{\"end\":64070,\"start\":64067},{\"end\":64084,\"start\":64078},{\"end\":64097,\"start\":64093},{\"end\":64114,\"start\":64109},{\"end\":64129,\"start\":64126},{\"end\":64144,\"start\":64137},{\"end\":64163,\"start\":64155},{\"end\":64177,\"start\":64172},{\"end\":64190,\"start\":64185},{\"end\":65652,\"start\":65650},{\"end\":65661,\"start\":65659},{\"end\":65670,\"start\":65668},{\"end\":65682,\"start\":65679},{\"end\":65720,\"start\":65692},{\"end\":65727,\"start\":65725},{\"end\":65741,\"start\":65737},{\"end\":65750,\"start\":65746},{\"end\":65764,\"start\":65760},{\"end\":65780,\"start\":65775},{\"end\":65793,\"start\":65789},{\"end\":66130,\"start\":66128},{\"end\":66156,\"start\":66150},{\"end\":66171,\"start\":66163},{\"end\":66178,\"start\":66176},{\"end\":66190,\"start\":66186},{\"end\":66202,\"start\":66199},{\"end\":66215,\"start\":66211},{\"end\":66625,\"start\":66621},{\"end\":66642,\"start\":66634},{\"end\":66656,\"start\":66651},{\"end\":66671,\"start\":66666},{\"end\":66691,\"start\":66682},{\"end\":66710,\"start\":66704},{\"end\":66722,\"start\":66718},{\"end\":66740,\"start\":66734},{\"end\":66751,\"start\":66748},{\"end\":66764,\"start\":66759},{\"end\":66779,\"start\":66773},{\"end\":67542,\"start\":67535},{\"end\":67561,\"start\":67555},{\"end\":67573,\"start\":67570},{\"end\":67588,\"start\":67583},{\"end\":67603,\"start\":67596},{\"end\":67618,\"start\":67615},{\"end\":67632,\"start\":67626},{\"end\":67648,\"start\":67641},{\"end\":67669,\"start\":67659},{\"end\":67686,\"start\":67675},{\"end\":67704,\"start\":67698},{\"end\":68184,\"start\":68177},{\"end\":68199,\"start\":68196},{\"end\":68214,\"start\":68207},{\"end\":68229,\"start\":68223},{\"end\":68242,\"start\":68239},{\"end\":68260,\"start\":68253},{\"end\":68275,\"start\":68269},{\"end\":68290,\"start\":68284},{\"end\":68306,\"start\":68299},{\"end\":68318,\"start\":68313},{\"end\":68336,\"start\":68329},{\"end\":68352,\"start\":68343},{\"end\":69195,\"start\":69189},{\"end\":69209,\"start\":69202},{\"end\":69223,\"start\":69216},{\"end\":69238,\"start\":69235},{\"end\":69253,\"start\":69247},{\"end\":69269,\"start\":69263},{\"end\":69281,\"start\":69277},{\"end\":69289,\"start\":69287},{\"end\":69302,\"start\":69299},{\"end\":69682,\"start\":69675},{\"end\":69698,\"start\":69690},{\"end\":70643,\"start\":70640},{\"end\":70654,\"start\":70652},{\"end\":70664,\"start\":70661},{\"end\":70680,\"start\":70676},{\"end\":70694,\"start\":70691},{\"end\":70702,\"start\":70700},{\"end\":70716,\"start\":70712},{\"end\":70729,\"start\":70726},{\"end\":71568,\"start\":71561},{\"end\":71586,\"start\":71580},{\"end\":71601,\"start\":71595},{\"end\":71616,\"start\":71611},{\"end\":71631,\"start\":71625},{\"end\":71643,\"start\":71640},{\"end\":72638,\"start\":72632},{\"end\":72654,\"start\":72645},{\"end\":72663,\"start\":72661},{\"end\":72677,\"start\":72671},{\"end\":72700,\"start\":72693},{\"end\":72706,\"start\":72704},{\"end\":72725,\"start\":72720},{\"end\":73156,\"start\":73150},{\"end\":73170,\"start\":73163},{\"end\":73186,\"start\":73180},{\"end\":73207,\"start\":73197},{\"end\":73223,\"start\":73215},{\"end\":73703,\"start\":73697},{\"end\":73720,\"start\":73713},{\"end\":73749,\"start\":73731},{\"end\":73763,\"start\":73757},{\"end\":74692,\"start\":74684},{\"end\":74706,\"start\":74702},{\"end\":74722,\"start\":74715},{\"end\":74743,\"start\":74729},{\"end\":74754,\"start\":74747},{\"end\":74766,\"start\":74761},{\"end\":74775,\"start\":74773},{\"end\":74788,\"start\":74781},{\"end\":74806,\"start\":74798},{\"end\":74812,\"start\":74808},{\"end\":75230,\"start\":75221},{\"end\":75244,\"start\":75240},{\"end\":75257,\"start\":75252},{\"end\":75747,\"start\":75741},{\"end\":75762,\"start\":75759},{\"end\":75771,\"start\":75769},{\"end\":75785,\"start\":75781},{\"end\":75807,\"start\":75797},{\"end\":75820,\"start\":75815},{\"end\":75841,\"start\":75831},{\"end\":76389,\"start\":76385},{\"end\":76399,\"start\":76395},{\"end\":76415,\"start\":76410},{\"end\":76429,\"start\":76425},{\"end\":76442,\"start\":76438},{\"end\":76455,\"start\":76450},{\"end\":76472,\"start\":76464},{\"end\":76482,\"start\":76479},{\"end\":76834,\"start\":76830},{\"end\":76846,\"start\":76843},{\"end\":76861,\"start\":76856},{\"end\":76870,\"start\":76866},{\"end\":76880,\"start\":76877},{\"end\":77312,\"start\":77306},{\"end\":77332,\"start\":77325},{\"end\":77348,\"start\":77341},{\"end\":77367,\"start\":77358},{\"end\":77385,\"start\":77379},{\"end\":77400,\"start\":77394},{\"end\":77415,\"start\":77410},{\"end\":78134,\"start\":78129},{\"end\":78149,\"start\":78144},{\"end\":78156,\"start\":78154},{\"end\":78172,\"start\":78168},{\"end\":78184,\"start\":78181},{\"end\":78200,\"start\":78193},{\"end\":78214,\"start\":78209},{\"end\":78231,\"start\":78223},{\"end\":78794,\"start\":78792},{\"end\":78804,\"start\":78801},{\"end\":78814,\"start\":78810},{\"end\":78830,\"start\":78823},{\"end\":79214,\"start\":79210},{\"end\":79223,\"start\":79221},{\"end\":79239,\"start\":79234},{\"end\":79254,\"start\":79248},{\"end\":79269,\"start\":79264},{\"end\":79291,\"start\":79278},{\"end\":79314,\"start\":79307}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":64908139},\"end\":38631,\"start\":38104},{\"attributes\":{\"doi\":\"10.1162/tacl_a_00288\",\"id\":\"b1\",\"matched_paper_id\":56895585},\"end\":39112,\"start\":38633},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3177797},\"end\":39877,\"start\":39114},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":251385004},\"end\":40522,\"start\":39879},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":14355670},\"end\":40929,\"start\":40524},{\"attributes\":{\"doi\":\"10.18653/v1/D15-1075\",\"id\":\"b5\",\"matched_paper_id\":14604520},\"end\":41691,\"start\":40931},{\"attributes\":{\"id\":\"b6\"},\"end\":42219,\"start\":41693},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":218971783},\"end\":43329,\"start\":42221},{\"attributes\":{\"doi\":\"abs/1611.09268\",\"id\":\"b8\",\"matched_paper_id\":1289517},\"end\":43782,\"start\":43331},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":211068995},\"end\":44321,\"start\":43784},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b10\",\"matched_paper_id\":211096730},\"end\":44984,\"start\":44323},{\"attributes\":{\"doi\":\"arXiv:2110.06918\",\"id\":\"b11\"},\"end\":45482,\"start\":44986},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":215768677},\"end\":46073,\"start\":45484},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3932228},\"end\":46701,\"start\":46075},{\"attributes\":{\"doi\":\"10.18653/v1/D17-1070\",\"id\":\"b14\",\"matched_paper_id\":28971531},\"end\":47533,\"start\":46703},{\"attributes\":{\"doi\":\"abs/2209.11755\",\"id\":\"b15\"},\"end\":47932,\"start\":47535},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3252915},\"end\":48329,\"start\":47934},{\"attributes\":{\"doi\":\"10.18653/v1/N19-1423\",\"id\":\"b17\",\"matched_paper_id\":52967399},\"end\":49316,\"start\":48331},{\"attributes\":{\"doi\":\"arXiv:2012.00614\",\"id\":\"b18\"},\"end\":49724,\"start\":49318},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":225366984},\"end\":50564,\"start\":49726},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":220347683},\"end\":51109,\"start\":50566},{\"attributes\":{\"doi\":\"abs/2101.00027\",\"id\":\"b21\",\"matched_paper_id\":230435736},\"end\":51638,\"start\":51111},{\"attributes\":{\"doi\":\"10.18653/v1/2021.emnlp-main.552\",\"id\":\"b22\",\"matched_paper_id\":233296292},\"end\":52401,\"start\":51640},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":52065462},\"end\":53421,\"start\":52403},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3675602},\"end\":54120,\"start\":53423},{\"attributes\":{\"doi\":\"10.1109/CVPR42600.2020.00975\",\"id\":\"b25\",\"matched_paper_id\":207930212},\"end\":54745,\"start\":54122},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2699406},\"end\":55213,\"start\":54747},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b27\",\"matched_paper_id\":59599816},\"end\":56134,\"start\":55215},{\"attributes\":{\"doi\":\"abs/2112.09118\",\"id\":\"b28\",\"matched_paper_id\":245218527},\"end\":56571,\"start\":56136},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b29\",\"matched_paper_id\":231879586},\"end\":57480,\"start\":56573},{\"attributes\":{\"doi\":\"doi: 10. 18653/v1/2020.emnlp-main.550\",\"id\":\"b30\",\"matched_paper_id\":215737187},\"end\":58353,\"start\":57482},{\"attributes\":{\"doi\":\"10.1145/3397271.3401075\",\"id\":\"b31\",\"matched_paper_id\":216553223},\"end\":59367,\"start\":58355},{\"attributes\":{\"doi\":\"10.1162/tacl_a_00276\",\"id\":\"b32\",\"matched_paper_id\":86611921},\"end\":60293,\"start\":59369},{\"attributes\":{\"doi\":\"10.18653/v1/2021.acl-long.518\",\"id\":\"b33\",\"matched_paper_id\":229363636},\"end\":61248,\"start\":60295},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":235829052},\"end\":61624,\"start\":61250},{\"attributes\":{\"doi\":\"abs/1907.11692\",\"id\":\"b35\"},\"end\":62050,\"start\":61626},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.447\",\"id\":\"b36\",\"matched_paper_id\":215416146},\"end\":62783,\"start\":62052},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":13866508},\"end\":63275,\"start\":62785},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":5959482},\"end\":63542,\"start\":63277},{\"attributes\":{\"doi\":\"abs/2202.08904\",\"id\":\"b39\",\"matched_paper_id\":246996947},\"end\":63735,\"start\":63544},{\"attributes\":{\"doi\":\"abs/2210.07316\",\"id\":\"b40\"},\"end\":63996,\"start\":63737},{\"attributes\":{\"id\":\"b41\"},\"end\":64966,\"start\":63998},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":203737303},\"end\":65591,\"start\":64968},{\"attributes\":{\"doi\":\"abs/2112.07899\",\"id\":\"b43\",\"matched_paper_id\":245144556},\"end\":66041,\"start\":65593},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":237260023},\"end\":66558,\"start\":66043},{\"attributes\":{\"doi\":\"10.18653/v1/2022.findings-naacl.114\",\"id\":\"b45\",\"matched_paper_id\":236493551},\"end\":67469,\"start\":66560},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":221507798},\"end\":68099,\"start\":67471},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b47\",\"matched_paper_id\":231591445},\"end\":69098,\"start\":68101},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":204838007},\"end\":69604,\"start\":69100},{\"attributes\":{\"doi\":\"10.18653/v1/D19-1410\",\"id\":\"b49\",\"matched_paper_id\":201646309},\"end\":70542,\"start\":69606},{\"attributes\":{\"doi\":\"10.18653/v1/2021.emnlp-main.224\",\"id\":\"b50\",\"matched_paper_id\":238857121},\"end\":71479,\"start\":70544},{\"attributes\":{\"doi\":\"10.18653/v1/2021.acl-long.507\",\"id\":\"b51\",\"matched_paper_id\":207863306},\"end\":72543,\"start\":71481},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":990233},\"end\":73051,\"start\":72545},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":233296016},\"end\":73622,\"start\":73053},{\"attributes\":{\"doi\":\"10.18653/v1/N18-1074\",\"id\":\"b54\",\"matched_paper_id\":4711425},\"end\":74601,\"start\":73624},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":218581058},\"end\":75142,\"start\":74603},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":51880268},\"end\":75687,\"start\":75144},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":216867133},\"end\":76297,\"start\":75689},{\"attributes\":{\"doi\":\"abs/2207.02578\",\"id\":\"b58\",\"matched_paper_id\":250311114},\"end\":76722,\"start\":76299},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":229923069},\"end\":77221,\"start\":76724},{\"attributes\":{\"doi\":\"979-10-95546-34-4\",\"id\":\"b60\",\"matched_paper_id\":207870323},\"end\":78038,\"start\":77223},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":220302524},\"end\":78703,\"start\":78040},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":247411106},\"end\":79126,\"start\":78705},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":52822214},\"end\":79780,\"start\":79128},{\"attributes\":{\"id\":\"b64\"},\"end\":80931,\"start\":79782},{\"attributes\":{\"id\":\"b65\"},\"end\":81249,\"start\":80933}]", "bib_title": "[{\"end\":38163,\"start\":38104},{\"end\":38722,\"start\":38633},{\"end\":39141,\"start\":39114},{\"end\":39922,\"start\":39879},{\"end\":40594,\"start\":40524},{\"end\":40995,\"start\":40931},{\"end\":42258,\"start\":42221},{\"end\":43396,\"start\":43331},{\"end\":43844,\"start\":43784},{\"end\":44392,\"start\":44323},{\"end\":45568,\"start\":45484},{\"end\":46145,\"start\":46075},{\"end\":46797,\"start\":46703},{\"end\":47970,\"start\":47934},{\"end\":48411,\"start\":48331},{\"end\":49815,\"start\":49726},{\"end\":50606,\"start\":50566},{\"end\":51175,\"start\":51111},{\"end\":51698,\"start\":51640},{\"end\":52483,\"start\":52403},{\"end\":53477,\"start\":53423},{\"end\":54187,\"start\":54122},{\"end\":54822,\"start\":54747},{\"end\":55260,\"start\":55215},{\"end\":56210,\"start\":56136},{\"end\":56662,\"start\":56573},{\"end\":57540,\"start\":57482},{\"end\":58439,\"start\":58355},{\"end\":59431,\"start\":59369},{\"end\":60345,\"start\":60295},{\"end\":61306,\"start\":61250},{\"end\":62100,\"start\":62052},{\"end\":62855,\"start\":62785},{\"end\":63337,\"start\":63277},{\"end\":63593,\"start\":63544},{\"end\":65025,\"start\":64968},{\"end\":65641,\"start\":65593},{\"end\":66119,\"start\":66043},{\"end\":66612,\"start\":66560},{\"end\":67527,\"start\":67471},{\"end\":68170,\"start\":68101},{\"end\":69181,\"start\":69100},{\"end\":69668,\"start\":69606},{\"end\":70630,\"start\":70544},{\"end\":71552,\"start\":71481},{\"end\":72622,\"start\":72545},{\"end\":73141,\"start\":73053},{\"end\":73689,\"start\":73624},{\"end\":74676,\"start\":74603},{\"end\":75211,\"start\":75144},{\"end\":75733,\"start\":75689},{\"end\":76377,\"start\":76299},{\"end\":76821,\"start\":76724},{\"end\":77294,\"start\":77223},{\"end\":78123,\"start\":78040},{\"end\":78783,\"start\":78705},{\"end\":79201,\"start\":79128}]", "bib_author": "[{\"end\":38180,\"start\":38165},{\"end\":38194,\"start\":38180},{\"end\":38205,\"start\":38194},{\"end\":38739,\"start\":38724},{\"end\":38755,\"start\":38739},{\"end\":39157,\"start\":39143},{\"end\":39170,\"start\":39157},{\"end\":39188,\"start\":39170},{\"end\":39946,\"start\":39924},{\"end\":39958,\"start\":39946},{\"end\":39975,\"start\":39958},{\"end\":39989,\"start\":39975},{\"end\":40003,\"start\":39989},{\"end\":40020,\"start\":40003},{\"end\":40041,\"start\":40020},{\"end\":40056,\"start\":40041},{\"end\":40069,\"start\":40056},{\"end\":40088,\"start\":40069},{\"end\":40609,\"start\":40596},{\"end\":40627,\"start\":40609},{\"end\":40642,\"start\":40627},{\"end\":40658,\"start\":40642},{\"end\":41007,\"start\":40997},{\"end\":41021,\"start\":41007},{\"end\":41041,\"start\":41021},{\"end\":41062,\"start\":41041},{\"end\":41071,\"start\":41062},{\"end\":41708,\"start\":41695},{\"end\":41723,\"start\":41708},{\"end\":41735,\"start\":41723},{\"end\":41752,\"start\":41735},{\"end\":41766,\"start\":41752},{\"end\":41785,\"start\":41766},{\"end\":41805,\"start\":41785},{\"end\":42277,\"start\":42260},{\"end\":42289,\"start\":42277},{\"end\":42309,\"start\":42289},{\"end\":42321,\"start\":42309},{\"end\":42332,\"start\":42321},{\"end\":42348,\"start\":42332},{\"end\":42362,\"start\":42348},{\"end\":42377,\"start\":42362},{\"end\":42389,\"start\":42377},{\"end\":42408,\"start\":42389},{\"end\":42420,\"start\":42408},{\"end\":42437,\"start\":42420},{\"end\":42451,\"start\":42437},{\"end\":42468,\"start\":42451},{\"end\":42476,\"start\":42468},{\"end\":43426,\"start\":43398},{\"end\":43438,\"start\":43426},{\"end\":43453,\"start\":43438},{\"end\":43468,\"start\":43453},{\"end\":43481,\"start\":43468},{\"end\":43496,\"start\":43481},{\"end\":43509,\"start\":43496},{\"end\":43523,\"start\":43509},{\"end\":43530,\"start\":43523},{\"end\":43863,\"start\":43846},{\"end\":43875,\"start\":43863},{\"end\":43890,\"start\":43875},{\"end\":43903,\"start\":43890},{\"end\":43917,\"start\":43903},{\"end\":44405,\"start\":44394},{\"end\":44422,\"start\":44405},{\"end\":44440,\"start\":44422},{\"end\":44459,\"start\":44440},{\"end\":45080,\"start\":45068},{\"end\":45097,\"start\":45080},{\"end\":45110,\"start\":45097},{\"end\":45124,\"start\":45110},{\"end\":45139,\"start\":45124},{\"end\":45157,\"start\":45139},{\"end\":45172,\"start\":45157},{\"end\":45185,\"start\":45172},{\"end\":45198,\"start\":45185},{\"end\":45583,\"start\":45570},{\"end\":45599,\"start\":45583},{\"end\":45611,\"start\":45599},{\"end\":45624,\"start\":45611},{\"end\":45639,\"start\":45624},{\"end\":46163,\"start\":46147},{\"end\":46176,\"start\":46163},{\"end\":46815,\"start\":46799},{\"end\":46828,\"start\":46815},{\"end\":46844,\"start\":46828},{\"end\":46859,\"start\":46844},{\"end\":46875,\"start\":46859},{\"end\":47547,\"start\":47535},{\"end\":47561,\"start\":47547},{\"end\":47568,\"start\":47561},{\"end\":47577,\"start\":47568},{\"end\":47588,\"start\":47577},{\"end\":47597,\"start\":47588},{\"end\":47612,\"start\":47597},{\"end\":47624,\"start\":47612},{\"end\":47638,\"start\":47624},{\"end\":47654,\"start\":47638},{\"end\":47990,\"start\":47972},{\"end\":47999,\"start\":47990},{\"end\":48016,\"start\":47999},{\"end\":48024,\"start\":48016},{\"end\":48034,\"start\":48024},{\"end\":48052,\"start\":48034},{\"end\":48062,\"start\":48052},{\"end\":48427,\"start\":48413},{\"end\":48443,\"start\":48427},{\"end\":48455,\"start\":48443},{\"end\":48475,\"start\":48455},{\"end\":49409,\"start\":49390},{\"end\":49429,\"start\":49409},{\"end\":49444,\"start\":49429},{\"end\":49468,\"start\":49444},{\"end\":49485,\"start\":49468},{\"end\":49833,\"start\":49817},{\"end\":49848,\"start\":49833},{\"end\":50625,\"start\":50608},{\"end\":50638,\"start\":50625},{\"end\":50650,\"start\":50638},{\"end\":50670,\"start\":50650},{\"end\":50680,\"start\":50670},{\"end\":51186,\"start\":51177},{\"end\":51208,\"start\":51186},{\"end\":51219,\"start\":51208},{\"end\":51237,\"start\":51219},{\"end\":51251,\"start\":51237},{\"end\":51267,\"start\":51251},{\"end\":51280,\"start\":51267},{\"end\":51291,\"start\":51280},{\"end\":51304,\"start\":51291},{\"end\":51319,\"start\":51304},{\"end\":51334,\"start\":51319},{\"end\":51348,\"start\":51334},{\"end\":51712,\"start\":51700},{\"end\":51727,\"start\":51712},{\"end\":51739,\"start\":51727},{\"end\":52493,\"start\":52485},{\"end\":52507,\"start\":52493},{\"end\":52519,\"start\":52507},{\"end\":52529,\"start\":52519},{\"end\":52538,\"start\":52529},{\"end\":52549,\"start\":52538},{\"end\":52563,\"start\":52549},{\"end\":52581,\"start\":52563},{\"end\":52590,\"start\":52581},{\"end\":52601,\"start\":52590},{\"end\":52618,\"start\":52601},{\"end\":53495,\"start\":53479},{\"end\":53511,\"start\":53495},{\"end\":53526,\"start\":53511},{\"end\":53543,\"start\":53526},{\"end\":53565,\"start\":53543},{\"end\":53582,\"start\":53565},{\"end\":53596,\"start\":53582},{\"end\":54201,\"start\":54189},{\"end\":54212,\"start\":54201},{\"end\":54222,\"start\":54212},{\"end\":54235,\"start\":54222},{\"end\":54252,\"start\":54235},{\"end\":54841,\"start\":54824},{\"end\":54850,\"start\":54841},{\"end\":54868,\"start\":54850},{\"end\":54877,\"start\":54868},{\"end\":55276,\"start\":55262},{\"end\":55292,\"start\":55276},{\"end\":55315,\"start\":55292},{\"end\":55330,\"start\":55315},{\"end\":55354,\"start\":55330},{\"end\":55371,\"start\":55354},{\"end\":55387,\"start\":55371},{\"end\":55402,\"start\":55387},{\"end\":56229,\"start\":56212},{\"end\":56245,\"start\":56229},{\"end\":56261,\"start\":56245},{\"end\":56279,\"start\":56261},{\"end\":56297,\"start\":56279},{\"end\":56312,\"start\":56297},{\"end\":56327,\"start\":56312},{\"end\":56674,\"start\":56664},{\"end\":56687,\"start\":56674},{\"end\":56695,\"start\":56687},{\"end\":56709,\"start\":56695},{\"end\":56724,\"start\":56709},{\"end\":56735,\"start\":56724},{\"end\":56746,\"start\":56735},{\"end\":56762,\"start\":56746},{\"end\":56771,\"start\":56762},{\"end\":56783,\"start\":56771},{\"end\":57562,\"start\":57542},{\"end\":57575,\"start\":57562},{\"end\":57586,\"start\":57575},{\"end\":57601,\"start\":57586},{\"end\":57612,\"start\":57601},{\"end\":57627,\"start\":57612},{\"end\":57639,\"start\":57627},{\"end\":57652,\"start\":57639},{\"end\":58455,\"start\":58441},{\"end\":58470,\"start\":58455},{\"end\":58479,\"start\":58470},{\"end\":59450,\"start\":59433},{\"end\":59471,\"start\":59450},{\"end\":59488,\"start\":59471},{\"end\":59505,\"start\":59488},{\"end\":59519,\"start\":59505},{\"end\":59534,\"start\":59519},{\"end\":59552,\"start\":59534},{\"end\":59570,\"start\":59552},{\"end\":59584,\"start\":59570},{\"end\":59596,\"start\":59584},{\"end\":59616,\"start\":59596},{\"end\":59629,\"start\":59616},{\"end\":59645,\"start\":59629},{\"end\":59661,\"start\":59645},{\"end\":59675,\"start\":59661},{\"end\":59692,\"start\":59675},{\"end\":59701,\"start\":59692},{\"end\":59714,\"start\":59701},{\"end\":60360,\"start\":60347},{\"end\":60373,\"start\":60360},{\"end\":60386,\"start\":60373},{\"end\":60398,\"start\":60386},{\"end\":61323,\"start\":61308},{\"end\":61340,\"start\":61323},{\"end\":61356,\"start\":61340},{\"end\":61371,\"start\":61356},{\"end\":61384,\"start\":61371},{\"end\":61406,\"start\":61384},{\"end\":61424,\"start\":61406},{\"end\":61638,\"start\":61626},{\"end\":61648,\"start\":61638},{\"end\":61661,\"start\":61648},{\"end\":61673,\"start\":61661},{\"end\":61687,\"start\":61673},{\"end\":61699,\"start\":61687},{\"end\":61710,\"start\":61699},{\"end\":61722,\"start\":61710},{\"end\":61740,\"start\":61722},{\"end\":61758,\"start\":61740},{\"end\":61767,\"start\":61758},{\"end\":62111,\"start\":62102},{\"end\":62125,\"start\":62111},{\"end\":62139,\"start\":62125},{\"end\":62154,\"start\":62139},{\"end\":62167,\"start\":62154},{\"end\":62870,\"start\":62857},{\"end\":62891,\"start\":62870},{\"end\":62906,\"start\":62891},{\"end\":62919,\"start\":62906},{\"end\":62935,\"start\":62919},{\"end\":62950,\"start\":62935},{\"end\":62969,\"start\":62950},{\"end\":63354,\"start\":63339},{\"end\":63364,\"start\":63354},{\"end\":63383,\"start\":63364},{\"end\":63397,\"start\":63383},{\"end\":63615,\"start\":63595},{\"end\":63757,\"start\":63737},{\"end\":63772,\"start\":63757},{\"end\":64020,\"start\":64000},{\"end\":64028,\"start\":64020},{\"end\":64039,\"start\":64028},{\"end\":64053,\"start\":64039},{\"end\":64072,\"start\":64053},{\"end\":64086,\"start\":64072},{\"end\":64099,\"start\":64086},{\"end\":64116,\"start\":64099},{\"end\":64131,\"start\":64116},{\"end\":64146,\"start\":64131},{\"end\":64165,\"start\":64146},{\"end\":64179,\"start\":64165},{\"end\":64192,\"start\":64179},{\"end\":65654,\"start\":65643},{\"end\":65663,\"start\":65654},{\"end\":65672,\"start\":65663},{\"end\":65684,\"start\":65672},{\"end\":65722,\"start\":65684},{\"end\":65729,\"start\":65722},{\"end\":65743,\"start\":65729},{\"end\":65752,\"start\":65743},{\"end\":65766,\"start\":65752},{\"end\":65782,\"start\":65766},{\"end\":65795,\"start\":65782},{\"end\":66132,\"start\":66121},{\"end\":66158,\"start\":66132},{\"end\":66173,\"start\":66158},{\"end\":66180,\"start\":66173},{\"end\":66192,\"start\":66180},{\"end\":66204,\"start\":66192},{\"end\":66217,\"start\":66204},{\"end\":66627,\"start\":66614},{\"end\":66644,\"start\":66627},{\"end\":66658,\"start\":66644},{\"end\":66673,\"start\":66658},{\"end\":66693,\"start\":66673},{\"end\":66712,\"start\":66693},{\"end\":66724,\"start\":66712},{\"end\":66742,\"start\":66724},{\"end\":66753,\"start\":66742},{\"end\":66766,\"start\":66753},{\"end\":66781,\"start\":66766},{\"end\":67544,\"start\":67529},{\"end\":67563,\"start\":67544},{\"end\":67575,\"start\":67563},{\"end\":67590,\"start\":67575},{\"end\":67605,\"start\":67590},{\"end\":67620,\"start\":67605},{\"end\":67634,\"start\":67620},{\"end\":67650,\"start\":67634},{\"end\":67671,\"start\":67650},{\"end\":67688,\"start\":67671},{\"end\":67706,\"start\":67688},{\"end\":68186,\"start\":68172},{\"end\":68201,\"start\":68186},{\"end\":68216,\"start\":68201},{\"end\":68231,\"start\":68216},{\"end\":68244,\"start\":68231},{\"end\":68262,\"start\":68244},{\"end\":68277,\"start\":68262},{\"end\":68292,\"start\":68277},{\"end\":68308,\"start\":68292},{\"end\":68320,\"start\":68308},{\"end\":68338,\"start\":68320},{\"end\":68354,\"start\":68338},{\"end\":69197,\"start\":69183},{\"end\":69211,\"start\":69197},{\"end\":69225,\"start\":69211},{\"end\":69240,\"start\":69225},{\"end\":69255,\"start\":69240},{\"end\":69271,\"start\":69255},{\"end\":69283,\"start\":69271},{\"end\":69291,\"start\":69283},{\"end\":69304,\"start\":69291},{\"end\":69684,\"start\":69670},{\"end\":69700,\"start\":69684},{\"end\":70645,\"start\":70632},{\"end\":70656,\"start\":70645},{\"end\":70666,\"start\":70656},{\"end\":70682,\"start\":70666},{\"end\":70696,\"start\":70682},{\"end\":70704,\"start\":70696},{\"end\":70718,\"start\":70704},{\"end\":70731,\"start\":70718},{\"end\":71570,\"start\":71554},{\"end\":71588,\"start\":71570},{\"end\":71603,\"start\":71588},{\"end\":71618,\"start\":71603},{\"end\":71633,\"start\":71618},{\"end\":71645,\"start\":71633},{\"end\":72640,\"start\":72624},{\"end\":72656,\"start\":72640},{\"end\":72665,\"start\":72656},{\"end\":72679,\"start\":72665},{\"end\":72702,\"start\":72679},{\"end\":72708,\"start\":72702},{\"end\":72727,\"start\":72708},{\"end\":73158,\"start\":73143},{\"end\":73172,\"start\":73158},{\"end\":73188,\"start\":73172},{\"end\":73209,\"start\":73188},{\"end\":73225,\"start\":73209},{\"end\":73705,\"start\":73691},{\"end\":73722,\"start\":73705},{\"end\":73751,\"start\":73722},{\"end\":73765,\"start\":73751},{\"end\":74694,\"start\":74678},{\"end\":74708,\"start\":74694},{\"end\":74724,\"start\":74708},{\"end\":74745,\"start\":74724},{\"end\":74756,\"start\":74745},{\"end\":74768,\"start\":74756},{\"end\":74777,\"start\":74768},{\"end\":74790,\"start\":74777},{\"end\":74808,\"start\":74790},{\"end\":74814,\"start\":74808},{\"end\":75232,\"start\":75213},{\"end\":75246,\"start\":75232},{\"end\":75259,\"start\":75246},{\"end\":75749,\"start\":75735},{\"end\":75764,\"start\":75749},{\"end\":75773,\"start\":75764},{\"end\":75787,\"start\":75773},{\"end\":75809,\"start\":75787},{\"end\":75822,\"start\":75809},{\"end\":75843,\"start\":75822},{\"end\":76391,\"start\":76379},{\"end\":76401,\"start\":76391},{\"end\":76417,\"start\":76401},{\"end\":76431,\"start\":76417},{\"end\":76444,\"start\":76431},{\"end\":76457,\"start\":76444},{\"end\":76474,\"start\":76457},{\"end\":76484,\"start\":76474},{\"end\":76836,\"start\":76823},{\"end\":76848,\"start\":76836},{\"end\":76863,\"start\":76848},{\"end\":76872,\"start\":76863},{\"end\":76882,\"start\":76872},{\"end\":77314,\"start\":77296},{\"end\":77334,\"start\":77314},{\"end\":77350,\"start\":77334},{\"end\":77369,\"start\":77350},{\"end\":77387,\"start\":77369},{\"end\":77402,\"start\":77387},{\"end\":77417,\"start\":77402},{\"end\":78136,\"start\":78125},{\"end\":78151,\"start\":78136},{\"end\":78158,\"start\":78151},{\"end\":78174,\"start\":78158},{\"end\":78186,\"start\":78174},{\"end\":78202,\"start\":78186},{\"end\":78216,\"start\":78202},{\"end\":78233,\"start\":78216},{\"end\":78796,\"start\":78785},{\"end\":78806,\"start\":78796},{\"end\":78816,\"start\":78806},{\"end\":78832,\"start\":78816},{\"end\":79216,\"start\":79203},{\"end\":79225,\"start\":79216},{\"end\":79241,\"start\":79225},{\"end\":79256,\"start\":79241},{\"end\":79271,\"start\":79256},{\"end\":79293,\"start\":79271},{\"end\":79316,\"start\":79293}]", "bib_venue": "[{\"end\":38277,\"start\":38263},{\"end\":39398,\"start\":39363},{\"end\":41266,\"start\":41179},{\"end\":43996,\"start\":43975},{\"end\":44586,\"start\":44533},{\"end\":45800,\"start\":45728},{\"end\":46380,\"start\":46279},{\"end\":47073,\"start\":46983},{\"end\":48788,\"start\":48639},{\"end\":50841,\"start\":50769},{\"end\":51970,\"start\":51858},{\"end\":52812,\"start\":52787},{\"end\":53805,\"start\":53709},{\"end\":54365,\"start\":54349},{\"end\":54994,\"start\":54944},{\"end\":55621,\"start\":55530},{\"end\":56959,\"start\":56895},{\"end\":57864,\"start\":57785},{\"end\":58861,\"start\":58733},{\"end\":60738,\"start\":60591},{\"end\":62357,\"start\":62285},{\"end\":65223,\"start\":65202},{\"end\":66913,\"start\":66887},{\"end\":68530,\"start\":68466},{\"end\":70073,\"start\":69897},{\"end\":70939,\"start\":70850},{\"end\":71985,\"start\":71838},{\"end\":74078,\"start\":73929},{\"end\":74848,\"start\":74831},{\"end\":75420,\"start\":75348},{\"end\":76018,\"start\":75939},{\"end\":77574,\"start\":77504},{\"end\":78324,\"start\":78317},{\"end\":79475,\"start\":79404},{\"end\":38261,\"start\":38205},{\"end\":38836,\"start\":38775},{\"end\":39302,\"start\":39188},{\"end\":40174,\"start\":40088},{\"end\":40702,\"start\":40658},{\"end\":41177,\"start\":41091},{\"end\":42593,\"start\":42476},{\"end\":43549,\"start\":43544},{\"end\":43973,\"start\":43917},{\"end\":44531,\"start\":44463},{\"end\":45066,\"start\":44986},{\"end\":45726,\"start\":45639},{\"end\":46277,\"start\":46176},{\"end\":46981,\"start\":46895},{\"end\":47722,\"start\":47668},{\"end\":48117,\"start\":48062},{\"end\":48637,\"start\":48495},{\"end\":49388,\"start\":49318},{\"end\":49965,\"start\":49848},{\"end\":50767,\"start\":50680},{\"end\":51367,\"start\":51362},{\"end\":51856,\"start\":51770},{\"end\":52730,\"start\":52618},{\"end\":53707,\"start\":53596},{\"end\":54347,\"start\":54280},{\"end\":54942,\"start\":54877},{\"end\":55485,\"start\":55406},{\"end\":56346,\"start\":56341},{\"end\":56866,\"start\":56787},{\"end\":57783,\"start\":57689},{\"end\":58640,\"start\":58502},{\"end\":59795,\"start\":59734},{\"end\":60589,\"start\":60427},{\"end\":61427,\"start\":61424},{\"end\":61827,\"start\":61781},{\"end\":62283,\"start\":62196},{\"end\":63005,\"start\":62969},{\"end\":63401,\"start\":63397},{\"end\":63634,\"start\":63629},{\"end\":63861,\"start\":63786},{\"end\":65083,\"start\":65027},{\"end\":65814,\"start\":65809},{\"end\":66284,\"start\":66217},{\"end\":66885,\"start\":66816},{\"end\":67777,\"start\":67706},{\"end\":68437,\"start\":68358},{\"end\":69340,\"start\":69304},{\"end\":69895,\"start\":69720},{\"end\":70848,\"start\":70762},{\"end\":71836,\"start\":71674},{\"end\":72789,\"start\":72727},{\"end\":73319,\"start\":73225},{\"end\":73927,\"start\":73785},{\"end\":74829,\"start\":74814},{\"end\":75346,\"start\":75259},{\"end\":75937,\"start\":75843},{\"end\":76503,\"start\":76498},{\"end\":76956,\"start\":76882},{\"end\":77502,\"start\":77434},{\"end\":78315,\"start\":78233},{\"end\":78899,\"start\":78832},{\"end\":79402,\"start\":79316},{\"end\":80212,\"start\":79782},{\"end\":81043,\"start\":80933}]"}}}, "year": 2023, "month": 12, "day": 17}