{"id": 249146827, "updated": "2023-10-24 13:43:30.51", "metadata": {"title": "ByteGNN: Efficient Graph Neural Network Training at Large Scale", "authors": "[{\"first\":\"Chenguang\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Hongzhi\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Yuxuan\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Zhezheng\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Yifan\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Changji\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"James\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Shuai\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "Proceedings of the VLDB Endowment", "journal": "Proceedings of the VLDB Endowment", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Graph neural networks (GNNs) have shown excellent performance in a wide range of applications such as recommendation, risk control, and drug discovery. With the increase in the volume of graph data, distributed GNN systems become essential to support efficient GNN training. However, existing distributed GNN training systems suffer from various performance issues including high network communication cost, low CPU utilization, and poor end-to-end performance. In this paper, we propose ByteGNN, which addresses the limitations in existing distributed GNN systems with three key designs: (1) an abstraction of mini-batch graph sampling to support high parallelism, (2) a two-level scheduling strategy to improve resource utilization and to reduce the end-to-end GNN training time, and (3) a graph partitioning algorithm tailored for GNN workloads. Our experiments show that ByteGNN outperforms the state-of-the-art distributed GNN systems with up to 3.5-23.8 times faster end-to-end execution, 2-6 times higher CPU utilization, and around half of the network communication cost.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/pvldb/ZhengCCSWLCYZ22", "doi": "10.14778/3514061.3514069"}}, "content": {"source": {"pdf_hash": "1df8b8cc125f667c6495b76e347da621109b0f73", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "336bbd48f874524c35564bb814e18cf537624283", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1df8b8cc125f667c6495b76e347da621109b0f73.txt", "contents": "\nByteGNN: Efficient Graph Neural Network Training at Large Scale\n2022\n\nChenguang Zheng \nThe Chinese University of Hong Kong\n\n\nByteDacne Inc\n\n\nHongzhi Chen \nYuxuan Cheng jcheng@cse.cuhk.edu.hk \nByteDacne Inc\n\n\nZhezheng Song \nThe Chinese University of Hong Kong\n\n\nByteDacne Inc\n\n\nYifan Wu 3yifanwu@pku.edu.cn \nByteDacne Inc\n\n\nPeking University\n\n\nChangji Li \nThe Chinese University of Hong Kong\n\n\nByteDacne Inc\n\n\nJames Cheng \nThe Chinese University of Hong Kong\n\n\nHao Yang \nByteDacne Inc\n\n\nShuai Zhang \nThe Chinese University of Hong Kong\n\n\nByteDacne Inc\n\n\nChenguang Zheng \nHongzhi Chen \nYuxuan Cheng \nZhezheng Song \nYifan Wu \nChangji Li \nJames Cheng \nHao Yang \nShuai Zhang \nBytegnn \nByteGNN: Efficient Graph Neural Network Training at Large Scale\n\nEfficient Graph Neural Network Training at Large Scale. PVLDB\n156202210.14778/3514061.3514069\nGraph neural networks (GNNs) have shown excellent performance in a wide range of applications such as recommendation, risk control, and drug discovery. With the increase in the volume of graph data, distributed GNN systems become essential to support efficient GNN training. However, existing distributed GNN training systems suffer from various performance issues including high network communication cost, low CPU utilization, and poor end-to-end performance. In this paper, we propose ByteGNN, which addresses the limitations in existing distributed GNN systems with three key designs: (1) an abstraction of mini-batch graph sampling to support high parallelism, (2) a two-level scheduling strategy to improve resource utilization and to reduce the end-to-end GNN training time, and (3) a graph partitioning algorithm tailored for GNN workloads. Our experiments show that ByteGNN outperforms the state-ofthe-art distributed GNN systems with up to 3.5-23.8 times faster end-to-end execution, 2-6 times higher CPU utilization, and around half of the network communication cost. PVLDB Reference Format:\n\nfor machine learning. Recent research results [16,31,43,64,71,75] have shown that GNNs achieve significant performance improvements over traditional methods on many important tasks such as node classification, link predication, and graph clustering. GNNs have been applied in a broad range of applications including recommendation systems [52,75], computer vision [50,58], natural language processing [55,73], drug discovery [24], and social networks [68].\n\nAlthough many graph computing systems [8,9,18,22,27,42,49,51,67,72,79,81] have been proposed, they are designed for batch graph analytics workloads such as the computation of PageRank, shortest paths, label propagation and connected components, and thus they lack of operators for neural network training. Thus, dedicated GNN systems have been developed upon neural network training systems (e.g., TensorFlow, PyTorch) for GNN training.\n\nAmong existing GNN systems, most of them are still singlemachine systems, e.g., DGL [66], PyTorch Geometric (PyG) [21], NeuGraph [48], FeatGraph [33] and Seastar [70], which are optimized for training GNN models on a relatively small graph but cannot scale to process large graphs generally available in industry today. Note that for GNNs, a graph not only contains the graph topology information (which is typically used for computations such as PageRank, shortest paths, etc.), but each vertex and edge in the graph also contain a feature vector. Thus, depending on the dimensions of the feature vectors (typically around 100 to hundreds), the size of a graph for GNNs can be easily many times larger than the graph topology processed by existing graph computing systems. For example, for the Ogbn-Papers [32] graph used in our experiments, a feature vector has 128 dimensions and the size of the features is 4 times larger than the size of the graph topology.\n\nFor GNN training on large graphs, distributed systems such as Euler [1], GraphLearn (also called AliGraph) [80], AGL [77] and DistDGL [78] have been proposed. During the training, these systems collect and aggregate the feature vectors of the -hop neighbors in order to compute the feature vector of each vertex, where is the number of layers of the GNN model to be trained. However, the -hop neighbors of a vertex can be many, especially for a power-law graph, and a large portion of them can be located in remote machines. Thus, fetching all the -hop neighbors to a local machine for each vertex (referred to as full mini-batch training) incurs high network communication overheads and memory consumption.\n\nTo address the problem of full mini-batch training, mini-batch sampling training was proposed [13,31,34], which works as follows. Distributed GNN training is conducted in iterations and for each iteration, a machine processes a mini-batch of vertices in While distributed mini-batch sampling has become the default method for GNN training on a large graph (for which full-batch training and full mini-batch training are not practical), existing distributed GNN training systems suffer from a number of performance problems. One main problem is that sampling can take significantly longer time to complete than training, due to large amounts of random data access and remote data fetching involved in the sampling phase. For example, Figure 1 reports the average sampling time and training time in an epoch of training a 2-layered and 3-layered GraphSAGE model (both supervised and unsupervised) on four machines by GraphLearn [80] on the Reddit dataset [31] and Ogbn-Product dataset [32], which shows that the sampling phase takes an order of magnitude longer time than the training phase. For example, in the 2-layer supervised GraphSAGE training on Ogbn-Product, GraphLearn's training time is only 2.66s while its sampling time is 24.17s. Under the same setting, the sampling time of DistDGL [78] is also 4.22x of its training time.\n\nThe imbalance between the sampling and training phases also leads to the under-utilization of computing resources and the problem is worsen if GPUs are used for training (which further widens the gap between the sampling and training time) [59]. To address this imbalanced computing pattern in mini-batch GNN training, existing systems have attempted to apply neighborhood caching [46] and fixed size prefetching [78] to shorten the sampling time. However, it is difficult to set the right hyper-parameters (i.e., cache ratio and prefetching number) for training different GNN models on different graphs. Nextdoor [36] proposed to sample neighborhood using GPUs, but the GPU memory capacity limits the size of the graph it can handle. Graph partitioning has also been applied to reduce the cost of remote data fetching [80]. However, existing graph partitioning algorithms are designed for traditional graph workloads (e.g., distributed PageRank) and they do not consider the data access pattern and load balancing in GNN training.\n\nIn this paper, we propose ByteGNN, a distributed GNN training framework to support fast end-to-end GNN training in large graphs. To improve the efficiency of sampling, we abstract the sampling phase of a mini-batch as a directed acyclic graph (DAG) of small tasks, so that we can run DAGs and tasks within each DAG in parallel. The fine-grained task abstraction in DAG modeling also leads to the design of a two-level scheduling. First, coarse-grained scheduling determines how much resources should be used for minibatch sampling, in order to dynamically adjust the computation loads between the sampling and training phases to avoid resource contention and maximize CPU utilization. Then, fine-grained scheduling decides the execution order of tasks in the DAGs in order to pipeline the sampling outputs to be consumed by the training phase at the right pace. The two scheduling strategies work together to minimize the end-to-end GNN training time. We also propose an effective graph partitioning algorithm tailored for mini-batch graph sampling, which maintains the data locality according to the data access pattern of mini-batch sampling and balances the computation loads in the training, validation and testing stages.\n\nWe implemented ByteGNN based on GraphLearn [4]. Our performance evaluation shows that ByteGNN achieves significantly higher training throughput and is more scalable than the state-ofthe-art distributed GNN systems. Experimental results show that ByteGNN achieves up to 23.8x speedup over GraphLearn and 3.5x over DistDGL. The results verify that our system designs lead to efficient GNN training.\n\n\nBACKGROUND AND MOTIVATION\n\nWe first introduce the necessary background of GNN and briefly discuss sampling-based GNN training. Then, we motivate our work by presenting the limitations of existing systems for large-scale GNN training.\n\n\nGraph Neural Networks\n\nGNN models are designed to capture the information contained in both the relationship among vertices in a graph and the vertex/edge attributes. The core idea of GNNs is recursively aggregating the neighbor information, including the features of the neighbors and the features of the connecting edges, and then applying feature transformation functions. Take the GraphSAGE model [31] as an example. The training process for one layer of the model can be expressed as follows:\n\u210e N ( ) \u2190 ( {\u210e \u22121 , \u2200 \u2208 N ( ) }),(1)\u210e \u2190 ( \u00b7 (\u210e \u22121 , \u210e N ( ) )),(2)\nwhere N ( ) is the set of neighbors of vertex . In this -th convolution layer, each vertex first uses the AGGREGATE function to collect the feature vectors of 's neighbors from the ( \u2212 1)-th layer. The aggregation result is then concatenated with 's feature vector from the ( \u22121)-th layer, followed by a dot-product operation with a learnable weight matrix W. The dot-product result is further transformed by a nonlinearity activation function such as the sigmoid function, which gives the feature vector of for the -th layer.\n\nAs the number of layers increases, the vertices are required to gather and aggregate more and more information from neighbors that are farther away (i.e., expanding from the -hop neighbors to the -hop neighbors for > ). When the training for all the layers completes (for a user-specified ), the final feature vector \u210e for each vertex is fed into a mapping function for a specific downstream task (e.g., node classification, link prediction).\n\n\nDistributed Mini-Batch Graph Sampling\n\nExisting distributed GNN systems adopt data parallelism and sampling is commonly applied in order to train a GNN model on a large graph efficiently. However, the sampling process in distributed GNN training is quite different from that in training DNN models for computer vision and natural language processing, for which each sample is independent and small. For GNNs, the distributed training may access the entire neighborhood of a sampled vertex, including both vertices and edges along with their feature vectors. Due to the structural connection among vertices in different partitions, the data access pattern usually leads to high network communication cost.\n\nIn a -layered GNN model, for each sampled seed vertex , we need to obtain the -hop neighborhood of to construct a neighborhood subgraph to update 's feature vector. As most real-world graphs have a power-law degree distribution, the size of the -hop neighborhood subgraph of a vertex grows exponentially as the number of hops increases. To address this problem, mini-batch neighborhood sampling [46,78,80] has been used to sample a limited number of neighbors for each sampled seed vertex. Figure 2 illustrates how mini-batch graph sampling is applied in the training of a 2-layered GNN model. We show the sampled 2-hop neighborhood subgraphs of two seed vertices, 1 and 2 , where we set the sampling configuration 1 = 2 and 2 = 2, meaning that a vertex first samples at most 1 of its 1-hop neighbors, and then each of 's sampled 1-hop neighbors further samples 2 of 's neighbors. Remote sampling requests are sent to remote devices to access the -hop neighbors that are stored there. After the sampling finishes, the sampled neighbors, along with their attributes (which are used to construct the initial feature vectors), are fetched to the local device of 1 (and 2 ) to construct its sampled 2-hop neighborhood subgraph, which is then fed into the GNN model to calculate the gradients and update the model parameters.\n\nAlthough the tradeoff is a potential loss in the model accuracy, mini-batch graph neighborhood sampling still converges to the required model accuracy. Take the mean aggregator in Graph-SAGE [31] as an example, using the Monte Carlo estimation, for the layer , we obtain:\nE[\u210e N ( ) ] = E[ 1 | N ( ) | \u2211\ufe01 \u2208N ( ) \u210e \u22121 ] = 1 | N ( ) | \u2211\ufe01 \u2208N ( ) \u210e \u22121 = \u210e N ( ) ,\nwhere N ( ) is the set of random sampled neighbors of vertex and |N ( )| = , which is the fanout of layer . Unfortunately, though \u210e N ( ) is an unbiased estimator of \u210e N ( ) , \u210e is not an unbiased estimator of \u210e due to the non-linearity of (\u00b7) in Equation (2) [14]. Thus, the gradient is biased and the convergence of SGD is not guaranteed, unless the fanout goes to infinity. But in practice, GraphSAGE sets 1 = 25 and 2 = 10 to provide statistically significant gains over existing approaches [43,56]. In addition, AGL [77] also reported that with a suitable sample size, the sample can well approximate the ground truth. \n\n\nLimitations of Existing GNN Systems\n\nExisting systems for distributed GNN training suffer from the following major limitations.\n\n(1) The overhead of network communication is large. As the sampling procedure shows in Section 2.2, for every sampled seed vertex in each iteration, we need to construct 's -hop neighborhood subgraph together with the feature vectors of the vertices and edges in the subgraph. For example, in a 3-hop neighborhood subgraph where the sampling configuration is set at 1 = 15, 2 = 10 and 3 = 5 in the Reddit dataset [31], there are 915 vertices each with a 602-dimension feature vector, which are what we need to prepare for one sampled seed vertex. As many of the neighbors and their features may be stored in remote machines, the -hop neighborhood subgraph construction incurs a high network communication overhead. Figure 13(a) shows that the number of remote vertices is about six times that of local vertices with the widely used Hash partitioning. In fact, existing graph partitioning algorithms only consider to reduce the inter-partition edges, but do not consider the data access pattern and load balancing of graph sampling in GNN training. This calls for a new design of a more effective graph partitioning strategy tailored for GNN training.\n\n(2) CPU utilization is low. Our performance profiling shows that existing distributed GNN systems had poor CPU utilization as shown in Figure 3. By analyzing their system designs, we list the main causes to their low CPU utilization below.\n\nThe sampling phase of GraphLearn [80] is handled using the Gremlin semantics [2, 3] to express each sampling step. For each step, the Gremlin statement is translated by a parser and converted into several execution operations. An operation is a minimum execution unit in GraphLearn. GraphLearn has low CPU utilization since all the graph sampling operations within each device do not overlap with each other. DistDGL [78] takes a similar approach but also has many optimizations such as replicating the neighbors of its local vertices.\n\nEuler [1], on the other hand, wraps each graph operator into one TensorFlow dataflow operator. This design is convenient for users to build the whole computation graph in TensorFlow. However, as all the sampling operators and the training operators are contained in one big computation dataflow graph, existing deep learning systems (including TensorFlow) cannot process it efficiently. It is difficult to have the optimal execution order for the dataflow graph with the newly defined graph sampling operators, which is totally different from the normal tensor computation. Besides, due to the convergence requirement, TensorFlow only runs one dataflow graph at a time. Each iteration always starts graph sampling after the previous training process finishes. This design also eliminates the opportunities to apply the data prefetching mechanism to the independent sampling stages.\n\n(3) GPU does not bring enough benefit for GNN training in large graphs. As mentioned in Section 1, distributed GNN training on large graphs consists of the sampling phase and training phase. Due to limited GPU memory capacity, graph data are stored in the host memory of the machines and thus the sampling phase is conducted by CPUs. When GPUs are used to conduct the training phase, the sampling results are loaded into GPU memory from CPU host memory via PCIe links. As shown in [46,59], even in the case of single-machine GNN training using GPU (i.e., data are not fetched through network), the sampling and data loading time still take a significant portion of the end-to-end training time. The training phase can indeed be accelerated using GPUs (compared with using CPUs), but this only reduces the model updating time while the sampling phase still dominates the overall processing cost. This is because most GNN models are considerably small (unlike DNN models) and the training phase only needs to conduct model computation on densely packed vectors, while sampling a large graph involves large amounts of random data access and remote data fetching in order to construct the neighborhood subgraph for each sampled seed vertex.\n\nWe evaluated the performance of DistDGL [78] on a GPU server (40 cores Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz, 256 GB Memory, and one Nvidia RTX 2080Ti graphics card). We tested DistDGL with different fanout and hidden sizes to show the influence from the workloads of sampling and training. As Figure 4 shows, the largest difference in epoch time is only 10% between using GPU and using CPU. The average GPU utilization for GNN training is only around 20%, which is consistent with the GPU utilization of DGL reported in [46]. In fact, even if we purely use CPUs for the training phase, the sampling phase still dominates the overall cost as we have shown in Figure 1. In addition, we also need to consider the operational costs. GPU servers are expensive and GPU quota is more restricted to training DNNs even in big companies like ByteDance. In the cloud environment, Dorylus [61] also shows that \n\n\nWorkload Balancing Partitioner\n\n\nS-Worker\n\n\nS-Worker\n\n\nGraphStore\n\n\nRPC\n\n\nS-Worker S-Worker\n\n\nT-Worker T-Worker T-Worker T-Worker\n\n\nPS PS\n\n\n\u2026 \u2026\n\n\nS-Worker\n\n\nFigure 5: System architecture of ByteGNN\n\nGPU-based training is only cost-effective for small, dense graphs. All the above concerns therefore divert our focus to designing a CPU-based framework for large-scale GNN training.\n\nMotivation summary. The analysis above motivates us to design (1) a computation paradigm that uses only CPUs and aims to maximize CPU utilization by adaptively allocating computing resources to the sampling and training phases according to their needs, and (2) a new graph partitioning algorithm in order to reduce massive network communication caused by graph sampling in GNN training. Figure 5 shows the architecture of ByteGNN, which consists of four main components in each machine where ByteGNN is deployed. Graph Store stores a partition of the input graph data and the Graph Stores of all machines form a distributed Graph Store. PS is a parameter server that stores the model parameters. Sampling Worker (S-Worker), handles the sampling phase and constructs sampled neighborhood subgraphs for sampled seed vertices. Training Worker (T-Worker), handles the training phase, which computes model gradients on the sampled neighborhood subgraphs constructed by the S-Worker in the same machine and synchronizes the gradients with PS to update the model parameters.\n\n\nSYSTEM DESIGN\n\nIn Sections 3.1-3.3 we focus on three key designs in ByteGNN, which address the limitations of existing GNN systems discussed in Section 2.3. \n\n\nAbstraction of Mini-Batch Graph Sampling\n\nThe sampling process in existing GNN systems [21,78,80] is not well-organized as the tasks in each sampling phase are executed without overlapping, which often leads to CPU under-utilization.\n\nIn addition, sampling is conducted for one iteration (i.e., one minibatch) after another, even though different mini-batches are independent of each other. To support parallel sampling within a mini-batch and among mini-batches, so as to maximize CPU utilization, we model the sampling process as a DAG of tasks. Then, we can execute the DAGs of sampling multiple mini-batches in parallel. We also introduce a scheduler in Section 3.2 to effectively utilize the computing resources for both intra-DAG and inter-DAG parallelization, while balancing the loads between sampling and training so that one is not waiting for the other to finish in order to continue. To construct this DAG for a general GNN model, we analyzed the sampling phase of a broad range of existing GNN models, i.e., those that follow a similar neighborhood aggregation as described in Section 2.1, which cover most of the widely-adopted models such as GCN [43], GAT [64], GraphSAGE [31], PinSAGE [75], and Graph-SAINT [76]. We provide a common abstraction for the sampling phase of these GNN models with a set of five operators: (1) Seed Sampler: sampling a set of vertices as seeds from the local graph store; (2) Positive Sampler: sampling vertices from the direct neighbors of each seed; (3) Negative Sampler: sampling vertices from those that are not the direct neighbors of each seed; (4) Neighborhood Subgraph Construction (NSC): sampling vertices from the multi-hop neighborhood of a given vertex and constructing the sampled neighborhood subgraph; (5) Feature Fetching: fetching the attributes of a given vertex/edge to construct its feature vector.\n\nWith the above five operators, we can present the workflow of the sampling phase as a DAG, as shown in Figure 6. The DAG on the left of Figure 6 models supervised training, which consists of three tasks: Seed Sampling, NSC, and Feature Fetching. For unsupervised training, we also need to construct the neighborhood subgraphs of each positively and negatively sampled vertices of the seed vertices, as shown in the DAG on the right of Figure 6. The three branches in the DAG for unsupervised training can be executed in parallel, and the results are then collected in the \"End\" node to be fed into a T-Worker for training.\n\nTo enable higher parallelism for both supervised and unsupervised training, we create an instance of the two dominating operations (i.e., NSC and Feature Fetching, as they access multi-hop neighbors and their attributes) for each sampled vertex and execute these instances in parallel. In addition, as NSC (along with Feature Fetching) is executed repeatedly for each hop of neighborhood expansion, we can break the multi-hop operations into many smaller tasks of one-hop operations. As shown in Figure 6, each small task of Feature Fetching can start immediately when the corresponding small NSC task finishes. The more fine-grained task abstraction results in higher parallelism and better resource utilization (e.g., less head-of-line blocking and stragglers, less fragmentation in resource utilization).\n\nTo construct a DAG, users only need to specify the customized sampling functions in Seed Sampler, Positive Sampler, Negative Sampler, and also in NSC (e.g., how and how many neighbors in each hop should be sampled). This design also leaves space for researchers and engineers to explore new, high-quality sampling strategies using the framework. Note that the logical DAG is created only once and physical instances are generated and executed for each mini-batch by the S-Workers.\n\n\nTwo-Level Scheduling\n\nByteGNN adopts a two-level scheduling strategy to improve CPU utilization and reduce the end-to-end GNN training time. Although many scheduling strategies have been proposed, they are mostly for job scheduling at the cluster level [17, 19, 20, 25, 28-30, 35, 40, 47, 57, 60, 63, 74] or heterogeneous jobs/tasks in dataflow systems [39], which are over-complicated and incur extra overheads for scheduling the simple tasks in our system (note that for the training of a GNN model, we only need to schedule instances of the same DAG instead of many different DAGs).\n\nCoarse-grained scheduling. The S-Worker in each machine executes multiple DAGs in parallel to increase throughput and reduce the end-to-end GNN training time. The first question we need to answer is how many DAGs should be launched in a machine. If we launch too many DAGs, which means more resource is needed by sampling, then resource contention becomes a problem. Resource contention does not just occur among the DAGs, but also between sampling (i.e., DAG execution) and training (i.e., model computation). The training time increases significantly when too many DAGs are launched. On the other hand, if too few DAGs are running, the resource is under-utilized. The training phase finishes quickly and the next iteration's training waits for the neighborhood subgraphs to be produced by the DAGs.\n\nTo control the resource utilization, we need to decide when to launch a DAG. We can model this problem as a variation of the classical Job-Shop Scheduling Problem (JSP) [7]. Each DAG can be regarded as a job, where a set of operations (tasks) in each job need to be processed in a specific order, and we have a set of jobs that are to be processed on a given set of workers. Knowing the best timing for DAG launching is equivalent to getting the earliest starting time of each job in the solution to this special Job-Shop Scheduling Problem. The Job-Shop Scheduling Problem has been well studied and to find a schedule that minimizes the makespan or minimizes the sum of the job completion time was proved to be strongly NP-hard [11]. Some new research also shows that the currently best approximation algorithms have worse than logarithmic performance guarantee [26].\n\nWe propose a heuristic strategy to decide when to launch a DAG based on three runtime measures: util , size , and gap . util is the CPU utilization rate. If util is low, we may launch a new DAG; otherwise, we may wait until util drops to a suitable level. Note that high util does not necessarily result in better performance because there could be much contention and switching among DAGs and between sampling and training.\n\nIn addition to CPU utilization, We also need to consider the memory footprint. The neighborhood subgraph constructed from each DAG execution is kept in the DAG output queue in the S-Worker and size is the size of this queue. The neighborhood subgraphs are then consumed by the T-Worker for training. Thus, size is essentially an indicator of the speed of production (by the S-Worker) and the speed of consumption (by the T-Worker) of the neighborhood subgraphs. If size is small, we may launch new DAGs; otherwise, we pause the launching. If size is large, it implies an over-supply of neighborhood subgraphs and we may shift more computing resource from sampling to accelerate training. Thus, size not only controls the memory usage, but also balances the overall resource usage between sampling and training.\n\nWe also found that the real-time measure for util is not sensitive enough since newly launched DAGs may not change the CPU utilization in a short time period and many DAGs may be launched during the period. Later, when the tasks in these DAGs start to run in parallel and use up the computing resource, the system suffers from severe resource contention. To avoid such delayed performance punishments, we introduce gap , which is the time gap elapsed since the previous DAG launch. If gap is too small, we may want to wait for a bit longer before we launch a new DAG.\n\nIt would be undesirable if users need to set the thresholds for the three measures, as it is hard to determine what values of util , size , and gap are good and how to relate them to each other. To this end, we integrate them into one single score, launch-score, to decide whether we should launch a new DAG. The idea is to maintain the balance between the production speed and the consumption speed of neighborhood subgraphs, while keeping CPU utilization high. Ideally, we hope that the output of each DAG will be consumed immediately by the training phase, which means that size should be close to 0 all the time. However, in most of the cases a very low size happens with a very low util . Thus, we need to consider size together with util .\n\nAlgorithm 1 shows the algorithm for coarse-grained scheduling.\n\nFirst, we want to maintain = avg_sample avg_train * size = 1, where avg_sample and avg_train are the average time for sampling and training a mini-batch. If balance > 1, it means that it would take less time to consume the current size sampling results than to produce a new sampling result, which is an indicator that a new DAG should be launched. Next, we first attempt to use (100 \u2212 util ) to give a higher weight to balance if util is low and penalize balance (i.e., delay new DAG launching) when util is high. However, simply using (100 \u2212 util ) does not work well as it is a linear scale. Instead, we want to quickly increase CPU utilization when util is low and prevent contention promptly when util is already very high. Thus, we use an exponential function, ( util ) = 101 \u2212 util / , where = 100 101 is a constant used to align the range of ( util ) with that of util , i.e., (0) = 100, (100) = 0, and 0 \u2264 ( util ) \u2264 100. Finally, we also put gap as a weight to reflect the delay in the real-time measurement of util , which leads to the definition of launch-score in Algorithm 1.\n\nWe monitor launch-score in real time and launch a new DAG when launch-score \u2265 , where is a threshold set as follows. As shown in Algorithm 1 and explained above, launch-score connects , ( util ) and together to determine whether a new DAG job should be launched. In practice, there exist reasonable values of , ( util ) and for which a new DAG should be launched; Note that there are always trade-offs between and ( util ), e.g., a higher and a lower ( util ), to achieve a high launch-score. Such tradeoffs in runtime allows the system to automatically adjust the resource allocation to balance the sampling and training progress.\n\nFine-grained scheduling. After new DAGs are launched, the S-Worker executes the tasks in the DAGs, in parallel with the tasks in other DAGs. These tasks are put in a queue when their dependency is cleared (i.e., their parent tasks in the DAG are completed) and are handled by a pool of processing threads. If we execute the tasks in an FIFO order, some tasks of newly launched DAGs could be in front of the tasks in those almost-finished DAGs. For example, when the 1 pushes the \"END\" node in the task queue and there are already \"NSC\" tasks from 2 and 3 in the queue, the \"NSC\" tasks will be executed first and the \"END\" task will be processed later even although the \"END\" task is the last task in 1 , completing which will immediately return the sampled data to the T-Worker for training. Meanwhile, one task may unlock a lot of downstream tasks in the same DAG, and heavy tasks may block many light tasks. Thus, the average completion time of the DAG jobs and hence the end-to-end GNN training time can be significantly increased.\n\nWe schedule tasks according to the following orders: (1) tasks in a DAG with a smaller ID will be executed first; (2) tasks in the same DAG will be executed in ascending order of their costs. We assign a smaller ID to a DAG launched earlier to prioritize earlier DAGs to be \n- * | (val) | (val) - * | (test) | (test) ) end = argmax 1\u2264 \u2264 {CE[t] * BS[t]} = \u222a end return 1 , 2 , 3 , ......,\ncompleted first. We calculate the cost of a task by the data it needs to handle. For example, for sampling tasks in each hop of NSC, the cost is equal to the total number of neighbors of the input vertices; for Feature Fetching, the cost is the number of vertices/edges to be fetched multiplied by the vertex/edge feature dimension. As tasks may require data from remote machines, the S-Worker sends data fetching requests to the local Graph Store, which communicates with remote Graph Stores to fetch the data. The remote requests are also scheduled in a similar way and the network operations are processed concurrently with the CPU operations.\n\n\nGNN-based Graph Partitioning\n\nExisting graph partitioning algorithms [37,41,46] are mainly designed to reduce inter-partition edges and balance the workload. They have been widely adopted in distributed graph processing systems [27,53,81] to reduce inter-machine communication. However, sample-based GNN training focuses on the -hop neighborhood of only the vertices in the training, validation and test sets (instead of all vertices). For example, in Figure 7, traditional partitioning strategies cut the graph into two parts by the left dotted line since it not only balances the vertices but also has the least cut edge. But for a 2-layer GNN training, since vertex and vertex are the labeled vertices, partitioning by the right dotted line is actually a better choice. Even if this results in two cut edges, it would not cause any data movement in the training process as only the 2-hop neighbors of the labeled vertices are required.\n\nIn addition, the ratio of the sizes of the training, validation, and test sets of different real-world graphs may differ significantly. For example, in the Ogbn-Product dataset, the test set size is 11 times the training set size and 56 times the validation set size; while in the Ogbn-Papers dataset, the test set size is only 0.18 times the training set size and 1.7 times the validation set size. Thus, the partitioning algorithm should consider both the special data access pattern of -layer GNN training and the balanced distribution of the training, validation, and test sets.\n\nIt is known that the traditional graph partitioning problem is proved to be APX-hard [6]. Thus, our graph partitioning problem is also APX-hard as it can be reduced to the traditional graph partitioning problem. We propose a heuristic two-step graph partitioning strategy tailored for GNN sampling workloads. The main idea is to group vertices into multi-hop neighborhood-based blocks and Step (1) neighborhood block construction. To better preserve the locality of graph data for GNN sampling workloads, we construct a neighborhood block for each vertex in the training, validation and test sets. We start a -hop breadth-first search from each vertex ( is called the block center) and broadcast the unique block ID of to its -hop neighbors being visited. Every vertex only keeps the first block ID it receives, except for block centers which keep their own block ID. A block is then formed of all the vertices that keep the same block ID. Figure 7 demonstrates how to construct the blocks.\n\nStep (2) block assignment. Just as existing graph partitioning algorithms aim to balance the number of vertices in the partitions, our objective is to also balance the number of training, validation and test vertices in the partitions so that the work of training, validation and test is also balanced among the machines. Algorithm 2 shows how to assign the blocks. For each block , it is assigned to the partition with the highest score. is the set of vertices that have already been assigned to partition .\n\n[ ] is the number of cross-edges between and , which will be eliminated if is assigned to . Thus, the larger [ ] is, the more likely is assigned to . As the size of different partitions may vary during the assignment, we normalize [ ] by | |. [ ] is the balancing score that controls the number of training/validation/test vertices in partition to be close to the average value. means that more training vertices can be assigned to partition . The above applies to the validation and test vertices as well. In addition, we also use a weight to put more attention on a specific type of vertices according to the scale of that type in order to obtain a better overall performance. For example, if the number of training vertices is significantly more, we may set a larger to favor the training process, which can improve the end-to-end processing time.\n\nBefore the block assignment, we sort the blocks in descending order of max{| (train)|, | (val)|, | (test)|}. Then, we start the block assignment according to this order. In this way, larger blocks are assigned to different partitions first, so that smaller blocks may be used later to fill the partitions more easily when the partitions begin to fill up.\n\n\nSYSTEM IMPLEMENTATION\n\nWe implemented ByteGNN based on GraphLearn [4], using Tensor-Flow [5] as the backend deep learning framework for the training phase. We used the data loader and distributed graph storage in GraphLearn, where the graph topology data is stored in adjacency list format and the features are stored separately and indexed by their vertex/edge ID. Our implementation focuses on efficient DAG construction and execution, graph partitioning, and gradient synchronization.\n\nDAG construction and execution. We adopt the Gremlin syntax to help us construct the DAG. We redesigned the parsing method to encode necessary metadata from a Gremlin query for generating DAG nodes. Since one Gremlin statement may become several nodes in the final DAG, we implemented the parsing phase to carefully handle the complex dependency among the task nodes. We also changed all the communication methods from synchronous in GraphLearn to asynchronous in ByteGNN.\n\nGraph partitioning. We implemented our graph partitioning strategy on the streaming graph partitioning framework in [12,46]. The random start seed vertices in [12] were replaced with labeled vertices/edges. The framework first does the multi-source distributed BFS to build the -hop neighborhood blocks, and then applies our block assignment strategy in Section 3.3 to assign blocks to the partitions. The partitions are written into HDFS and then loaded by the system for sampling and training.\n\nGradient synchronization. To address the potential convergence issue, we implemented the bulk synchronous parallel (BSP) and stale synchronous parallel (SSP) models based on the Tensor-Flow API, so that users may also choose to use BSP or SSP to obtain faster model convergence and reduce the training time.\n\n\nSYSTEM EVALUATION\n\nWe evaluate the performance of ByteGNN by comparing with Graph-Learn [4], Euler [1] and Distributed DGL (DistDGL) [78]. We also examine the effects of our system designs on the performance.\n\nTestbed. We ran our experiments on a cluster of machines where each machine is equipped with 1T DDR4 main memory and two 2.40GHz Intel(R) Xeon(R) Platinum 8260 CPU (each CPU has 24 cores or 48 virtual cores by hyper-threading). All the machines are connected by a 25Gbps network and the OS is the Debian 9.13 with Linux kernel 4.19.117.\n\nDatasets. We used three datasets in the evaluation, as shown in Table 1. and -are the largest two graphs in the Open Graph Benchmark (OGB).\n\nis an undirected and unweighted graph modeling an Amazon product copurchasing network [32].\n\nis a directed citation graph of 111 million papers indexed by MAG [65]. The dataset is a directed graph in industry from the social network scenario.\n\nModels. We used three representative GNN models, Graph Convolutional Network (GCN) [43], GraphSAGE [31] and Graph Attention network(GAT) [64], in our evaluation. In order to demonstrate the expressiveness and efficiency of ByteGNN, we also tested the unsupervised variants of these three models. Although unsupervised learning shares most of the GNN architectures with supervised learning, it involves the negative sampling operator in the sampling phase and is also widely used in important tasks such as link prediction. Since many works are proposed to improve the sampling of GNN models, we used GraphSAINT [76] as a typical example to show how our sampling abstraction can be applied. As shown by prior works [16,32,37], deeper and larger GNN architectures can achieve better model accuracy. We used three network layers for the models and set the sampling configuration to 1 = 10, 2 = 5 and 3 = 3 for the neighborhood sampling models. The mini-batch size was set to 512 in all the experiments.\n\nSystems. We compared with three distributed GNN training systems, GraphLearn, Euler (v1.0) and DistDGL (DGL v0.5.3). GraphLearn is a distributed framework designed for the development and application of GNNs on large scale graphs within Alibaba. Euler is also developed by Alibaba but it has been used in many companies for large scale GNN training. Both GraphLearn and Euler use Ten-sorFlow as the backend system. DistDGL is a popular GNN system and its latest version (v0.5.3) supports distributed GNN training. The computational patterns of DistDGL are highly optimized by dedicated sparse tensor operations, which are currently lacking in ByteGNN as this work focuses on improving the sampling performance. Unless otherwise stated, we used the default configuration of these systems in our experiments. ByteGNN used the BSP model to obtain better test accuracy. All the systems adopt the random neighborhood sampling method as the default sampling method and use the same hop number and fanout.\n\n\nOverall Performance\n\nWe first compared the overall performance of the systems. We report the throughput of each system, i.e., the number of samples being processed per second, which is a metric commonly used to measure the performance of model training of a system. The throughput is calculated as the total number of seed vertices processed divided by the end-to-end GNN training time. Thus, the larger the throughput of a system, the shorter is the end-to-end GNN training time of the system. The hidden size is set to 32 in GCN and GraphSAGE. For GAT, we used 4 attention heads with hidden size 16. Since Euler failed to run unsupervised GAT training, we ignore this result. Figure 8 reports the results. ByteGNN achieves 7.5 to 16.2 times speedup compared with GraphLearn on supervised training and up to 23 times on unsupervised training. As ByteGNN is implemented on GraphLearn and the key differences from GraphLearn are the three system designs presented in Sections 3.  that our designs are effective. In particular, the performance improvement obtained by ByteGNN is more significant for unsupervised training that has more parallel sampling tasks, which is as a result of the high parallelism enabled for tasks within a DAG and among DAGs. Compared with Euler, although Euler also adopts the data-flow graph by TensorFlow for sampling the mini-batch neighborhood and training, ByteGNN can still achieve up to 4.7 times performance speedup. Euler cannot run two TensorFlow's computation graphs at the same time as otherwise it would lead to a convergence problem. In contrast, the separation of sampling phase and training phase in ByteGNN enables concurrent execution of multiple DAGs to maximize CPU utilization. , ByteGNN still has better performance. In the supervised training, ByteGNN is 1.5x and 1.3x faster than Dist-DGL in GCN and GraphSAGE. But the speedup is less significant compared with that on the dense graph, especially for GAT. This is because sparse tensor operations in the training phase of Dist-DGL have been highly optimized, while currently there is no such optimization in ByteGNN. For unsupervised training that has heavier sampling workloads, Figures 8\n(b)&(d)&(f)\nshow that ByteGNN achieves considerably better performance as ByteGNN's design enables higher parallelism in sampling execution. (e.g., 2.4x for unsupervised GraphSAGE and 1.6x for unsupervised GAT).\n\nWe also report the average CPU utilization of ByteGNN for training all the models in Figure 9. The result is reported for -, while ByteGNN's CPU utilization for the other two datasets is similar. Compared with the average CPU utilization of GraphLearn, Euler and DistDGL as shown in Figure 3, ByteGNN achieves 3 -6 times higher CPU utilization. ByteGNN has lower CPU utilization for supervised GCN and GraphSAGE because the number of neighborhood subgraphs in the DAG output queue is sufficient, S-Worker dynamically frees up some resource to T-Worker and the training workload for GCN is not heavy. Figure 10 reports the throughput scalability of the systems for the dataset, where we increase the number of machines from 4 to 64. ByteGNN achieves better scalability than all the other three systems. We omit the results for the other two datasets due to the page limitation, but the patterns are similar and ByteGNN's performance on the dense graph -is even better. The hidden size is set to 256 here to demonstrate the performance of our system in different configuration.\n\n\nScalability\n\nIn general, the throughput performance in distributed GNN training has sub-linear scalability due to the synchronization overhead (when the BSP model is used to achieve high accuracy) and heavy network I/O among the machines. GraphLearn   cause much synchronization overhead (though with potential accuracy loss). However, without an effective graph partitioning algorithm to preserve the locality of neighborhood access, remote data fetching results in high network communication overhead. In contrast, DistDGL's main issue in scalability is due to the synchronization overhead for gradient update. If the sampling output of a mini-batch cannot return on time, the trainer will get the forward loss later and all the other machines will wait for this loss to begin the back propagation. Even with the fixed prefetching mechanism, the possibility of the back propagation waiting increases as the number of machines increases. In comparison, ByteGNN's scheduling allows the sampling outputs to be pipelined to the trainers while other sampling processes continue, which results in better resource utilization. ByteGNN's GNN-tailored graph partitioning algorithm also leads to lower network communication overhead as the number of machines increases. As a result, ByteGNN achieves better scalability than the other systems.\n\n\nModel Accuracy\n\nWe also report the correctness of ByteGNN by evaluating the test accuracy of the GraphSAGE model on the -dataset, comparing with GraphLearn and DistDGL. Euler has similar accuracy as GraphLearn. In Figure 11, we report the test accuracy of different systems at every epoch until the training converges. The result shows that the systems achieve similar or the same accuracy eventually, but ByteGNN converges the fastest, in both the single-machine setting (1M) and distributed 4-machine setting (4M). We also note that as the mini-batch training can update the model many times in one epoch, the accuracy increases quickly in the first several epochs. The single-machine accuracy of GraphLearn can also be seen as the baseline to demonstrate that our code changes to GraphLearn do not affect the semantics of the GNN models. And as ByteGNN uses BSP to ensure model convergence in distributed training, it achieves approximately the same accuracy as DistDGL but uses less time. \n\n\nEvaluation on System Designs\n\nWe further evaluate the effectiveness of each individual system design in ByteGNN.\n\n\nSampling Abstraction.\n\nWe used the GraphSAINT model to demonstrate how to build a DAG using our sampling abstraction. Different from sampling neighbors across the layers, GraphSAINT constructs mini-batches by sampling the whole input graph once and then building a full GCN on the sampled subgraph. It provides three light-weight and efficient samplers, NodeSampler, EdgeSampler, and RandomWalkSampler. Due to space constraints, we only show the implementation of the Seed Sampler function in our sampling abstraction for GraphSAINT's NodeSampler and EdgeSampler. Note that the training part is the same for different samplers. . by ( \" E d g e W e i g h t \" ) . bothV ( ) For GraphSAINT's NodeSampler, we sample vertices from all the training vertices according to a vertex probability distribution ( ) \u221d ||\u02dc: , || 2 . We call this \"InDegree\" sampling as it is associated with the in-degree of each vertex. For EdgeSampler, the edge probability distribution follows ( , ) \u221d 1 ( ) + 1 ( ) . Normally, it can be pre-calculated dependent on the graph topology only and become the weight of edges. The code above shows the Seed Sampler function of these two samplers using our sampling abstraction. Using Gremlin, users can easily write the sampling logic. Then, the sampling stage can be completed by the NSC and Feature Fetching functions as discussed in Section 3.1.\n\nWe also implemented the GraphSAINT model in GraphLearn to compare the end-to-end training performance. Table 2 reports the speedup ratio of ByteGNN over GraphLearn for training Graph-SAINT on different graphs using four machines, using the same sampling setting from [76]. Even though GraphSAINT has a light sampling workload, ByteGNN can still achieve significant speedup compared with GraphLearn. Note that ByteGNN has better performance with EdgeSampler because EdgeSampler needs to obtain the two end-vertices of the sampled edge and has a higher workload than NodeSampler.  We first evaluate the performance of the coarse-grained scheduling strategy. We used three different batch sizes: 512, 1024 and 2048. We created a light sampling workload by setting the sampling configuration to 1 = 10, 2 = 5 and 3 = 3. We also created a heavy sampling workload by setting the sampling configuration to 1 = 15, 2 = 10 and 3 = 5.\n\nWe used two baselines. The first baseline is sequential DAG execution, which runs DAGs one after another. The second baseline is running a fixed number of DAGs at any time. The DAG size is set to 16 which is the same as the default in DistDGL prefetching. Table 3 shows that coarse-grained scheduling achieves the best performance in all cases. For sequential DAG execution, the execution of a single DAG at a time results in resource under-utilization and thus has poor performance. For the light sampling workload, the fixed number of DAGs has performance close to that of coarsegrained scheduling. This is because the sampling workload is light and can be finished quickly so that DAGs can already produce the sampling results fast enough for the trainer to consume. However, the lower sampling rate leads to more biased results and the light workload also results in resource under-utilization. When the sampling workload is heavier, the higher random data access overhead and higher network I/O cost to retrieve remote neighbors become the performance bottleneck. In this case, our coarse-grained scheduling strategy becomes effective as it dynamically adjusts the resource allocation to sampling and training in order to maximize resource utilization.\n\n\nFine-Grained Scheduling.\n\nWe further show the impact of the fine-grained scheduling strategy on the DAG completion time. We ran ByteGNN for 10 epochs and measured the completion time of each DAG of mini-batch sampling under two settings: using the priority-based scheduling in the fine-grained scheduling strategy and using FIFO-based scheduling. We use the box plot to report the distribution of the DAG completion time. Figure 12 shows that the priority-based scheduling can significantly reduce the completion time of the DAGs, and the DAG completion time is also more stable, which avoids the short time period of under-supply or over-supply of the sampling outputs. In supervised training, when the number of DAGs is in the suitable range, there are not too many small tasks in the DAGs so that the FIFO scheduling can handle it. However, unsupervised training launches more sampling tasks during the DAG execution. The median DAG completion time of the FIFO scheduling is almost two times greater than the median of the priority-based scheduling.\n\n\nGraph Partitioning.\n\nTo validate the effectiveness of our GNNtailored graph partitioning (GNN-P) algorithm, we compared GNN-P with three well-known graph partitioning methods: hash partitioning, Fennel partitioning [62] and METIS partitioning [41]. Both hash and METIS partitioning have been widely adopted in distributed graph computing systems. Fennel is the representative of one-pass streaming partitioning algorithms. Figure 13 reports the distribution of the requests for remote and local neighborhood data in one training epoch by each machine (the distributions of the requests for validation and test show a similar pattern). First, Figure 13(a) shows that hash partitioning achieves the best balanced distribution because hash partitioning assigns each type of vertices to different partitions with the same possibility. However, it does not consider the locality of neighborhood data access and thus incurs much higher remote data requests, which result in high network communication overhead. The number of remote requests is about 6.32 times the local data requests. Although METIS keeps the total number of vertices similar in each partition, the number of training vertices varies significantly among the partitions (also true for validation and test vertices). Half of the training vertices are assigned to one partition in Machine 1, which indeed reduces remote data requests; however, the imbalanced distribution results in Machine 1 being a severe straggler, which processes around 80% of the data requests in each training epoch. Fennel roughly balances the total load in each partition. Fennel considers data locality but it is only limited to direct neighbors, and thus remote data requests still take a major portion of the total number of data requests in each partition. In contrast, the multi-hop block construction of GNN-P significantly improves the data locality of each partition. The ratio of remote data requests and local data requests in the partitions of GNN-P is also considerably \n\n\nRELATED WORK\n\nSingle-machine GNN systems. PyG [21] integrates with Py-Torch [54] to provide a message-passing API for GNN training. Incorporated with the Apache TVM compiler [15], FeatGraph [33] generates optimized kernels for both CPU and GPU. But to implement new GNN operators, users need to have the background of TVM primitives. NeuGraph [48] proposes Scatter-ApplyEdge-Gather-ApplyVertex programming model to express GNN models and supports full-batch training in a single machine with multiple GPUs. It divides a graph into 2-D chunks and introduces a streaming scheduler to handle the CPU-GPU data transfer when GNN computation for a graph cannot fit in the GPU memory. Seastar [69,70] proposes a vertex-centric programming model to express GNN models using native Python syntax and identifies a common seastar computation pattern in GNN training to generate high-performance fused kernels. There are also works [36,46] that focus on addressing the bottleneck of mini-batch sampling. PaGraph [46] is a samplingbased training framework on multi-GPUs that addresses the expensive subgraph data loading issue by a GNN computation-aware cache policy. NextDoor [36] enables users to express the sampling tasks in GPUs by a high-level API and also proposes a novel transit parallelism approach to parallelize graph sampling. However, these single-machine systems have the limitation of processing large industrial graphs due to limited GPU memory.\n\nDistributed GNN systems. For training GNNs on large graphs in a distributed manner, AliGraph [80] provides sampling-based distributed GNN training and reduces network communication by caching vertices on local machines. DistDGL [78] uses a distributed in-memory key-value store to support efficient access to graph topology and feature data in distributed GNN training. DGCL [10] proposes an efficient communication library for distributed fullbatch GNN training on multi-GPUs using NVLink. DGCL needs to load all the graph data into GPUs first and is not suitable for training large graphs. Based on FlexFlow [38], a distributed DNN training framework, Roc [37] also adopts full-batch training in multi-GPUs using dynamic programming to minimize data swapping between host DRAM and GPU memory. P3 [23] reduces communication by model parallelism for the first layer, while it uses data parallelism for the remaining layers. However, if the hidden size is larger than the input dimension, it still incurs a high cost for the synchronization of the output of the first layer. AGL [77] uses MapReduce to preprocess a graph, which samples multiple neighborhood subgraphs for each vertex and stores them in a distributed file system. During training, AGL loads the required samples of neighborhood subgraphs of the vertices directly from the disk. However, the preprocessing cost is high and the storage overhead can also be large. Dorylus [61] designs a computation separation mechanism and pipelines the different computation patterns in the Amazon EC2 machine and serverless Lambdas in the cloud environment.\n\nGraph partitioning in GNN. METIS [41] is commonly used for graph partitioning in GNN algorithms [16,44,45] and systems [10,48,78]. Cluster-GCN [16] adopts METIS to build small clusters and then uses the partitions to perform an SGD update. DistDGL [78] adjusts the METIS algorithm to balance the training vertices in each partition. NeuGraph [48] uses the Kernighan-Lin algorithm to make the chunks in the diagonal have as many edges as possible. Roc [37] uses an linear-regression based algorithm to achieve balanced partitioning for both GNN training and inference; but it still treats all the vertices equally, which makes the computation load unbalanced. PaGraph [46] partitions a graph based on the neighborhood of a training vertex. However, with the multi-hop feature cache to avoid feature communication between different trainers, the memory overhead is too high.\n\n\nCONCLUSIONS\n\nWe presented ByteGNN, a distributed GNN training system for GNN training in large graphs. ByteGNN abstracts the sampling phase of a mini-batch as a DAG of small tasks to support high parallelism. Leveraging the DAG abstraction, ByteGNN designs a two-level scheduling to improve resource utilization and reduce the end-toend GNN training time. ByteGNN also tailors graph partitioning for GNN workloads to reduce network I/O and balance the workload. Experimental results show that ByteGNN can significantly shorten the end-to-end training time compared with existing distributed GNN systems.\n\nFigure 1 :\n1Sampling and training time of GraphLearn its partition in two phases: (1) the sampling phase -for each vertex in the mini-batch, the sampler samples a limited number of neighbors of in each hop, fetches the sampled remote neighbors to the local machine, and constructs the neighborhood subgraph of from its sampled neighbors locally; (2) the training phasethe trainer trains the model on the neighborhood subgraphs of the vertices in the mini-batch locally.\n\nFigure 3 :\n3The CPU utilization of different systems\n\nFigure 4 :\n4The epoch time of DistDGL with different hidden sizes and fanout on the Ogbn-product dataset GraphStore GraphStore GraphStore\n\nFigure 6 :\n6The DAG of the sampling workflow\n\nFigure 7 :\n7Traditional partitioning vs. GNN partitioning then assign these blocks to partitions by balancing the numbers of training, validation and test vertices in the partitions.\n\n\nFor example, the expected number of training vertices in each partition is (train) = | (train)|/ , where (train) is the set of all training vertices and is the total number of partitions. Let (train) be the set of training vertices currently in partition . Thus, a smaller | (train) | (train)\n\nFigure 8 :\n8The throughput of GraphLearn, Euler, DistDGL, and ByteGNN for training different models on 4 machines\n\nFigure 9 :\n9Average CPU utilization of ByteGNN Compared with DistDGL, ByteGNN achieves 2.1~3.5 times speedup for training the dense graph -in both supervised and unsupervised training. For the sparse graphs and\n\nFigure 10 :\n10Scalability comparison\n\nFigure 11 :\n11Accuracy comparison\n\nFigure 12 :\n12Distribution of DAG completion time\n\nFigure 13 :\n13The distribution of remote/local data requests smaller than that of the hash and Fennel partitions. In addition, with the balance-aware assignment algorithm, GNN-P achieves a much more balanced distribution of the total workload.\n\n\nAlgorithm 1: The Coarse-Grained Scheduling StrategyVariable: util , size , Given: =launch-score while more_dag do //more_dag=1 when more DAGs can be launched= \n\navg_sample \n\navg_train  * Q size ; \n( util ) = (101 \u2212 \n\n/ ), where = 100 \n\nln 101 \n; \nlaunch-score = \n* ( util ) * \n; \nif launch-score \u2265 then \nmore_dag = Launch_DAG(); \n// launches a new DAG; returns 0 when no more DAG to \nlaunch \n\n_ \n\n\u210e = \n() // used to calculate \nelse \nsleep(5ms); \nend \nend \n\n\n\nTable 1 :\n1Graph datasetsDataset \nOgbn-Product Ogbn-Papers \nSocial \n(Product) \n(Papers) \n\nVertices \n2,449,029 \n111,059,956 \n66,351,656 \nEdges \n123,718,280 \n1,615,685,872 1,751,915,191 \nFeature \n100 \n128 \n150 \nClasses \n47 \n172 \n2 \nTraining set \n196,615 \n1,207,179 \n6,631,989 \nValidation set \n39,323 \n125,265 \n19,908,461 \nTest set \n2,213,091 \n214,338 \n39,811,206 \n\n\n\n\nand Euler scale poorly and their throughputs are relatively low. Although GraphLearn and Euler are built on top of TensorFlow, the default asynchronous gradient update in distributed TensorFlow does not4 8 \n\n16 \n32 \n64 \n\n0 K \n\n100 K \n\n200 K \n\n300 K \n\n400 K \n\nNumber of Machines \n\nThroughput(Sample/s) \n\nGraphLearn \nEuler \nDistDGL \nByteGNN \n\n(a) GraphSAGE \n\n4 8 \n16 \n32 \n64 \n\n0 K \n\n100 K \n\n200 K \n\n300 K \n\n400 K \n\nNumber of Machines \n\nThroughput(Sample/s) \n\nGraphLearn \nEuler \nDistDGL \nByteGNN \n\n\n\nTable 2 :\n2Speedup ratio of ByteGNN over GraphLearnType \nOgbn-Product \nOgbn-Papers \nSocial \n\nNodeSampler \n3.40 \n2.80 \n4.05 \nEdgeSampler \n4.72 \n3.25 \n5.89 \n\n\n\nTable 3 :\n3The execution time (sec) of one epoch for different Coarse-Grained Scheduling.sampling settings, running on \n-\nusing 8 machines \n\n(a) The execution time of light sampling workload \n\nSequential Fixed DAGs Coarse-Grained \n\n512 \n79.04 \n19.56 \n18.62 \n1024 \n75.29 \n19.21 \n17.52 \n2048 \n74.52 \n19.90 \n17.75 \n\n(b) The execution time of heavy sampling workload \n\nSequential Fixed DAGs Coarse-Grained \n\n512 \n314.04 \n63.41 \n56.70 \n1024 \n312.14 \n68.72 \n57.20 \n2048 \n310.43 \n78.10 \n62.46 \n\n5.4.2 \nACKNOWLEDGMENTSWe thank the anonymous VLDB reviewers, for their constructive comments and suggestions that have helped greatly improve the quality of the paper.\nTensorFlow: A System for Large-Scale Machine Learning. Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, Xiaoqiang Zheng, 12th USENIX Symposium on Operating Systems Design and Implementation. Savannah, GA, USAMart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manju- nath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for Large-Scale Machine Learning. In 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016, Savannah, GA, USA, November 2-4, 2016. USENIX Association, 265-283. https://www.usenix.org/conference/osdi16/technical- sessions/presentation/abadi\n\nBalanced graph partitioning. Konstantin Andreev, Harald R\u00e4cke, 10.1145/1007912.1007931SPAA 2004: Proceedings of the Sixteenth Annual ACM Symposium on Parallelism in Algorithms and Architectures. Barcelona, SpainACMKonstantin Andreev and Harald R\u00e4cke. 2004. Balanced graph partitioning. In SPAA 2004: Proceedings of the Sixteenth Annual ACM Symposium on Parallelism in Algorithms and Architectures, June 27-30, 2004, Barcelona, Spain. ACM, 120-124. https://doi.org/10.1145/1007912.1007931\n\nA Computational Study of the Job-Shop Scheduling Problem. L David, William J Applegate, Cook, 10.1287/ijoc.3.2.149INFORMS J. Comput. 3David L. Applegate and William J. Cook. 1991. A Computational Study of the Job-Shop Scheduling Problem. INFORMS J. Comput. 3, 2 (1991), 149-156. https://doi.org/10.1287/ijoc.3.2.149\n\nGiraph: Large-scale graph processing infrastructure on hadoop. Ching Avery, Proceedings of the Hadoop Summit. the Hadoop SummitSanta Clara11Ching Avery. 2011. Giraph: Large-scale graph processing infrastructure on hadoop. Proceedings of the Hadoop Summit. Santa Clara 11 (2011).\n\nA faster algorithm for betweenness centrality. https:/arxiv.org/abs/https:/doi.org/10.1080/0022250X.2001.9990249The Journal of Mathematical Sociology. 25Ulrik BrandesUlrik Brandes. 2001. A faster algorithm for betweenness centrality. The Journal of Mathematical Sociology 25, 2 (2001), 163-177. https://doi.org/10.1080/0022250X. 2001.9990249 arXiv:https://doi.org/10.1080/0022250X.2001.9990249\n\nDGCL: an efficient communication library for distributed GNN training. Zhenkun Cai, Xiao Yan, Yidi Wu, Kaihao Ma, James Cheng, Fan Yu, 10.1145/3447786.3456233EuroSys '21: Sixteenth European Conference on Computer Systems, Online Event. United KingdomACMZhenkun Cai, Xiao Yan, Yidi Wu, Kaihao Ma, James Cheng, and Fan Yu. 2021. DGCL: an efficient communication library for distributed GNN training. In EuroSys '21: Sixteenth European Conference on Computer Systems, Online Event, United Kingdom, April 26-28, 2021. ACM, 130-144. https://doi.org/10.1145/ 3447786.3456233\n\nA Review of Machine Scheduling: Complexity, Algorithms and Approximability. Bo Chen, Chris Potts, Gerhard Woeginger, 10.1007/978-1-4613-0303-9_25Bo Chen, Chris Potts, and Gerhard Woeginger. 1998. A Review of Machine Scheduling: Complexity, Algorithms and Approximability. 21-169. https://doi. org/10.1007/978-1-4613-0303-9_25\n\nG-Miner: an efficient task-oriented graph mining system. Hongzhi Chen, Miao Liu, Yunjian Zhao, Xiao Yan, Da Yan, James Cheng, 10.1145/3190508.3190545Proceedings of the Thirteenth EuroSys Conference. the Thirteenth EuroSys ConferencePorto, PortugalACM32Hongzhi Chen, Miao Liu, Yunjian Zhao, Xiao Yan, Da Yan, and James Cheng. 2018. G-Miner: an efficient task-oriented graph mining system. In Proceedings of the Thirteenth EuroSys Conference, EuroSys 2018, Porto, Portugal, April 23-26, 2018. ACM, 32:1-32:12. https://doi.org/10.1145/3190508.3190545\n\nFastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling. Jie Chen, Tengfei Ma, Cao Xiao, 6th International Conference on Learning Representations. Vancouver, BC, CanadaConference Track Proceedings. OpenReview.netJie Chen, Tengfei Ma, and Cao Xiao. 2018. FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -May 3, 2018, Conference Track Proceedings. OpenReview.net. https: //openreview.net/forum?id=rytstxWAW\n\nStochastic Training of Graph Convolutional Networks with Variance Reduction. Jianfei Chen, Jun Zhu, Le Song, PMLRProceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningStockholmsm\u00e4ssan, Stockholm, Sweden80Proceedings of Machine Learning Research)Jianfei Chen, Jun Zhu, and Le Song. 2018. Stochastic Training of Graph Convolu- tional Networks with Variance Reduction. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Swe- den, July 10-15, 2018 (Proceedings of Machine Learning Research), Vol. 80. PMLR, 941-949. http://proceedings.mlr.press/v80/chen18p.html\n\nTVM: An Automated End-to-End Optimizing Compiler for Deep Learning. Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Q Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, Arvind Krishnamurthy, 13th USENIX Symposium on Operating Systems Design and Implementation. Carlsbad, CA, USAUSENIX AssociationTianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Q. Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: An Automated End-to-End Optimizing Compiler for Deep Learning. In 13th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2018, Carlsbad, CA, USA, October 8-10, 2018. USENIX Association, 578-594. https://www.usenix.org/conference/osdi18/ presentation/chen\n\nCluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks. Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, Cho-Jui Hsieh, 10.1145/3292500.3330925Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019. the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019Anchorage, AK, USAACMWei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. 2019. Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019. ACM, 257-266. https://doi.org/10.1145/3292500.3330925\n\nResource central: Understanding and predicting workloads for improved resource management in large cloud platforms. Eli Cortez, Anand Bonde, Alexandre Muzio, Mark Russinovich, Marcus Fontoura, Ricardo Bianchini, Proceedings of the 26th ACM Symposium on Operating Systems Principles. ACM. the 26th ACM Symposium on Operating Systems Principles. ACMEli Cortez, Anand Bonde, Alexandre Muzio, Mark Russinovich, Marcus Fontoura, and Ricardo Bianchini. 2017. Resource central: Understanding and predicting workloads for improved resource management in large cloud platforms. In Pro- ceedings of the 26th ACM Symposium on Operating Systems Principles. ACM, 153-167.\n\nWork-Efficient Parallel GPU Methods for Single-Source Shortest Paths. Andrew A Davidson, Sean Baxter, Michael Garland, John D Owens, 10.1109/IPDPS.2014.45IEEE 28th International Parallel and Distributed Processing Symposium. Phoenix, AZ, USAIEEE Computer SocietyAndrew A. Davidson, Sean Baxter, Michael Garland, and John D. Owens. 2014. Work-Efficient Parallel GPU Methods for Single-Source Shortest Paths. In 2014 IEEE 28th International Parallel and Distributed Processing Symposium, Phoenix, AZ, USA, May 19-23, 2014. IEEE Computer Society, 349-359. https://doi.org/10. 1109/IPDPS.2014.45\n\nParagon: QoS-aware scheduling for heterogeneous datacenters. Christina Delimitrou, Christos Kozyrakis, Proceedings of the 18th International Conference on Architectural Support for Programming Languages and Operating Systems. the 18th International Conference on Architectural Support for Programming Languages and Operating SystemsACM48Christina Delimitrou and Christos Kozyrakis. 2013. Paragon: QoS-aware sched- uling for heterogeneous datacenters. In Proceedings of the 18th International Conference on Architectural Support for Programming Languages and Operating Systems, Vol. 48. ACM, 77-88.\n\nQuasar: resource-efficient and QoS-aware cluster management. Christina Delimitrou, Christos Kozyrakis, Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems. the 19th International Conference on Architectural Support for Programming Languages and Operating SystemsACM42Christina Delimitrou and Christos Kozyrakis. 2014. Quasar: resource-efficient and QoS-aware cluster management. In Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems, Vol. 42. ACM, 127-144.\n\nFast Graph Representation Learning with PyTorch Geometric. Matthias Fey, Jan Eric Lenssen, arXiv:1903.02428Matthias Fey and Jan Eric Lenssen. 2019. Fast Graph Representation Learning with PyTorch Geometric. CoRR abs/1903.02428. arXiv:1903.02428 http://arxiv. org/abs/1903.02428\n\nMapGraph: A High Level API for Fast Development of High Performance Graph Analytics on GPUs. Zhisong Fu, Bryan B Thompson, Michael Personick, 10.1145/2621934.2621936Second International Workshop on Graph Data Management Experiences and Systems. Snowbird, Utah, USACWI/ACM2GRADES 2014Zhisong Fu, Bryan B. Thompson, and Michael Personick. 2014. MapGraph: A High Level API for Fast Development of High Performance Graph Analytics on GPUs. In Second International Workshop on Graph Data Management Experiences and Systems, GRADES 2014, co-loated with SIGMOD/PODS 2014, Snowbird, Utah, USA, June 22, 2014. CWI/ACM, 2:1-2:6. https://doi.org/10.1145/2621934.2621936\n\nP3: Distributed Deep Graph Learning at Scale. Swapnil Gandhi, Anand Padmanabha Iyer, 15th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2021. USENIX AssociationSwapnil Gandhi and Anand Padmanabha Iyer. 2021. P3: Distributed Deep Graph Learning at Scale. In 15th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2021, July 14-16, 2021. USENIX Association, 551-568. https://www.usenix.org/conference/osdi21/presentation/gandhi\n\nUtilising Graph Machine Learning within Drug Discovery and Development. Thomas Gaudelet, Ben Day, Arian R Jamasb, Jyothish Soman, Cristian Regep, Gertrude Liu, Jeremy B R Hayter, Richard Vickers, Charles Roberts, Jian Tang, David Roblin, Tom L Blundell, Michael M Bronstein, Jake P Taylor-King, arXiv:2012.05716Thomas Gaudelet, Ben Day, Arian R. Jamasb, Jyothish Soman, Cristian Regep, Gertrude Liu, Jeremy B. R. Hayter, Richard Vickers, Charles Roberts, Jian Tang, David Roblin, Tom L. Blundell, Michael M. Bronstein, and Jake P. Taylor-King. 2020. Utilising Graph Machine Learning within Drug Discovery and Develop- ment. CoRR abs/2012.05716 (2020). arXiv:2012.05716 https://arxiv.org/abs/2012. 05716\n\nFirmament: Fast, centralized cluster scheduling at scale. Ionel Gog, Malte Schwarzkopf, Adam Gleave, N M Robert, Steven Watson, Hand, Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation. the 12th USENIX Symposium on Operating Systems Design and ImplementationIonel Gog, Malte Schwarzkopf, Adam Gleave, Robert NM Watson, and Steven Hand. 2016. Firmament: Fast, centralized cluster scheduling at scale. In Proceed- ings of the 12th USENIX Symposium on Operating Systems Design and Implemen- tation. 99-115.\n\nBetter Approximation Guarantees for Job-shop Scheduling. Mike Leslie Ann Goldberg, Aravind Paterson, Elizabeth Srinivasan, Sweedyk, Proceedings of the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms. the Eighth Annual ACM-SIAM Symposium on Discrete AlgorithmsNew Orleans, Louisiana, USAACM/SIAMLeslie Ann Goldberg, Mike Paterson, Aravind Srinivasan, and Elizabeth Sweedyk. 1997. Better Approximation Guarantees for Job-shop Scheduling. In Proceedings of the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, 5-7 January 1997, New Orleans, Louisiana, USA. ACM/SIAM, 599-608. http://dl.acm.org/ citation.cfm?id=314161.314395\n\nPowerGraph: Distributed Graph-Parallel Computation on Natural Graphs. Joseph E Gonzalez, Yucheng Low, Haijie Gu, Danny Bickson, Carlos Guestrin, 10th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2012. Hollywood, CA, USAUSENIX AssociationJoseph E. Gonzalez, Yucheng Low, Haijie Gu, Danny Bickson, and Carlos Guestrin. 2012. PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs. In 10th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2012, Hollywood, CA, USA, October 8-10, 2012. USENIX Association, 17- 30. https://www.usenix.org/conference/osdi12/technical-sessions/presentation/ gonzalez\n\nMulti-resource packing for cluster schedulers. Robert Grandl, Ganesh Ananthanarayanan, Srikanth Kandula, Sriram Rao, Aditya Akella, Proceedings of the ACM SIGCOMM Computer Communication Review. the ACM SIGCOMM Computer Communication ReviewACM44Robert Grandl, Ganesh Ananthanarayanan, Srikanth Kandula, Sriram Rao, and Aditya Akella. 2015. Multi-resource packing for cluster schedulers. In Proceedings of the ACM SIGCOMM Computer Communication Review, Vol. 44. ACM, 455-466.\n\nAltruistic scheduling in multi-resource clusters. Robert Grandl, Mosharaf Chowdhury, Aditya Akella, Ganesh Ananthanarayanan, Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation. the 12th USENIX Symposium on Operating Systems Design and ImplementationRobert Grandl, Mosharaf Chowdhury, Aditya Akella, and Ganesh Anantha- narayanan. 2016. Altruistic scheduling in multi-resource clusters. In Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation. 65-80.\n\nGRAPHENE: Packing and Dependency-Aware Scheduling for Data-Parallel Clusters. Robert Grandl, Srikanth Kandula, Sriram Rao, Aditya Akella, Janardhan Kulkarni, Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation. the 12th USENIX Symposium on Operating Systems Design and ImplementationRobert Grandl, Srikanth Kandula, Sriram Rao, Aditya Akella, and Janardhan Kulkarni. 2016. GRAPHENE: Packing and Dependency-Aware Scheduling for Data-Parallel Clusters. In Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation. 81-97.\n\nInductive Representation Learning on Large Graphs. William L Hamilton, Zhitao Ying, Jure Leskovec, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Long Beach, CA, USADecember 4-9William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Represen- tation Learning on Large Graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, De- cember 4-9, 2017, Long Beach, CA, USA. 1024-1034. https://proceedings.neurips. cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html\n\nOpen Graph Benchmark: Datasets for Machine Learning on Graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. 2020Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020. Open Graph Benchmark: Datasets for Machine Learning on Graphs. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. https://proceedings.neurips.cc/paper/ 2020/hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html\n\nFeatGraph: A Flexible and Efficient Backend for Graph Neural Network Systems. Yuwei Hu, Zihao Ye, Minjie Wang, Jiali Yu, Da Zheng, Mu Li, Zheng Zhang, Zhiru Zhang, Yida Wang, arXiv:2008.11359Yuwei Hu, Zihao Ye, Minjie Wang, Jiali Yu, Da Zheng, Mu Li, Zheng Zhang, Zhiru Zhang, and Yida Wang. 2020. FeatGraph: A Flexible and Efficient Backend for Graph Neural Network Systems. CoRR abs/2008.11359 (2020). arXiv:2008.11359 https://arxiv.org/abs/2008.11359\n\nAdaptive Sampling Towards Fast Graph Representation Learning. Wen-Bing Huang, Tong Zhang, Yu Rong, Junzhou Huang, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. NeurIPS; Montr\u00e9al, CanadaWen-bing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. 2018. Adap- tive Sampling Towards Fast Graph Representation Learning. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural In- formation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Mon- tr\u00e9al, Canada. 4563-4572. https://proceedings.neurips.cc/paper/2018/hash/ 01eee509ee2f68dc6014898c309e86bf-Abstract.html\n\nQuincy: fair scheduling for distributed computing clusters. Michael Isard, Vijayan Prabhakaran, Jon Currey, Udi Wieder, Kunal Talwar, Andrew Goldberg, Proceedings of the 22nd ACM Symposium on Operating Systems Principles. ACM. the 22nd ACM Symposium on Operating Systems Principles. ACMMichael Isard, Vijayan Prabhakaran, Jon Currey, Udi Wieder, Kunal Talwar, and Andrew Goldberg. 2009. Quincy: fair scheduling for distributed computing clus- ters. In Proceedings of the 22nd ACM Symposium on Operating Systems Principles. ACM, 261-276.\n\nAccelerating graph sampling for graph machine learning using GPUs. Abhinav Jangda, Sandeep Polisetty, Arjun Guha, Marco Serafini, 10.1145/3447786.3456244EuroSys '21: Sixteenth European Conference on Computer Systems, Online Event. United KingdomACMAbhinav Jangda, Sandeep Polisetty, Arjun Guha, and Marco Serafini. 2021. Accel- erating graph sampling for graph machine learning using GPUs. In EuroSys '21: Sixteenth European Conference on Computer Systems, Online Event, United King- dom, April 26-28, 2021. ACM, 311-326. https://doi.org/10.1145/3447786.3456244\n\nImproving the Accuracy, Scalability, and Performance of Graph Neural Networks with Roc. Zhihao Jia, Sina Lin, Mingyu Gao, Matei Zaharia, Alex Aiken, Proceedings of Machine Learning and Systems 2020. Machine Learning and Systems 2020Austin, TX, USA2020Zhihao Jia, Sina Lin, Mingyu Gao, Matei Zaharia, and Alex Aiken. 2020. Improv- ing the Accuracy, Scalability, and Performance of Graph Neural Networks with Roc. In Proceedings of Machine Learning and Systems 2020, MLSys 2020, Austin, TX, USA, March 2-4, 2020. mlsys.org. https://proceedings.mlsys.org/book/300.pdf\n\nBeyond Data and Model Parallelism for Deep Neural Networks. Zhihao Jia, Matei Zaharia, Alex Aiken, Proceedings of Machine Learning and Systems. Machine Learning and SystemsMLSys; Stanford, CA, USAZhihao Jia, Matei Zaharia, and Alex Aiken. 2019. Beyond Data and Model Parallelism for Deep Neural Networks. In Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 -April 2, 2019. mlsys.org. https://proceedings.mlsys.org/book/265.pdf\n\nImproving resource utilization by timely fine-grained scheduling. Tatiana Jin, Zhenkun Cai, Boyang Li, Chengguang Zheng, Guanxian Jiang, James Cheng, 10.1145/3342195.3387551EuroSys '20: Fifteenth EuroSys Conference 2020. Heraklion, GreeceACM2016Tatiana Jin, Zhenkun Cai, Boyang Li, Chengguang Zheng, Guanxian Jiang, and James Cheng. 2020. Improving resource utilization by timely fine-grained sched- uling. In EuroSys '20: Fifteenth EuroSys Conference 2020, Heraklion, Greece, April 27-30, 2020. ACM, 20:1-20:16. https://doi.org/10.1145/3342195.3387551\n\nMIFO: A Query-Semantic Aware Resource Allocation Policy. Prajakta Kalmegh, Shivnath Babu, Proceedings of the 2019 ACM International Conference on Management of Data. the 2019 ACM International Conference on Management of DataACMPrajakta Kalmegh and Shivnath Babu. 2019. MIFO: A Query-Semantic Aware Re- source Allocation Policy. In Proceedings of the 2019 ACM International Conference on Management of Data. ACM, 1678-1695.\n\nA Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs. George Karypis, Vipin Kumar, 10.1137/S1064827595287997SIAM J. Sci. Comput. 20George Karypis and Vipin Kumar. 1998. A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs. SIAM J. Sci. Comput. 20, 1 (1998), 359-392. https://doi.org/10.1137/S1064827595287997\n\nCuSha: vertex-centric graph processing on GPUs. Farzad Khorasani, Keval Vora, Rajiv Gupta, Laxmi N Bhuyan, 10.1145/2600212.2600227The 23rd International Symposium on High-Performance Parallel and Distributed Computing, HPDC'14. Vancouver, BC, CanadaACMFarzad Khorasani, Keval Vora, Rajiv Gupta, and Laxmi N. Bhuyan. 2014. CuSha: vertex-centric graph processing on GPUs. In The 23rd International Symposium on High-Performance Parallel and Distributed Computing, HPDC'14, Vancouver, BC, Canada -June 23 -27, 2014. ACM, 239-252. https://doi.org/10.1145/2600212. 2600227\n\nSemi-Supervised Classification with Graph Convolutional Networks. N Thomas, Max Kipf, Welling, 5th International Conference on Learning Representations. Toulon, FranceConference Track Proceedings. OpenReview.netThomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. https://openreview.net/forum?id=SJU4ayYgl\n\nFast Haar Transforms for Graph Neural Networks. Ming Li, Zheng Ma, Yu Guang Wang, Xiaosheng Zhuang, 10.1016/j.neunet.2020.04.028Neural Networks. 128Ming Li, Zheng Ma, Yu Guang Wang, and Xiaosheng Zhuang. 2020. Fast Haar Transforms for Graph Neural Networks. Neural Networks 128 (2020), 188-198. https://doi.org/10.1016/j.neunet.2020.04.028\n\nGraph Neural Network Based Coarse-Grained Mapping Prediction. Zhiheng Li, P Geemi, Maghesree Wellawatte, Chakraborty, A Heta, Chenliang Gandhi, Andrew D Xu, White, arXiv:2007.04921Zhiheng Li, Geemi P. Wellawatte, Maghesree Chakraborty, Heta A. Gandhi, Chenliang Xu, and Andrew D. White. 2020. Graph Neural Network Based Coarse- Grained Mapping Prediction. CoRR abs/2007.04921 (2020). arXiv:2007.04921 https://arxiv.org/abs/2007.04921\n\nPaGraph: Scaling GNN training on large graphs via computation-aware caching. Zhiqi Lin, Cheng Li, Youshan Miao, Yunxin Liu, Yinlong Xu, 10.1145/3419111.3421281SoCC '20: ACM Symposium on Cloud Computing, Virtual Event. USAACMZhiqi Lin, Cheng Li, Youshan Miao, Yunxin Liu, and Yinlong Xu. 2020. PaGraph: Scaling GNN training on large graphs via computation-aware caching. In SoCC '20: ACM Symposium on Cloud Computing, Virtual Event, USA, October 19-21, 2020. ACM, 401-415. https://doi.org/10.1145/3419111.3421281\n\nElasecutor: Elastic Executor Scheduling in Data Analytics Systems. Libin Liu, Hong Xu, Proceedings of the ACM Symposium on Cloud Computing. ACM. the ACM Symposium on Cloud Computing. ACMLibin Liu and Hong Xu. 2018. Elasecutor: Elastic Executor Scheduling in Data Analytics Systems. In Proceedings of the ACM Symposium on Cloud Computing. ACM, 107-120.\n\nNeuGraph: Parallel Deep Neural Network Computation on Large Graphs. Lingxiao Ma, Zhi Yang, Youshan Miao, Jilong Xue, Ming Wu, Lidong Zhou, Yafei Dai, 2019 USENIX Annual Technical Conference, USENIX ATC 2019. Renton, WA, USAUSENIX AssociationLingxiao Ma, Zhi Yang, Youshan Miao, Jilong Xue, Ming Wu, Lidong Zhou, and Yafei Dai. 2019. NeuGraph: Parallel Deep Neural Network Computation on Large Graphs. In 2019 USENIX Annual Technical Conference, USENIX ATC 2019, Renton, WA, USA, July 10-12, 2019. USENIX Association, 443-458. https: //www.usenix.org/conference/atc19/presentation/ma\n\nPregel: a system for largescale graph processing. Grzegorz Malewicz, Matthew H Austern, J C Aart, James C Bik, Ilan Dehnert, Naty Horn, Grzegorz Leiser, Czajkowski, 10.1145/1807167.1807184SIGMOD. Grzegorz Malewicz, Matthew H. Austern, Aart J. C. Bik, James C. Dehnert, Ilan Horn, Naty Leiser, and Grzegorz Czajkowski. 2010. Pregel: a system for large- scale graph processing. In SIGMOD. 135-146. https://doi.org/10.1145/1807167. 1807184\n\nThe More You Know: Using Knowledge Graphs for Image Classification. Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta, 10.1109/CVPR.2017.102017 IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USAIEEE Computer SocietyKenneth Marino, Ruslan Salakhutdinov, and Abhinav Gupta. 2017. The More You Know: Using Knowledge Graphs for Image Classification. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017. IEEE Computer Society, 20-28. https://doi.org/10.1109/CVPR.2017.10\n\nScalable GPU graph traversal. Duane Merrill, Michael Garland, Andrew S Grimshaw, 10.1145/2145816.2145832Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel ProgrammingNew Orleans, LA, USAACM2012Duane Merrill, Michael Garland, and Andrew S. Grimshaw. 2012. Scalable GPU graph traversal. In Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP 2012, New Orleans, LA, USA, February 25-29, 2012. ACM, 117-128. https://doi.org/10.1145/2145816.2145832\n\nGeometric Matrix Completion with Recurrent Multi-Graph Neural Networks. Federico Monti, Michael M Bronstein, Xavier Bresson, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Long Beach, CA, USAFederico Monti, Michael M. Bronstein, and Xavier Bresson. 2017. Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural In- formation Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA. 3697-3707. http://papers.nips.cc/paper/6960-geometric-matrix-completion- with-recurrent-multi-graph-neural-networks\n\nDo We Need Specialized Graph Databases? Benchmarking Real-Time Social Networking Applications. Anil Pacaci, Alice Zhou, Jimmy Lin, M Tamer, \u00d6zsu, 10.1145/3078447.3078459Proceedings of the Fifth International Workshop on Graph Datamanagement Experiences & Systems, GRADES@SIGMOD/PODS 2017. the Fifth International Workshop on Graph Datamanagement Experiences & Systems, GRADES@SIGMOD/PODS 2017Chicago, IL, USAACM12Anil Pacaci, Alice Zhou, Jimmy Lin, and M. Tamer \u00d6zsu. 2017. Do We Need Specialized Graph Databases? Benchmarking Real-Time Social Networking Ap- plications. In Proceedings of the Fifth International Workshop on Graph Data- management Experiences & Systems, GRADES@SIGMOD/PODS 2017, Chicago, IL, USA, May 14 -19, 2017. ACM, 12:1-12:7. https://doi.org/10.1145/3078447.3078459\n\nPyTorch: An Imperative Style, High-Performance Deep Learning Library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, CanadaAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre- gory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems 32: An- nual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada. 8024-8035. https://proceedings. neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html\n\nLarge-Scale Hierarchical Text Classification with Recursively Regularized Deep Graph-CNN. Hao Peng, Jianxin Li, Yu He, Yaopeng Liu, Mengjiao Bao, Lihong Wang, Proceedings of the 2018 World Wide Web Conference on World Wide Web. the 2018 World Wide Web Conference on World Wide WebLyon, FranceYangqiu Song, and Qiang YangHao Peng, Jianxin Li, Yu He, Yaopeng Liu, Mengjiao Bao, Lihong Wang, Yangqiu Song, and Qiang Yang. 2018. Large-Scale Hierarchical Text Classification with Recursively Regularized Deep Graph-CNN. In Proceedings of the 2018 World Wide Web Conference on World Wide Web, WWW 2018, Lyon, France, April 23-27, 2018.\n\n. 10.1145/3178876.3186005ACMACM, 1063-1072. https://doi.org/10.1145/3178876.3186005\n\nDeepWalk: online learning of social representations. Bryan Perozzi, Rami Al-Rfou, Steven Skiena, 10.1145/2623330.2623732The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '14. New York, NY, USAACMBryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: online learning of social representations. In The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '14, New York, NY, USA -August 24 - 27, 2014. ACM, 701-710. https://doi.org/10.1145/2623330.2623732\n\nAutopilot: workload autoscaling at Google. Krzysztof Rzadca, Pawel Findeisen, Jacek Swiderski, Przemyslaw Zych, Przemyslaw Broniek, Jarek Kusmierek, Pawel Nowak, Beata Strack, Piotr Witusowski, Steven Hand, John Wilkes, 10.1145/3342195.3387524EuroSys '20: Fifteenth EuroSys Conference 2020. Heraklion, GreeceACM1616Krzysztof Rzadca, Pawel Findeisen, Jacek Swiderski, Przemyslaw Zych, Przemys- law Broniek, Jarek Kusmierek, Pawel Nowak, Beata Strack, Piotr Witusowski, Steven Hand, and John Wilkes. 2020. Autopilot: workload autoscaling at Google. In EuroSys '20: Fifteenth EuroSys Conference 2020, Heraklion, Greece, April 27-30, 2020. ACM, 16:1-16:16. https://doi.org/10.1145/3342195.3387524\n\nFew-Shot Learning with Graph Neural Networks. Garcia Victor, Joan Bruna Satorras, Estrach, 6th International Conference on Learning Representations. Vancouver, BC, CanadaConference Track Proceedings. OpenReview.netVictor Garcia Satorras and Joan Bruna Estrach. 2018. Few-Shot Learning with Graph Neural Networks. In 6th International Conference on Learning Representa- tions, ICLR 2018, Vancouver, BC, Canada, April 30 -May 3, 2018, Conference Track Proceedings. OpenReview.net. https://openreview.net/forum?id=BJj6qGbRW\n\nScalable Graph Neural Network Training: The Case for Sampling. Marco Serafini, 10.1145/3469379.3469387ACM SIGOPS Oper. Syst. Rev. 55Marco Serafini. 2021. Scalable Graph Neural Network Training: The Case for Sampling. ACM SIGOPS Oper. Syst. Rev. 55, 1 (2021), 68-76. https://doi.org/10. 1145/3469379.3469387\n\nROSE: Cluster Resource Scheduling via Speculative Over-Subscription. Xiaoyang Sun, Chunming Hu, Renyu Yang, Peter Garraghan, Tianyu Wo, Jie Xu, Jianyong Zhu, Chao Li, IEEE 38th International Conference on Distributed Computing Systems. IEEEXiaoyang Sun, Chunming Hu, Renyu Yang, Peter Garraghan, Tianyu Wo, Jie Xu, Jianyong Zhu, and Chao Li. 2018. ROSE: Cluster Resource Scheduling via Speculative Over-Subscription. In 2018 IEEE 38th International Conference on Distributed Computing Systems. IEEE, 949-960.\n\nDorylus: Affordable, Scalable, and Accurate GNN Training with Distributed CPU Servers and Serverless Threads. John Thorpe, Yifan Qiao, Jonathan Eyolfson, Shen Teng, Guanzhou Hu, Zhihao Jia, Jinliang Wei, Keval Vora, Ravi Netravali, Miryung Kim, Guoqing Harry Xu, 15th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2021. USENIX AssociationJohn Thorpe, Yifan Qiao, Jonathan Eyolfson, Shen Teng, Guanzhou Hu, Zhihao Jia, Jinliang Wei, Keval Vora, Ravi Netravali, Miryung Kim, and Guoqing Harry Xu. 2021. Dorylus: Affordable, Scalable, and Accurate GNN Training with Dis- tributed CPU Servers and Serverless Threads. In 15th USENIX Symposium on Op- erating Systems Design and Implementation, OSDI 2021, July 14-16, 2021. USENIX Association, 495-514. https://www.usenix.org/conference/osdi21/presentation/ thorpe\n\nFENNEL: streaming graph partitioning for massive scale graphs. Charalampos E Tsourakakis, Christos Gkantsidis, Bozidar Radunovic, Milan Vojnovic, 10.1145/2556195.2556213Seventh ACM International Conference on Web Search and Data Mining, WSDM 2014. New York, NY, USAACMCharalampos E. Tsourakakis, Christos Gkantsidis, Bozidar Radunovic, and Milan Vojnovic. 2014. FENNEL: streaming graph partitioning for massive scale graphs. In Seventh ACM International Conference on Web Search and Data Mining, WSDM 2014, New York, NY, USA, February 24-28, 2014. ACM, 333-342. https://doi.org/ 10.1145/2556195.2556213\n\nTetriSched: global rescheduling with adaptive plan-ahead in dynamic heterogeneous clusters. Alexey Tumanov, Timothy Zhu, Jun Woo Park, A Michael, Mor Kozuch, Gregory R Harchol-Balter, Ganger, Proceedings of the 11th European Conference on Computer Systems. the 11th European Conference on Computer SystemsACM35Alexey Tumanov, Timothy Zhu, Jun Woo Park, Michael A Kozuch, Mor Harchol- Balter, and Gregory R Ganger. 2016. TetriSched: global rescheduling with adap- tive plan-ahead in dynamic heterogeneous clusters. In Proceedings of the 11th European Conference on Computer Systems. ACM, 35.\n\nGraph Attention Networks. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio, 6th International Conference on Learning Representations. Vancouver, BC, CanadaConference Track Proceedings. OpenReview.netPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. 2018. Graph Attention Networks. In 6th Interna- tional Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -May 3, 2018, Conference Track Proceedings. OpenReview.net. https: //openreview.net/forum?id=rJXMpikCZ\n\nMicrosoft Academic Graph: When experts are not enough. Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, Anshul Kanakia, 10.1162/qss_a_00021Quant. Sci. Stud. 1Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul Kanakia. 2020. Microsoft Academic Graph: When experts are not enough. Quant. Sci. Stud. 1, 1 (2020), 396-413. https://doi.org/10.1162/qss_a_ 00021\n\nDeep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs. Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li, Jinjing Zhou, Qi Huang, Chao Ma, Ziyue Huang, Qipeng Guo, Hao Zhang, Haibin Lin, Junbo Zhao, Jinyang Li, Alexander J Smola, Zheng Zhang, arXiv:1909.01315Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li, Jinjing Zhou, Qi Huang, Chao Ma, Ziyue Huang, Qipeng Guo, Hao Zhang, Haibin Lin, Junbo Zhao, Jinyang Li, Alexander J. Smola, and Zheng Zhang. 2019. Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs. CoRR abs/1909.01315 (2019). arXiv:1909.01315 http://arxiv.org/abs/1909.01315\n\nGunrock: a high-performance graph processing library on the GPU. Yangzihao Wang, Andrew A Davidson, Yuechao Pan, Yuduo Wu, Andy Riffel, John D Owens, 10.1145/2851141.2851145Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel ProgrammingBarcelona, SpainACM11Yangzihao Wang, Andrew A. Davidson, Yuechao Pan, Yuduo Wu, Andy Riffel, and John D. Owens. 2016. Gunrock: a high-performance graph processing library on the GPU. In Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP 2016, Barcelona, Spain, March 12-16, 2016. ACM, 11:1-11:12. https://doi.org/10.1145/2851141.2851145\n\nDeep Reasoning with Knowledge Graph for Social Relationship Understanding. Zhouxia Wang, Tianshui Chen, Jimmy S J Ren, Weihao Yu, Hui Cheng, Liang Lin, 10.24963/ijcai.2018/142Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018. the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018Stockholm, Sweden. ijcai.orgZhouxia Wang, Tianshui Chen, Jimmy S. J. Ren, Weihao Yu, Hui Cheng, and Liang Lin. 2018. Deep Reasoning with Knowledge Graph for Social Relationship Understanding. In Proceedings of the Twenty-Seventh International Joint Confer- ence on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden. ijcai.org, 1021-1028. https://doi.org/10.24963/ijcai.2018/142\n\nVertex-Centric Visual Programming for Graph Neural Networks. Yidi Wu, Yuntao Gui, Tatiana Jin, James Cheng, Xiao Yan, Peiqi Yin, Yufei Cai, Bo Tang, Fan Yu, 10.1145/3448016.3452770SIGMOD '21: International Conference on Management of Data, Virtual Event. Guoliang Li, Zhanhuai Li, Stratos Idreos, and Divesh SrivastavaChinaACMYidi Wu, Yuntao Gui, Tatiana Jin, James Cheng, Xiao Yan, Peiqi Yin, Yufei Cai, Bo Tang, and Fan Yu. 2021. Vertex-Centric Visual Programming for Graph Neural Networks. In SIGMOD '21: International Conference on Management of Data, Virtual Event, China, June 20-25, 2021, Guoliang Li, Zhanhuai Li, Stratos Idreos, and Divesh Srivastava (Eds.). ACM, 2803-2807. https://doi.org/10.1145/3448016. 3452770\n\nSeastar: vertex-centric programming for graph neural networks. Yidi Wu, Kaihao Ma, Zhenkun Cai, Tatiana Jin, Boyang Li, Chengguang Zheng, James Cheng, Fan Yu, 10.1145/3447786.3456247EuroSys '21: Sixteenth European Conference on Computer Systems, Online Event. United KingdomACMYidi Wu, Kaihao Ma, Zhenkun Cai, Tatiana Jin, Boyang Li, Chengguang Zheng, James Cheng, and Fan Yu. 2021. Seastar: vertex-centric programming for graph neural networks. In EuroSys '21: Sixteenth European Conference on Computer Systems, Online Event, United Kingdom, April 26-28, 2021. ACM, 359-375. https: //doi.org/10.1145/3447786.3456247\n\nHow Powerful are Graph Neural Networks. Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USAKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Powerful are Graph Neural Networks?. In 7th International Conference on Learning Rep- resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=ryGs6iA5Km\n\nBlogel: A Block-Centric Framework for Distributed Computation on Real-World Graphs. Da Yan, James Cheng, Yi Lu, Wilfred Ng, Proc. VLDB. VLDBDa Yan, James Cheng, Yi Lu, and Wilfred Ng. 2014. Blogel: A Block-Centric Framework for Distributed Computation on Real-World Graphs. Proc. VLDB\n\n. Endow, 10.14778/2733085.27331037Endow. 7, 14 (2014), 1981-1992. https://doi.org/10.14778/2733085.2733103\n\nGraph Convolutional Networks for Text Classification. Liang Yao, Chengsheng Mao, Yuan Luo, 10.1609/aaai.v33i01.33017370The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019. Honolulu, Hawaii, USAAAAI PressLiang Yao, Chengsheng Mao, and Yuan Luo. 2019. Graph Convolutional Net- works for Text Classification. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelli- gence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -February 1, 2019. AAAI Press, 7370-7377. https://doi.org/10.1609/aaai.v33i01.33017370\n\nOpERA: opportunistic and efficient resource allocation in Hadoop YARN by harnessing idle resources. Yi Yao, Han Gao, Jiayin Wang, Ningfang Mi, Bo Sheng, Proceedings of the 25th International Conference on Computer Communication and Networks. the 25th International Conference on Computer Communication and NetworksIEEEYi Yao, Han Gao, Jiayin Wang, Ningfang Mi, and Bo Sheng. 2016. OpERA: opportunistic and efficient resource allocation in Hadoop YARN by harnessing idle resources. In Proceedings of the 25th International Conference on Computer Communication and Networks. IEEE, 1-9.\n\nGraph Convolutional Neural Networks for Web-Scale Recommender Systems. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, Jure Leskovec, 10.1145/3219819.3219890Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningLondon, UKACMRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale Recommender Systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018. ACM, 974-983. https://doi.org/10.1145/3219819.3219890\n\nGraphSAINT: Graph Sampling Based Inductive Learning Method. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, Viktor K Prasanna, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna. 2020. GraphSAINT: Graph Sampling Based Inductive Learning Method. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https: //openreview.net/forum?id=BJe8pkHFwS\n\nAGL: A Scalable System for Industrial-purpose Graph Machine Learning. Dalong Zhang, Xin Huang, Ziqi Liu, Jun Zhou, Zhiyang Hu, Xianzheng Song, Zhibang Ge, Lin Wang, Zhiqiang Zhang, Yuan Qi, 10.14778/3415478.3415539Proc. VLDB Endow. VLDB Endow13Dalong Zhang, Xin Huang, Ziqi Liu, Jun Zhou, Zhiyang Hu, Xianzheng Song, Zhibang Ge, Lin Wang, Zhiqiang Zhang, and Yuan Qi. 2020. AGL: A Scalable System for Industrial-purpose Graph Machine Learning. Proc. VLDB Endow. 13, 12 (2020), 3125-3137. https://doi.org/10.14778/3415478.3415539\n\nDistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs. Da Zheng, Chao Ma, Minjie Wang, Jinjing Zhou, Qidong Su, Xiang Song, Quan Gan, Zheng Zhang, George Karypis, arXiv:2010.05337Da Zheng, Chao Ma, Minjie Wang, Jinjing Zhou, Qidong Su, Xiang Song, Quan Gan, Zheng Zhang, and George Karypis. 2020. DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs. CoRR abs/2010.05337 (2020). arXiv:2010.05337 https://arxiv.org/abs/2010.05337\n\nMedusa: Simplified Graph Processing on GPUs. Jianlong Zhong, Bingsheng He, 10.1109/TPDS.2013.111IEEE Trans. Parallel Distributed Syst. 25Jianlong Zhong and Bingsheng He. 2014. Medusa: Simplified Graph Processing on GPUs. IEEE Trans. Parallel Distributed Syst. 25, 6 (2014), 1543-1552. https: //doi.org/10.1109/TPDS.2013.111\n\nAliGraph: A Comprehensive Graph Neural Network Platform. Rong Zhu, Kun Zhao, Hongxia Yang, Wei Lin, Chang Zhou, Baole Ai, Yong Li, Jingren Zhou, 10.14778/3352063.3352127Proc. VLDB Endow. VLDB Endow12Rong Zhu, Kun Zhao, Hongxia Yang, Wei Lin, Chang Zhou, Baole Ai, Yong Li, and Jingren Zhou. 2019. AliGraph: A Comprehensive Graph Neural Network Platform. Proc. VLDB Endow. 12, 12 (2019), 2094-2105. https://doi.org/10.14778/ 3352063.3352127\n\nGemini: A Computation-Centric Distributed Graph Processing System. Xiaowei Zhu, Wenguang Chen, Weimin Zheng, Xiaosong Ma, 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016, Savannah. GA, USAUSENIX AssociationXiaowei Zhu, Wenguang Chen, Weimin Zheng, and Xiaosong Ma. 2016. Gemini: A Computation-Centric Distributed Graph Processing System. In 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016, Sa- vannah, GA, USA, November 2-4, 2016. USENIX Association, 301-316. https: //www.usenix.org/conference/osdi16/technical-sessions/presentation/zhu\n", "annotations": {"author": "[{\"end\":141,\"start\":71},{\"end\":155,\"start\":142},{\"end\":208,\"start\":156},{\"end\":277,\"start\":209},{\"end\":343,\"start\":278},{\"end\":409,\"start\":344},{\"end\":460,\"start\":410},{\"end\":486,\"start\":461},{\"end\":553,\"start\":487},{\"end\":570,\"start\":554},{\"end\":584,\"start\":571},{\"end\":598,\"start\":585},{\"end\":613,\"start\":599},{\"end\":623,\"start\":614},{\"end\":635,\"start\":624},{\"end\":648,\"start\":636},{\"end\":658,\"start\":649},{\"end\":671,\"start\":659},{\"end\":680,\"start\":672}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":81},{\"end\":154,\"start\":150},{\"end\":168,\"start\":163},{\"end\":222,\"start\":218},{\"end\":286,\"start\":284},{\"end\":354,\"start\":352},{\"end\":421,\"start\":416},{\"end\":469,\"start\":465},{\"end\":498,\"start\":493},{\"end\":569,\"start\":564},{\"end\":583,\"start\":579},{\"end\":597,\"start\":592},{\"end\":612,\"start\":608},{\"end\":622,\"start\":620},{\"end\":634,\"start\":632},{\"end\":647,\"start\":642},{\"end\":657,\"start\":653},{\"end\":670,\"start\":665},{\"end\":679,\"start\":672}]", "author_first_name": "[{\"end\":80,\"start\":71},{\"end\":149,\"start\":142},{\"end\":162,\"start\":156},{\"end\":217,\"start\":209},{\"end\":283,\"start\":278},{\"end\":351,\"start\":344},{\"end\":415,\"start\":410},{\"end\":464,\"start\":461},{\"end\":492,\"start\":487},{\"end\":563,\"start\":554},{\"end\":578,\"start\":571},{\"end\":591,\"start\":585},{\"end\":607,\"start\":599},{\"end\":619,\"start\":614},{\"end\":631,\"start\":624},{\"end\":641,\"start\":636},{\"end\":652,\"start\":649},{\"end\":664,\"start\":659}]", "author_affiliation": "[{\"end\":124,\"start\":88},{\"end\":140,\"start\":126},{\"end\":207,\"start\":193},{\"end\":260,\"start\":224},{\"end\":276,\"start\":262},{\"end\":322,\"start\":308},{\"end\":342,\"start\":324},{\"end\":392,\"start\":356},{\"end\":408,\"start\":394},{\"end\":459,\"start\":423},{\"end\":485,\"start\":471},{\"end\":536,\"start\":500},{\"end\":552,\"start\":538}]", "title": "[{\"end\":64,\"start\":1},{\"end\":744,\"start\":681}]", "venue": "[{\"end\":807,\"start\":746}]", "abstract": "[{\"end\":1942,\"start\":840}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1994,\"start\":1990},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1997,\"start\":1994},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2000,\"start\":1997},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":2003,\"start\":2000},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":2006,\"start\":2003},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":2009,\"start\":2006},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2287,\"start\":2283},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":2290,\"start\":2287},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2312,\"start\":2308},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2315,\"start\":2312},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2349,\"start\":2345},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":2352,\"start\":2349},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2373,\"start\":2369},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":2399,\"start\":2395},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2443,\"start\":2440},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2445,\"start\":2443},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2448,\"start\":2445},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2451,\"start\":2448},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2454,\"start\":2451},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2457,\"start\":2454},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2460,\"start\":2457},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2463,\"start\":2460},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":2466,\"start\":2463},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":2469,\"start\":2466},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":2472,\"start\":2469},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":2475,\"start\":2472},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":2928,\"start\":2924},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2958,\"start\":2954},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2973,\"start\":2969},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2989,\"start\":2985},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":3006,\"start\":3002},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3651,\"start\":3647},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":3915,\"start\":3911},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":3925,\"start\":3921},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":3942,\"start\":3938},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4611,\"start\":4607},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4614,\"start\":4611},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4617,\"start\":4614},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":5443,\"start\":5439},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5470,\"start\":5466},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5500,\"start\":5496},{\"end\":5749,\"start\":5746},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":5811,\"start\":5807},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":6093,\"start\":6089},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6234,\"start\":6230},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":6266,\"start\":6262},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6467,\"start\":6463},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":6672,\"start\":6668},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9150,\"start\":9146},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11388,\"start\":11384},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":11391,\"start\":11388},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":11394,\"start\":11391},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12506,\"start\":12502},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12934,\"start\":12930},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13169,\"start\":13165},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":13172,\"start\":13169},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":13195,\"start\":13191},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13843,\"start\":13839},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":14856,\"start\":14852},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":15240,\"start\":15236},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":16724,\"start\":16720},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":16727,\"start\":16724},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":17521,\"start\":17517},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18007,\"start\":18003},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":18364,\"start\":18360},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20087,\"start\":20083},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":20090,\"start\":20087},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":20093,\"start\":20090},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21161,\"start\":21157},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":21171,\"start\":21167},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21187,\"start\":21183},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":21201,\"start\":21197},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":21223,\"start\":21219},{\"end\":24080,\"start\":24029},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24133,\"start\":24129},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25337,\"start\":25334},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25898,\"start\":25894},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26032,\"start\":26028},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":32523,\"start\":32519},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":32526,\"start\":32523},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32529,\"start\":32526},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32682,\"start\":32678},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":32685,\"start\":32682},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":32688,\"start\":32685},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34062,\"start\":34059},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":36777,\"start\":36774},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":37768,\"start\":37764},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":37771,\"start\":37768},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":37811,\"start\":37807},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":38592,\"start\":38588},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":39234,\"start\":39230},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":39307,\"start\":39303},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":39475,\"start\":39471},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":39491,\"start\":39487},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":39529,\"start\":39525},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":40003,\"start\":39999},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":40106,\"start\":40102},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":40109,\"start\":40106},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":40112,\"start\":40109},{\"end\":45120,\"start\":45110},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":48958,\"start\":48954},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":52147,\"start\":52143},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":52175,\"start\":52171},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":53998,\"start\":53994},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":54028,\"start\":54024},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":54126,\"start\":54122},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":54142,\"start\":54138},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":54295,\"start\":54291},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":54638,\"start\":54634},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":54641,\"start\":54638},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":54872,\"start\":54868},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":54875,\"start\":54872},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":54952,\"start\":54948},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":55116,\"start\":55112},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":55496,\"start\":55492},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":55631,\"start\":55627},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":55778,\"start\":55774},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":56013,\"start\":56009},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":56061,\"start\":56057},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":56201,\"start\":56197},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":56481,\"start\":56477},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":56838,\"start\":56834},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":57044,\"start\":57040},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":57107,\"start\":57103},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":57110,\"start\":57107},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":57113,\"start\":57110},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":57130,\"start\":57126},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":57133,\"start\":57130},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":57136,\"start\":57133},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":57154,\"start\":57150},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":57259,\"start\":57255},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":57353,\"start\":57349},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":57462,\"start\":57458},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":57678,\"start\":57674}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":58956,\"start\":58486},{\"attributes\":{\"id\":\"fig_1\"},\"end\":59010,\"start\":58957},{\"attributes\":{\"id\":\"fig_2\"},\"end\":59149,\"start\":59011},{\"attributes\":{\"id\":\"fig_3\"},\"end\":59195,\"start\":59150},{\"attributes\":{\"id\":\"fig_4\"},\"end\":59379,\"start\":59196},{\"attributes\":{\"id\":\"fig_5\"},\"end\":59674,\"start\":59380},{\"attributes\":{\"id\":\"fig_6\"},\"end\":59789,\"start\":59675},{\"attributes\":{\"id\":\"fig_7\"},\"end\":60001,\"start\":59790},{\"attributes\":{\"id\":\"fig_8\"},\"end\":60039,\"start\":60002},{\"attributes\":{\"id\":\"fig_9\"},\"end\":60074,\"start\":60040},{\"attributes\":{\"id\":\"fig_11\"},\"end\":60125,\"start\":60075},{\"attributes\":{\"id\":\"fig_12\"},\"end\":60370,\"start\":60126},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":60830,\"start\":60371},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":61195,\"start\":60831},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":61693,\"start\":61196},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":61851,\"start\":61694},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":62347,\"start\":61852}]", "paragraph": "[{\"end\":2400,\"start\":1944},{\"end\":2838,\"start\":2402},{\"end\":3802,\"start\":2840},{\"end\":4511,\"start\":3804},{\"end\":5847,\"start\":4513},{\"end\":6880,\"start\":5849},{\"end\":8108,\"start\":6882},{\"end\":8506,\"start\":8110},{\"end\":8742,\"start\":8536},{\"end\":9242,\"start\":8768},{\"end\":9836,\"start\":9310},{\"end\":10280,\"start\":9838},{\"end\":10987,\"start\":10322},{\"end\":12309,\"start\":10989},{\"end\":12582,\"start\":12311},{\"end\":13294,\"start\":12670},{\"end\":13424,\"start\":13334},{\"end\":14576,\"start\":13426},{\"end\":14817,\"start\":14578},{\"end\":15354,\"start\":14819},{\"end\":16237,\"start\":15356},{\"end\":17475,\"start\":16239},{\"end\":18381,\"start\":17477},{\"end\":18764,\"start\":18583},{\"end\":19833,\"start\":18766},{\"end\":19993,\"start\":19851},{\"end\":20229,\"start\":20038},{\"end\":21858,\"start\":20231},{\"end\":22482,\"start\":21860},{\"end\":23291,\"start\":22484},{\"end\":23773,\"start\":23293},{\"end\":24361,\"start\":23798},{\"end\":25163,\"start\":24363},{\"end\":26033,\"start\":25165},{\"end\":26459,\"start\":26035},{\"end\":27271,\"start\":26461},{\"end\":27840,\"start\":27273},{\"end\":28587,\"start\":27842},{\"end\":28651,\"start\":28589},{\"end\":29742,\"start\":28653},{\"end\":30375,\"start\":29744},{\"end\":31411,\"start\":30377},{\"end\":31687,\"start\":31413},{\"end\":32447,\"start\":31801},{\"end\":33388,\"start\":32480},{\"end\":33972,\"start\":33390},{\"end\":34964,\"start\":33974},{\"end\":35474,\"start\":34966},{\"end\":36326,\"start\":35476},{\"end\":36682,\"start\":36328},{\"end\":37172,\"start\":36708},{\"end\":37646,\"start\":37174},{\"end\":38143,\"start\":37648},{\"end\":38452,\"start\":38145},{\"end\":38663,\"start\":38474},{\"end\":39001,\"start\":38665},{\"end\":39142,\"start\":39003},{\"end\":39235,\"start\":39144},{\"end\":39386,\"start\":39237},{\"end\":40387,\"start\":39388},{\"end\":41387,\"start\":40389},{\"end\":43579,\"start\":41411},{\"end\":43791,\"start\":43592},{\"end\":44868,\"start\":43793},{\"end\":46205,\"start\":44884},{\"end\":47201,\"start\":46224},{\"end\":47316,\"start\":47234},{\"end\":48685,\"start\":47342},{\"end\":49611,\"start\":48687},{\"end\":50870,\"start\":49613},{\"end\":51925,\"start\":50899},{\"end\":53945,\"start\":51949},{\"end\":55397,\"start\":53962},{\"end\":57005,\"start\":55399},{\"end\":57879,\"start\":57007},{\"end\":58485,\"start\":57895}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9279,\"start\":9243},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9309,\"start\":9279},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12669,\"start\":12583},{\"attributes\":{\"id\":\"formula_3\"},\"end\":31800,\"start\":31688},{\"attributes\":{\"id\":\"formula_4\"},\"end\":43591,\"start\":43580}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":39075,\"start\":39067},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":48797,\"start\":48790},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":49876,\"start\":49869}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":8534,\"start\":8509},{\"attributes\":{\"n\":\"2.1\"},\"end\":8766,\"start\":8745},{\"attributes\":{\"n\":\"2.2\"},\"end\":10320,\"start\":10283},{\"attributes\":{\"n\":\"2.3\"},\"end\":13332,\"start\":13297},{\"end\":18414,\"start\":18384},{\"end\":18425,\"start\":18417},{\"end\":18436,\"start\":18428},{\"end\":18449,\"start\":18439},{\"end\":18455,\"start\":18452},{\"end\":18475,\"start\":18458},{\"end\":18513,\"start\":18478},{\"end\":18521,\"start\":18516},{\"end\":18527,\"start\":18524},{\"end\":18538,\"start\":18530},{\"end\":18581,\"start\":18541},{\"attributes\":{\"n\":\"3\"},\"end\":19849,\"start\":19836},{\"attributes\":{\"n\":\"3.1\"},\"end\":20036,\"start\":19996},{\"attributes\":{\"n\":\"3.2\"},\"end\":23796,\"start\":23776},{\"attributes\":{\"n\":\"3.3\"},\"end\":32478,\"start\":32450},{\"attributes\":{\"n\":\"4\"},\"end\":36706,\"start\":36685},{\"attributes\":{\"n\":\"5\"},\"end\":38472,\"start\":38455},{\"attributes\":{\"n\":\"5.1\"},\"end\":41409,\"start\":41390},{\"attributes\":{\"n\":\"5.2\"},\"end\":44882,\"start\":44871},{\"attributes\":{\"n\":\"5.3\"},\"end\":46222,\"start\":46208},{\"attributes\":{\"n\":\"5.4\"},\"end\":47232,\"start\":47204},{\"attributes\":{\"n\":\"5.4.1\"},\"end\":47340,\"start\":47319},{\"attributes\":{\"n\":\"5.4.3\"},\"end\":50897,\"start\":50873},{\"attributes\":{\"n\":\"5.4.4\"},\"end\":51947,\"start\":51928},{\"attributes\":{\"n\":\"6\"},\"end\":53960,\"start\":53948},{\"attributes\":{\"n\":\"7\"},\"end\":57893,\"start\":57882},{\"end\":58497,\"start\":58487},{\"end\":58968,\"start\":58958},{\"end\":59022,\"start\":59012},{\"end\":59161,\"start\":59151},{\"end\":59207,\"start\":59197},{\"end\":59686,\"start\":59676},{\"end\":59801,\"start\":59791},{\"end\":60014,\"start\":60003},{\"end\":60052,\"start\":60041},{\"end\":60087,\"start\":60076},{\"end\":60138,\"start\":60127},{\"end\":60841,\"start\":60832},{\"end\":61704,\"start\":61695},{\"end\":61862,\"start\":61853}]", "table": "[{\"end\":60830,\"start\":60530},{\"end\":61195,\"start\":60857},{\"end\":61693,\"start\":61400},{\"end\":61851,\"start\":61746},{\"end\":62347,\"start\":61942}]", "figure_caption": "[{\"end\":58956,\"start\":58499},{\"end\":59010,\"start\":58970},{\"end\":59149,\"start\":59024},{\"end\":59195,\"start\":59163},{\"end\":59379,\"start\":59209},{\"end\":59674,\"start\":59382},{\"end\":59789,\"start\":59688},{\"end\":60001,\"start\":59803},{\"end\":60039,\"start\":60017},{\"end\":60074,\"start\":60055},{\"end\":60125,\"start\":60090},{\"end\":60370,\"start\":60141},{\"end\":60530,\"start\":60373},{\"end\":60857,\"start\":60843},{\"end\":61400,\"start\":61198},{\"end\":61746,\"start\":61706},{\"end\":61942,\"start\":61864}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5254,\"start\":5246},{\"end\":11487,\"start\":11479},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14150,\"start\":14141},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14721,\"start\":14713},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17784,\"start\":17776},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18149,\"start\":18141},{\"end\":19161,\"start\":19153},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21971,\"start\":21963},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22004,\"start\":21996},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22303,\"start\":22295},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22988,\"start\":22980},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32910,\"start\":32902},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":34922,\"start\":34914},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":42076,\"start\":42068},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":43579,\"start\":43570},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":43886,\"start\":43878},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":44084,\"start\":44076},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44402,\"start\":44393},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46431,\"start\":46422},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51304,\"start\":51295},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":52360,\"start\":52351},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":52579,\"start\":52570}]", "bib_author_first_name": "[{\"end\":62570,\"start\":62564},{\"end\":62582,\"start\":62578},{\"end\":62598,\"start\":62591},{\"end\":62612,\"start\":62605},{\"end\":62623,\"start\":62619},{\"end\":62638,\"start\":62631},{\"end\":62653,\"start\":62645},{\"end\":62667,\"start\":62661},{\"end\":62686,\"start\":62678},{\"end\":62702,\"start\":62695},{\"end\":62719,\"start\":62710},{\"end\":62732,\"start\":62728},{\"end\":62749,\"start\":62744},{\"end\":62763,\"start\":62757},{\"end\":62776,\"start\":62771},{\"end\":62783,\"start\":62777},{\"end\":62798,\"start\":62792},{\"end\":62812,\"start\":62808},{\"end\":62814,\"start\":62813},{\"end\":62828,\"start\":62823},{\"end\":62844,\"start\":62840},{\"end\":62859,\"start\":62853},{\"end\":62871,\"start\":62867},{\"end\":62885,\"start\":62876},{\"end\":63649,\"start\":63639},{\"end\":63665,\"start\":63659},{\"end\":64158,\"start\":64157},{\"end\":64173,\"start\":64166},{\"end\":64175,\"start\":64174},{\"end\":64484,\"start\":64479},{\"end\":65169,\"start\":65162},{\"end\":65179,\"start\":65175},{\"end\":65189,\"start\":65185},{\"end\":65200,\"start\":65194},{\"end\":65210,\"start\":65205},{\"end\":65221,\"start\":65218},{\"end\":65739,\"start\":65737},{\"end\":65751,\"start\":65746},{\"end\":65766,\"start\":65759},{\"end\":66052,\"start\":66045},{\"end\":66063,\"start\":66059},{\"end\":66076,\"start\":66069},{\"end\":66087,\"start\":66083},{\"end\":66095,\"start\":66093},{\"end\":66106,\"start\":66101},{\"end\":66622,\"start\":66619},{\"end\":66636,\"start\":66629},{\"end\":66644,\"start\":66641},{\"end\":67190,\"start\":67183},{\"end\":67200,\"start\":67197},{\"end\":67208,\"start\":67206},{\"end\":67870,\"start\":67864},{\"end\":67884,\"start\":67877},{\"end\":67899,\"start\":67893},{\"end\":67914,\"start\":67907},{\"end\":67927,\"start\":67922},{\"end\":67929,\"start\":67928},{\"end\":67942,\"start\":67935},{\"end\":67955,\"start\":67949},{\"end\":67969,\"start\":67963},{\"end\":67981,\"start\":67976},{\"end\":67990,\"start\":67986},{\"end\":68003,\"start\":67997},{\"end\":68020,\"start\":68014},{\"end\":68706,\"start\":68699},{\"end\":68723,\"start\":68715},{\"end\":68731,\"start\":68729},{\"end\":68740,\"start\":68736},{\"end\":68749,\"start\":68745},{\"end\":68765,\"start\":68758},{\"end\":69517,\"start\":69514},{\"end\":69531,\"start\":69526},{\"end\":69548,\"start\":69539},{\"end\":69560,\"start\":69556},{\"end\":69580,\"start\":69574},{\"end\":69598,\"start\":69591},{\"end\":70134,\"start\":70128},{\"end\":70136,\"start\":70135},{\"end\":70151,\"start\":70147},{\"end\":70167,\"start\":70160},{\"end\":70181,\"start\":70177},{\"end\":70183,\"start\":70182},{\"end\":70721,\"start\":70712},{\"end\":70742,\"start\":70734},{\"end\":71320,\"start\":71311},{\"end\":71341,\"start\":71333},{\"end\":71916,\"start\":71908},{\"end\":71925,\"start\":71922},{\"end\":71930,\"start\":71926},{\"end\":72228,\"start\":72221},{\"end\":72238,\"start\":72233},{\"end\":72240,\"start\":72239},{\"end\":72258,\"start\":72251},{\"end\":72841,\"start\":72834},{\"end\":72866,\"start\":72850},{\"end\":73337,\"start\":73331},{\"end\":73351,\"start\":73348},{\"end\":73362,\"start\":73357},{\"end\":73364,\"start\":73363},{\"end\":73381,\"start\":73373},{\"end\":73397,\"start\":73389},{\"end\":73413,\"start\":73405},{\"end\":73425,\"start\":73419},{\"end\":73429,\"start\":73426},{\"end\":73445,\"start\":73438},{\"end\":73462,\"start\":73455},{\"end\":73476,\"start\":73472},{\"end\":73488,\"start\":73483},{\"end\":73500,\"start\":73497},{\"end\":73502,\"start\":73501},{\"end\":73520,\"start\":73513},{\"end\":73522,\"start\":73521},{\"end\":73538,\"start\":73534},{\"end\":73540,\"start\":73539},{\"end\":74026,\"start\":74021},{\"end\":74037,\"start\":74032},{\"end\":74055,\"start\":74051},{\"end\":74065,\"start\":74064},{\"end\":74067,\"start\":74066},{\"end\":74082,\"start\":74076},{\"end\":74566,\"start\":74562},{\"end\":74595,\"start\":74588},{\"end\":74615,\"start\":74606},{\"end\":75220,\"start\":75214},{\"end\":75222,\"start\":75221},{\"end\":75240,\"start\":75233},{\"end\":75252,\"start\":75246},{\"end\":75262,\"start\":75257},{\"end\":75278,\"start\":75272},{\"end\":75851,\"start\":75845},{\"end\":75866,\"start\":75860},{\"end\":75893,\"start\":75885},{\"end\":75909,\"start\":75903},{\"end\":75921,\"start\":75915},{\"end\":76329,\"start\":76323},{\"end\":76346,\"start\":76338},{\"end\":76364,\"start\":76358},{\"end\":76379,\"start\":76373},{\"end\":76880,\"start\":76874},{\"end\":76897,\"start\":76889},{\"end\":76913,\"start\":76907},{\"end\":76925,\"start\":76919},{\"end\":76943,\"start\":76934},{\"end\":77441,\"start\":77434},{\"end\":77443,\"start\":77442},{\"end\":77460,\"start\":77454},{\"end\":77471,\"start\":77467},{\"end\":78080,\"start\":78074},{\"end\":78093,\"start\":78085},{\"end\":78106,\"start\":78099},{\"end\":78121,\"start\":78115},{\"end\":78134,\"start\":78128},{\"end\":78145,\"start\":78140},{\"end\":78158,\"start\":78151},{\"end\":78172,\"start\":78168},{\"end\":78833,\"start\":78828},{\"end\":78843,\"start\":78838},{\"end\":78854,\"start\":78848},{\"end\":78866,\"start\":78861},{\"end\":78873,\"start\":78871},{\"end\":78883,\"start\":78881},{\"end\":78893,\"start\":78888},{\"end\":78906,\"start\":78901},{\"end\":78918,\"start\":78914},{\"end\":79275,\"start\":79267},{\"end\":79287,\"start\":79283},{\"end\":79297,\"start\":79295},{\"end\":79311,\"start\":79304},{\"end\":79936,\"start\":79929},{\"end\":79951,\"start\":79944},{\"end\":79968,\"start\":79965},{\"end\":79980,\"start\":79977},{\"end\":79994,\"start\":79989},{\"end\":80009,\"start\":80003},{\"end\":80481,\"start\":80474},{\"end\":80497,\"start\":80490},{\"end\":80514,\"start\":80509},{\"end\":80526,\"start\":80521},{\"end\":81064,\"start\":81058},{\"end\":81074,\"start\":81070},{\"end\":81086,\"start\":81080},{\"end\":81097,\"start\":81092},{\"end\":81111,\"start\":81107},{\"end\":81602,\"start\":81596},{\"end\":81613,\"start\":81608},{\"end\":81627,\"start\":81623},{\"end\":82078,\"start\":82071},{\"end\":82091,\"start\":82084},{\"end\":82103,\"start\":82097},{\"end\":82118,\"start\":82108},{\"end\":82134,\"start\":82126},{\"end\":82147,\"start\":82142},{\"end\":82624,\"start\":82616},{\"end\":82642,\"start\":82634},{\"end\":83067,\"start\":83061},{\"end\":83082,\"start\":83077},{\"end\":83394,\"start\":83388},{\"end\":83411,\"start\":83406},{\"end\":83423,\"start\":83418},{\"end\":83436,\"start\":83431},{\"end\":83438,\"start\":83437},{\"end\":83976,\"start\":83975},{\"end\":83988,\"start\":83985},{\"end\":84472,\"start\":84468},{\"end\":84482,\"start\":84477},{\"end\":84489,\"start\":84487},{\"end\":84495,\"start\":84490},{\"end\":84511,\"start\":84502},{\"end\":84830,\"start\":84823},{\"end\":84836,\"start\":84835},{\"end\":84853,\"start\":84844},{\"end\":84880,\"start\":84879},{\"end\":84896,\"start\":84887},{\"end\":84911,\"start\":84905},{\"end\":84913,\"start\":84912},{\"end\":85278,\"start\":85273},{\"end\":85289,\"start\":85284},{\"end\":85301,\"start\":85294},{\"end\":85314,\"start\":85308},{\"end\":85327,\"start\":85320},{\"end\":85781,\"start\":85776},{\"end\":85791,\"start\":85787},{\"end\":86138,\"start\":86130},{\"end\":86146,\"start\":86143},{\"end\":86160,\"start\":86153},{\"end\":86173,\"start\":86167},{\"end\":86183,\"start\":86179},{\"end\":86194,\"start\":86188},{\"end\":86206,\"start\":86201},{\"end\":86704,\"start\":86696},{\"end\":86722,\"start\":86715},{\"end\":86724,\"start\":86723},{\"end\":86735,\"start\":86734},{\"end\":86737,\"start\":86736},{\"end\":86749,\"start\":86744},{\"end\":86751,\"start\":86750},{\"end\":86761,\"start\":86757},{\"end\":86775,\"start\":86771},{\"end\":86790,\"start\":86782},{\"end\":87159,\"start\":87152},{\"end\":87174,\"start\":87168},{\"end\":87197,\"start\":87190},{\"end\":87678,\"start\":87673},{\"end\":87695,\"start\":87688},{\"end\":87711,\"start\":87705},{\"end\":87713,\"start\":87712},{\"end\":88337,\"start\":88329},{\"end\":88352,\"start\":88345},{\"end\":88354,\"start\":88353},{\"end\":88372,\"start\":88366},{\"end\":89031,\"start\":89027},{\"end\":89045,\"start\":89040},{\"end\":89057,\"start\":89052},{\"end\":89064,\"start\":89063},{\"end\":89795,\"start\":89791},{\"end\":89807,\"start\":89804},{\"end\":89824,\"start\":89815},{\"end\":89836,\"start\":89832},{\"end\":89849,\"start\":89844},{\"end\":89867,\"start\":89860},{\"end\":89882,\"start\":89876},{\"end\":89898,\"start\":89892},{\"end\":89911,\"start\":89904},{\"end\":89928,\"start\":89924},{\"end\":89942,\"start\":89937},{\"end\":89961,\"start\":89954},{\"end\":89974,\"start\":89968},{\"end\":89988,\"start\":89981},{\"end\":90003,\"start\":89997},{\"end\":90019,\"start\":90012},{\"end\":90034,\"start\":90028},{\"end\":90055,\"start\":90049},{\"end\":90067,\"start\":90065},{\"end\":90080,\"start\":90074},{\"end\":90093,\"start\":90086},{\"end\":91025,\"start\":91022},{\"end\":91039,\"start\":91032},{\"end\":91046,\"start\":91044},{\"end\":91058,\"start\":91051},{\"end\":91072,\"start\":91064},{\"end\":91084,\"start\":91078},{\"end\":91706,\"start\":91701},{\"end\":91720,\"start\":91716},{\"end\":91736,\"start\":91730},{\"end\":92234,\"start\":92225},{\"end\":92248,\"start\":92243},{\"end\":92265,\"start\":92260},{\"end\":92287,\"start\":92277},{\"end\":92304,\"start\":92294},{\"end\":92319,\"start\":92314},{\"end\":92336,\"start\":92331},{\"end\":92349,\"start\":92344},{\"end\":92363,\"start\":92358},{\"end\":92382,\"start\":92376},{\"end\":92393,\"start\":92389},{\"end\":92928,\"start\":92922},{\"end\":92941,\"start\":92937},{\"end\":92947,\"start\":92942},{\"end\":93466,\"start\":93461},{\"end\":93783,\"start\":93775},{\"end\":93797,\"start\":93789},{\"end\":93807,\"start\":93802},{\"end\":93819,\"start\":93814},{\"end\":93837,\"start\":93831},{\"end\":93845,\"start\":93842},{\"end\":93858,\"start\":93850},{\"end\":93868,\"start\":93864},{\"end\":94330,\"start\":94326},{\"end\":94344,\"start\":94339},{\"end\":94359,\"start\":94351},{\"end\":94374,\"start\":94370},{\"end\":94389,\"start\":94381},{\"end\":94400,\"start\":94394},{\"end\":94414,\"start\":94406},{\"end\":94425,\"start\":94420},{\"end\":94436,\"start\":94432},{\"end\":94455,\"start\":94448},{\"end\":94474,\"start\":94461},{\"end\":95123,\"start\":95112},{\"end\":95125,\"start\":95124},{\"end\":95147,\"start\":95139},{\"end\":95167,\"start\":95160},{\"end\":95184,\"start\":95179},{\"end\":95751,\"start\":95745},{\"end\":95768,\"start\":95761},{\"end\":95781,\"start\":95774},{\"end\":95789,\"start\":95788},{\"end\":95802,\"start\":95799},{\"end\":95818,\"start\":95811},{\"end\":95820,\"start\":95819},{\"end\":96276,\"start\":96271},{\"end\":96296,\"start\":96289},{\"end\":96314,\"start\":96307},{\"end\":96332,\"start\":96325},{\"end\":96347,\"start\":96341},{\"end\":96359,\"start\":96353},{\"end\":96896,\"start\":96889},{\"end\":96910,\"start\":96903},{\"end\":96924,\"start\":96917},{\"end\":96941,\"start\":96932},{\"end\":96952,\"start\":96946},{\"end\":96965,\"start\":96959},{\"end\":97324,\"start\":97318},{\"end\":97338,\"start\":97331},{\"end\":97345,\"start\":97343},{\"end\":97357,\"start\":97353},{\"end\":97365,\"start\":97363},{\"end\":97376,\"start\":97371},{\"end\":97386,\"start\":97381},{\"end\":97398,\"start\":97391},{\"end\":97407,\"start\":97405},{\"end\":97419,\"start\":97415},{\"end\":97429,\"start\":97424},{\"end\":97443,\"start\":97437},{\"end\":97452,\"start\":97449},{\"end\":97466,\"start\":97460},{\"end\":97477,\"start\":97472},{\"end\":97491,\"start\":97484},{\"end\":97505,\"start\":97496},{\"end\":97507,\"start\":97506},{\"end\":97520,\"start\":97515},{\"end\":97993,\"start\":97984},{\"end\":98006,\"start\":98000},{\"end\":98008,\"start\":98007},{\"end\":98026,\"start\":98019},{\"end\":98037,\"start\":98032},{\"end\":98046,\"start\":98042},{\"end\":98059,\"start\":98055},{\"end\":98061,\"start\":98060},{\"end\":98744,\"start\":98737},{\"end\":98759,\"start\":98751},{\"end\":98771,\"start\":98766},{\"end\":98775,\"start\":98772},{\"end\":98787,\"start\":98781},{\"end\":98795,\"start\":98792},{\"end\":98808,\"start\":98803},{\"end\":99497,\"start\":99493},{\"end\":99508,\"start\":99502},{\"end\":99521,\"start\":99514},{\"end\":99532,\"start\":99527},{\"end\":99544,\"start\":99540},{\"end\":99555,\"start\":99550},{\"end\":99566,\"start\":99561},{\"end\":99574,\"start\":99572},{\"end\":99584,\"start\":99581},{\"end\":100225,\"start\":100221},{\"end\":100236,\"start\":100230},{\"end\":100248,\"start\":100241},{\"end\":100261,\"start\":100254},{\"end\":100273,\"start\":100267},{\"end\":100288,\"start\":100278},{\"end\":100301,\"start\":100296},{\"end\":100312,\"start\":100309},{\"end\":100822,\"start\":100816},{\"end\":100833,\"start\":100827},{\"end\":100842,\"start\":100838},{\"end\":100861,\"start\":100853},{\"end\":101321,\"start\":101319},{\"end\":101332,\"start\":101327},{\"end\":101342,\"start\":101340},{\"end\":101354,\"start\":101347},{\"end\":101688,\"start\":101683},{\"end\":101704,\"start\":101694},{\"end\":101714,\"start\":101710},{\"end\":102613,\"start\":102611},{\"end\":102622,\"start\":102619},{\"end\":102634,\"start\":102628},{\"end\":102649,\"start\":102641},{\"end\":102656,\"start\":102654},{\"end\":103170,\"start\":103167},{\"end\":103184,\"start\":103177},{\"end\":103196,\"start\":103189},{\"end\":103207,\"start\":103203},{\"end\":103229,\"start\":103222},{\"end\":103231,\"start\":103230},{\"end\":103246,\"start\":103242},{\"end\":103909,\"start\":103902},{\"end\":103924,\"start\":103916},{\"end\":103938,\"start\":103931},{\"end\":103959,\"start\":103951},{\"end\":103974,\"start\":103968},{\"end\":103976,\"start\":103975},{\"end\":104477,\"start\":104471},{\"end\":104488,\"start\":104485},{\"end\":104500,\"start\":104496},{\"end\":104509,\"start\":104506},{\"end\":104523,\"start\":104516},{\"end\":104537,\"start\":104528},{\"end\":104551,\"start\":104544},{\"end\":104559,\"start\":104556},{\"end\":104574,\"start\":104566},{\"end\":104586,\"start\":104582},{\"end\":105010,\"start\":105008},{\"end\":105022,\"start\":105018},{\"end\":105033,\"start\":105027},{\"end\":105047,\"start\":105040},{\"end\":105060,\"start\":105054},{\"end\":105070,\"start\":105065},{\"end\":105081,\"start\":105077},{\"end\":105092,\"start\":105087},{\"end\":105106,\"start\":105100},{\"end\":105459,\"start\":105451},{\"end\":105476,\"start\":105467},{\"end\":105792,\"start\":105788},{\"end\":105801,\"start\":105798},{\"end\":105815,\"start\":105808},{\"end\":105825,\"start\":105822},{\"end\":105836,\"start\":105831},{\"end\":105848,\"start\":105843},{\"end\":105857,\"start\":105853},{\"end\":105869,\"start\":105862},{\"end\":106246,\"start\":106239},{\"end\":106260,\"start\":106252},{\"end\":106273,\"start\":106267},{\"end\":106289,\"start\":106281}]", "bib_author_last_name": "[{\"end\":62576,\"start\":62571},{\"end\":62589,\"start\":62583},{\"end\":62603,\"start\":62599},{\"end\":62617,\"start\":62613},{\"end\":62629,\"start\":62624},{\"end\":62643,\"start\":62639},{\"end\":62659,\"start\":62654},{\"end\":62676,\"start\":62668},{\"end\":62693,\"start\":62687},{\"end\":62708,\"start\":62703},{\"end\":62726,\"start\":62720},{\"end\":62742,\"start\":62733},{\"end\":62755,\"start\":62750},{\"end\":62769,\"start\":62764},{\"end\":62790,\"start\":62784},{\"end\":62806,\"start\":62799},{\"end\":62821,\"start\":62815},{\"end\":62838,\"start\":62829},{\"end\":62851,\"start\":62845},{\"end\":62865,\"start\":62860},{\"end\":62874,\"start\":62872},{\"end\":62891,\"start\":62886},{\"end\":63657,\"start\":63650},{\"end\":63671,\"start\":63666},{\"end\":64164,\"start\":64159},{\"end\":64185,\"start\":64176},{\"end\":64191,\"start\":64187},{\"end\":64490,\"start\":64485},{\"end\":65173,\"start\":65170},{\"end\":65183,\"start\":65180},{\"end\":65192,\"start\":65190},{\"end\":65203,\"start\":65201},{\"end\":65216,\"start\":65211},{\"end\":65224,\"start\":65222},{\"end\":65744,\"start\":65740},{\"end\":65757,\"start\":65752},{\"end\":65776,\"start\":65767},{\"end\":66057,\"start\":66053},{\"end\":66067,\"start\":66064},{\"end\":66081,\"start\":66077},{\"end\":66091,\"start\":66088},{\"end\":66099,\"start\":66096},{\"end\":66112,\"start\":66107},{\"end\":66627,\"start\":66623},{\"end\":66639,\"start\":66637},{\"end\":66649,\"start\":66645},{\"end\":67195,\"start\":67191},{\"end\":67204,\"start\":67201},{\"end\":67213,\"start\":67209},{\"end\":67875,\"start\":67871},{\"end\":67891,\"start\":67885},{\"end\":67905,\"start\":67900},{\"end\":67920,\"start\":67915},{\"end\":67933,\"start\":67930},{\"end\":67947,\"start\":67943},{\"end\":67961,\"start\":67956},{\"end\":67974,\"start\":67970},{\"end\":67984,\"start\":67982},{\"end\":67995,\"start\":67991},{\"end\":68012,\"start\":68004},{\"end\":68034,\"start\":68021},{\"end\":68713,\"start\":68707},{\"end\":68727,\"start\":68724},{\"end\":68734,\"start\":68732},{\"end\":68743,\"start\":68741},{\"end\":68756,\"start\":68750},{\"end\":68771,\"start\":68766},{\"end\":69524,\"start\":69518},{\"end\":69537,\"start\":69532},{\"end\":69554,\"start\":69549},{\"end\":69572,\"start\":69561},{\"end\":69589,\"start\":69581},{\"end\":69608,\"start\":69599},{\"end\":70145,\"start\":70137},{\"end\":70158,\"start\":70152},{\"end\":70175,\"start\":70168},{\"end\":70189,\"start\":70184},{\"end\":70732,\"start\":70722},{\"end\":70752,\"start\":70743},{\"end\":71331,\"start\":71321},{\"end\":71351,\"start\":71342},{\"end\":71920,\"start\":71917},{\"end\":71938,\"start\":71931},{\"end\":72231,\"start\":72229},{\"end\":72249,\"start\":72241},{\"end\":72268,\"start\":72259},{\"end\":72848,\"start\":72842},{\"end\":72871,\"start\":72867},{\"end\":73346,\"start\":73338},{\"end\":73355,\"start\":73352},{\"end\":73371,\"start\":73365},{\"end\":73387,\"start\":73382},{\"end\":73403,\"start\":73398},{\"end\":73417,\"start\":73414},{\"end\":73436,\"start\":73430},{\"end\":73453,\"start\":73446},{\"end\":73470,\"start\":73463},{\"end\":73481,\"start\":73477},{\"end\":73495,\"start\":73489},{\"end\":73511,\"start\":73503},{\"end\":73532,\"start\":73523},{\"end\":73552,\"start\":73541},{\"end\":74030,\"start\":74027},{\"end\":74049,\"start\":74038},{\"end\":74062,\"start\":74056},{\"end\":74074,\"start\":74068},{\"end\":74089,\"start\":74083},{\"end\":74095,\"start\":74091},{\"end\":74586,\"start\":74567},{\"end\":74604,\"start\":74596},{\"end\":74626,\"start\":74616},{\"end\":74635,\"start\":74628},{\"end\":75231,\"start\":75223},{\"end\":75244,\"start\":75241},{\"end\":75255,\"start\":75253},{\"end\":75270,\"start\":75263},{\"end\":75287,\"start\":75279},{\"end\":75858,\"start\":75852},{\"end\":75883,\"start\":75867},{\"end\":75901,\"start\":75894},{\"end\":75913,\"start\":75910},{\"end\":75928,\"start\":75922},{\"end\":76336,\"start\":76330},{\"end\":76356,\"start\":76347},{\"end\":76371,\"start\":76365},{\"end\":76396,\"start\":76380},{\"end\":76887,\"start\":76881},{\"end\":76905,\"start\":76898},{\"end\":76917,\"start\":76914},{\"end\":76932,\"start\":76926},{\"end\":76952,\"start\":76944},{\"end\":77452,\"start\":77444},{\"end\":77465,\"start\":77461},{\"end\":77480,\"start\":77472},{\"end\":78083,\"start\":78081},{\"end\":78097,\"start\":78094},{\"end\":78113,\"start\":78107},{\"end\":78126,\"start\":78122},{\"end\":78138,\"start\":78135},{\"end\":78149,\"start\":78146},{\"end\":78166,\"start\":78159},{\"end\":78181,\"start\":78173},{\"end\":78836,\"start\":78834},{\"end\":78846,\"start\":78844},{\"end\":78859,\"start\":78855},{\"end\":78869,\"start\":78867},{\"end\":78879,\"start\":78874},{\"end\":78886,\"start\":78884},{\"end\":78899,\"start\":78894},{\"end\":78912,\"start\":78907},{\"end\":78923,\"start\":78919},{\"end\":79281,\"start\":79276},{\"end\":79293,\"start\":79288},{\"end\":79302,\"start\":79298},{\"end\":79317,\"start\":79312},{\"end\":79942,\"start\":79937},{\"end\":79963,\"start\":79952},{\"end\":79975,\"start\":79969},{\"end\":79987,\"start\":79981},{\"end\":80001,\"start\":79995},{\"end\":80018,\"start\":80010},{\"end\":80488,\"start\":80482},{\"end\":80507,\"start\":80498},{\"end\":80519,\"start\":80515},{\"end\":80535,\"start\":80527},{\"end\":81068,\"start\":81065},{\"end\":81078,\"start\":81075},{\"end\":81090,\"start\":81087},{\"end\":81105,\"start\":81098},{\"end\":81117,\"start\":81112},{\"end\":81606,\"start\":81603},{\"end\":81621,\"start\":81614},{\"end\":81633,\"start\":81628},{\"end\":82082,\"start\":82079},{\"end\":82095,\"start\":82092},{\"end\":82106,\"start\":82104},{\"end\":82124,\"start\":82119},{\"end\":82140,\"start\":82135},{\"end\":82153,\"start\":82148},{\"end\":82632,\"start\":82625},{\"end\":82647,\"start\":82643},{\"end\":83075,\"start\":83068},{\"end\":83088,\"start\":83083},{\"end\":83404,\"start\":83395},{\"end\":83416,\"start\":83412},{\"end\":83429,\"start\":83424},{\"end\":83445,\"start\":83439},{\"end\":83983,\"start\":83977},{\"end\":83993,\"start\":83989},{\"end\":84002,\"start\":83995},{\"end\":84475,\"start\":84473},{\"end\":84485,\"start\":84483},{\"end\":84500,\"start\":84496},{\"end\":84518,\"start\":84512},{\"end\":84833,\"start\":84831},{\"end\":84842,\"start\":84837},{\"end\":84864,\"start\":84854},{\"end\":84877,\"start\":84866},{\"end\":84885,\"start\":84881},{\"end\":84903,\"start\":84897},{\"end\":84916,\"start\":84914},{\"end\":84923,\"start\":84918},{\"end\":85282,\"start\":85279},{\"end\":85292,\"start\":85290},{\"end\":85306,\"start\":85302},{\"end\":85318,\"start\":85315},{\"end\":85330,\"start\":85328},{\"end\":85785,\"start\":85782},{\"end\":85794,\"start\":85792},{\"end\":86141,\"start\":86139},{\"end\":86151,\"start\":86147},{\"end\":86165,\"start\":86161},{\"end\":86177,\"start\":86174},{\"end\":86186,\"start\":86184},{\"end\":86199,\"start\":86195},{\"end\":86210,\"start\":86207},{\"end\":86713,\"start\":86705},{\"end\":86732,\"start\":86725},{\"end\":86742,\"start\":86738},{\"end\":86755,\"start\":86752},{\"end\":86769,\"start\":86762},{\"end\":86780,\"start\":86776},{\"end\":86797,\"start\":86791},{\"end\":86809,\"start\":86799},{\"end\":87166,\"start\":87160},{\"end\":87188,\"start\":87175},{\"end\":87203,\"start\":87198},{\"end\":87686,\"start\":87679},{\"end\":87703,\"start\":87696},{\"end\":87722,\"start\":87714},{\"end\":88343,\"start\":88338},{\"end\":88364,\"start\":88355},{\"end\":88380,\"start\":88373},{\"end\":89038,\"start\":89032},{\"end\":89050,\"start\":89046},{\"end\":89061,\"start\":89058},{\"end\":89070,\"start\":89065},{\"end\":89076,\"start\":89072},{\"end\":89802,\"start\":89796},{\"end\":89813,\"start\":89808},{\"end\":89830,\"start\":89825},{\"end\":89842,\"start\":89837},{\"end\":89858,\"start\":89850},{\"end\":89874,\"start\":89868},{\"end\":89890,\"start\":89883},{\"end\":89902,\"start\":89899},{\"end\":89922,\"start\":89912},{\"end\":89935,\"start\":89929},{\"end\":89952,\"start\":89943},{\"end\":89966,\"start\":89962},{\"end\":89979,\"start\":89975},{\"end\":89995,\"start\":89989},{\"end\":90010,\"start\":90004},{\"end\":90026,\"start\":90020},{\"end\":90047,\"start\":90035},{\"end\":90063,\"start\":90056},{\"end\":90072,\"start\":90068},{\"end\":90084,\"start\":90081},{\"end\":90102,\"start\":90094},{\"end\":91030,\"start\":91026},{\"end\":91042,\"start\":91040},{\"end\":91049,\"start\":91047},{\"end\":91062,\"start\":91059},{\"end\":91076,\"start\":91073},{\"end\":91089,\"start\":91085},{\"end\":91714,\"start\":91707},{\"end\":91728,\"start\":91721},{\"end\":91743,\"start\":91737},{\"end\":92241,\"start\":92235},{\"end\":92258,\"start\":92249},{\"end\":92275,\"start\":92266},{\"end\":92292,\"start\":92288},{\"end\":92312,\"start\":92305},{\"end\":92329,\"start\":92320},{\"end\":92342,\"start\":92337},{\"end\":92356,\"start\":92350},{\"end\":92374,\"start\":92364},{\"end\":92387,\"start\":92383},{\"end\":92400,\"start\":92394},{\"end\":92935,\"start\":92929},{\"end\":92956,\"start\":92948},{\"end\":92965,\"start\":92958},{\"end\":93475,\"start\":93467},{\"end\":93787,\"start\":93784},{\"end\":93800,\"start\":93798},{\"end\":93812,\"start\":93808},{\"end\":93829,\"start\":93820},{\"end\":93840,\"start\":93838},{\"end\":93848,\"start\":93846},{\"end\":93862,\"start\":93859},{\"end\":93871,\"start\":93869},{\"end\":94337,\"start\":94331},{\"end\":94349,\"start\":94345},{\"end\":94368,\"start\":94360},{\"end\":94379,\"start\":94375},{\"end\":94392,\"start\":94390},{\"end\":94404,\"start\":94401},{\"end\":94418,\"start\":94415},{\"end\":94430,\"start\":94426},{\"end\":94446,\"start\":94437},{\"end\":94459,\"start\":94456},{\"end\":94477,\"start\":94475},{\"end\":95137,\"start\":95126},{\"end\":95158,\"start\":95148},{\"end\":95177,\"start\":95168},{\"end\":95193,\"start\":95185},{\"end\":95759,\"start\":95752},{\"end\":95772,\"start\":95769},{\"end\":95786,\"start\":95782},{\"end\":95797,\"start\":95790},{\"end\":95809,\"start\":95803},{\"end\":95835,\"start\":95821},{\"end\":95843,\"start\":95837},{\"end\":96287,\"start\":96277},{\"end\":96305,\"start\":96297},{\"end\":96323,\"start\":96315},{\"end\":96339,\"start\":96333},{\"end\":96351,\"start\":96348},{\"end\":96366,\"start\":96360},{\"end\":96901,\"start\":96897},{\"end\":96915,\"start\":96911},{\"end\":96930,\"start\":96925},{\"end\":96944,\"start\":96942},{\"end\":96957,\"start\":96953},{\"end\":96973,\"start\":96966},{\"end\":97329,\"start\":97325},{\"end\":97341,\"start\":97339},{\"end\":97351,\"start\":97346},{\"end\":97361,\"start\":97358},{\"end\":97369,\"start\":97366},{\"end\":97379,\"start\":97377},{\"end\":97389,\"start\":97387},{\"end\":97403,\"start\":97399},{\"end\":97413,\"start\":97408},{\"end\":97422,\"start\":97420},{\"end\":97435,\"start\":97430},{\"end\":97447,\"start\":97444},{\"end\":97458,\"start\":97453},{\"end\":97470,\"start\":97467},{\"end\":97482,\"start\":97478},{\"end\":97494,\"start\":97492},{\"end\":97513,\"start\":97508},{\"end\":97526,\"start\":97521},{\"end\":97998,\"start\":97994},{\"end\":98017,\"start\":98009},{\"end\":98030,\"start\":98027},{\"end\":98040,\"start\":98038},{\"end\":98053,\"start\":98047},{\"end\":98067,\"start\":98062},{\"end\":98749,\"start\":98745},{\"end\":98764,\"start\":98760},{\"end\":98779,\"start\":98776},{\"end\":98790,\"start\":98788},{\"end\":98801,\"start\":98796},{\"end\":98812,\"start\":98809},{\"end\":99500,\"start\":99498},{\"end\":99512,\"start\":99509},{\"end\":99525,\"start\":99522},{\"end\":99538,\"start\":99533},{\"end\":99548,\"start\":99545},{\"end\":99559,\"start\":99556},{\"end\":99570,\"start\":99567},{\"end\":99579,\"start\":99575},{\"end\":99587,\"start\":99585},{\"end\":100228,\"start\":100226},{\"end\":100239,\"start\":100237},{\"end\":100252,\"start\":100249},{\"end\":100265,\"start\":100262},{\"end\":100276,\"start\":100274},{\"end\":100294,\"start\":100289},{\"end\":100307,\"start\":100302},{\"end\":100315,\"start\":100313},{\"end\":100825,\"start\":100823},{\"end\":100836,\"start\":100834},{\"end\":100851,\"start\":100843},{\"end\":100869,\"start\":100862},{\"end\":101325,\"start\":101322},{\"end\":101338,\"start\":101333},{\"end\":101345,\"start\":101343},{\"end\":101357,\"start\":101355},{\"end\":101528,\"start\":101523},{\"end\":101692,\"start\":101689},{\"end\":101708,\"start\":101705},{\"end\":101718,\"start\":101715},{\"end\":102617,\"start\":102614},{\"end\":102626,\"start\":102623},{\"end\":102639,\"start\":102635},{\"end\":102652,\"start\":102650},{\"end\":102662,\"start\":102657},{\"end\":103175,\"start\":103171},{\"end\":103187,\"start\":103185},{\"end\":103201,\"start\":103197},{\"end\":103220,\"start\":103208},{\"end\":103240,\"start\":103232},{\"end\":103255,\"start\":103247},{\"end\":103914,\"start\":103910},{\"end\":103929,\"start\":103925},{\"end\":103949,\"start\":103939},{\"end\":103966,\"start\":103960},{\"end\":103985,\"start\":103977},{\"end\":104483,\"start\":104478},{\"end\":104494,\"start\":104489},{\"end\":104504,\"start\":104501},{\"end\":104514,\"start\":104510},{\"end\":104526,\"start\":104524},{\"end\":104542,\"start\":104538},{\"end\":104554,\"start\":104552},{\"end\":104564,\"start\":104560},{\"end\":104580,\"start\":104575},{\"end\":104589,\"start\":104587},{\"end\":105016,\"start\":105011},{\"end\":105025,\"start\":105023},{\"end\":105038,\"start\":105034},{\"end\":105052,\"start\":105048},{\"end\":105063,\"start\":105061},{\"end\":105075,\"start\":105071},{\"end\":105085,\"start\":105082},{\"end\":105098,\"start\":105093},{\"end\":105114,\"start\":105107},{\"end\":105465,\"start\":105460},{\"end\":105479,\"start\":105477},{\"end\":105796,\"start\":105793},{\"end\":105806,\"start\":105802},{\"end\":105820,\"start\":105816},{\"end\":105829,\"start\":105826},{\"end\":105841,\"start\":105837},{\"end\":105851,\"start\":105849},{\"end\":105860,\"start\":105858},{\"end\":105874,\"start\":105870},{\"end\":106250,\"start\":106247},{\"end\":106265,\"start\":106261},{\"end\":106279,\"start\":106274},{\"end\":106292,\"start\":106290}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6287870},\"end\":63608,\"start\":62509},{\"attributes\":{\"doi\":\"10.1145/1007912.1007931\",\"id\":\"b1\",\"matched_paper_id\":7330838},\"end\":64097,\"start\":63610},{\"attributes\":{\"doi\":\"10.1287/ijoc.3.2.149\",\"id\":\"b2\",\"matched_paper_id\":38624544},\"end\":64414,\"start\":64099},{\"attributes\":{\"id\":\"b3\"},\"end\":64694,\"start\":64416},{\"attributes\":{\"doi\":\"https:/arxiv.org/abs/https:/doi.org/10.1080/0022250X.2001.9990249\",\"id\":\"b4\",\"matched_paper_id\":13971996},\"end\":65089,\"start\":64696},{\"attributes\":{\"doi\":\"10.1145/3447786.3456233\",\"id\":\"b5\",\"matched_paper_id\":233328082},\"end\":65659,\"start\":65091},{\"attributes\":{\"doi\":\"10.1007/978-1-4613-0303-9_25\",\"id\":\"b6\"},\"end\":65986,\"start\":65661},{\"attributes\":{\"doi\":\"10.1145/3190508.3190545\",\"id\":\"b7\",\"matched_paper_id\":4949699},\"end\":66535,\"start\":65988},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":22191393},\"end\":67104,\"start\":66537},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b9\",\"matched_paper_id\":3636539},\"end\":67794,\"start\":67106},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":52939079},\"end\":68603,\"start\":67796},{\"attributes\":{\"doi\":\"10.1145/3292500.3330925\",\"id\":\"b11\",\"matched_paper_id\":159042192},\"end\":69396,\"start\":68605},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":207600624},\"end\":70056,\"start\":69398},{\"attributes\":{\"doi\":\"10.1109/IPDPS.2014.45\",\"id\":\"b13\",\"matched_paper_id\":9370067},\"end\":70649,\"start\":70058},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7798270},\"end\":71248,\"start\":70651},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":207209661},\"end\":71847,\"start\":71250},{\"attributes\":{\"doi\":\"arXiv:1903.02428\",\"id\":\"b16\"},\"end\":72126,\"start\":71849},{\"attributes\":{\"doi\":\"10.1145/2621934.2621936\",\"id\":\"b17\",\"matched_paper_id\":1706843},\"end\":72786,\"start\":72128},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":236992607},\"end\":73257,\"start\":72788},{\"attributes\":{\"doi\":\"arXiv:2012.05716\",\"id\":\"b19\"},\"end\":73961,\"start\":73259},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":808053},\"end\":74503,\"start\":73963},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5089320},\"end\":75142,\"start\":74505},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":13396177},\"end\":75796,\"start\":75144},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7387997},\"end\":76271,\"start\":75798},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":15116254},\"end\":76794,\"start\":76273},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6862943},\"end\":77381,\"start\":76796},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4755450},\"end\":78009,\"start\":77383},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":218487328},\"end\":78748,\"start\":78011},{\"attributes\":{\"doi\":\"arXiv:2008.11359\",\"id\":\"b28\"},\"end\":79203,\"start\":78750},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":52279871},\"end\":79867,\"start\":79205},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":207175615},\"end\":80405,\"start\":79869},{\"attributes\":{\"doi\":\"10.1145/3447786.3456244\",\"id\":\"b31\",\"matched_paper_id\":233328073},\"end\":80968,\"start\":80407},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":219850480},\"end\":81534,\"start\":80970},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":49868726},\"end\":82003,\"start\":81536},{\"attributes\":{\"doi\":\"10.1145/3342195.3387551\",\"id\":\"b34\",\"matched_paper_id\":218489479},\"end\":82557,\"start\":82005},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":195259568},\"end\":82982,\"start\":82559},{\"attributes\":{\"doi\":\"10.1137/S1064827595287997\",\"id\":\"b36\",\"matched_paper_id\":3628209},\"end\":83338,\"start\":82984},{\"attributes\":{\"doi\":\"10.1145/2600212.2600227\",\"id\":\"b37\",\"matched_paper_id\":207213081},\"end\":83907,\"start\":83340},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3144218},\"end\":84418,\"start\":83909},{\"attributes\":{\"doi\":\"10.1016/j.neunet.2020.04.028\",\"id\":\"b39\",\"matched_paper_id\":198179822},\"end\":84759,\"start\":84420},{\"attributes\":{\"doi\":\"arXiv:2007.04921\",\"id\":\"b40\"},\"end\":85194,\"start\":84761},{\"attributes\":{\"doi\":\"10.1145/3419111.3421281\",\"id\":\"b41\",\"matched_paper_id\":222296394},\"end\":85707,\"start\":85196},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":52903050},\"end\":86060,\"start\":85709},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":196171782},\"end\":86644,\"start\":86062},{\"attributes\":{\"doi\":\"10.1145/1807167.1807184\",\"id\":\"b44\"},\"end\":87082,\"start\":86646},{\"attributes\":{\"doi\":\"10.1109/CVPR.2017.10\",\"id\":\"b45\",\"matched_paper_id\":2021646},\"end\":87641,\"start\":87084},{\"attributes\":{\"doi\":\"10.1145/2145816.2145832\",\"id\":\"b46\",\"matched_paper_id\":15847307},\"end\":88255,\"start\":87643},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":18052422},\"end\":88930,\"start\":88257},{\"attributes\":{\"doi\":\"10.1145/3078447.3078459\",\"id\":\"b48\",\"matched_paper_id\":9015470},\"end\":89719,\"start\":88932},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":202786778},\"end\":90930,\"start\":89721},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":4897546},\"end\":91561,\"start\":90932},{\"attributes\":{\"doi\":\"10.1145/3178876.3186005\",\"id\":\"b51\"},\"end\":91646,\"start\":91563},{\"attributes\":{\"doi\":\"10.1145/2623330.2623732\",\"id\":\"b52\",\"matched_paper_id\":3051291},\"end\":92180,\"start\":91648},{\"attributes\":{\"doi\":\"10.1145/3342195.3387524\",\"id\":\"b53\",\"matched_paper_id\":218489692},\"end\":92874,\"start\":92182},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":3431470},\"end\":93396,\"start\":92876},{\"attributes\":{\"doi\":\"10.1145/3469379.3469387\",\"id\":\"b55\",\"matched_paper_id\":236490236},\"end\":93704,\"start\":93398},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":50777103},\"end\":94214,\"start\":93706},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":235167057},\"end\":95047,\"start\":94216},{\"attributes\":{\"doi\":\"10.1145/2556195.2556213\",\"id\":\"b58\",\"matched_paper_id\":9590483},\"end\":95651,\"start\":95049},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":15136963},\"end\":96243,\"start\":95653},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":3292002},\"end\":96832,\"start\":96245},{\"attributes\":{\"doi\":\"10.1162/qss_a_00021\",\"id\":\"b61\",\"matched_paper_id\":210872675},\"end\":97240,\"start\":96834},{\"attributes\":{\"doi\":\"arXiv:1909.01315\",\"id\":\"b62\"},\"end\":97917,\"start\":97242},{\"attributes\":{\"doi\":\"10.1145/2851141.2851145\",\"id\":\"b63\",\"matched_paper_id\":2770614},\"end\":98660,\"start\":97919},{\"attributes\":{\"doi\":\"10.24963/ijcai.2018/142\",\"id\":\"b64\",\"matched_paper_id\":49558620},\"end\":99430,\"start\":98662},{\"attributes\":{\"doi\":\"10.1145/3448016.3452770\",\"id\":\"b65\",\"matched_paper_id\":235474025},\"end\":100156,\"start\":99432},{\"attributes\":{\"doi\":\"10.1145/3447786.3456247\",\"id\":\"b66\",\"matched_paper_id\":233328074},\"end\":100774,\"start\":100158},{\"attributes\":{\"doi\":\"ICLR 2019\",\"id\":\"b67\",\"matched_paper_id\":52895589},\"end\":101233,\"start\":100776},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":7254792},\"end\":101519,\"start\":101235},{\"attributes\":{\"doi\":\"10.14778/2733085.2733103\",\"id\":\"b69\"},\"end\":101627,\"start\":101521},{\"attributes\":{\"doi\":\"10.1609/aaai.v33i01.33017370\",\"id\":\"b70\",\"matched_paper_id\":52284222},\"end\":102509,\"start\":101629},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":17755914},\"end\":103094,\"start\":102511},{\"attributes\":{\"doi\":\"10.1145/3219819.3219890\",\"id\":\"b72\",\"matched_paper_id\":46949657},\"end\":103840,\"start\":103096},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":195886159},\"end\":104399,\"start\":103842},{\"attributes\":{\"doi\":\"10.14778/3415478.3415539\",\"id\":\"b74\",\"matched_paper_id\":212414958},\"end\":104929,\"start\":104401},{\"attributes\":{\"doi\":\"arXiv:2010.05337\",\"id\":\"b75\"},\"end\":105404,\"start\":104931},{\"attributes\":{\"doi\":\"10.1109/TPDS.2013.111\",\"id\":\"b76\",\"matched_paper_id\":3607461},\"end\":105729,\"start\":105406},{\"attributes\":{\"doi\":\"10.14778/3352063.3352127\",\"id\":\"b77\",\"matched_paper_id\":67855424},\"end\":106170,\"start\":105731},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":3625061},\"end\":106773,\"start\":106172}]", "bib_title": "[{\"end\":62562,\"start\":62509},{\"end\":63637,\"start\":63610},{\"end\":64155,\"start\":64099},{\"end\":64477,\"start\":64416},{\"end\":64741,\"start\":64696},{\"end\":65160,\"start\":65091},{\"end\":66043,\"start\":65988},{\"end\":66617,\"start\":66537},{\"end\":67181,\"start\":67106},{\"end\":67862,\"start\":67796},{\"end\":68697,\"start\":68605},{\"end\":69512,\"start\":69398},{\"end\":70126,\"start\":70058},{\"end\":70710,\"start\":70651},{\"end\":71309,\"start\":71250},{\"end\":72219,\"start\":72128},{\"end\":72832,\"start\":72788},{\"end\":74019,\"start\":73963},{\"end\":74560,\"start\":74505},{\"end\":75212,\"start\":75144},{\"end\":75843,\"start\":75798},{\"end\":76321,\"start\":76273},{\"end\":76872,\"start\":76796},{\"end\":77432,\"start\":77383},{\"end\":78072,\"start\":78011},{\"end\":79265,\"start\":79205},{\"end\":79927,\"start\":79869},{\"end\":80472,\"start\":80407},{\"end\":81056,\"start\":80970},{\"end\":81594,\"start\":81536},{\"end\":82069,\"start\":82005},{\"end\":82614,\"start\":82559},{\"end\":83059,\"start\":82984},{\"end\":83386,\"start\":83340},{\"end\":83973,\"start\":83909},{\"end\":84466,\"start\":84420},{\"end\":85271,\"start\":85196},{\"end\":85774,\"start\":85709},{\"end\":86128,\"start\":86062},{\"end\":86694,\"start\":86646},{\"end\":87150,\"start\":87084},{\"end\":87671,\"start\":87643},{\"end\":88327,\"start\":88257},{\"end\":89025,\"start\":88932},{\"end\":89789,\"start\":89721},{\"end\":91020,\"start\":90932},{\"end\":91699,\"start\":91648},{\"end\":92223,\"start\":92182},{\"end\":92920,\"start\":92876},{\"end\":93459,\"start\":93398},{\"end\":93773,\"start\":93706},{\"end\":94324,\"start\":94216},{\"end\":95110,\"start\":95049},{\"end\":95743,\"start\":95653},{\"end\":96269,\"start\":96245},{\"end\":96887,\"start\":96834},{\"end\":97982,\"start\":97919},{\"end\":98735,\"start\":98662},{\"end\":99491,\"start\":99432},{\"end\":100219,\"start\":100158},{\"end\":100814,\"start\":100776},{\"end\":101317,\"start\":101235},{\"end\":101681,\"start\":101629},{\"end\":102609,\"start\":102511},{\"end\":103165,\"start\":103096},{\"end\":103900,\"start\":103842},{\"end\":104469,\"start\":104401},{\"end\":105449,\"start\":105406},{\"end\":105786,\"start\":105731},{\"end\":106237,\"start\":106172}]", "bib_author": "[{\"end\":62578,\"start\":62564},{\"end\":62591,\"start\":62578},{\"end\":62605,\"start\":62591},{\"end\":62619,\"start\":62605},{\"end\":62631,\"start\":62619},{\"end\":62645,\"start\":62631},{\"end\":62661,\"start\":62645},{\"end\":62678,\"start\":62661},{\"end\":62695,\"start\":62678},{\"end\":62710,\"start\":62695},{\"end\":62728,\"start\":62710},{\"end\":62744,\"start\":62728},{\"end\":62757,\"start\":62744},{\"end\":62771,\"start\":62757},{\"end\":62792,\"start\":62771},{\"end\":62808,\"start\":62792},{\"end\":62823,\"start\":62808},{\"end\":62840,\"start\":62823},{\"end\":62853,\"start\":62840},{\"end\":62867,\"start\":62853},{\"end\":62876,\"start\":62867},{\"end\":62893,\"start\":62876},{\"end\":63659,\"start\":63639},{\"end\":63673,\"start\":63659},{\"end\":64166,\"start\":64157},{\"end\":64187,\"start\":64166},{\"end\":64193,\"start\":64187},{\"end\":64492,\"start\":64479},{\"end\":65175,\"start\":65162},{\"end\":65185,\"start\":65175},{\"end\":65194,\"start\":65185},{\"end\":65205,\"start\":65194},{\"end\":65218,\"start\":65205},{\"end\":65226,\"start\":65218},{\"end\":65746,\"start\":65737},{\"end\":65759,\"start\":65746},{\"end\":65778,\"start\":65759},{\"end\":66059,\"start\":66045},{\"end\":66069,\"start\":66059},{\"end\":66083,\"start\":66069},{\"end\":66093,\"start\":66083},{\"end\":66101,\"start\":66093},{\"end\":66114,\"start\":66101},{\"end\":66629,\"start\":66619},{\"end\":66641,\"start\":66629},{\"end\":66651,\"start\":66641},{\"end\":67197,\"start\":67183},{\"end\":67206,\"start\":67197},{\"end\":67215,\"start\":67206},{\"end\":67877,\"start\":67864},{\"end\":67893,\"start\":67877},{\"end\":67907,\"start\":67893},{\"end\":67922,\"start\":67907},{\"end\":67935,\"start\":67922},{\"end\":67949,\"start\":67935},{\"end\":67963,\"start\":67949},{\"end\":67976,\"start\":67963},{\"end\":67986,\"start\":67976},{\"end\":67997,\"start\":67986},{\"end\":68014,\"start\":67997},{\"end\":68036,\"start\":68014},{\"end\":68715,\"start\":68699},{\"end\":68729,\"start\":68715},{\"end\":68736,\"start\":68729},{\"end\":68745,\"start\":68736},{\"end\":68758,\"start\":68745},{\"end\":68773,\"start\":68758},{\"end\":69526,\"start\":69514},{\"end\":69539,\"start\":69526},{\"end\":69556,\"start\":69539},{\"end\":69574,\"start\":69556},{\"end\":69591,\"start\":69574},{\"end\":69610,\"start\":69591},{\"end\":70147,\"start\":70128},{\"end\":70160,\"start\":70147},{\"end\":70177,\"start\":70160},{\"end\":70191,\"start\":70177},{\"end\":70734,\"start\":70712},{\"end\":70754,\"start\":70734},{\"end\":71333,\"start\":71311},{\"end\":71353,\"start\":71333},{\"end\":71922,\"start\":71908},{\"end\":71940,\"start\":71922},{\"end\":72233,\"start\":72221},{\"end\":72251,\"start\":72233},{\"end\":72270,\"start\":72251},{\"end\":72850,\"start\":72834},{\"end\":72873,\"start\":72850},{\"end\":73348,\"start\":73331},{\"end\":73357,\"start\":73348},{\"end\":73373,\"start\":73357},{\"end\":73389,\"start\":73373},{\"end\":73405,\"start\":73389},{\"end\":73419,\"start\":73405},{\"end\":73438,\"start\":73419},{\"end\":73455,\"start\":73438},{\"end\":73472,\"start\":73455},{\"end\":73483,\"start\":73472},{\"end\":73497,\"start\":73483},{\"end\":73513,\"start\":73497},{\"end\":73534,\"start\":73513},{\"end\":73554,\"start\":73534},{\"end\":74032,\"start\":74021},{\"end\":74051,\"start\":74032},{\"end\":74064,\"start\":74051},{\"end\":74076,\"start\":74064},{\"end\":74091,\"start\":74076},{\"end\":74097,\"start\":74091},{\"end\":74588,\"start\":74562},{\"end\":74606,\"start\":74588},{\"end\":74628,\"start\":74606},{\"end\":74637,\"start\":74628},{\"end\":75233,\"start\":75214},{\"end\":75246,\"start\":75233},{\"end\":75257,\"start\":75246},{\"end\":75272,\"start\":75257},{\"end\":75289,\"start\":75272},{\"end\":75860,\"start\":75845},{\"end\":75885,\"start\":75860},{\"end\":75903,\"start\":75885},{\"end\":75915,\"start\":75903},{\"end\":75930,\"start\":75915},{\"end\":76338,\"start\":76323},{\"end\":76358,\"start\":76338},{\"end\":76373,\"start\":76358},{\"end\":76398,\"start\":76373},{\"end\":76889,\"start\":76874},{\"end\":76907,\"start\":76889},{\"end\":76919,\"start\":76907},{\"end\":76934,\"start\":76919},{\"end\":76954,\"start\":76934},{\"end\":77454,\"start\":77434},{\"end\":77467,\"start\":77454},{\"end\":77482,\"start\":77467},{\"end\":78085,\"start\":78074},{\"end\":78099,\"start\":78085},{\"end\":78115,\"start\":78099},{\"end\":78128,\"start\":78115},{\"end\":78140,\"start\":78128},{\"end\":78151,\"start\":78140},{\"end\":78168,\"start\":78151},{\"end\":78183,\"start\":78168},{\"end\":78838,\"start\":78828},{\"end\":78848,\"start\":78838},{\"end\":78861,\"start\":78848},{\"end\":78871,\"start\":78861},{\"end\":78881,\"start\":78871},{\"end\":78888,\"start\":78881},{\"end\":78901,\"start\":78888},{\"end\":78914,\"start\":78901},{\"end\":78925,\"start\":78914},{\"end\":79283,\"start\":79267},{\"end\":79295,\"start\":79283},{\"end\":79304,\"start\":79295},{\"end\":79319,\"start\":79304},{\"end\":79944,\"start\":79929},{\"end\":79965,\"start\":79944},{\"end\":79977,\"start\":79965},{\"end\":79989,\"start\":79977},{\"end\":80003,\"start\":79989},{\"end\":80020,\"start\":80003},{\"end\":80490,\"start\":80474},{\"end\":80509,\"start\":80490},{\"end\":80521,\"start\":80509},{\"end\":80537,\"start\":80521},{\"end\":81070,\"start\":81058},{\"end\":81080,\"start\":81070},{\"end\":81092,\"start\":81080},{\"end\":81107,\"start\":81092},{\"end\":81119,\"start\":81107},{\"end\":81608,\"start\":81596},{\"end\":81623,\"start\":81608},{\"end\":81635,\"start\":81623},{\"end\":82084,\"start\":82071},{\"end\":82097,\"start\":82084},{\"end\":82108,\"start\":82097},{\"end\":82126,\"start\":82108},{\"end\":82142,\"start\":82126},{\"end\":82155,\"start\":82142},{\"end\":82634,\"start\":82616},{\"end\":82649,\"start\":82634},{\"end\":83077,\"start\":83061},{\"end\":83090,\"start\":83077},{\"end\":83406,\"start\":83388},{\"end\":83418,\"start\":83406},{\"end\":83431,\"start\":83418},{\"end\":83447,\"start\":83431},{\"end\":83985,\"start\":83975},{\"end\":83995,\"start\":83985},{\"end\":84004,\"start\":83995},{\"end\":84477,\"start\":84468},{\"end\":84487,\"start\":84477},{\"end\":84502,\"start\":84487},{\"end\":84520,\"start\":84502},{\"end\":84835,\"start\":84823},{\"end\":84844,\"start\":84835},{\"end\":84866,\"start\":84844},{\"end\":84879,\"start\":84866},{\"end\":84887,\"start\":84879},{\"end\":84905,\"start\":84887},{\"end\":84918,\"start\":84905},{\"end\":84925,\"start\":84918},{\"end\":85284,\"start\":85273},{\"end\":85294,\"start\":85284},{\"end\":85308,\"start\":85294},{\"end\":85320,\"start\":85308},{\"end\":85332,\"start\":85320},{\"end\":85787,\"start\":85776},{\"end\":85796,\"start\":85787},{\"end\":86143,\"start\":86130},{\"end\":86153,\"start\":86143},{\"end\":86167,\"start\":86153},{\"end\":86179,\"start\":86167},{\"end\":86188,\"start\":86179},{\"end\":86201,\"start\":86188},{\"end\":86212,\"start\":86201},{\"end\":86715,\"start\":86696},{\"end\":86734,\"start\":86715},{\"end\":86744,\"start\":86734},{\"end\":86757,\"start\":86744},{\"end\":86771,\"start\":86757},{\"end\":86782,\"start\":86771},{\"end\":86799,\"start\":86782},{\"end\":86811,\"start\":86799},{\"end\":87168,\"start\":87152},{\"end\":87190,\"start\":87168},{\"end\":87205,\"start\":87190},{\"end\":87688,\"start\":87673},{\"end\":87705,\"start\":87688},{\"end\":87724,\"start\":87705},{\"end\":88345,\"start\":88329},{\"end\":88366,\"start\":88345},{\"end\":88382,\"start\":88366},{\"end\":89040,\"start\":89027},{\"end\":89052,\"start\":89040},{\"end\":89063,\"start\":89052},{\"end\":89072,\"start\":89063},{\"end\":89078,\"start\":89072},{\"end\":89804,\"start\":89791},{\"end\":89815,\"start\":89804},{\"end\":89832,\"start\":89815},{\"end\":89844,\"start\":89832},{\"end\":89860,\"start\":89844},{\"end\":89876,\"start\":89860},{\"end\":89892,\"start\":89876},{\"end\":89904,\"start\":89892},{\"end\":89924,\"start\":89904},{\"end\":89937,\"start\":89924},{\"end\":89954,\"start\":89937},{\"end\":89968,\"start\":89954},{\"end\":89981,\"start\":89968},{\"end\":89997,\"start\":89981},{\"end\":90012,\"start\":89997},{\"end\":90028,\"start\":90012},{\"end\":90049,\"start\":90028},{\"end\":90065,\"start\":90049},{\"end\":90074,\"start\":90065},{\"end\":90086,\"start\":90074},{\"end\":90104,\"start\":90086},{\"end\":91032,\"start\":91022},{\"end\":91044,\"start\":91032},{\"end\":91051,\"start\":91044},{\"end\":91064,\"start\":91051},{\"end\":91078,\"start\":91064},{\"end\":91091,\"start\":91078},{\"end\":91716,\"start\":91701},{\"end\":91730,\"start\":91716},{\"end\":91745,\"start\":91730},{\"end\":92243,\"start\":92225},{\"end\":92260,\"start\":92243},{\"end\":92277,\"start\":92260},{\"end\":92294,\"start\":92277},{\"end\":92314,\"start\":92294},{\"end\":92331,\"start\":92314},{\"end\":92344,\"start\":92331},{\"end\":92358,\"start\":92344},{\"end\":92376,\"start\":92358},{\"end\":92389,\"start\":92376},{\"end\":92402,\"start\":92389},{\"end\":92937,\"start\":92922},{\"end\":92958,\"start\":92937},{\"end\":92967,\"start\":92958},{\"end\":93477,\"start\":93461},{\"end\":93789,\"start\":93775},{\"end\":93802,\"start\":93789},{\"end\":93814,\"start\":93802},{\"end\":93831,\"start\":93814},{\"end\":93842,\"start\":93831},{\"end\":93850,\"start\":93842},{\"end\":93864,\"start\":93850},{\"end\":93873,\"start\":93864},{\"end\":94339,\"start\":94326},{\"end\":94351,\"start\":94339},{\"end\":94370,\"start\":94351},{\"end\":94381,\"start\":94370},{\"end\":94394,\"start\":94381},{\"end\":94406,\"start\":94394},{\"end\":94420,\"start\":94406},{\"end\":94432,\"start\":94420},{\"end\":94448,\"start\":94432},{\"end\":94461,\"start\":94448},{\"end\":94479,\"start\":94461},{\"end\":95139,\"start\":95112},{\"end\":95160,\"start\":95139},{\"end\":95179,\"start\":95160},{\"end\":95195,\"start\":95179},{\"end\":95761,\"start\":95745},{\"end\":95774,\"start\":95761},{\"end\":95788,\"start\":95774},{\"end\":95799,\"start\":95788},{\"end\":95811,\"start\":95799},{\"end\":95837,\"start\":95811},{\"end\":95845,\"start\":95837},{\"end\":96289,\"start\":96271},{\"end\":96307,\"start\":96289},{\"end\":96325,\"start\":96307},{\"end\":96341,\"start\":96325},{\"end\":96353,\"start\":96341},{\"end\":96368,\"start\":96353},{\"end\":96903,\"start\":96889},{\"end\":96917,\"start\":96903},{\"end\":96932,\"start\":96917},{\"end\":96946,\"start\":96932},{\"end\":96959,\"start\":96946},{\"end\":96975,\"start\":96959},{\"end\":97331,\"start\":97318},{\"end\":97343,\"start\":97331},{\"end\":97353,\"start\":97343},{\"end\":97363,\"start\":97353},{\"end\":97371,\"start\":97363},{\"end\":97381,\"start\":97371},{\"end\":97391,\"start\":97381},{\"end\":97405,\"start\":97391},{\"end\":97415,\"start\":97405},{\"end\":97424,\"start\":97415},{\"end\":97437,\"start\":97424},{\"end\":97449,\"start\":97437},{\"end\":97460,\"start\":97449},{\"end\":97472,\"start\":97460},{\"end\":97484,\"start\":97472},{\"end\":97496,\"start\":97484},{\"end\":97515,\"start\":97496},{\"end\":97528,\"start\":97515},{\"end\":98000,\"start\":97984},{\"end\":98019,\"start\":98000},{\"end\":98032,\"start\":98019},{\"end\":98042,\"start\":98032},{\"end\":98055,\"start\":98042},{\"end\":98069,\"start\":98055},{\"end\":98751,\"start\":98737},{\"end\":98766,\"start\":98751},{\"end\":98781,\"start\":98766},{\"end\":98792,\"start\":98781},{\"end\":98803,\"start\":98792},{\"end\":98814,\"start\":98803},{\"end\":99502,\"start\":99493},{\"end\":99514,\"start\":99502},{\"end\":99527,\"start\":99514},{\"end\":99540,\"start\":99527},{\"end\":99550,\"start\":99540},{\"end\":99561,\"start\":99550},{\"end\":99572,\"start\":99561},{\"end\":99581,\"start\":99572},{\"end\":99589,\"start\":99581},{\"end\":100230,\"start\":100221},{\"end\":100241,\"start\":100230},{\"end\":100254,\"start\":100241},{\"end\":100267,\"start\":100254},{\"end\":100278,\"start\":100267},{\"end\":100296,\"start\":100278},{\"end\":100309,\"start\":100296},{\"end\":100317,\"start\":100309},{\"end\":100827,\"start\":100816},{\"end\":100838,\"start\":100827},{\"end\":100853,\"start\":100838},{\"end\":100871,\"start\":100853},{\"end\":101327,\"start\":101319},{\"end\":101340,\"start\":101327},{\"end\":101347,\"start\":101340},{\"end\":101359,\"start\":101347},{\"end\":101530,\"start\":101523},{\"end\":101694,\"start\":101683},{\"end\":101710,\"start\":101694},{\"end\":101720,\"start\":101710},{\"end\":102619,\"start\":102611},{\"end\":102628,\"start\":102619},{\"end\":102641,\"start\":102628},{\"end\":102654,\"start\":102641},{\"end\":102664,\"start\":102654},{\"end\":103177,\"start\":103167},{\"end\":103189,\"start\":103177},{\"end\":103203,\"start\":103189},{\"end\":103222,\"start\":103203},{\"end\":103242,\"start\":103222},{\"end\":103257,\"start\":103242},{\"end\":103916,\"start\":103902},{\"end\":103931,\"start\":103916},{\"end\":103951,\"start\":103931},{\"end\":103968,\"start\":103951},{\"end\":103987,\"start\":103968},{\"end\":104485,\"start\":104471},{\"end\":104496,\"start\":104485},{\"end\":104506,\"start\":104496},{\"end\":104516,\"start\":104506},{\"end\":104528,\"start\":104516},{\"end\":104544,\"start\":104528},{\"end\":104556,\"start\":104544},{\"end\":104566,\"start\":104556},{\"end\":104582,\"start\":104566},{\"end\":104591,\"start\":104582},{\"end\":105018,\"start\":105008},{\"end\":105027,\"start\":105018},{\"end\":105040,\"start\":105027},{\"end\":105054,\"start\":105040},{\"end\":105065,\"start\":105054},{\"end\":105077,\"start\":105065},{\"end\":105087,\"start\":105077},{\"end\":105100,\"start\":105087},{\"end\":105116,\"start\":105100},{\"end\":105467,\"start\":105451},{\"end\":105481,\"start\":105467},{\"end\":105798,\"start\":105788},{\"end\":105808,\"start\":105798},{\"end\":105822,\"start\":105808},{\"end\":105831,\"start\":105822},{\"end\":105843,\"start\":105831},{\"end\":105853,\"start\":105843},{\"end\":105862,\"start\":105853},{\"end\":105876,\"start\":105862},{\"end\":106252,\"start\":106239},{\"end\":106267,\"start\":106252},{\"end\":106281,\"start\":106267},{\"end\":106294,\"start\":106281}]", "bib_venue": "[{\"end\":62961,\"start\":62893},{\"end\":63803,\"start\":63696},{\"end\":64230,\"start\":64213},{\"end\":64524,\"start\":64492},{\"end\":64845,\"start\":64808},{\"end\":65325,\"start\":65249},{\"end\":65735,\"start\":65661},{\"end\":66185,\"start\":66137},{\"end\":66707,\"start\":66651},{\"end\":67287,\"start\":67219},{\"end\":68104,\"start\":68036},{\"end\":68902,\"start\":68796},{\"end\":69684,\"start\":69610},{\"end\":70281,\"start\":70212},{\"end\":70875,\"start\":70754},{\"end\":71474,\"start\":71353},{\"end\":71906,\"start\":71849},{\"end\":72371,\"start\":72293},{\"end\":72952,\"start\":72873},{\"end\":73329,\"start\":73259},{\"end\":74184,\"start\":74097},{\"end\":74711,\"start\":74637},{\"end\":75368,\"start\":75289},{\"end\":75990,\"start\":75930},{\"end\":76485,\"start\":76398},{\"end\":77041,\"start\":76954},{\"end\":77594,\"start\":77482},{\"end\":78300,\"start\":78183},{\"end\":78826,\"start\":78750},{\"end\":79431,\"start\":79319},{\"end\":80094,\"start\":80020},{\"end\":80636,\"start\":80560},{\"end\":81167,\"start\":81119},{\"end\":81678,\"start\":81635},{\"end\":82224,\"start\":82178},{\"end\":82723,\"start\":82649},{\"end\":83134,\"start\":83115},{\"end\":83566,\"start\":83470},{\"end\":84060,\"start\":84004},{\"end\":84563,\"start\":84548},{\"end\":84821,\"start\":84761},{\"end\":85412,\"start\":85355},{\"end\":85852,\"start\":85796},{\"end\":86268,\"start\":86212},{\"end\":86840,\"start\":86834},{\"end\":87288,\"start\":87225},{\"end\":87843,\"start\":87747},{\"end\":88494,\"start\":88382},{\"end\":89219,\"start\":89101},{\"end\":90216,\"start\":90104},{\"end\":91158,\"start\":91091},{\"end\":91860,\"start\":91768},{\"end\":92471,\"start\":92425},{\"end\":93023,\"start\":92967},{\"end\":93526,\"start\":93500},{\"end\":93940,\"start\":93873},{\"end\":94558,\"start\":94479},{\"end\":95295,\"start\":95218},{\"end\":95908,\"start\":95845},{\"end\":96424,\"start\":96368},{\"end\":97010,\"start\":96994},{\"end\":97316,\"start\":97242},{\"end\":98188,\"start\":98092},{\"end\":98940,\"start\":98837},{\"end\":99685,\"start\":99612},{\"end\":100416,\"start\":100340},{\"end\":100936,\"start\":100880},{\"end\":101369,\"start\":101359},{\"end\":101997,\"start\":101748},{\"end\":102751,\"start\":102664},{\"end\":103376,\"start\":103280},{\"end\":104043,\"start\":103987},{\"end\":104631,\"start\":104615},{\"end\":105006,\"start\":104931},{\"end\":105539,\"start\":105502},{\"end\":105916,\"start\":105900},{\"end\":106383,\"start\":106294},{\"end\":62980,\"start\":62963},{\"end\":63821,\"start\":63805},{\"end\":64554,\"start\":64526},{\"end\":65341,\"start\":65327},{\"end\":66235,\"start\":66187},{\"end\":66730,\"start\":66709},{\"end\":67377,\"start\":67289},{\"end\":68123,\"start\":68106},{\"end\":69013,\"start\":68904},{\"end\":69745,\"start\":69686},{\"end\":70299,\"start\":70283},{\"end\":70983,\"start\":70877},{\"end\":71582,\"start\":71476},{\"end\":72392,\"start\":72373},{\"end\":74258,\"start\":74186},{\"end\":74799,\"start\":74713},{\"end\":75388,\"start\":75370},{\"end\":76037,\"start\":75992},{\"end\":76559,\"start\":76487},{\"end\":77115,\"start\":77043},{\"end\":77615,\"start\":77596},{\"end\":79458,\"start\":79433},{\"end\":80155,\"start\":80096},{\"end\":80652,\"start\":80638},{\"end\":81217,\"start\":81169},{\"end\":81732,\"start\":81680},{\"end\":82243,\"start\":82226},{\"end\":82784,\"start\":82725},{\"end\":83589,\"start\":83568},{\"end\":84076,\"start\":84062},{\"end\":85417,\"start\":85414},{\"end\":85895,\"start\":85854},{\"end\":86285,\"start\":86270},{\"end\":87307,\"start\":87290},{\"end\":87946,\"start\":87845},{\"end\":88515,\"start\":88496},{\"end\":89340,\"start\":89221},{\"end\":90248,\"start\":90218},{\"end\":91224,\"start\":91160},{\"end\":91879,\"start\":91862},{\"end\":92490,\"start\":92473},{\"end\":93046,\"start\":93025},{\"end\":95314,\"start\":95297},{\"end\":95958,\"start\":95910},{\"end\":96447,\"start\":96426},{\"end\":98287,\"start\":98190},{\"end\":99058,\"start\":98942},{\"end\":99755,\"start\":99750},{\"end\":100432,\"start\":100418},{\"end\":100958,\"start\":100938},{\"end\":101375,\"start\":101371},{\"end\":102020,\"start\":101999},{\"end\":102825,\"start\":102753},{\"end\":103469,\"start\":103378},{\"end\":104066,\"start\":104045},{\"end\":104643,\"start\":104633},{\"end\":105928,\"start\":105918},{\"end\":106392,\"start\":106385}]"}}}, "year": 2023, "month": 12, "day": 17}