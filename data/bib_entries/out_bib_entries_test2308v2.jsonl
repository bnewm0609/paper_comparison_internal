{"bib_hash_or_arxiv_id": "dd352e3c3918b783377312dd4399a284589a24d1", "title": "The Kinetics Human Action Video Dataset", "corpus_id": 27300853, "metadata": {"paperId": "86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6", "externalIds": {"DBLP": "journals/corr/KayCSZHVVGBNSZ17", "ArXiv": "1705.06950", "MAG": "2619947201", "CorpusId": 27300853}, "corpusId": 27300853, "title": "The Kinetics Human Action Video Dataset", "abstract": "We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.", "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "Will Kay, Jo\u00e3o Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. 2017. The Kinetics Human Action Video Dataset. CoRR abs/1705.06950 (2017).", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "6e2fa15158ea3b4f5b8d04f72da2aa372af27b50", "title": "OpenXAI: Towards a Transparent Evaluation of Model Explanations", "corpus_id": 249926576, "metadata": {"paperId": "bc45043633e933bd0969a94ef442587585c764b7", "externalIds": {"ArXiv": "2206.11104", "CorpusId": 249926576}, "corpusId": 249926576, "title": "OpenXAI: Towards a Transparent Evaluation of Model Explanations", "abstract": "While several types of post hoc explanation methods (e.g., feature attribution methods) have been proposed in recent literature, there is little to no work on systematically benchmarking these methods in an efficient and transparent manner. Here, we introduce OpenXAI, a comprehensive and extensible open source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, (ii) open-source implementations of twenty-two quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, and (iii) the first ever public XAI leaderboards to benchmark explanations. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. Overall, OpenXAI provides an automated end-to-end pipeline that not only simplifies and standardizes the evaluation of post hoc explanation methods, but also promotes transparency and reproducibility in benchmarking these methods. OpenXAI datasets and data loaders, implementations of state-of-the-art explanation methods and evaluation metrics, as well as leaderboards are publicly available at https://open-xai.github.io/.", "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "Chirag Agarwal, Satyapriya Krishna, Eshika Saxena, Martin Pawelczyk, Nari Johnson, Isha Puri, Marinka Zitnik, and Himabindu Lakkaraju. OpenXAI: Towards a Transparent Evaluation of Model Explanations, January 2023.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "a36f740e218f582e8283c0072f42067d2088a222", "title": "\"Saam: Stealthy adversarial attack on monoculor depth estimation,\"", "corpus_id": 260682704, "metadata": {"paperId": "79411a597f24ef32126c388b3813a8070f93c75d", "externalIds": {"ArXiv": "2308.03108", "DOI": "10.1109/access.2024.3353042", "CorpusId": 260682704}, "corpusId": 260682704, "title": "SAAM: Stealthy Adversarial Attack on Monocular Depth Estimation", "abstract": "In this paper, we investigate the vulnerability of MDE to adversarial patches. We propose a novel \\underline{S}tealthy \\underline{A}dversarial \\underline{A}ttacks on \\underline{M}DE (SAAM) that compromises MDE by either corrupting the estimated distance or causing an object to seamlessly blend into its surroundings. Our experiments, demonstrate that the designed stealthy patch successfully causes a DNN-based MDE to misestimate the depth of objects. In fact, our proposed adversarial patch achieves a significant 60\\% depth error with 99\\% ratio of the affected region. Importantly, despite its adversarial nature, the patch maintains a naturalistic appearance, making it inconspicuous to human observers. We believe that this work sheds light on the threat of adversarial attacks in the context of MDE on edge devices. We hope it raises awareness within the community about the potential real-life harm of such attacks and encourages further research into developing more robust and adaptive defense mechanisms.", "isOpenAccess": true, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10388324.pdf", "status": "GOLD"}}, "bib_entry_raw": "A. Guesmi, M. A. Hanif, B. Ouni, and M. Shafique, \u201cSaam: Stealthy adversarial attack on monoculor depth estimation,\u201d 2023.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "95e54b29cd6da1bfd4a7bac1c7214e83ff2d18d0", "title": "\"Adversarial patch attacks on monocular depth estimation networks,\"", "corpus_id": 222177159, "metadata": {"paperId": "54065d21ad9f7b8597f14a39debcee383114b092", "externalIds": {"MAG": "3090358134", "ArXiv": "2010.03072", "DBLP": "journals/access/YamanakaM0F20", "DOI": "10.1109/ACCESS.2020.3027372", "CorpusId": 222177159}, "corpusId": 222177159, "title": "Adversarial Patch Attacks on Monocular Depth Estimation Networks", "abstract": "Thanks to the excellent learning capability of deep convolutional neural networks (CNN), monocular depth estimation using CNNs has achieved great success in recent years. However, depth estimation from a monocular image alone is essentially an ill-posed problem, and thus, it seems that this approach would have inherent vulnerabilities. To reveal this limitation, we propose a method of adversarial patch attack on monocular depth estimation. More specifically, we generate artificial patterns (adversarial patches) that can fool the target methods into estimating an incorrect depth for the regions where the patterns are placed. Our method can be implemented in the real world by physically placing the printed patterns in real scenes. We also analyze the behavior of monocular depth estimation under attacks by visualizing the activation levels of the intermediate layers and the regions potentially affected by the adversarial attack.", "isOpenAccess": true, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09207958.pdf", "status": "GOLD"}}, "bib_entry_raw": "K. Yamanaka, R. Matsumoto, K. Takahashi, and T. Fujii, \u201cAdversarial patch attacks on monocular depth estimation networks,\u201d IEEE Access, vol. 8, pp. 179 094\u2013179 104, 2020.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "e3191321ede4b3f9ba57ea0129e6361c1f0fc230", "title": "\"Aparate: Adaptive adversarial patch for cnn-based monocular depth estimation for autonomous navigation,\"", "corpus_id": 257279866, "metadata": {"paperId": "1bd2ff4afa465b40340847335cb9b8e943b3f878", "externalIds": {"ArXiv": "2303.01351", "DBLP": "journals/corr/abs-2303-01351", "DOI": "10.48550/arXiv.2303.01351", "CorpusId": 257279866}, "corpusId": 257279866, "title": "APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth Estimation for Autonomous Navigation", "abstract": "In recent times, monocular depth estimation (MDE) has experienced significant advancements in performance, largely attributed to the integration of innovative architectures, i.e., convolutional neural networks (CNNs) and Transformers. Nevertheless, the susceptibility of these models to adversarial attacks has emerged as a noteworthy concern, especially in domains where safety and security are paramount. This concern holds particular weight for MDE due to its critical role in applications like autonomous driving and robotic navigation, where accurate scene understanding is pivotal. To assess the vulnerability of CNN-based depth prediction methods, recent work tries to design adversarial patches against MDE. However, the existing approaches fall short of inducing a comprehensive and substantially disruptive impact on the vision system. Instead, their influence is partial and confined to specific local areas. These methods lead to erroneous depth predictions only within the overlapping region with the input image, without considering the characteristics of the target object, such as its size, shape, and position. In this paper, we introduce a novel adversarial patch named APARATE. This patch possesses the ability to selectively undermine MDE in two distinct ways: by distorting the estimated distances or by creating the illusion of an object disappearing from the perspective of the autonomous system. Notably, APARATE is designed to be sensitive to the shape and scale of the target object, and its influence extends beyond immediate proximity. APARATE, results in a mean depth estimation error surpassing $0.5$, significantly impacting as much as $99\\%$ of the targeted region when applied to CNN-based MDE models. Furthermore, it yields a significant error of $0.34$ and exerts substantial influence over $94\\%$ of the target region in the context of Transformer-based MDE.", "isOpenAccess": true, "openAccessPdf": {"url": "http://arxiv.org/pdf/2303.01351", "status": "CLOSED"}}, "bib_entry_raw": "A. Guesmi, M. A. Hanif, I. Alouani, and M. Shafique, \u201cAparate: Adaptive adversarial patch for cnn-based monocular depth estimation for autonomous navigation,\u201d 2023.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "07d35d7967a43f96ce0a6e9237a0909ac112a9e3", "title": "\"On adversarial patches: Real-world attack on arcface-100 face recognition system,\"", "corpus_id": 204734223, "metadata": {"paperId": "072c716ededeee9c574f598eb6e1af37fa81f5aa", "externalIds": {"MAG": "3101267861", "ArXiv": "1910.07067", "DBLP": "journals/corr/abs-1910-07067", "DOI": "10.1109/SIBIRCON48586.2019.8958134", "CorpusId": 204734223}, "corpusId": 204734223, "title": "On Adversarial Patches: Real-World Attack on ArcFace-100 Face Recognition System", "abstract": "Recent works showed the vulnerability of image classifiers to adversarial attacks in the digital domain. However, the majority of attacks involve adding small perturbation to an image to fool the classifier. Unfortunately, such procedures can not be used to conduct a real-world attack, where adding an adversarial attribute to the photo is a more practical approach. In this paper, we study the problem of real-world attacks on face recognition systems. We examine security of one of the best public face recognition systems, LResNet100E-IR with ArcFace loss, and propose a simple method to attack it in the physical world. The method suggests creating an adversarial patch that can be printed, added as a face attribute and photographed; the photo of a person with such attribute is then passed to the classifier such that the classifier's recognized class changes from correct to the desired one. Proposed generating procedure allows projecting adversarial patches not only on different areas of the face, such as nose or forehead but also on some wearable accessory, such as eyeglasses.", "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1910.07067", "status": "GREEN"}}, "bib_entry_raw": "M. A. Pautov, G. Melnikov, E. Kaziakhmedov, K. Kireev, and A. Petiushko, \u201cOn adversarial patches: Real-world attack on arcface-100 face recognition system,\u201d 2019 International Multi-Conference on Engineering, Computer and Information Sciences (SIBIRCON), pp. 0391\u20130396, 2019.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "3a44dac4ee488d48c809f412a148cc20569a0a06", "title": null, "corpus_id": -1, "metadata": null, "bib_entry_raw": "Hart R. E., et al., 2016, Monthly Notices of the Royal Astronomical Society, 461, 3663", "contained_arXiv_ids": [], "contained_links": [{"url": "http://dx.doi.org/10.1093/mnras/stw1588", "text": "Monthly Notices of the Royal Astronomical Society, 461, 3663", "start": 26, "end": 86}]}
{"bib_hash_or_arxiv_id": "65b562eaeea6c4ffe62c65924a681454f9a32f8b", "title": "Narcissus: A practical clean-label backdoor attack with limited information", "corpus_id": 248084906, "metadata": {"paperId": "175828c468666dde43947b8c8f45a1f0045f7419", "externalIds": {"DBLP": "journals/corr/abs-2204-05255", "ArXiv": "2204.05255", "DOI": "10.1145/3576915.3616617", "CorpusId": 248084906}, "corpusId": 248084906, "title": "Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information", "abstract": "Backdoor attacks introduce manipulated data into a machine learning model's training set, causing the model to misclassify inputs with a trigger during testing to achieve a desired outcome by the attacker. For backdoor attacks to bypass human inspection, it is essential that the injected data appear to be correctly labeled. The attacks with such property are often referred to as \"clean-label attacks.\" The success of current clean-label backdoor methods largely depends on access to the complete training set. Yet, accessing the complete dataset is often challenging or unfeasible since it frequently comes from varied, independent sources, like images from distinct users. It remains a question of whether backdoor attacks still present real threats. In this paper, we provide an affirmative answer to this question by designing an algorithm to launch clean-label backdoor attacks using only samples from the target class and public out-of-distribution data. By inserting carefully crafted malicious examples totaling less than 0.5% of the target class size and 0.05% of the full training set size, we can manipulate the model to misclassify arbitrary inputs into the target class when they contain the backdoor trigger. Importantly, the trained poisoned model retains high accuracy for regular test samples without the trigger, as if the model is trained on untainted data. Our technique is consistently effective across various datasets, models, and even when the trigger is injected into the physical world. We explore the space of defenses and find that Narcissus can evade the latest state-of-the-art defenses in their vanilla form or after a simple adaptation. We analyze the effectiveness of our attack - the synthesized Narcissus trigger contains durable features as persistent as the original target class features. Attempts to remove the trigger inevitably hurt model accuracy first.", "isOpenAccess": true, "openAccessPdf": {"url": "https://dl.acm.org/doi/pdf/10.1145/3576915.3616617", "status": "HYBRID"}}, "bib_entry_raw": "Y. Zeng, M. Pan, H. A. Just, L. Lyu, M. Qiu, R. Jia, Narcissus: A practical clean-label backdoor attack with limited information, arXiv preprint arXiv:2204.05255 (2022).", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "b3584710b9953fd2496e1e8d3009588c237048b4", "title": "\"Improving transferability of adversarial patches on face recognition with generative models,\"", "corpus_id": 235670046, "metadata": {"paperId": "cef2370a769df907e0badcf1c80081f7efcdb1f3", "externalIds": {"DBLP": "journals/corr/abs-2106-15058", "ArXiv": "2106.15058", "DOI": "10.1109/CVPR46437.2021.01167", "CorpusId": 235670046}, "corpusId": 235670046, "title": "Improving Transferability of Adversarial Patches on Face Recognition with Generative Models", "abstract": "Face recognition is greatly improved by deep convolutional neural networks (CNNs). Recently, these face recognition models have been used for identity authentication in security sensitive applications. However, deep CNNs are vulnerable to adversarial patches, which are physically realizable and stealthy, raising new security concerns on the real-world applications of these models. In this paper, we evaluate the robustness of face recognition models using adversarial patches based on transferability, where the attacker has limited accessibility to the target models. First, we extend the existing transfer-based attack techniques to generate transferable adversarial patches. However, we observe that the transferability is sensitive to initialization and degrades when the perturbation magnitude is large, indicating the overfitting to the substitute models. Second, we propose to regularize the adversarial patches on the low dimensional data manifold. The manifold is represented by generative models pre-trained on legitimate human face images. Using face-like features as adversarial perturbations through optimization on the manifold, we show that the gaps between the responses of substitute models and the target models dramatically decrease, exhibiting a better transferability. Extensive digital world experiments are conducted to demonstrate the superiority of the proposed method in the black-box setting. We apply the proposed method in the physical world as well.", "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2106.15058", "status": "GREEN"}}, "bib_entry_raw": "Z. Xiao, X. Gao, C. Fu, Y. Dong, W. Gao, X. Zhang, J. Zhou, and J. Zhu, \u201cImproving transferability of adversarial patches on face recognition with generative models,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 11 845\u201311 854.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "f8c190a84137e57162fcf91f7288b9eeba166211", "title": "A hybrid deep learning based intrusion detection system using spatial-temporal representation of in-vehicle network traffic", "corpus_id": 247877345, "metadata": {"paperId": "fff3aca509ab1ec379d55b6fb05aa942b1a16b6c", "externalIds": {"DBLP": "journals/vcomm/LoATACK22", "DOI": "10.1016/j.vehcom.2022.100471", "CorpusId": 247877345}, "corpusId": 247877345, "title": "A hybrid deep learning based intrusion detection system using spatial-temporal representation of in-vehicle network traffic", "abstract": null, "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "Wei Lo, Hamed Alqahtani, Kutub Thakur, Ahmad Almadhor, Subhash Chander, and Gulshan Kumar. A hybrid deep learning based intrusion detection system using spatial-temporal representation of in-vehicle network traffic. Vehicular Communications, 35:100471, 2022.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "25e110173e7d46b727eeb2a8fa66d3336eb85d0f", "title": "Self-supervised anomaly detection for in-vehicle network using noised pseudo normal data", "corpus_id": 232235061, "metadata": {"paperId": "7cdc2915f69c9d87ecb734eb9d2bcbedb8e1b60e", "externalIds": {"DBLP": "journals/tvt/SongK21", "DOI": "10.1109/TVT.2021.3051026", "CorpusId": 232235061}, "corpusId": 232235061, "title": "Self-Supervised Anomaly Detection for In-Vehicle Network Using Noised Pseudo Normal Data", "abstract": "As the risk of cyber and safety threats to vehicle systems has increased, the anomaly detection in in-vehicle networks (IVN) has received the attention of researchers. Although, machine-learning-based anomaly detection methods have been proposed, there are limitations in detecting unknown attacks that the model has not learned because general supervised learning-based approaches depend on training dataset. To solve this problem, we propose a novel self-supervised method for IVN anomaly detection using noised pseudo normal data. The proposed method consists of two deep-learning models of the generator and the detector, which generates noised pseudo normal data and detects anomalies, respectively. Firstly, the generator is trained with only normal network traffic to generate pseudo normal traffic data. Then, the anomaly detector is trained to classify normal traffic and noised pseudo normal traffic as normal and abnormal, respectively. The experimental results demonstrate that the anomaly detection models, trained with the proposed method, not only significantly improved in the detection of unknown attacks, but also outperformed other semi-supervised learning-based methods.", "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "Hyun Min Song and Huy Kang Kim. Self-supervised anomaly detection for in-vehicle network using noised pseudo normal data. IEEE Transactions on Vehicular Technology, 70(2):1098\u20131108, 2021.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "813cafe807e03737a636e48e95a2240af371ec7a", "title": null, "corpus_id": -1, "metadata": null, "bib_entry_raw": "Willett K. W., et al., 2013, , 435, 2835", "contained_arXiv_ids": [], "contained_links": [{"url": "http://dx.doi.org/10.1093/mnras/stt1458", "text": ",", "start": 13, "end": 14}, {"url": "https://ui.adsabs.harvard.edu/abs/2013MNRAS.435.2835W", "text": "435, 2835", "start": 31, "end": 40}]}
{"bib_hash_or_arxiv_id": "fa0a6102d90ff8fde4b0f7f613b7228af29ecd4a", "title": "Gids: Gan based intrusion detection system for in-vehicle network", "corpus_id": 53234493, "metadata": {"paperId": "21b7dea0b9751c6bc7db2086109b6b1a4f9e60b5", "externalIds": {"MAG": "2959120033", "ArXiv": "1907.07377", "DBLP": "conf/pst/SeoSK18", "DOI": "10.1109/PST.2018.8514157", "CorpusId": 53234493}, "corpusId": 53234493, "title": "GIDS: GAN based Intrusion Detection System for In-Vehicle Network", "abstract": "A Controller Area Network (CAN) bus in the vehicles is an efficient standard bus enabling communication between all Electronic Control Units (ECU). However, CAN bus is not enough to protect itself because of lack of security features. To detect suspicious network connections effectively, the intrusion detection system (IDS) is strongly required. Unlike the traditional IDS for Internet, there are small number of known attack signatures for vehicle networks. Also, IDS for vehicle requires high accuracy because any false-positive error can seriously affect the safety of the driver. To solve this problem, we propose a novel IDS model for in-vehicle networks, GIDS (GAN based Intrusion Detection System) using deep-learning model, Generative Adversarial Nets. GIDS can learn to detect unknown attacks using only normal data. As experiment result, GIDS shows high detection accuracy for four unknown attacks.", "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1907.07377", "status": "GREEN"}}, "bib_entry_raw": "Eunbi Seo, Hyun Min Song, and Huy Kang Kim. Gids: Gan based intrusion detection system for in-vehicle network. In 2018 16th Annual Conference on Privacy, Security and Trust (PST), pages 1\u20136. IEEE, 2018.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "932ba84aadd95c3dc7cf60ccd1c53777c81b00c7", "title": "\"Projection-based physical adversarial attack for monocular depth estimation,\"", "corpus_id": 255364274, "metadata": {"paperId": "62d9d40628bbbf6b33110e295f8cb4f203bcbee7", "externalIds": {"DBLP": "journals/ieicetd/DaimoO23", "DOI": "10.1587/transinf.2022mul0001", "CorpusId": 255364274}, "corpusId": 255364274, "title": "Projection-Based Physical Adversarial Attack for Monocular Depth Estimation", "abstract": null, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.jstage.jst.go.jp/article/transinf/E106.D/1/E106.D_2022MUL0001/_pdf", "status": "GOLD"}}, "bib_entry_raw": "R. Daimo and S. Ono, \u201cProjection-based physical adversarial attack for monocular depth estimation,\u201d IEICE TRANSACTIONS on Information and Systems, vol. 106, no. 1, pp. 31\u201335, 2023.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "639acf6a4253a7fbdd254233b8bf77f3a7fbf8ca", "title": "Intrusion detection system using deep neural network for in-vehicle network security", "corpus_id": 18256723, "metadata": {"paperId": "23cff307ec800043b83103deefc27a8f3a15fbda", "externalIds": {"PubMedCentral": "4896428", "MAG": "2414564754", "DOI": "10.1371/journal.pone.0155781", "CorpusId": 18256723, "PubMed": "27271802"}, "corpusId": 18256723, "title": "Intrusion Detection System Using Deep Neural Network for In-Vehicle Network Security", "abstract": "A novel intrusion detection system (IDS) using a deep neural network (DNN) is proposed to enhance the security of in-vehicular network. The parameters building the DNN structure are trained with probability-based feature vectors that are extracted from the in-vehicular network packets. For a given packet, the DNN provides the probability of each class discriminating normal and attack packets, and, thus the sensor can identify any malicious attack to the vehicle. As compared to the traditional artificial neural network applied to the IDS, the proposed technique adopts recent advances in deep learning studies such as initializing the parameters through the unsupervised pre-training of deep belief networks (DBN), therefore improving the detection accuracy. It is demonstrated with experimental results that the proposed technique can provide a real-time response to the attack with a significantly improved detection ratio in controller area network (CAN) bus.", "isOpenAccess": true, "openAccessPdf": {"url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0155781&type=printable", "status": "GOLD"}}, "bib_entry_raw": "Min-Joo Kang and Je-Won Kang. Intrusion detection system using deep neural network for in-vehicle network security. PloS one, 11(6):e0155781, 2016.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "ff7d6c24f75f31384a49a8a300ecccfe3eed3f7d", "title": "\"Advhat: Real-world adversarial attack on arcface face id system,\"", "corpus_id": 201645162, "metadata": {"paperId": "e3dacba4399fc66f31779060db730ea423f341d8", "externalIds": {"DBLP": "conf/icpr/KomkovP20", "ArXiv": "1908.08705", "MAG": "2969664989", "DOI": "10.1109/ICPR48806.2021.9412236", "CorpusId": 201645162}, "corpusId": 201645162, "title": "AdvHat: Real-World Adversarial Attack on ArcFace Face ID System", "abstract": "In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.", "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1908.08705", "status": "GREEN"}}, "bib_entry_raw": "S. Komkov and A. Petiushko, \u201cAdvhat: Real-world adversarial attack on arcface face id system,\u201d in 2020 25th International Conference on Pattern Recognition (ICPR). IEEE, 2021, pp. 819\u2013826.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "4b43f58c0cec14130cef6a97c1211082d9b8dba1", "title": "Can-bert do it? controller area network intrusion detection system based on bert language model", "corpus_id": 252967697, "metadata": {"paperId": "1a205b0e91393d3f1837ce54fb2e44b56f12169a", "externalIds": {"ArXiv": "2210.09439", "DBLP": "journals/corr/abs-2210-09439", "DOI": "10.1109/AICCSA56895.2022.10017800", "CorpusId": 252967697}, "corpusId": 252967697, "title": "CAN-BERT do it? Controller Area Network Intrusion Detection System based on BERT Language Model", "abstract": "Due to the rising number of sophisticated customer functionalities, electronic control units (ECUs) are increasingly integrated into modern automotive systems. However, the high connectivity between the in-vehicle and the external networks paves the way for hackers who could exploit in-vehicle network protocols' vulnerabilities. Among these protocols, the Controller Area Network (CAN), known as the most widely used in-vehicle networking technology, lacks encryption and authentication mechanisms, making the communications delivered by distributed ECUs insecure. Inspired by the outstanding performance of bidirectional encoder representations from transformers (BERT) for improving many natural language processing tasks, we propose in this paper \u201cCAN-BERT\u201d, a deep learning based network intrusion detection system, to detect cyber attacks on CAN bus protocol. We show that the BERT model can learn the sequence of arbitration identifiers (IDs) in the CAN bus for anomaly detection using the \u201cmasked language model\u201d unsupervised training objective. The experimental results on the \u201cCar Hacking: Attack & Defense Challenge 2020\u201d dataset show that \u201cCAN-BERT\u201d outperforms state-of-the-art approaches. In addition to being able to identify in-vehicle intrusions in real-time within 0.8 ms to 3 ms w.r.t CAN ID sequence length, it can also detect a wide variety of cyberattacks with an F1-score of between 0.81 and 0.99.", "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2210.09439", "status": "GREEN"}}, "bib_entry_raw": "Natasha Alkhatib, Maria Mushtaq, Hadi Ghauch, and Jean-Luc Danger. Can-bert do it? controller area network intrusion detection system based on bert language model. In 2022 IEEE/ACS 19th International Conference on Computer Systems and Applications (AICCSA), pages 1\u20138. IEEE, 2022.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "8c2c61fc0c96472286beca676c64331a020711c8", "title": "\"On brightness agnostic adversarial examples against face recognition systems,\"", "corpus_id": 238215344, "metadata": {"paperId": "57f58f0c4321ab8c230f710aec375841db7deef5", "externalIds": {"ArXiv": "2109.14205", "DBLP": "conf/biosig/SinghMKA21", "DOI": "10.1109/BIOSIG52210.2021.9548291", "CorpusId": 238215344}, "corpusId": 238215344, "title": "On Brightness Agnostic Adversarial Examples Against Face Recognition Systems", "abstract": "This paper introduces a novel adversarial example generation method against face recognition systems (FRSs). An adversarial example (AX) is an image with deliberately crafted noise to cause incorrect predictions by a target system. The AXs generated from our method remain robust under real-world brightness changes. Our method performs nonlinear brightness transformations while leveraging the concept of curriculum learning during the attack generation procedure. We demonstrate that our method outperforms conventional techniques from comprehensive experimental investigations in the digital and physical world. Furthermore, this method enables practical risk assessment of FRSs against brightness agnostic AXs.", "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2109.14205", "status": "GREEN"}}, "bib_entry_raw": "I. Singh, S. Momiyama, K. Kakizaki, and T. Araki, \u201cOn brightness agnostic adversarial examples against face recognition systems,\u201d in 2021 International Conference of the Biometrics Special Interest Group (BIOSIG). IEEE, 2021, pp. 1\u20135.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "3dfd446db36563cc1eeb028310478bbb775a0237", "title": "Patch-VQ: 'Patching Up' the Video Quality Problem", "corpus_id": 227210156, "metadata": {"paperId": "2e41d8b57af02799b4f9e89977f62c748eb995fe", "externalIds": {"DBLP": "journals/corr/abs-2011-13544", "MAG": "3108337142", "ArXiv": "2011.13544", "DOI": "10.1109/CVPR46437.2021.01380", "CorpusId": 227210156}, "corpusId": 227210156, "title": "Patch-VQ: \u2018Patching Up\u2019 the Video Quality Problem", "abstract": "No-reference (NR) perceptual video quality assessment (VQA) is a complex, unsolved, and important problem for social and streaming media applications. Efficient and accurate video quality predictors are needed to monitor and guide the processing of billions of shared, often imperfect, user-generated content (UGC). Unfortunately, current NR models are limited in their prediction capabilities on real-world, \"in-the-wild\" UGC video data. To advance progress on this problem, we created the largest (by far) subjective video quality dataset, containing 38,811 real-world distorted videos and 116,433 space-time localized video patches (\u2018v-patches\u2019), and 5.5M human perceptual quality annotations. Using this, we created two unique NR-VQA models: (a) a local-to-global region-based NR VQA architecture (called PVQ) that learns to predict global video quality and achieves state-of-the-art performance on 3 UGC datasets, and (b) a first-of-a-kind space-time video quality mapping engine (called PVQ Mapper) that helps localize and visualize perceptual distortions in space and time. The entire dataset and prediction models are freely available at https://live.ece.utexas.edu/research.php.", "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2011.13544", "status": "GREEN"}}, "bib_entry_raw": "Zhenqiang Ying, Maniratnam Mandal, Deepti Ghadiyaram, and Alan Bovik. 2021. Patch-VQ: 'Patching Up' the Video Quality Problem. In CVPR. Computer Vision Foundation / IEEE, 14019\u201314029.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "67c1efc7cab13a0850d56c2b24101d4f53ec0db8", "title": "Stc-ids: Spatial-temporal correlation feature analyzing based intrusion detection system for intelligent connected vehicles", "corpus_id": 248377504, "metadata": {"paperId": "816bc1846ace6cdc4197cde9bba2440c5949d6d8", "externalIds": {"DBLP": "journals/corr/abs-2204-10990", "ArXiv": "2204.10990", "DOI": "10.1002/int.23012", "CorpusId": 248377504}, "corpusId": 248377504, "title": "STC\u2010IDS: Spatial\u2013temporal correlation feature analyzing based intrusion detection system for intelligent connected vehicles", "abstract": "Intrusion detection is an important defensive measure for automotive communications security. Accurate frame detection models assist vehicles to avoid malicious attacks. Uncertainty and diversity regarding attack methods make this task challenging. However, the existing works have the limitation of only considering local features or the weak feature mapping of multifeatures. To address these limitations, we present a novel model for automotive intrusion detection by spatial\u2013temporal correlation (STC) features of in\u2010vehicle communication traffic (intrusion detection system [IDS]). Specifically, the proposed model exploits an encoding\u2010detection architecture. In the encoder part, spatial and temporal relations are encoded simultaneously. To strengthen the relationship between features, the attention\u2010based convolutional network still captures spatial and channel features to increase the receptive field, while attention\u2010long short\u2010term memory builds meaningful relationships from previous time series or crucial bytes. The encoded information is then passed to detector for generating forceful spatial\u2013temporal attention features and enabling anomaly classification. In particular, single\u2010frame and multiframe models are constructed to present different advantages, respectively. Under automatic hyperparameter selection based on Bayesian optimization, the model is trained to attain the best performance. Extensive empirical studies based on a real\u2010world vehicle attack data set demonstrate that STC\u2010IDS has outperformed baseline methods and obtains fewer false\u2010alarm rates while maintaining efficiency.", "isOpenAccess": true, "openAccessPdf": null}, "bib_entry_raw": "Pengzhou Cheng, Mu Han, Aoxue Li, and Fengwei Zhang. Stc-ids: Spatial\u2013temporal correlation feature analyzing based intrusion detection system for intelligent connected vehicles. International Journal of Intelligent Systems, 37(11):9532\u20139561, 2022.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "5e18118b535c0e44d7d004b4c8114668660f16e2", "title": "\"Ipatch: A remote adversarial patch,\"", "corpus_id": 233481695, "metadata": {"paperId": "6b60da73fd18a73d12482c09558c07d6a48a575f", "externalIds": {"DBLP": "journals/cybersec/Mirsky23", "ArXiv": "2105.00113", "DOI": "10.1186/s42400-023-00145-0", "CorpusId": 233481695}, "corpusId": 233481695, "title": "IPatch: a remote adversarial patch", "abstract": null, "isOpenAccess": true, "openAccessPdf": {"url": "https://cybersecurity.springeropen.com/counter/pdf/10.1186/s42400-023-00145-0", "status": "GOLD"}}, "bib_entry_raw": "Y. Mirsky, \u201cIpatch: A remote adversarial patch,\u201d 2021.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "0be83d97ac938367712de08a5210e1a8d6239c49", "title": "Scanqa: 3d question answering for spatial scene understanding", "corpus_id": 245334889, "metadata": {"paperId": "8f6c652a392995bd047a2f7b94474ab1e6e23ff0", "externalIds": {"ArXiv": "2112.10482", "DBLP": "journals/corr/abs-2112-10482", "DOI": "10.1109/CVPR52688.2022.01854", "CorpusId": 245334889}, "corpusId": 245334889, "title": "ScanQA: 3D Question Answering for Spatial Scene Understanding", "abstract": "We propose a new 3D spatial understanding task for 3D question answering (3D-QA). In the 3D-QA task, models receive visual information from the entire 3D scene of a rich RGB-D indoor scan and answer given textual questions about the 3D scene. Unlike the 2D-question answering of visual question answering, the conventional 2D-QA models suffer from problems with spatial understanding of object alignment and directions and fail in object localization from the textual questions in 3D-QA. We propose a baseline model for 3D-QA, called the ScanQA11https://github.com/ATR-DBI/ScanQA, which learns a fused descriptor from 3D object proposals and encoded sentence embeddings. This learned descriptor correlates language expressions with the underlying geometric features of the 3D scan and facilitates the regression of 3D bounding boxes to determine the described objects in textual questions. We collected human-edited question-answer pairs with free-form answers grounded in 3D objects in each 3D scene. Our new ScanQA dataset contains over 41k question-answer pairs from 800 indoor scenes obtained from the ScanNet dataset. To the best of our knowledge, ScanQA is the first large-scale effort to perform object-grounded question answering in 3D environments.", "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2112.10482", "status": "GREEN"}}, "bib_entry_raw": "Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 19129\u201319139, 2022.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "015d8801c962d00d765fba000d88d4e4468d82c9", "title": "\"Adversarial light projection attacks on face recognition systems: A feasibility study,\"", "corpus_id": 214641445, "metadata": {"paperId": "823ba4ff0f0162dfa0e2525acfb6327a5f2283fc", "externalIds": {"MAG": "3034823537", "DBLP": "conf/cvpr/NguyenAWY20", "ArXiv": "2003.11145", "DOI": "10.1109/CVPRW50498.2020.00415", "CorpusId": 214641445}, "corpusId": 214641445, "title": "Adversarial Light Projection Attacks on Face Recognition Systems: A Feasibility Study", "abstract": "Deep learning-based systems have been shown to be vulnerable to adversarial attacks in both digital and physical domains. While feasible, digital attacks have limited applicability in attacking deployed systems, including face recognition systems, where an adversary typically has access to the input and not the transmission channel. In such setting, physical attacks that directly provide a malicious input through the input channel pose a bigger threat. We investigate the feasibility of conducting real-time physical attacks on face recognition systems using adversarial light projections. A setup comprising a commercially available web camera and a projector is used to conduct the attack. The adversary uses a transformation-invariant adversarial pattern generation method to generate a digital adversarial pattern using one or more images of the target available to the adversary. The digital adversarial pattern is then projected onto the adversary\u2019s face in the physical domain to either impersonate a target (impersonation) or evade recognition (obfuscation). We conduct preliminary experiments using two open-source and one commercial face recognition system on a pool of 50 subjects. Our experimental results demonstrate the vulnerability of face recognition systems to light projection attacks in both white-box and black-box attack settings.", "isOpenAccess": true, "openAccessPdf": {"url": "http://arxiv.org/pdf/2003.11145", "status": "GREEN"}}, "bib_entry_raw": "D.-L. Nguyen, S. S. Arora, Y. Wu, and H. Yang, \u201cAdversarial light projection attacks on face recognition systems: A feasibility study,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, 2020, pp. 814\u2013815.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "f170ed4453867c68a5407ab2d59dca611a38b06f", "title": "An ensemble intrusion detection method for train ethernet consist network based on cnn and rnn", "corpus_id": 233377063, "metadata": {"paperId": "d0d4e8602fb08d8118cf216c54f221c039b89748", "externalIds": {"DBLP": "journals/access/YueWWDN21", "DOI": "10.1109/ACCESS.2021.3073413", "CorpusId": 233377063}, "corpusId": 233377063, "title": "An Ensemble Intrusion Detection Method for Train Ethernet Consist Network Based on CNN and RNN", "abstract": "The train Ethernet Consist Network (ECN) undertakes the task of transmitting critical train control instructions. With the increasing interactions between the train network and the outside environment, masses of network intrusions are threatening the data security of railway vehicles. The intrusion detection system has been proved to be an efficient method to detect network attacks. In this paper, a novel ensemble intrusion detection method is proposed to defense network attacks against the train ECN, in particular IP Scan, Port Scan, Denial of Service (DoS) and Man in the Middle (MITM). Thirty-four features of different protocol contents are extracted from the raw data generated from our ECN testbed to form a specific dataset. A data imaging method and a temporal sequence building method are designed to optimize the dataset. Six base classifiers are built based on several typical convolutional neural networks and recurrent neural networks: LeNet-5, AlexNet, VGGNet, SimpleRNN, LSTM and GRU. A dynamic weight matrix voting method is proposed to integrate all the base classifiers. The proposed method is evaluated based on our dataset. The experiment results show that our method has an outstanding ability to aggregate advantages of all the base classifiers and achieves a superior detection performance with the accuracy of 0.975.", "isOpenAccess": true, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/9312710/09405676.pdf", "status": "GOLD"}}, "bib_entry_raw": "Chuan Yue, Lide Wang, Dengrui Wang, Ruifeng Duo, and Xiaobo Nie. An ensemble intrusion detection method for train ethernet consist network based on cnn and rnn. IEEE Access, 9:59527\u201359539, 2021.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "35555727c219a9c9d24f8eb9e80e3fa9d3c65721", "title": null, "corpus_id": -1, "metadata": null, "bib_entry_raw": "Ashish Bhardwaj. Framingham heart study dataset.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "8f3eb869445ed0622a9879a6f2010828f6039e8b", "title": "Large-Scale Study of Perceptual Video Quality", "corpus_id": 52285071, "metadata": {"paperId": "7954410ea395fe0b74db571e4bf87cc5b312ca6b", "externalIds": {"DBLP": "journals/tip/SinnoB19", "MAG": "3104836973", "ArXiv": "1803.01761", "DOI": "10.1109/TIP.2018.2869673", "CorpusId": 52285071, "PubMed": "30222561"}, "corpusId": 52285071, "title": "Large-Scale Study of Perceptual Video Quality", "abstract": "The great variations of videographic skills in videography, camera designs, compression and processing protocols, communication and bandwidth environments, and displays leads to an enormous variety of video impairments. Current no-reference (NR) video quality models are unable to handle this diversity of distortions. This is true in part because available video quality assessment databases contain very limited content, fixed resolutions, were captured using a small number of camera devices by a few videographers and have been subjected to a modest number of distortions. As such, these databases fail to adequately represent real world videos, which contain very different kinds of content obtained under highly diverse imaging conditions and are subject to authentic, complex, and often commingled distortions that are difficult or impossible to simulate. As a result, NR video quality predictors tested on real-world video data often perform poorly. Toward advancing NR video quality prediction, we have constructed a large-scale video quality assessment database containing 585 videos of unique content, captured by a large number of users, with wide ranges of levels of complex, authentic distortions. We collected a large number of subjective video quality scores via crowdsourcing. A total of 4776 unique participants took part in the study, yielding over 205 000 opinion scores, resulting in an average of 240 recorded human opinions per video. We demonstrate the value of the new resource, which we call the live video quality challenge database (LIVE-VQC), by conducting a comparison with leading NR video quality predictors on it. This paper is the largest video quality assessment study ever conducted along several key dimensions: number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores. The database is available for download on this link: http://live.ece.utexas.edu/research/LIVEVQC/index.html.", "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1803.01761", "status": "GREEN"}}, "bib_entry_raw": "Zeina Sinno and Alan Conrad Bovik. 2019. Large-Scale Study of Perceptual Video Quality. IEEE Trans. Image Process. 28, 2 (2019), 612\u2013627.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "b2ac2714d9b73b80f4b12e5f27fd5f122f979634", "title": "Hidden trigger backdoor attacks", "corpus_id": 203610516, "metadata": {"paperId": "31675c0dea68199af03a8c58b638b89df4b9db3f", "externalIds": {"ArXiv": "1910.00033", "MAG": "2996800219", "DBLP": "conf/aaai/SahaSP20", "DOI": "10.1609/AAAI.V34I07.6871", "CorpusId": 203610516}, "corpusId": 203610516, "title": "Hidden Trigger Backdoor Attacks", "abstract": "With the success of deep learning algorithms in various domains, studying adversarial attacks to secure deep models in real world applications has become an important research topic. Backdoor attacks are a form of adversarial attacks on deep networks where the attacker provides poisoned data to the victim to train the model with, and then activates the attack by showing a specific small trigger pattern at the test time. Most state-of-the-art backdoor attacks either provide mislabeled poisoning data that is possible to identify by visual inspection, reveal the trigger in the poisoned data, or use noise to hide the trigger. We propose a novel form of backdoor attack where poisoned data look natural with correct labels and also more importantly, the attacker hides the trigger in the poisoned data and keeps the trigger secret until the test time. We perform an extensive study on various image classification settings and show that our attack can fool the model by pasting the trigger at random locations on unseen images although the model performs well on clean data. We also show that our proposed attack cannot be easily defended using a state-of-the-art defense algorithm for backdoor attacks.", "isOpenAccess": true, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/6871/6725", "status": "GOLD"}}, "bib_entry_raw": "A. Saha, A. Subramanya, H. Pirsiavash, Hidden trigger backdoor attacks, in: Proceedings of the AAAI conference on artificial intelligence, Vol. 34, 2020, pp. 11957\u201311965.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "5b37deac1ed581d064d42dd5b50f279f34e1bca8", "title": "Multi-view transformer for 3d visual grounding", "corpus_id": 247957775, "metadata": {"paperId": "44d1b81911e35e2aa2c03a5347b88ae479602837", "externalIds": {"ArXiv": "2204.02174", "DBLP": "conf/cvpr/HuangCJ022", "DOI": "10.1109/CVPR52688.2022.01508", "CorpusId": 247957775}, "corpusId": 247957775, "title": "Multi-View Transformer for 3D Visual Grounding", "abstract": "The 3D visual grounding task aims to ground a natural language description to the targeted object in a 3D scene, which is usually represented in 3D point clouds. Previous works studied visual grounding under specific views. The vision-language correspondence learned by this way can easily fail once the view changes. In this paper, we propose a Multi-View Transformer (MVT) for 3D visual grounding. We project the 3D scene to a multi-view space, in which the position information of the 3D scene under different views are modeled simultaneously and aggregated together. The multi-view space enables the network to learn a more robust multi-modal representation for 3D visual grounding and eliminates the dependence on specific views. Extensive experiments show that our approach significantly outperforms all state-of-the-art methods. Specifically, on Nr3D and Sr3D datasets, our method outperforms the best competitor by 11.2% and 7.1% and even surpasses recent work with extra 2D assistance by 5.9% and 6.6%. Our code is available at https://github.com/sega-hsj/MVT-3DVG.", "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2204.02174", "status": "GREEN"}}, "bib_entry_raw": "Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. Multi-view transformer for 3d visual grounding. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 15524\u201315533, 2022.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "394f94ef93fc6a84fe38229fb0f104706e2e463b", "title": "\"Adversarial sticker: A stealthy attack method in the physical world,\"", "corpus_id": 248986228, "metadata": {"paperId": "d19fbb5bef5a817028d668ca54ea43a9603057da", "externalIds": {"ArXiv": "2104.06728", "DBLP": "journals/pami/WeiGY23", "DOI": "10.1109/TPAMI.2022.3176760", "CorpusId": 248986228, "PubMed": "35604977"}, "corpusId": 248986228, "title": "Adversarial Sticker: A Stealthy Attack Method in the Physical World", "abstract": "To assess the vulnerability of deep learning in the physical world, recent works introduce adversarial patches and apply them on different tasks. In this paper, we propose another kind of adversarial patch: the Meaningful Adversarial Sticker, a physically feasible and stealthy attack method by using real stickers existing in our life. Unlike the previous adversarial patches by designing perturbations, our method manipulates the sticker's pasting position and rotation angle on the objects to perform physical attacks. Because the position and rotation angle are less affected by the printing loss and color distortion, adversarial stickers can keep good attacking performance in the physical world. Besides, to make adversarial stickers more practical in real scenes, we conduct attacks in the black-box setting with the limited information rather than the white-box setting with all the details of threat models. To effectively solve for the sticker's parameters, we design the Region based Heuristic Differential Evolution Algorithm, which utilizes the new-found regional aggregation of effective solutions and the adaptive adjustment strategy of the evaluation criteria. Our method is comprehensively verified in the face recognition and then extended to the image retrieval and traffic sign recognition. Extensive experiments show the proposed method is effective and efficient in complex physical conditions and has a good generalization for different tasks.", "isOpenAccess": true, "openAccessPdf": {"url": "http://arxiv.org/pdf/2104.06728", "status": "GREEN"}}, "bib_entry_raw": "X. Wei, Y. Guo, and J. Yu, \u201cAdversarial sticker: A stealthy attack method in the physical world,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 3, pp. 2711\u20132725, 2023.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "df55db7c0c9432aff18de38019f5e490047a3519", "title": null, "corpus_id": -1, "metadata": null, "bib_entry_raw": "Hans Hofmann. Statlog (German Credit Data), 1994.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "935b9f53040da5646e8ad6f12786a48e460e2eba", "title": "\"Evaluating the robustness of semantic segmentation for autonomous driving against real-world adversarial patch attacks,\"", "corpus_id": 237048246, "metadata": {"paperId": "fc8a115c92896d0aaa5a84e6fc776630af7ade22", "externalIds": {"DBLP": "journals/corr/abs-2108-06179", "ArXiv": "2108.06179", "DOI": "10.1109/WACV51458.2022.00288", "CorpusId": 237048246}, "corpusId": 237048246, "title": "Evaluating the Robustness of Semantic Segmentation for Autonomous Driving against Real-World Adversarial Patch Attacks", "abstract": "Deep learning and convolutional neural networks allow achieving impressive performance in computer vision tasks, such as object detection and semantic segmentation (SS). However, recent studies have shown evident weaknesses of such models against adversarial perturbations. In a real-world scenario instead, like autonomous driving, more attention should be devoted to real-world adversarial examples (RWAEs), which are physical objects (e.g., billboards and printable patches) optimized to be adversarial to the entire perception pipeline. This paper presents an in-depth evaluation of the robustness of popular SS models by testing the effects of both digital and real-world adversarial patches. These patches are crafted with powerful attacks enriched with a novel loss function. Firstly, an investigation on the Cityscapes dataset is conducted by extending the Expectation Over Transformation (EOT) paradigm to cope with SS. Then, a novel attack optimization, called scene-specific attack, is proposed. Such an attack leverages the CARLA driving simulator to improve the transferability of the proposed EOT-based attack to a real 3D environment. Finally, a printed physical billboard containing an adversarial patch was tested in an outdoor driving scenario to assess the feasibility of the studied attacks in the real world. Exhaustive experiments revealed that the proposed attack formulations outperform previous work to craft both digital and real-world adversarial patches for SS. At the same time, the experimental results showed how these attacks are notably less effective in the real world, hence questioning the practical relevance of adversarial attacks to SS models for autonomous/assisted driving.", "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2108.06179", "status": "GREEN"}}, "bib_entry_raw": "F. Nesti, G. Rossolini, S. Nair, A. Biondi, and G. Buttazzo, \u201cEvaluating the robustness of semantic segmentation for autonomous driving against real-world adversarial patch attacks,\u201d 2021.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "c3038b9e76df9bdf8431c44e1a0da77ad5f87274", "title": "\"Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition,\"", "corpus_id": 207241700, "metadata": {"paperId": "7f57e9939560562727344c1c987416285ef76cda", "externalIds": {"DBLP": "conf/ccs/SharifBBR16", "MAG": "2535873859", "DOI": "10.1145/2976749.2978392", "CorpusId": 207241700}, "corpusId": 207241700, "title": "Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition", "abstract": "Machine learning is enabling a myriad innovations, including new algorithms for cancer diagnosis and self-driving cars. The broad use of machine learning makes it important to understand the extent to which machine-learning algorithms are subject to attack, particularly when used in applications where physical security or safety is at risk. In this paper, we focus on facial biometric systems, which are widely used in surveillance and access control. We define and investigate a novel class of attacks: attacks that are physically realizable and inconspicuous, and allow an attacker to evade recognition or impersonate another individual. We develop a systematic method to automatically generate such attacks, which are realized through printing a pair of eyeglass frames. When worn by the attacker whose image is supplied to a state-of-the-art face-recognition algorithm, the eyeglasses allow her to evade being recognized or to impersonate another individual. Our investigation focuses on white-box face-recognition systems, but we also demonstrate how similar techniques can be used in black-box scenarios, as well as to avoid face detection.", "isOpenAccess": true, "openAccessPdf": {"url": "http://dl.acm.org/ft_gateway.cfm?id=2978392&type=pdf", "status": "BRONZE"}}, "bib_entry_raw": "M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, \u201cAccessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition,\u201d in Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, ser. CCS '16. New York, NY, USA: Association for Computing Machinery, 2016, p. 1528\u20131540. [Online]. Available: https://doi.org/10.1145/2976749.2978392", "contained_arXiv_ids": [], "contained_links": [{"url": "https://doi.org/10.1145/2976749.2978392", "text": "https://doi.org/10.1145/2976749.2978392", "start": 346, "end": 385}]}
{"bib_hash_or_arxiv_id": "24b4d04b01098cffe3cb975171aa05132c6a0903", "title": "The Konstanz natural video database (KoNViD-1k)", "corpus_id": 9136312, "metadata": {"paperId": "ae95abd2406ddfab1aa2ffa2413d68b98b1ba21b", "externalIds": {"DBLP": "conf/qomex/HosuHJLMSLS17", "MAG": "2611434713", "DOI": "10.1109/QoMEX.2017.7965673", "CorpusId": 9136312}, "corpusId": 9136312, "title": "The Konstanz natural video database (KoNViD-1k)", "abstract": "Subjective video quality assessment (VQA) strongly depends on semantics, context, and the types of visual distortions. Currently, all existing VQA databases include only a small number of video sequences with artificial distortions. The development and evaluation of objective quality assessment methods would benefit from having larger datasets of real-world video sequences with corresponding subjective mean opinion scores (MOS), in particular for deep learning purposes. In addition, the training and validation of any VQA method intended to be \u2018general purpose\u2019 requires a large dataset of video sequences that are representative of the whole spectrum of available video content and all types of distortions. We report our work on KoNViD-1k, a subjectively annotated VQA database consisting of 1,200 public-domain video sequences, fairly sampled from a large public video dataset, YFCC100m. We present the challenges and choices we have made in creating such a database aimed at \u2018in the wild\u2019 authentic distortions, depicting a wide variety of content.", "isOpenAccess": true, "openAccessPdf": {"url": "https://kar.kent.ac.uk/69606/1/konstanz-natural-video.pdf", "status": "GREEN"}}, "bib_entry_raw": "Vlad Hosu, Franz Hahn, Mohsen Jenadeleh, Hanhe Lin, Hui Men, Tam\u00e1s Szir\u00e1nyi, Shujun Li, and Dietmar Saupe. 2017. The Konstanz natural video database (KoNViD-1k). In QoMEX. IEEE, 1\u20136.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "8b696311541e376264afd2550734d2d448c56062", "title": "Application of controller area network (can) bus anomaly detection based on time series prediction", "corpus_id": 225264248, "metadata": {"paperId": "5b65a4a72c5b1599be08196cecbce859ceed13bc", "externalIds": {"MAG": "3083938246", "DBLP": "journals/vcomm/QinYJ21", "DOI": "10.1016/j.vehcom.2020.100291", "CorpusId": 225264248}, "corpusId": 225264248, "title": "Application of Controller Area Network (CAN) bus anomaly detection based on time series prediction", "abstract": null, "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "Hongmao Qin, Mengru Yan, and Haojie Ji. Application of controller area network (can) bus anomaly detection based on time series prediction. Vehicular Communications, 27:100291, 2021.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "aefc7f9f9e6901e1f8605ea533cc466fb5238e80", "title": "Correspondence learning via linearly-invariant embedding", "corpus_id": 225066835, "metadata": {"paperId": "43a834a433c05d920614aedcf081a4be08e0db71", "externalIds": {"MAG": "3099488909", "ArXiv": "2010.13136", "DBLP": "conf/nips/MarinRMO20", "CorpusId": 225066835}, "corpusId": 225066835, "title": "Correspondence Learning via Linearly-invariant Embedding", "abstract": "In this paper, we propose a fully differentiable pipeline for estimating accurate dense correspondences between 3D point clouds. The proposed pipeline is an extension and a generalization of the functional maps framework. However, instead of using the Laplace-Beltrami eigenfunctions as done in virtually all previous works in this domain, we demonstrate that learning the basis from data can both improve robustness and lead to better accuracy in challenging settings. We interpret the basis as a learned embedding into a higher dimensional space. Following the functional map paradigm the optimal transformation in this embedding space must be linear and we propose a separate architecture aimed at estimating the transformation by learning optimal descriptor functions. This leads to the first end-to-end trainable functional map-based correspondence approach in which both the basis and the descriptors are learned from data. Interestingly, we also observe that learning a \\emph{canonical} embedding leads to worse results, suggesting that leaving an extra linear degree of freedom to the embedding network gives it more robustness, thereby also shedding light onto the success of previous methods. Finally, we demonstrate that our approach achieves state-of-the-art results in challenging non-rigid 3D point cloud correspondence applications.", "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "Riccardo Marin, Marie-Julie Rakotosaona, Simone Melzi, and Maks Ovsjanikov. Correspondence learning via linearly-invariant embedding. Advances in Neural Information Processing Systems, 33:1608\u20131620, 2020.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "9be8207b7ba6b4f423a574f686110ed04d5a3d91", "title": "YouTube UGC Dataset for Video Compression Research", "corpus_id": 119309258, "metadata": {"paperId": "093561bf2c2112be497873d2e36dcc1648a035e0", "externalIds": {"MAG": "2985067372", "DBLP": "journals/corr/abs-1904-06457", "ArXiv": "1904.06457", "DOI": "10.1109/MMSP.2019.8901772", "CorpusId": 119309258}, "corpusId": 119309258, "title": "YouTube UGC Dataset for Video Compression Research", "abstract": "Non-professional video, commonly known as User Generated Content (UGC) has become very popular in today's video sharing applications. However, traditional metrics used in compression and quality assessment, like BD-Rate and PSNR, are designed for pristine originals. Thus, their accuracy drops significantly when being applied on non-pristine originals (the majority of UGC). Understanding difficulties for compression and quality assessment in the scenario of UGC is important, but there are few public UGC datasets available for research. This paper introduces a large scale UGC dataset (1500 20 sec video clips) sampled from millions of YouTube videos. The dataset covers popular categories like Gaming, Sports, and new features like High Dynamic Range (HDR). Besides a novel sampling method based on features extracted from encoding, challenges for UGC compression and quality evaluation are also discussed. Shortcomings of traditional reference-based metrics on UGC are addressed. We demonstrate a promising way to evaluate UGC quality by no-reference objective quality metrics, and evaluate the current dataset with three no-reference metrics (Noise, Banding, and SLEEQ).", "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1904.06457", "status": "GREEN"}}, "bib_entry_raw": "Yilin Wang, Sasi Inguva, and Balu Adsumilli. 2019. YouTube UGC Dataset for Video Compression Research. In MMSP. IEEE, 1\u20135.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "ca23f8210bcd16856c8ed66e1fc2f95218e66df2", "title": "\"Adversarial mask: Real-world universal adversarial attack on face recognition model,\"", "corpus_id": 250311842, "metadata": {"paperId": "fea7e10ef910d4c75724d1e502dbeaf759647879", "externalIds": {"DBLP": "conf/pkdd/ZolfiAES22", "ArXiv": "2111.10759", "DOI": "10.1007/978-3-031-26409-2_19", "CorpusId": 250311842}, "corpusId": 250311842, "title": "Adversarial Mask: Real-World Universal Adversarial Attack on Face Recognition Models", "abstract": null, "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "A. Zolfi, S. Avidan, Y. Elovici, and A. Shabtai, \u201cAdversarial mask: Real-world universal adversarial attack on face recognition model,\u201d 2022.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "3162307221bb662b0d76690c4de2452a38cd232f", "title": "Sqa3d: Situated question answering in 3d scenes", "corpus_id": 252907411, "metadata": {"paperId": "17a496e051dd4b8da26a29baeee01f7d072005ce", "externalIds": {"ArXiv": "2210.07474", "DBLP": "conf/iclr/MaYZ0LZH23", "DOI": "10.48550/arXiv.2210.07474", "CorpusId": 252907411}, "corpusId": 252907411, "title": "SQA3D: Situated Question Answering in 3D Scenes", "abstract": "We propose a new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g., 3D scan), SQA3D requires the tested agent to first understand its situation (position, orientation, etc.) in the 3D scene as described by text, then reason about its surrounding environment and answer a question under that situation. Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. These questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateur human participants can reach 90.06%. We believe SQA3D could facilitate future embodied AI research with stronger situation understanding and reasoning capability.", "isOpenAccess": true, "openAccessPdf": {"url": "http://arxiv.org/pdf/2210.07474", "status": "GREEN"}}, "bib_entry_raw": "Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated question answering in 3d scenes. International Conference on Learning Representations (ICLR), 2023.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "82b5a8e9e9a61df27edd5da521382a04f98e76ec", "title": "Cantransfer: Transfer learning based intrusion detection on a controller area network using convolutional lstm network", "corpus_id": 214694052, "metadata": {"paperId": "1ebe6ca661cce3fd8c4dbf988f847724c3454581", "externalIds": {"DBLP": "conf/sac/TariqLW20", "MAG": "3014212182", "DOI": "10.1145/3341105.3373868", "CorpusId": 214694052}, "corpusId": 214694052, "title": "CANTransfer: transfer learning based intrusion detection on a controller area network using convolutional LSTM network", "abstract": "In-vehicle communications, due to simplicity and reliability, a Controller Area Network (CAN) bus is widely used as the de facto standard to provide serial communications between Electronic Control Units (ECUs). However, prior research exhibits several network-level attacks can be easily performed and exploited in the CAN bus. Additionally, new types of intrusion attacks are discovered very frequently. However, unless we have a large amount of data about an intrusion, developing an efficient deep neural network-based detection mechanism is not easy. To address this challenge, we propose CANTransfer, an intrusion detection method using Transfer Learning for CAN bus, where a Convolutional LSTM based model is trained using known intrusion to detect new attacks. By applying one-shot learning, the model can be adaptable to detect new intrusions with a limited amount of new datasets. We performed extensive experimentation and achieved a performance gain of 26.60% over the best baseline model for detecting new intrusions.", "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "Shahroz Tariq, Sangyup Lee, and Simon S Woo. Cantransfer: Transfer learning based intrusion detection on a controller area network using convolutional lstm network. In Proceedings of the 35th annual ACM symposium on applied computing, pages 1048\u20131055, 2020.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "be41e7cf52d29dfc41d5c3040390f193accc5c43", "title": "\"Physical attack on monocular depth estimation with optimal adversarial patches,\"", "corpus_id": 250426022, "metadata": {"paperId": "35efa06e8c55a209677bcb48a6790b654d8b322f", "externalIds": {"DBLP": "conf/eccv/ChengLCTCLZ22", "ArXiv": "2207.04718", "DOI": "10.48550/arXiv.2207.04718", "CorpusId": 250426022}, "corpusId": 250426022, "title": "Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches", "abstract": "Deep learning has substantially boosted the performance of Monocular Depth Estimation (MDE), a critical component in fully vision-based autonomous driving (AD) systems (e.g., Tesla and Toyota). In this work, we develop an attack against learning-based MDE. In particular, we use an optimization-based method to systematically generate stealthy physical-object-oriented adversarial patches to attack depth estimation. We balance the stealth and effectiveness of our attack with object-oriented adversarial design, sensitive region localization, and natural style camouflage. Using real-world driving scenarios, we evaluate our attack on concurrent MDE models and a representative downstream task for AD (i.e., 3D object detection). Experimental results show that our method can generate stealthy, effective, and robust adversarial patches for different target objects and models and achieves more than 6 meters mean depth estimation error and 93% attack success rate (ASR) in object detection with a patch of 1/9 of the vehicle's rear area. Field tests on three different driving routes with a real vehicle indicate that we cause over 6 meters mean depth estimation error and reduce the object detection rate from 90.70% to 5.16% in continuous video frames.", "isOpenAccess": true, "openAccessPdf": {"url": "http://arxiv.org/pdf/2207.04718", "status": "GREEN"}}, "bib_entry_raw": "Z. Cheng, J. Liang, H. Choi, G. Tao, Z. Cao, D. Liu, and X. Zhang, \u201cPhysical attack on monocular depth estimation with optimal adversarial patches,\u201d in Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXVIII. Springer, 2022, pp. 514\u2013532.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "30cf9a9adfb667b600cf7a1b82cb9b51ea8f587e", "title": "Language conditioned spatial relation reasoning for 3d object grounding", "corpus_id": 253581736, "metadata": {"paperId": "88c741be2c0200f0eff9877d99fbce612ba64d07", "externalIds": {"DBLP": "journals/corr/abs-2211-09646", "ArXiv": "2211.09646", "DOI": "10.48550/arXiv.2211.09646", "CorpusId": 253581736}, "corpusId": 253581736, "title": "Language Conditioned Spatial Relation Reasoning for 3D Object Grounding", "abstract": "Localizing objects in 3D scenes based on natural language requires understanding and reasoning about spatial relations. In particular, it is often crucial to distinguish similar objects referred by the text, such as\"the left most chair\"and\"a chair next to the window\". In this work we propose a language-conditioned transformer model for grounding 3D objects and their spatial relations. To this end, we design a spatial self-attention layer that accounts for relative distances and orientations between objects in input 3D point clouds. Training such a layer with visual and language inputs enables to disambiguate spatial relations and to localize objects referred by the text. To facilitate the cross-modal learning of relations, we further propose a teacher-student approach where the teacher model is first trained using ground-truth object labels, and then helps to train a student model using point cloud inputs. We perform ablation studies showing advantages of our approach. We also demonstrate our model to significantly outperform the state of the art on the challenging Nr3D, Sr3D and ScanRefer 3D object grounding datasets.", "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Language conditioned spatial relation reasoning for 3d object grounding. Advances in Neural Information Processing Systems (NeurIPS), 2022.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "437511d9f9f2e1bcb63d3ce6e7fc29ef09c11ede", "title": "Anomaly detection in automobile control network data with long short-term memory networks", "corpus_id": 2947736, "metadata": {"paperId": "cb75fd79921fe0a5ee753fe1001c9039130b8a1d", "externalIds": {"MAG": "2561208905", "DBLP": "conf/dsaa/TaylorLJ16", "DOI": "10.1109/DSAA.2016.20", "CorpusId": 2947736}, "corpusId": 2947736, "title": "Anomaly Detection in Automobile Control Network Data with Long Short-Term Memory Networks", "abstract": "Modern automobiles have been proven vulnerable to hacking by security researchers. By exploiting vulnerabilities in the car's external interfaces, such as wifi, bluetooth, and physical connections, they can access a car's controller area network (CAN) bus. On the CAN bus, commands can be sent to control the car, for example cutting the brakes or stopping the engine. While securing the car's interfaces to the outside world is an important part of mitigating this threat, the last line of defence is detecting malicious behaviour on the CAN bus. We propose an anomaly detector based on a Long Short-Term Memory neural network to detect CAN bus attacks. The detector works by learning to predict the next data word originating from each sender on the bus. Highly surprising bits in the actual next word are flagged as anomalies. We evaluate the detector by synthesizing anomalies with modified CAN bus data. The synthesized anomalies are designed to mimic attacks reported in the literature. We show that the detector can detect anomalies we synthesized with low false alarm rates. Additionally, the granularity of the bit predictions can provide forensic investigators clues as to the nature of flagged anomalies.", "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "Adrian Taylor, Sylvain Leblanc, and Nathalie Japkowicz. Anomaly detection in automobile control network data with long short-term memory networks. In 2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA), pages 130\u2013139. IEEE, 2016.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "6895032a1f3468a1027079def9c11c2f4618fb6d", "title": "----", "corpus_id": -1, "metadata": null, "bib_entry_raw": "\u2014\u2014, \u201cA general framework for adversarial examples with objectives,\u201d ACM Transactions on Privacy and Security (TOPS), vol. 22, no. 3, pp. 1\u201330, 2019.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "18bf18c973beebbda88a6073400fe84bb63a1496", "title": "\"Adversarial examples for replay attacks against cnn-based face recognition with anti-spoofing capability,\"", "corpus_id": 219498404, "metadata": {"paperId": "80bdb63d9d707554c52be223d068b1383b6eb443", "externalIds": {"DBLP": "journals/cviu/ZhangTB20", "MAG": "3027378069", "DOI": "10.1016/j.cviu.2020.102988", "CorpusId": 219498404}, "corpusId": 219498404, "title": "Adversarial examples for replay attacks against CNN-based face recognition with anti-spoofing capability", "abstract": null, "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "B. Zhang, B. Tondi, and M. Barni, \u201cAdversarial examples for replay attacks against cnn-based face recognition with anti-spoofing capability,\u201d Comput. Vis. Image Underst., vol. 197-198, p. 102988, 2020.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "08531f89e92c14e63dfbab3f5c53b4cecc1ff7e1", "title": "Badnets: Identifying vulnerabilities in the machine learning model supply chain", "corpus_id": 26783139, "metadata": {"paperId": "573fd2ce97c70bb29097e8efb28a27af791225ca", "externalIds": {"DBLP": "journals/corr/abs-1708-06733", "MAG": "2748789698", "ArXiv": "1708.06733", "CorpusId": 26783139}, "corpusId": 26783139, "title": "BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain", "abstract": "Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a \\emph{BadNet}) that has state-of-the-art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of {25}\\% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and---because the behavior of neural networks is difficult to explicate---stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.", "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "T. Gu, B. Dolan-Gavitt, S. Garg, Badnets: Identifying vulnerabilities in the machine learning model supply chain, arXiv preprint arXiv:1708.06733 (2017).", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "1e703d981f17ca9cf0101caf049f065028d393ea", "title": "Intrusion detection method using bi-directional gpt for in-vehicle controller area networks", "corpus_id": 237519710, "metadata": {"paperId": "0ee17c9a630fe064f7d99650f0c15dca3f7a103e", "externalIds": {"DBLP": "journals/access/NamPK21", "DOI": "10.1109/ACCESS.2021.3110524", "CorpusId": 237519710}, "corpusId": 237519710, "title": "Intrusion Detection Method Using Bi-Directional GPT for in-Vehicle Controller Area Networks", "abstract": "The controller area network (CAN) bus protocol is exposed to threats from various attacks because it is designed without consideration of security. In a normal vehicle operation situation, controllers connected to a CAN bus transmit periodic and nonperiodic signals. Thus, if a CAN identifier (ID) sequence is configured by collecting the identifiers of CAN signals in their order of occurrence, it will have a certain pattern. However, if only a very small number of attack IDs are included in a CAN ID sequence, it will be difficult to detect the corresponding pattern change. Thus, a detection method that is different from the conventional one is required to detect such attacks. Since a CAN ID sequence can be regarded as a sentence consisting of words in the form of CAN IDs, a generative pretrained transformer (GPT) model can learn the pattern of a normal CAN ID sequence. Therefore, such a model is expected to be able to detect CAN ID sequences that contain a very small number of attack IDs better than the existing long short-term memory (LSTM)-based method. In this paper, we propose an intrusion detection model that combines two GPT networks in a bi-directional manner to allow both past and future CAN IDs (relative to the time of detection) to be used. The proposed model is trained to minimize the negative log-likelihood (NLL) value of the bi-directional GPT network for a normal sequence. When the NLL value for a CAN ID sequence is larger than a prespecified threshold, it is deemed an intrusion. The proposed model outperforms a single uni-directional GPT model with the same degree of complexity as other existing LSTM-based models because the bi-directional structure of the proposed model maintains the estimation performance for most CAN IDs, regardless of their positions in the sequence.", "isOpenAccess": true, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/09530394.pdf", "status": "GOLD"}}, "bib_entry_raw": "Minki Nam, Seungyoung Park, and Duk Soo Kim. Intrusion detection method using bi-directional gpt for in-vehicle controller area networks. IEEE Access, 9:124931\u2013124944, 2021.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "4dde40b697587eaf7d692b56b0e0931fb174f2ad", "title": "Using the ADAP Learning Algorithm to Forecast the Onset of Diabetes Mellitus", "corpus_id": 60837618, "metadata": {"paperId": "8674223cb2a4ff09c7b8d82e3a6ad187cc9008b5", "externalIds": {"MAG": "1541145887", "CorpusId": 60837618}, "corpusId": 60837618, "title": "Using the ADAP Learning Algorithm to Forecast the Onset of Diabetes Mellitus", "abstract": "Abstract \nNeural networks or connectionist models for parallel processing are not new. However, a resurgence of interest in the past half decade has occurred. In part, this is related to a better understanding of what are now referred to as hidden nodes. These algorithms are considered to be of marked value in pattern recognition problems. Because of that, we tested the ability of an early neural network model, ADAP, to forecast the onset of diabetes mellitus in a high risk population of Pima Indians. The algorithm's performance was analyzed using standard measures for clinical tests: sensitivity, specificity, and a receiver operating characteristic curve. The crossover point for sensitivity and specificity is 0.76. We are currently further examining these methods by comparing the ADAP results with those obtained from logistic regression and linear perceptron models using precisely the same training and forecasting sets. A description of the algorithm is included.", "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "Jack W. Smith, J.E. Everhart, W.C. Dickson, W.C. Knowler, and R.S. Johannes. Using the ADAP Learning Algorithm to Forecast the Onset of Diabetes Mellitus. pages 261\u2013265, 1988.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "d8196a9cf56d63c8148113a8087ea336c9ff5b9e", "title": "Canintelliids: Detecting in-vehicle intrusion attacks on a controller area network using cnn and attention-based gru", "corpus_id": 234222188, "metadata": {"paperId": "9a5d5a1b827276f73dbd7209c4022e694a8be1ea", "externalIds": {"DBLP": "journals/tnse/JavedRKAG21", "MAG": "3130982961", "DOI": "10.1109/TNSE.2021.3059881", "CorpusId": 234222188}, "corpusId": 234222188, "title": "CANintelliIDS: Detecting In-Vehicle Intrusion Attacks on a Controller Area Network Using CNN and Attention-Based GRU", "abstract": "Controller area network (CAN) is a communication protocol that provides reliable and productive transmission between in-vehicle nodes continuously. CAN bus protocol is broadly utilized standard channel to deliver sequential communications between electronic control units (ECUs) due to simple and reliable in-vehicle communication. Existing studies report how easily an attack can be performed on the CAN bus of in-vehicle due to weak security mechanisms that could lead to system malfunctions. Hence the security of communications inside a vehicle is a latent problem. In this paper, we propose a novel approach named CANintelliIDS, for vehicle intrusion attack detection on the CAN bus. CANintelliIDS is based on a combination of convolutional neural network (CNN) and attention-based gated recurrent unit (GRU) model to detect single intrusion attacks as well as mixed intrusion attacks on a CAN bus. The proposed CANintelliIDS model is evaluated extensively and it achieved a performance gain of 10.79% on test intrusion attacks over existing approaches.", "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "Abdul Rehman Javed, Saif Ur Rehman, Mohib Ullah Khan, Mamoun Alazab, and Thippa Reddy. Canintelliids: Detecting in-vehicle intrusion attacks on a controller area network using cnn and attention-based gru. IEEE transactions on network science and engineering, 8(2):1456\u20131466, 2021.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "8969334a6b7005bb5c1cfbc2d58663bbb560e559", "title": "Label-consistent backdoor attacks", "corpus_id": 208637053, "metadata": {"paperId": "02d1168344c969761397b575b64765405752c354", "externalIds": {"DBLP": "journals/corr/abs-1912-02771", "ArXiv": "1912.02771", "MAG": "2993846550", "CorpusId": 208637053}, "corpusId": 208637053, "title": "Label-Consistent Backdoor Attacks", "abstract": "Deep neural networks have been demonstrated to be vulnerable to backdoor attacks. Specifically, by injecting a small number of maliciously constructed inputs into the training set, an adversary is able to plant a backdoor into the trained model. This backdoor can then be activated during inference by a backdoor trigger to fully control the model's behavior. While such attacks are very effective, they crucially rely on the adversary injecting arbitrary inputs that are---often blatantly---mislabeled. Such samples would raise suspicion upon human inspection, potentially revealing the attack. Thus, for backdoor attacks to remain undetected, it is crucial that they maintain label-consistency---the condition that injected inputs are consistent with their labels. In this work, we leverage adversarial perturbations and generative models to execute efficient, yet label-consistent, backdoor attacks. Our approach is based on injecting inputs that appear plausible, yet are hard to classify, hence causing the model to rely on the (easier-to-learn) backdoor trigger.", "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "A. Turner, D. Tsipras, A. Madry, Label-consistent backdoor attacks, arXiv preprint arXiv:1912.02771 (2019).", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "9c04ab9115e73e7300a6077937ec1b23e3fdf820", "title": "KonVid-150k: A Dataset for No-Reference Video Quality Assessment of Videos in-the-Wild", "corpus_id": 234788066, "metadata": {"paperId": "0518d5abe3926ca8d6bd097c04c246e69f7d2da5", "externalIds": {"DBLP": "journals/access/Gotz-HahnHLS21", "DOI": "10.1109/ACCESS.2021.3077642", "CorpusId": 234788066}, "corpusId": 234788066, "title": "KonVid-150k: A Dataset for No-Reference Video Quality Assessment of Videos in-the-Wild", "abstract": "Video quality assessment (VQA) methods focus on particular degradation types, usually artificially induced on a small set of reference videos. Hence, most traditional VQA methods under-perform in-the-wild. Deep learning approaches have had limited success due to the small size and diversity of existing VQA datasets, either artificial or authentically distorted. We introduce a new in-the-wild VQA dataset that is substantially larger and diverse: KonVid-150k. It consists of a coarsely annotated set of 153,841 videos having five quality ratings each, and 1,596 videos with a minimum of 89 ratings each. Additionally, we propose new efficient VQA approaches (MLSP-VQA) relying on multi-level spatially pooled deep-features (MLSP). They are exceptionally well suited for training at scale, compared to deep transfer learning approaches. Our best method, MLSP-VQA-FF, improves the Spearman rank-order correlation coefficient (SRCC) performance metric on the commonly used KoNViD-1k in-the-wild benchmark dataset to 0.82. It surpasses the best existing deep-learning model (0.80 SRCC) and hand-crafted feature-based method (0.78 SRCC). We further investigate how alternative approaches perform under different levels of label noise, and dataset size, showing that MLSP-VQA-FF is the overall best method for videos in-the-wild. Finally, we show that the MLSP-VQA models trained on KonVid-150k sets the new state-of-the-art for cross-test performance on KoNViD-1k and LIVE-Qualcomm with a 0.83 and 0.64 SRCC, respectively. For KoNViD-1k this inter-dataset testing outperforms intra-dataset experiments, showing excellent generalization.", "isOpenAccess": true, "openAccessPdf": null}, "bib_entry_raw": "Franz G\u00f6tz-Hahn, Vlad Hosu, Hanhe Lin, and Dietmar Saupe. 2021. KonVid-150k: A Dataset for No-Reference Video Quality Assessment of Videos in-the-Wild. IEEE Access 9 (2021), 72139\u201372160. https://doi.org/10.1109/ACCESS.2021.3077642", "contained_arXiv_ids": [], "contained_links": [{"url": "https://doi.org/10.1109/ACCESS.2021.3077642", "text": "https://doi.org/10.1109/ACCESS.2021.3077642", "start": 187, "end": 230}]}
{"bib_hash_or_arxiv_id": "cb57006b5e3cd0586ad3fc0e649c32c57d82a682", "title": "The Effect of Race/Ethnicity on Sentencing: Examining Sentence Type, Jail Length, and Prison Length", "corpus_id": 143653771, "metadata": {"paperId": "022cf7caf2395da170430fe72b3d28525a6d41e6", "externalIds": {"MAG": "1975982181", "DOI": "10.1080/15377938.2014.984045", "CorpusId": 143653771}, "corpusId": 143653771, "title": "The Effect of Race/Ethnicity on Sentencing: Examining Sentence Type, Jail Length, and Prison Length", "abstract": "The purpose of this research was to examine the impact of race/ethnicity on criminal sentencing outcomes. The findings from prior studies tend to be mixed on this issue. Using 4 years of data from the State Court Processing Statistics (2000, 2002, 2004, and 2006) and propensity score matching, we examined the impact of race/ethnicity on sentencing outcomes among Blacks, Hispanics, and Whites. The findings suggest that racial/ethnic biases occur in the sentence type (community sanction, jail, or prison) and jail length decisions though not in the prison length decision. It is important to separate jail length and prison length when examining incarceration time. Combining the 2 distinct sentences may confound the true impact of factors on these outcomes.", "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "Kareem L. Jordan and Tina L. Freiburger. The Effect of Race/Ethnicity on Sentencing: Examining Sentence Type, Jail Length, and Prison Length. Journal of Ethnicity in Criminal Justice, 13(3):179\u2013196, July 2015. ISSN 1537-7938, 1537-7946. doi: 10.1080/15377938.2014.984045.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "06067f5b62b044923d16856bbef7fdbadc30a854", "title": "Towards a can ids based on a neural network data field predictor", "corpus_id": 57189430, "metadata": {"paperId": "d9554e5c96c02b078eee0370e83608eedfdd2397", "externalIds": {"ArXiv": "1812.11596", "DBLP": "conf/codaspy/PawelecBC19", "MAG": "2950251594", "DOI": "10.1145/3309171.3309180", "CorpusId": 57189430}, "corpusId": 57189430, "title": "Towards a CAN IDS Based on a Neural Network Data Field Predictor", "abstract": "Modern vehicles contain a few controller area networks (CANs), which allow scores of on-board electronic control units (ECUs) to communicate messages critical to vehicle functions and driver safety. CAN provides a lightweight and reliable broadcast protocol but is bereft of security features. As evidenced by many recent research works, CAN exploits are possible both remotely and with direct access, fueling a growing CAN intrusion detection system (IDS) body of research. A challenge for pioneering vehicle-agnostic IDSs is that passenger vehicles' CAN message encodings are proprietary, defined and held secret by original equipment manufacturers (OEMs). Targeting detection of next-generation attacks, in which messages are sent from the expected ECU at the expected time but with malicious content, researchers are now seeking to leverage \"CAN data models'', which predict future CAN messages and use prediction error to identify anomalous, hopefully malicious CAN messages. Yet, current works model CAN signals post-translation, i.e., after applying OEM-donated or reverse-engineered translations from raw data. We present initial IDS results testing deep neural networks used to predict CAN data at the bit level, targeting IDS capabilities that avoiding reverse engineering proprietary encodings. Our results suggest the method is promising for data with signals exhibiting dependence on previous or concurrent inputs.", "isOpenAccess": true, "openAccessPdf": {"url": "https://dl.acm.org/doi/pdf/10.1145/3309171.3309180", "status": "BRONZE"}}, "bib_entry_raw": "Krzysztof Pawelec, Robert A Bridges, and Frank L Combs. Towards a can ids based on a neural network data field predictor. In Proceedings of the ACM Workshop on Automotive Cybersecurity, pages 31\u201334, 2019.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "fbaab966ad1e0efdeb5b96e80b5b467ba60eeced", "title": "Large-scale Video Classification with Convolutional Neural Networks", "corpus_id": 206592218, "metadata": {"paperId": "6d4c9c923e9f145d1c01a2de2afc38ec23c44253", "externalIds": {"MAG": "2308045930", "DBLP": "conf/cvpr/KarpathyTSLSF14", "DOI": "10.1109/CVPR.2014.223", "CorpusId": 206592218}, "corpusId": 206592218, "title": "Large-Scale Video Classification with Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).", "isOpenAccess": true, "openAccessPdf": {"url": "http://www.cs.cmu.edu/~rahuls/pub/cvpr2014-deepvideo-rahuls.pdf", "status": "GREEN"}}, "bib_entry_raw": "Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. 2014. Large-scale Video Classification with Convolutional Neural Networks. In CVPR.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "acd3b95fbd1a60fbdbe48398c47817680f12eef8", "title": "\"Simultaneously optimizing perturbations and positions for black-box adversarial patch attacks,\"", "corpus_id": 255125520, "metadata": {"paperId": "0921720dc5da00317b5af5fbbda00936d433a1ee", "externalIds": {"DBLP": "journals/corr/abs-2212-12995", "ArXiv": "2212.12995", "DOI": "10.1109/TPAMI.2022.3231886", "CorpusId": 255125520, "PubMed": "37015667"}, "corpusId": 255125520, "title": "Simultaneously Optimizing Perturbations and Positions for Black-Box Adversarial Patch Attacks", "abstract": "Adversarial patch is an important form of real-world adversarial attack that brings serious risks to the robustness of deep neural networks. Previous methods generate adversarial patches by either optimizing their perturbation values while fixing the pasting position or manipulating the position while fixing the patch's content. This reveals that the positions and perturbations are both important to the adversarial attack. For that, in this article, we propose a novel method to simultaneously optimize the position and perturbation for an adversarial patch, and thus obtain a high attack success rate in the black-box setting. Technically, we regard the patch's position, the pre-designed hyper-parameters to determine the patch's perturbations as the variables, and utilize the reinforcement learning framework to simultaneously solve for the optimal solution based on the rewards obtained from the target model with a small number of queries. Extensive experiments are conducted on the Face Recognition (FR) task, and results on four representative FR models show that our method can significantly improve the attack success rate and query efficiency. Besides, experiments on the commercial FR service and physical environments confirm its practical application value. We also extend our method to the traffic sign recognition task to verify its generalization ability.", "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2212.12995", "status": "GREEN"}}, "bib_entry_raw": "X. Wei, Y. Guo, J. Yu, and B. Zhang, \u201cSimultaneously optimizing perturbations and positions for black-box adversarial patch attacks,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "dd165aba48b463b9724bf76f9c7ca09211f6482a", "title": "Targeted backdoor attacks on deep learning systems using data poisoning", "corpus_id": 36122023, "metadata": {"paperId": "cb4c2a2d7e50667914d1a648f1a9134056724780", "externalIds": {"DBLP": "journals/corr/abs-1712-05526", "MAG": "2774423163", "ArXiv": "1712.05526", "CorpusId": 36122023}, "corpusId": 36122023, "title": "Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning", "abstract": "Deep learning models have achieved high performance on many tasks, and thus have been applied to many security-critical scenarios. For example, deep learning-based face recognition systems have been used to authenticate users to access many security-sensitive applications like payment apps. Such usages of deep learning systems provide the adversaries with sufficient incentives to perform attacks against these systems for their adversarial purposes. In this work, we consider a new type of attacks, called backdoor attacks, where the attacker's goal is to create a backdoor into a learning-based authentication system, so that he can easily circumvent the system by leveraging the backdoor. Specifically, the adversary aims at creating backdoor instances, so that the victim learning system will be misled to classify the backdoor instances as a target label specified by the adversary. In particular, we study backdoor poisoning attacks, which achieve backdoor attacks using poisoning strategies. Different from all existing work, our studied poisoning strategies can apply under a very weak threat model: (1) the adversary has no knowledge of the model and the training set used by the victim system; (2) the attacker is allowed to inject only a small amount of poisoning samples; (3) the backdoor key is hard to notice even by human beings to achieve stealthiness. We conduct evaluation to demonstrate that a backdoor adversary can inject only around 50 poisoning samples, while achieving an attack success rate of above 90%. We are also the first work to show that a data poisoning attack can create physically implementable backdoors without touching the training process. Our work demonstrates that backdoor poisoning attacks pose real threats to a learning system, and thus highlights the importance of further investigation and proposing defense strategies against them.", "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "X. Chen, C. Liu, B. Li, K. Lu, D. Song, Targeted backdoor attacks on deep learning systems using data poisoning, arXiv preprint arXiv:1712.05526 (2017).", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "d0724e2e5b80b58f48ee6a8a33f90494bcb8c8a3", "title": "3djcg: A unified framework for joint dense captioning and visual grounding on 3d point clouds", "corpus_id": 250980730, "metadata": {"paperId": "fade0ef67bcad3369e83348111a73c0f9578786f", "externalIds": {"DBLP": "conf/cvpr/CaiZZSX22", "DOI": "10.1109/CVPR52688.2022.01597", "CorpusId": 250980730}, "corpusId": 250980730, "title": "3DJCG: A Unified Framework for Joint Dense Captioning and Visual Grounding on 3D Point Clouds", "abstract": "Observing that the 3D captioning task and the 3D grounding task contain both shared and complementary information in nature, in this work, we propose a unified framework to jointly solve these two distinct but closely related tasks in a synergistic fashion, which consists of both shared task-agnostic modules and lightweight task-specific modules. On one hand, the shared task-agnostic modules aim to learn precise locations of objects, fine-grained attribute features to characterize different objects, and complex relations between objects, which benefit both captioning and visual grounding. On the other hand, by casting each of the two tasks as the proxy task of another one, the lightweight task-specific modules solve the captioning task and the grounding task respectively. Extensive experiments and ablation study on three 3D vision and language datasets demonstrate that our joint training frame-work achieves significant performance gains for each individual task and finally improves the state-of-the-art performance for both captioning and grounding tasks.", "isOpenAccess": false, "openAccessPdf": null}, "bib_entry_raw": "Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong Xu. 3djcg: A unified framework for joint dense captioning and visual grounding on 3d point clouds. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 16464\u201316473, 2022.", "contained_arXiv_ids": [], "contained_links": []}
{"bib_hash_or_arxiv_id": "2308.07247v1", "title": "Can we Agree? On the Rash\\=omon Effect and the Reliability of Post-Hoc Explainable AI", "corpus_id": 260887734, "metadata": {"paperId": "2f2815cba7163777d6882c6d7f5d401c1cb94630", "externalIds": {"ArXiv": "2308.07247", "CorpusId": 260887734}, "corpusId": 260887734, "title": "Can we Agree? On the Rash\\=omon Effect and the Reliability of Post-Hoc Explainable AI", "abstract": "The Rash\\=omon effect poses challenges for deriving reliable knowledge from machine learning models. This study examined the influence of sample size on explanations from models in a Rash\\=omon set using SHAP. Experiments on 5 public datasets showed that explanations gradually converged as the sample size increased. Explanations from<128 samples exhibited high variability, limiting reliable knowledge extraction. However, agreement between models improved with more data, allowing for consensus. Bagging ensembles often had higher agreement. The results provide guidance on sufficient data to trust explanations. Variability at low samples suggests that conclusions may be unreliable without validation. Further work is needed with more model types, data domains, and explanation methods. Testing convergence in neural networks and with model-specific explanation methods would be impactful. The approaches explored here point towards principled techniques for eliciting knowledge from ambiguous models.", "isOpenAccess": false, "openAccessPdf": null}}
{"bib_hash_or_arxiv_id": "2308.13252v1", "title": "Kissing to Find a Match: Efficient Low-Rank Permutation Representation", "corpus_id": 261214490, "metadata": {"paperId": "ee6527ef003d9df3d853879b5178cfc002168a01", "externalIds": {"DBLP": "journals/corr/abs-2308-13252", "ArXiv": "2308.13252", "DOI": "10.48550/arXiv.2308.13252", "CorpusId": 261214490}, "corpusId": 261214490, "title": "Kissing to Find a Match: Efficient Low-Rank Permutation Representation", "abstract": "Permutation matrices play a key role in matching and assignment problems across the fields, especially in computer vision and robotics. However, memory for explicitly representing permutation matrices grows quadratically with the size of the problem, prohibiting large problem instances. In this work, we propose to tackle the curse of dimensionality of large permutation matrices by approximating them using low-rank matrix factorization, followed by a nonlinearity. To this end, we rely on the Kissing number theory to infer the minimal rank required for representing a permutation matrix of a given size, which is significantly smaller than the problem size. This leads to a drastic reduction in computation and memory costs, e.g., up to $3$ orders of magnitude less memory for a problem of size $n=20000$, represented using $8.4\\times10^5$ elements in two small matrices instead of using a single huge matrix with $4\\times 10^8$ elements. The proposed representation allows for accurate representations of large permutation matrices, which in turn enables handling large problems that would have been infeasible otherwise. We demonstrate the applicability and merits of the proposed approach through a series of experiments on a range of problems that involve predicting permutation matrices, from linear and quadratic assignment to shape matching problems.", "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2308.13252", "status": "CLOSED"}}}
{"bib_hash_or_arxiv_id": "2308.01237v2", "title": "LSF-IDM: Automotive Intrusion Detection Model with Lightweight Attribution and Semantic Fusion", "corpus_id": 260379010, "metadata": {"paperId": "aaa40ef8f2ceed58096a302802e592e585088df9", "externalIds": {"DBLP": "journals/corr/abs-2308-01237", "ArXiv": "2308.01237", "DOI": "10.48550/arXiv.2308.01237", "CorpusId": 260379010}, "corpusId": 260379010, "title": "LSF-IDM: Automotive Intrusion Detection Model with Lightweight Attribution and Semantic Fusion", "abstract": "Autonomous vehicles (AVs) are more vulnerable to network attacks due to the high connectivity and diverse communication modes between vehicles and external networks. Deep learning-based Intrusion detection, an effective method for detecting network attacks, can provide functional safety as well as a real-time communication guarantee for vehicles, thereby being widely used for AVs. Existing works well for cyber-attacks such as simple-mode but become a higher false alarm with a resource-limited environment required when the attack is concealed within a contextual feature. In this paper, we present a novel automotive intrusion detection model with lightweight attribution and semantic fusion, named LSF-IDM. Our motivation is based on the observation that, when injected the malicious packets to the in-vehicle networks (IVNs), the packet log presents a strict order of context feature because of the periodicity and broadcast nature of the CAN bus. Therefore, this model first captures the context as the semantic feature of messages by the BERT language framework. Thereafter, the lightweight model (e.g., BiLSTM) learns the fused feature from an input packet's classification and its output distribution in BERT based on knowledge distillation. Experiment results demonstrate the effectiveness of our methods in defending against several representative attacks from IVNs. We also perform the difference analysis of the proposed method with lightweight models and Bert to attain a deeper understanding of how the model balance detection performance and model complexity.", "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2308.01237", "status": "GREEN"}}}
{"bib_hash_or_arxiv_id": "2308.00729v1", "title": "Ada-DQA: Adaptive Diverse Quality-aware Feature Acquisition for Video Quality Assessment", "corpus_id": 260378902, "metadata": {"paperId": "b5e4c46ac09d5de2051998c9f741ef480fab2ebe", "externalIds": {"DBLP": "conf/mm/LiuWYSTZWL23", "ArXiv": "2308.00729", "DOI": "10.1145/3581783.3611795", "CorpusId": 260378902}, "corpusId": 260378902, "title": "Ada-DQA: Adaptive Diverse Quality-aware Feature Acquisition for Video Quality Assessment", "abstract": "Video quality assessment (VQA) has attracted growing attention in recent years. While the great expense of annotating large-scale VQA datasets has become the main obstacle for current deep-learning methods. To surmount the constraint of insufficient training data, in this paper, we first consider the complete range of video distribution diversity (i.e. content, distortion, motion) and employ diverse pretrained models (e.g. architecture, pretext task, pre-training dataset) to benefit quality representation. An Adaptive Diverse Quality-aware feature Acquisition (Ada-DQA) framework is proposed to capture desired quality-related features generated by these frozen pretrained models. By leveraging the Quality-aware Acquisition Module (QAM), the framework is able to extract more essential and relevant features to represent quality. Finally, the learned quality representation is utilized as supplementary supervisory information, along with the supervision of the labeled quality score, to guide the training of a relatively lightweight VQA model in a knowledge distillation manner, which largely reduces the computational cost during inference. Experimental results on three mainstream no-reference VQA benchmarks clearly show the superior performance of Ada-DQA in comparison with current state-of-the-art approaches without using extra training data of VQA.", "isOpenAccess": true, "openAccessPdf": {"url": "https://dl.acm.org/doi/pdf/10.1145/3581783.3611795", "status": "HYBRID"}}}
{"bib_hash_or_arxiv_id": "2308.11007v1", "title": "Mitigating Bias in Deep Learning: Training Unbiased Models on Biased Data for the Morphological Classification of Galaxies", "corpus_id": 261065025, "metadata": {"paperId": "d929f00eda548e1d5a4cf0e8fed8e4b67b059b68", "externalIds": {"ArXiv": "2308.11007", "CorpusId": 261065025}, "corpusId": 261065025, "title": "Mitigating Bias in Deep Learning: Training Unbiased Models on Biased Data for the Morphological Classification of Galaxies", "abstract": "Galaxy morphologies and their relation with physical properties have been a relevant subject of study in the past. Most galaxy morphology catalogs have been labelled by human annotators or by machine learning models trained on human labelled data. Human generated labels have been shown to contain biases in terms of the observational properties of the data, such as image resolution. These biases are independent of the annotators, that is, are present even in catalogs labelled by experts. In this work, we demonstrate that training deep learning models on biased galaxy data produce biased models, meaning that the biases in the training data are transferred to the predictions of the new models. We also propose a method to train deep learning models that considers this inherent labelling bias, to obtain a de-biased model even when training on biased data. We show that models trained using our deep de-biasing method are capable of reducing the bias of human labelled datasets.", "isOpenAccess": false, "openAccessPdf": null}}
{"bib_hash_or_arxiv_id": "2308.09487v3", "title": "DFB: A Data-Free, Low-Budget, and High-Efficacy Clean-Label Backdoor Attack", "corpus_id": 261030242, "metadata": {"paperId": "7d8e145fd56dc03f05ee3d368c5ea50582cbe951", "externalIds": {"ArXiv": "2308.09487", "CorpusId": 261030242}, "corpusId": 261030242, "title": "DFB: A Data-Free, Low-Budget, and High-Efficacy Clean-Label Backdoor Attack", "abstract": "In the domain of backdoor attacks, accurate labeling of injected data is essential for evading rudimentary detection mechanisms. This imperative has catalyzed the development of clean-label attacks, which are notably more elusive as they preserve the original labels of the injected data. Current clean-label attack methodologies primarily depend on extensive knowledge of the training dataset. However, practically, such comprehensive dataset access is often unattainable, given that training datasets are typically compiled from various independent sources. Departing from conventional clean-label attack methodologies, our research introduces DFB, a data-free, low-budget, and high-efficacy clean-label backdoor Attack. DFB is unique in its independence from training data access, requiring solely the knowledge of a specific target class. Tested on CIFAR10, Tiny-ImageNet, and TSRD, DFB demonstrates remarkable efficacy with minimal poisoning rates of just 0.1%, 0.025%, and 0.4%, respectively. These rates are significantly lower than those required by existing methods such as LC, HTBA, BadNets, and Blend, yet DFB achieves superior attack success rates. Furthermore, our findings reveal that DFB poses a formidable challenge to four established backdoor defense algorithms, indicating its potential as a robust tool in advanced clean-label attack strategies.", "isOpenAccess": false, "openAccessPdf": null}}
{"bib_hash_or_arxiv_id": "2308.06173v1", "title": "Physical Adversarial Attacks for Camera-Based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook", "corpus_id": 260865937, "metadata": {"paperId": "89a0f417d5d41e5acaebaa8fd695d9dcb9632a04", "externalIds": {"ArXiv": "2308.06173", "DBLP": "journals/access/GuesmiHOS23", "DOI": "10.1109/ACCESS.2023.3321118", "CorpusId": 260865937}, "corpusId": 260865937, "title": "Physical Adversarial Attacks for Camera-Based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook", "abstract": "Deep Neural Networks (DNNs) have shown impressive performance in computer vision tasks; however, their vulnerability to adversarial attacks raises concerns regarding their security and reliability. Extensive research has shown that DNNs can be compromised by carefully crafted perturbations, leading to significant performance degradation in both digital and physical domains. Therefore, ensuring the security of DNN-based systems is crucial, particularly in safety-critical domains such as autonomous driving, robotics, smart homes/cities, smart industries, video surveillance, and healthcare. In this paper, we present a comprehensive survey of the current trends focusing specifically on physical adversarial attacks. We aim to provide a thorough understanding of the concept of physical adversarial attacks, analyzing their key characteristics and distinguishing features. Furthermore, we explore the specific requirements and challenges associated with executing attacks in the physical world. Our article delves into various physical adversarial attack methods, categorized according to their target tasks in different applications, including classification, detection, face recognition, semantic segmentation and depth estimation. We assess the performance of these attack methods in terms of their effectiveness, stealthiness, and robustness. We examine how each technique strives to ensure the successful manipulation of DNNs while mitigating the risk of detection and withstanding real-world distortions. Lastly, we discuss the current challenges and outline potential future research directions in the field of physical adversarial attacks. We highlight the need for enhanced defense mechanisms, the exploration of novel attack strategies, the evaluation of attacks in different application domains, and the establishment of standardized benchmarks and evaluation criteria for physical adversarial attacks. Through this comprehensive survey, we aim to provide a valuable resource for researchers, practitioners, and policymakers to gain a holistic understanding of physical adversarial attacks in computer vision and facilitate the development of robust and secure DNN-based systems.", "isOpenAccess": true, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10268441.pdf", "status": "GOLD"}}}
{"bib_hash_or_arxiv_id": "2308.04352v1", "title": "3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment", "corpus_id": 260704493, "metadata": {"paperId": "64a80a33018a0fdc182b06111e32b2e08e186f6a", "externalIds": {"DBLP": "conf/iccv/ZhuMCD0023", "ArXiv": "2308.04352", "DOI": "10.1109/ICCV51070.2023.00272", "CorpusId": 260704493}, "corpusId": 260704493, "title": "3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment", "abstract": "3D vision-language grounding (3D-VL) is an emerging field that aims to connect the 3D physical world with natural language, which is crucial for achieving embodied intelligence. Current 3D-VL models rely heavily on sophisticated modules, auxiliary losses, and optimization tricks, which calls for a simple and unified model. In this paper, we propose 3D-VisTA, a pre-trained Transformer for 3D Vision and Text Alignment that can be easily adapted to various downstream tasks. 3D-VisTA simply utilizes self-attention layers for both single-modal modeling and multi-modal fusion without any sophisticated task-specific design. To further enhance its performance on 3D-VL tasks, we construct ScanScribe, the first large-scale 3D scene-text pairs dataset for 3D-VL pre-training. ScanScribe contains 2,995 RGB-D scans for 1,185 unique indoor scenes originating from ScanNet and 3R-Scan datasets, along with paired 278K scene descriptions generated from existing 3D-VL tasks, templates, and GPT-3. 3D-VisTA is pre-trained on ScanScribe via masked language/object modeling and scene-text matching. It achieves state-of-the-art results on various 3D-VL tasks, ranging from visual grounding and dense captioning to question answering and situated reasoning. Moreover, 3D-VisTA demonstrates superior data efficiency, obtaining strong performance even with limited annotations during downstream task fine-tuning.", "isOpenAccess": false, "openAccessPdf": null}}
