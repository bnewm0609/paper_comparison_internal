ID: ea633ba2-44f4-4a74-b096-dd319318ca97

GOLD TABLE:
|           | Language       | Pretrained from                | Corpora                                                     | Publicly Available   | Evaluation                                                                                     |
|----------:|:---------------|:-------------------------------|:------------------------------------------------------------|:---------------------|:-----------------------------------------------------------------------------------------------|
| 211678011 | ['Japanese']   | ['Scratch']                    | ['Japanese Clinical Text + Japanese Wikipedia']             | ['No']               | ['Text Classification']                                                                        |
| 226283634 | ['Portuguese'] | ['Multilingual BERT']          | ['Brazilian Clinical Text + Biomedical Text']               | ['Yes']              | ['Clinical Concept Extraction']                                                                |
| 215416118 | ['Russian']    | ['Multilingual BERT']          | ['Russian and English Health Reviews']                      | ['Yes']              | ['ADR Tweets Classification']                                                                  |
| 225452639 | ['German']     | ['General German BERT']        | ['Private Radiology Reports']                               | ['No']               | ['Radiology Reports Classification']                                                           |
| 218564040 | ['Spanish']    | ['Scratch']                    | ['Spanish Biomedical Text']                                 | ['No']               | ['Biomedical NER']                                                                             |
| 232283435 | ['Arabic']     | ['AraBERT']                    | ['General Arabic Text+ Arabic Biomedical Text']             | ['No']               | ['Biomedical NER']                                                                             |
| 219956480 | ['French']     | ['CamemBERT {{cite:a40b78e}}'] | ['French Biomedical Corpus']                                | ['No']               | ['Biomedical NER']                                                                             |
| 221293343 | ['Chinese']    | ['General Chinese BERT']       | ['Chinese Biomedical Text, Encyclopedia , Medical records'] | ['Yes']              | ['ChineseBLUE']                                                                                |
| 220409271 | ['Japanese']   | ['Scratch']                    | ['Japanese Clinical Text']                                  | ['Yes']              | ['Text Classification']                                                                        |
| 233240721 | ['Persian']    | ['ParsBERT {{cite:d02ce8d}}']  | ['Persian Medical Corpus']                                  | ['No']               | ['Medical Question Classification, Medical Question Retrieval and Medical Sentiment Analysis'] |
| 235077408 | ['Spanish']    | ['XLM-R {{cite:e715160}}']     | ['Spanish Clinical Text corpus']                            | ['Yes']              | ['Medical Coding']                                                                             |

GOLD SCHEMA:
0: Language
1: Pretrained from
2: Corpora
3: Publicly Available
4: Evaluation

PREDICTION PATH:../../metric_validation_0/ea633ba2-44f4-4a74-b096-dd319318ca97/gpt3.5/ours_outputs/try_0.json

PREDICTED TABLE:
|          | Language-specific BERT models                     | State-of-the-art performance on NLP tasks                                                                                           | Pre-training on a large corpus                                                                                                | Comparison with multilingual BERT                                                                                                                                                                                                    | Availability of pre-trained models                                                                                                                                         |
|:---------|:--------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| paper_1  | ['AraBERT']                                       | ['Achieved state-of-the-art performance on Arabic NLP tasks']                                                                       | ['Pre-trained BERT specifically for Arabic language']                                                                         | ['Compared AraBERT performance to multilingual BERT, achieving state-of-the-art results for most tested Arabic NLP tasks.']                                                                                                          | ['The pretrained araBERT models are publicly available at https://github.com/aub-mind/arabert.']                                                                           |
| paper_2  | ['BioBERTpt']                                     | ['Outperformed baseline model in Portuguese clinical NER']                                                                          | ['Transfer learned information from multilingual-BERT to a corpora of clinical narratives']                                   | ['BioBERTpt performance was evaluated in comparison to existing BERT models, outperforming the baseline model in F1-score by 2.72%.']                                                                                                | ['Transferred learned information encoded in BioBERTpt model to a corpora of clinical narratives in Brazilian Portuguese, enhancing the Portuguese biomedical NER model.'] |
| paper_3  | ['BioBERTpt']                                     | ['Outperformed baseline model in Portuguese clinical NER']                                                                          | ['Transfer learned information from multilingual-BERT to a corpora of clinical narratives']                                   | ['BioBERTpt performance was assessed in comparison to existing BERT models, outperforming the baseline model in F1-score by 2.72%.']                                                                                                 | ['Transferred learned information encoded in BioBERTpt model to a corpora of clinical narratives in Brazilian Portuguese, enhancing the Portuguese biomedical NER model.'] |
| paper_4  | ['BioBERTpt']                                     | ['Outperformed baseline model in Portuguese clinical NER']                                                                          | ['Transfer learned information from multilingual-BERT to a corpora of clinical narratives']                                   | ['BioBERTpt performance was evaluated in comparison to existing BERT models, outperforming the baseline model in F1-score by 2.72%.']                                                                                                | ['Transferred learned information encoded in BioBERTpt model to a corpora of clinical narratives in Brazilian Portuguese, enhancing the Portuguese biomedical NER model.'] |
| paper_5  | ['RuDR-BERT']                                     | ['Achieved 74.85% macro F1 score in Russian NER task']                                                                              | ['Pretrained RuDR-BERT model on Russian Drug Reaction Corpus']                                                                | ['RuDR-BERT achieved a macro F1 score of 74.85% in the NER task, surpassing the accuracy of previous approaches with little annotation effort.']                                                                                     | ['The RuDReC corpus and pretrained weights of domain-specific BERT models are available at https://github.com/cimm-kzn/RuDReC.']                                           |
| paper_6  | ['RuDR-BERT']                                     | ['Achieved 74.85% macro F1 score in Russian NER task']                                                                              | ['Pretrained RuDR-BERT model on Russian Drug Reaction Corpus']                                                                | ['RuDR-BERT achieved a macro F1 score of 74.85% in the NER task, surpassing the accuracy of previous approaches with little annotation effort.']                                                                                     | ['The RuDReC corpus and pretrained weights of domain-specific BERT models are available at https://github.com/cimm-kzn/RuDReC.']                                           |
| paper_7  | ['BERT']                                          | ['Achieved high accuracy in classifying chest radiographic reports']                                                                | ['Pre-trained on 3.8 million text reports for identifying findings in chest x-ray reports']                                   | ['Used BERT to identify findings in chest radiographic reports and achieved high areas under the receiver operation characteristics curve, surpassing the accuracy of previous approaches with little effort.']                      | ['N/A']                                                                                                                                                                    |
| paper_8  | ['BERT']                                          | ['Achieved high accuracy in classifying chest radiographic reports']                                                                | ['Pre-trained on 3.8 million text reports for identifying findings in chest x-ray reports']                                   | ['Used BERT to identify findings in chest radiographic reports and achieved high areas under the receiver operation characteristics curve, surpassing the accuracy of previous approaches with little effort.']                      | ['N/A']                                                                                                                                                                    |
| paper_9  | ['BERT']                                          | ['Outperformed standard word embeddings in Spanish biomedical NER task']                                                            | ['Train from scratch the BERT language representation model and fine-tune it for Spanish biomedical NER']                     | ['Trained BERT language representation model from scratch and fine-tuned it for NER tasks on Spanish Clinical Case Corpus, showing better results than NER model trained over standard word embeddings.']                            | ['N/A']                                                                                                                                                                    |
| paper_10 | ['ABioNER']                                       | ['Outperformed AraBERT and multilingual BERT cased with 85% F1-score']                                                              | ['Investigated the effectiveness of pretraining a BERT model with a small-scale biomedical dataset on Arabic biomedical NER'] | ['Performed comparative analysis with AraBERT and multilingual BERT cased models, outperforming both models with 85% F1-score.']                                                                                                     | ['N/A']                                                                                                                                                                    |
| paper_11 | ['French contextualized language models']         | ['Achieved top performance in biomedical NER in French text']                                                                       | ['Used a transfer-learning-based approach to pretrain BERT on a corpus of real-world oncology clinical cases']                | ['The contextualized French language models performed very effectively for NER in the biomedical domain, achieving the best results in the challenge.']                                                                              | ['N/A']                                                                                                                                                                    |
| paper_12 | ['BERT for Chinese biomedical corpora']           | ['Proposed a novel conceptualized representation learning approach']                                                                | ['Adapted for Chinese biomedical corpora using a conceptualized representation learning approach']                            | ['Investigated how BERT can be adapted for Chinese biomedical corpora and proposed a conceptualized representation learning approach that showed significant gain.']                                                                 | ['The pre-trained model is released on GitHub: this https URL.']                                                                                                           |
| paper_13 | ['clinical specific BERT model']                  | ['Trained on a huge size of Japanese clinical narrative with promising results']                                                    | ['Pre-trained with a huge size of Japanese clinical narrative']                                                               | ['Developed a clinical specific BERT model and evaluated it on NTCIR-13 MedWeb, showing higher performances on the task than other nonspecific BERTs.']                                                                              | ['N/A']                                                                                                                                                                    |
| paper_14 | ['SINA-BERT']                                     | ['Outperformed previous BERT-based models in Persian medical tasks']                                                                | ['Pre-trained on a large-scale corpus of medical contents for Persian language tasks']                                        | ['Sina-BERT, a language model pre-trained on BERT, was released to address the lack of a high-quality Persian language model in the medical domain.']                                                                                | ['N/A']                                                                                                                                                                    |
| paper_15 | ['transformer-based models (mBERT, BETO, XLM-R)'] | ['Resulted in improved performance with an ensemble approach leveraging the predictive capacities of three different transformers'] | ['Pretrained on a corpus of real-world oncology clinical cases to adapt to Spanish medical texts']                            | ['Systematically analyzed transformer-based models for automatic clinical coding in Spanish, and achieved state-of-the-art performance with the help of an ensemble approach leveraging the capabilities of existing transformers.'] | ['The mBERT, BETO, and XLMR transformers adapted to the Spanish clinical domain are publicly available at https://github.com/guilopgar/ClinicalCodingTransformerES.']      |
| paper_16 | ['transformer-based models (mBERT, BETO, XLM-R)'] | ['Resulted in improved performance with an ensemble approach leveraging the predictive capacities of three different transformers'] | ['Pretrained on a corpus of real-world oncology clinical cases to adapt to Spanish medical texts']                            | ['Systematically analyzed transformer-based models for automatic clinical coding in Spanish, and achieved state-of-the-art performance with the help of an ensemble approach leveraging the capabilities of existing transformers.'] | ['The mBERT, BETO, and XLMR transformers adapted to the Spanish clinical domain are publicly available at https://github.com/guilopgar/ClinicalCodingTransformerES.']      |
| paper_17 | ['transformer-based models (mBERT, BETO, XLM-R)'] | ['Resulted in improved performance with an ensemble approach leveraging the predictive capacities of three different transformers'] | ['Pretrained on a corpus of real-world oncology clinical cases to adapt to Spanish medical texts']                            | ['Systematically analyzed transformer-based models for automatic clinical coding in Spanish, and achieved state-of-the-art performance with the help of an ensemble approach leveraging the capabilities of existing transformers.'] | ['The mBERT, BETO, and XLMR transformers adapted to the Spanish clinical domain are publicly available at https://github.com/guilopgar/ClinicalCodingTransformerES.']      |

MATCHES:
Language-specific BERT models: 
State-of-the-art performance on NLP tasks: 
Pre-training on a large corpus: 
Comparison with multilingual BERT: 
Availability of pre-trained models: 