Table ID,Column,Corpus ID,Gold Value,Predicted Value
3d6eef29-85dd-420e-b68b-a929adc0a399,Name,6875312,A3C,Asynchronous Methods for Deep Reinforcement Learning
3d6eef29-85dd-420e-b68b-a929adc0a399,Name,16326763,DDPG,Proximal Policy Optimization Algorithms
3d6eef29-85dd-420e-b68b-a929adc0a399,Name,16046818,TRPO,Soft Actor-Critic
3d6eef29-85dd-420e-b68b-a929adc0a399,Approach,6875312,Distributed Actor Critic,Asynchronous Gradient Descent for Deep Neural Network Controllers
3d6eef29-85dd-420e-b68b-a929adc0a399,Approach,16326763,Derivative of continuous action function,Model-free Off-policy Actor-Critic Algorithm with Deep Function Approximators
3d6eef29-85dd-420e-b68b-a929adc0a399,Approach,16046818,Dynamically sized step size,Trust Region Policy Optimization with KL Divergence Constraint
3d6eef29-85dd-420e-b68b-a929adc0a399,Approach,28695052,"Improved TRPO, first order",Proximal Policy Optimization
3d6eef29-85dd-420e-b68b-a929adc0a399,Approach,28202810,Variance-based Actor Critic for robustness,Soft Actor-Critic based on Maximum Entropy Framework
