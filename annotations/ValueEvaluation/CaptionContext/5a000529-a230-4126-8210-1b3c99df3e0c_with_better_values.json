{"tabid": "5a000529-a230-4126-8210-1b3c99df3e0c", "table": {"# Images": {"173991173": "DAQUAR: 1,449, Visual Madlibs: 10,738, Visual 7W: 47,300, VQA (v2): 200,000, MovieQA: 408, CLEVR: 100,000, KB-VQA: 700, FVQA: 2,190, OK-VQA: 14,031", "214612106": "DAQUAR: 1,449, VQA: 204,000, VQA v2: 204,000", "231951663": "642"}, "# QA pairs": {"173991173": "DAQUAR: 12,468, Visual Madlibs: 360,001, Visual 7W: 327,939, VQA (v2): 1.1M, MovieQA: 14,944, CLEVR: 999,968, KB-VQA: 2,402, FVQA: 5,826, OK-VQA: 14,055", "214612106": "DAQUAR: 12,468, VQA: 614,000, VQA v2: 1,000,000", "231951663": "N/A"}, "Source of imagesand content": {"173991173": "Uses COCO dataset images and Wikipedia articles.", "214612106": "Uses general-domain images and captions; medical datasets use pathology images and captions from textbooks and libraries.", "231951663": "Uses radiology images (CT scans, MRIs, X-Rays) and doctor-generated questions, categorized into vision-only and knowledge-based."}, "QACreation": {"173991173": "Questions and answers generated through a two-step MTurk process, manually filtered for quality and knowledge requirement.", "214612106": "Questions and answers derived from pathology images and captions using NLP, manually checked for errors and inconsistencies.", "231951663": "Questions proposed by doctors using templates, balanced answers to mitigate bias, includes semantic labels."}, "Question Category": {"173991173": "Covers a wide range of knowledge categories including Vehicles and Transportation, Brands, Companies and Products, Objects, Materials and Clothing, Sports and Recreation, Cooking and Food, Geography, History, Language and Culture, People and Everyday Life, Plants and Animals, Science and Technology, Weather and Climate.", "214612106": "Covers pathology-related questions, clinical questions, and general-domain questions including aspects such as color, location, appearance, shape, modality, plane, organ system, and abnormality.", "231951663": "Addresses both vision-only and knowledge-based questions in radiology, covering categories such as diagnostic, anatomical, and general knowledge areas."}}, "row_bib_map": [{"bib_hash_or_arxiv_id": "6dfd5792688a8560a7f26036a8e45c15f426bdd8", "row": 0, "corpus_id": 173991173, "type": "ref", "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge", "abstract": "Visual Question Answering (VQA) in its ideal form lets us study reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most VQA benchmarks to date are focused on questions such as simple counting, visual attributes, and object detection that do not require reasoning or knowledge beyond what is in the image. In this paper, we address the task of knowledge-based visual question answering and provide a benchmark, called OK-VQA, where the image content is not sufficient to answer the questions, encouraging methods that rely on external knowledge resources. Our new dataset includes more than 14,000 questions that require external knowledge to answer. We show that the performance of the state-of-the-art VQA models degrades drastically in this new setting. Our analysis shows that our knowledge-based VQA task is diverse, difficult, and large compared to previous knowledge-based VQA datasets. We hope that this dataset enables researchers to open up new avenues for research in this domain."}, {"bib_hash_or_arxiv_id": "cad8f1bb7e8d5dc7d502d1cc02c5a919f3c274c0", "row": 1, "corpus_id": 214612106, "type": "ref", "title": "PathVQA: 30000+ Questions for Medical Visual Question Answering", "abstract": "Is it possible to develop an \"AI Pathologist\" to pass the board-certified examination of the American Board of Pathology? To achieve this goal, the first step is to create a visual question answering (VQA) dataset where the AI agent is presented with a pathology image together with a question and is asked to give the correct answer. Our work makes the first attempt to build such a dataset. Different from creating general-domain VQA datasets where the images are widely accessible and there are many crowdsourcing workers available and capable of generating question-answer pairs, developing a medical VQA dataset is much more challenging. First, due to privacy concerns, pathology images are usually not publicly available. Second, only well-trained pathologists can understand pathology images, but they barely have time to help create datasets for AI research. To address these challenges, we resort to pathology textbooks and online digital libraries. We develop a semi-automated pipeline to extract pathology images and captions from textbooks and generate question-answer pairs from captions using natural language processing. We collect 32,799 open-ended questions from 4,998 pathology images where each question is manually checked to ensure correctness. To our best knowledge, this is the first dataset for pathology VQA. Our dataset will be released publicly to promote research in medical VQA."}, {"bib_hash_or_arxiv_id": "29f7abceef54decdb49c568cecc5a74e4b9f8782", "row": 2, "corpus_id": 231951663, "type": "ref", "title": "Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering", "abstract": "Medical visual question answering (Med-VQA) has tremendous potential in healthcare. However, the development of this technology is hindered by the lacking of publicly-available and high-quality labeled datasets for training and evaluation. In this paper, we present a large bilingual dataset, SLAKE, with comprehensive semantic labels annotated by experienced physicians and a new structural medical knowledge base for Med-VQA. Besides, SLAKE includes richer modalities and covers more human body parts than the currently available dataset. We show that SLAKE can be used to facilitate the development and evaluation of Med-VQA systems. The dataset can be downloaded from http://www.med-vqa.com/slake."}]}