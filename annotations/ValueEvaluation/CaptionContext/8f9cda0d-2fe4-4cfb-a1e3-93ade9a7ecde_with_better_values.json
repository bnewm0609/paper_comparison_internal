{"tabid": "8f9cda0d-2fe4-4cfb-a1e3-93ade9a7ecde", "table": {"Development": {"239998781": "N/A", "219558583": "Developed GQA-OOD benchmark using a fine-grained reorganization of the GQA dataset; specific evaluation metrics designed for this benchmark.", "62902119": "Utilized human annotation, consensus score, and standard image captioning metrics.", "233168594": "Employed VQA-CE protocol, GMiner for rule extraction, and various bias-reduction methods.", "252780087": "Used mutual information, Shannon entropy, Jaccard Similarity, and head/tail splits in training data.", "235266220": "Developed AVQA dataset using HAMLET; trained models on Faster R-CNN features; metrics included model error rate and performance on robust VQA benchmarks.", "235352921": "Human-in-the-loop data collection; used Amazon Mechanical Turk for adversarial questions; metrics included model error rate."}, "Image Source": {"239998781": "Visual Genome", "219558583": "GQA", "62902119": "VQA v2.0", "233168594": "VQA v2", "252780087": "VQA v2", "235266220": "Multiple sources (Conceptual Captions, Fakeddit, VCR)", "235352921": "COCO"}, "Focus": {"239998781": "Development of CRIC benchmark for compositional reasoning in VQA", "219558583": "Investigation of natural language question challenges in VQA", "62902119": "Proposal of CRIC dataset for compositional reasoning in VQA", "233168594": "Exploration of challenges in VQA models on CRIC dataset", "252780087": "Evaluation and improvement of VQA models", "235266220": "Robustness to linguistic variations in VQA questions", "235352921": "Training for consistency and robustness in VQA models"}, "ID": {"239998781": "Unique identifier for datasets and evaluation metrics", "219558583": "Unique identifier for datasets and evaluation metrics", "62902119": "Unique identifier for datasets and evaluation metrics", "233168594": "N/A", "252780087": "Unique identifier for datasets and evaluation metrics", "235266220": "Unique identifier for datasets and evaluation metrics", "235352921": "Unique identifier for datasets and evaluation metrics"}, "OOD": {"239998781": "Exception while querying PaperQA endpoint: '239998781'", "219558583": "Introduces GQA-OOD benchmark with metrics Acc-tail, Acc-head, Acc-all; compares bias-reduction methods.", "62902119": "N/A", "233168594": "Introduces VQA-CE protocol; evaluates bias-reduction methods on OOD data.", "252780087": "Compares VQA-CP v2 and VQA-VS; introduces multiple OOD test sets and standardizes OOD evaluation.", "235266220": "Introduces Adversarial VQA (AVQA) benchmark; compares UNITER-B model performance on various VQA benchmarks.", "235352921": "Discusses limitations of current models on OOD data; introduces Adversarial VQA (AdVQA) dataset."}, "Metrics": {"239998781": "Evaluation criteria: Answer accuracy, grounding accuracy, final score; Performance on ID and OOD data; Analysis of various data types and datasets.", "219558583": "Evaluation criteria: Accuracy on infrequent and frequent concepts, overall accuracy; Metrics: Acc-tail, Acc-head, Acc-all; Performance on ID and OOD data; Analysis of prediction error distribution and model comparison.", "62902119": "Evaluation criteria: VQA accuracy, consensus score, failure prediction performance; Performance on ID and OOD data; Analysis on VQA v2.0 and VQA-Rephrasings datasets.", "233168594": "Evaluation criteria: Detection of shortcuts, impact on VQA models; Performance measures: Accuracy on counterexamples, subsets of validation set; Analysis of shortcuts and correlation coefficients.", "252780087": "Evaluation criteria: Accuracy on ID and OOD test sets; Performance measures: Difference between ID and OOD accuracy, accuracy on shortcut test sets; Analysis of language biases, reasoning abilities, impact of shortcuts.", "235266220": "Evaluation criteria: Model error rate, performance on different datasets; Analysis of question types and visual concepts; Performance on AVQA and other robust VQA benchmarks.", "235352921": "Evaluation criteria: Model error rate, inter-human agreement; Performance on different question categories; Analysis on AdVQA and VQA v2 datasets; Ability to reason about text, count, handle rare concepts, understand multimodal information."}}, "row_bib_map": [{"bib_hash_or_arxiv_id": "91440bcf751c3b084b1de2a8ea5cb483cc41460d", "row": 0, "corpus_id": 239998781, "type": "ref", "title": "CRIC: A VQA Dataset for Compositional Reasoning on Vision and Commonsense", "abstract": "Alternatively inferring on the visual facts and commonsense is fundamental for an advanced visual question answering (VQA) system. This ability requires models to go beyond the literal understanding of commonsense. The system should not just treat objects as the entrance to query background knowledge, but fully ground commonsense to the visual world and imagine the possible relationships between objects, e.g., \u201cfork, can lift, food\u201d. To comprehensively evaluate such abilities, we propose a VQA benchmark, Compositional Reasoning on vIsion and Commonsense(CRIC), which introduces new types of questions about CRIC, and an evaluation metric integrating the correctness of answering and commonsense grounding. To collect such questions and rich additional annotations to support the metric, we also propose an automatic algorithm to generate question samples from the scene graph associated with the images and the relevant knowledge graph. We further analyze several representative types of VQA models on the CRIC dataset. Experimental results show that grounding the commonsense to the image region and joint reasoning on vision and commonsense are still challenging for current approaches. The dataset is available at https://cricvqa.github.io."}, {"bib_hash_or_arxiv_id": "6c93e7887517b6694cba9fb84a1f527009469c2b", "row": 1, "corpus_id": 219558583, "type": "ref", "title": "Roses are Red, Violets are Blue\u2026 But Should VQA expect Them To?", "abstract": "Models for Visual Question Answering (VQA) are notorious for their tendency to rely on dataset biases, as the large and unbalanced diversity of questions and concepts involved and tends to prevent models from learning to \"reason\", leading them to perform \"educated guesses\" instead. In this paper, we claim that the standard evaluation metric, which consists in measuring the overall in-domain accuracy, is misleading. Since questions and concepts are unbalanced, this tends to favor models which exploit subtle training set statistics. Alternatively, naively introducing artificial distribution shifts between train and test splits is also not completely satisfying. First, the shifts do not reflect real-world tendencies, resulting in unsuitable models; second, since the shifts are handcrafted, trained models are specifically designed for this particular setting, and do not generalize to other configurations. We propose the GQAOOD benchmark designed to overcome these concerns: we measure and compare accuracy over both rare and frequent question-answer pairs, and argue that the former is better suited to the evaluation of reasoning abilities, which we experimentally validate with models trained to more or less exploit biases. In a large-scale study involving 7 VQA models and 3 bias reduction techniques, we also experimentally demonstrate that these models fail to address questions involving infrequent concepts and provide recommendations for future directions of research."}, {"bib_hash_or_arxiv_id": "69ebb504c11b61938cd057076cff80ab5fae880c", "row": 2, "corpus_id": 62902119, "type": "ref", "title": "Cycle-Consistency for Robust Visual Question Answering", "abstract": "Despite significant progress in Visual Question Answer-ing over the years, robustness of today\u2019s VQA models leave much to be desired. We introduce a new evaluation protocol and associated dataset (VQA-Rephrasings) and show that state-of-the-art VQA models are notoriously brittle to linguistic variations in questions. VQA-Rephrasings contains 3 human-provided rephrasings for 40k questions-image pairs from the VQA v2.0 validation dataset. As a step towards improving robustness of VQA models, we propose a model-agnostic framework that exploits cycle consistency. Specifically, we train a model to not only answer a question, but also generate a question conditioned on the answer, such that the answer predicted for the generated question is the same as the ground truth answer to the original question. Without the use of additional supervision, we show that our approach is significantly more robust to linguistic variations than state-of-the-art VQA models, when evaluated on the VQA-Rephrasings dataset. In addition, our approach also outperforms state-of-the-art approaches on the standard VQA and Visual Question Generation tasks on the challenging VQA v2.0 dataset. Code and models will be made publicly available."}, {"bib_hash_or_arxiv_id": "38f1dc4c631f8b84b83f903ff1191d45b6df327b", "row": 3, "corpus_id": 233168594, "type": "ref", "title": "Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering", "abstract": "We introduce an evaluation methodology for visual question answering (VQA) to better diagnose cases of shortcut learning. These cases happen when a model exploits spurious statistical regularities to produce correct answers but does not actually deploy the desired behavior. There is a need to identify possible shortcuts in a dataset and assess their use before deploying a model in the real world. The research community in VQA has focused exclusively on question-based shortcuts, where a model might, for example, answer \"What is the color of the sky\" with \"blue\" by relying mostly on the question-conditional training prior and give little weight to visual evidence. We go a step further and consider multimodal shortcuts that involve both questions and images. We first identify potential shortcuts in the popular VQA v2 training set by mining trivial predictive rules such as co-occurrences of words and visual elements. We then introduce VQA-CounterExamples (VQACE), an evaluation protocol based on our subset of CounterExamples i.e. image-question-answer triplets where our rules lead to incorrect answers. We use this new evaluation in a large-scale study of existing approaches for VQA. We demonstrate that even state-of-the-art models perform poorly and that existing techniques to reduce biases are largely ineffective in this context. Our findings suggest that past work on question-based biases in VQA has only addressed one facet of a complex issue. The code for our method is available at https://github.com/cdancette/detect-shortcuts"}, {"bib_hash_or_arxiv_id": "273f4c4db89bb2137b2a6dda2f679217b7a96abb", "row": 4, "corpus_id": 252780087, "type": "ref", "title": "Language Prior Is Not the Only Shortcut: A Benchmark for Shortcut Learning in VQA", "abstract": "Visual Question Answering (VQA) models are prone to learn the shortcut solution formed by dataset biases rather than the intended solution. To evaluate the VQA models' reasoning ability beyond shortcut learning, the VQA-CP v2 dataset introduces a distribution shift between the training and test set given a question type. In this way, the model cannot use the training set shortcut (from question type to answer) to perform well on the test set. However, VQA-CP v2 only considers one type of shortcut and thus still cannot guarantee that the model relies on the intended solution rather than a solution specific to this shortcut. To overcome this limitation, we propose a new dataset that considers varying types of shortcuts by constructing different distribution shifts in multiple OOD test sets. In addition, we overcome the three troubling practices in the use of VQA-CP v2, e.g., selecting models using OOD test sets, and further standardize OOD evaluation procedure. Our benchmark provides a more rigorous and comprehensive testbed for shortcut learning in VQA. We benchmark recent methods and find that methods specifically designed for particular shortcuts fail to simultaneously generalize to our varying OOD test sets. We also systematically study the varying shortcuts and provide several valuable findings, which may promote the exploration of shortcut learning in VQA."}, {"bib_hash_or_arxiv_id": "a8ed3d3fdb1008c166d5d1cf18dc6a18c6e288cc", "row": 5, "corpus_id": 235266220, "type": "ref", "title": "Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models", "abstract": "Benefiting from large-scale pre-training, we have witnessed significant performance boost on the popular Visual Question Answering (VQA) task. Despite rapid progress, it remains unclear whether these state-of-the-art (SOTA) models are robust when encountering examples in the wild. To study this, we introduce Adversarial VQA, a new large-scale VQA benchmark, collected iteratively via an adversarial human-and-model-in-the-loop procedure. Through this new benchmark, we discover several interesting findings. (i) Surprisingly, we find that during dataset collection, non-expert annotators can easily attack SOTA VQA models successfully. (ii) Both large-scale pre-trained models and adversarial training methods achieve far worse performance on the new benchmark than over standard VQA v2 dataset, revealing the fragility of these models while demonstrating the effectiveness of our adversarial dataset. (iii) When used for data augmentation, our dataset can effectively boost model performance on other robust VQA benchmarks. We hope our Adversarial VQA dataset can shed new light on robustness study in the community and serve as a valuable benchmark for future work."}, {"bib_hash_or_arxiv_id": "49d628f90823e57d23ed397516d62a5670a1c530", "row": 6, "corpus_id": 235352921, "type": "ref", "title": "Human-Adversarial Visual Question Answering", "abstract": "Performance on the most commonly used Visual Question Answering dataset (VQA v2) is starting to approach human accuracy. However, in interacting with state-of-the-art VQA models, it is clear that the problem is far from being solved. In order to stress test VQA models, we benchmark them against human-adversarial examples. Human subjects interact with a state-of-the-art VQA model, and for each image in the dataset, attempt to find a question where the model's predicted answer is incorrect. We find that a wide range of state-of-the-art models perform poorly when evaluated on these examples. We conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions. We hope that this Adversarial VQA (AdVQA) benchmark can help drive progress in the field and advance the state of the art."}]}