{"tabid": "46fd6d34-5004-405c-a1b5-d83d964d714f", "table": {"Architecture": {"10647804": "C-P-I-F", "2482804": "C-P-C-P-C-C-C-P-F-F-F"}, "Depth": {"10647804": "12 layers total", "2482804": "4C, 3P, 2N, 2F layers"}}, "row_bib_map": [{"bib_hash_or_arxiv_id": "163d13766025c44c5359ba46dc0b814b4b05fd09", "row": 0, "corpus_id": 10647804, "type": "ref", "title": "Going Deeper in Facial Expression Recognition using Deep Neural Networks", "abstract": "Automated Facial Expression Recognition (FER) has remained a challenging and interesting problem. Despite efforts made in developing various methods for FER, existing approaches traditionally lack generalizability when applied to unseen images or those that are captured in wild setting. Most of the existing approaches are based on engineered features (e.g. HOG, LBPH, and Gabor) where the classifier's hyperparameters are tuned to give best recognition accuracies across a single database, or a small collection of similar databases. Nevertheless, the results are not significant when they are applied to novel data. This paper proposes a deep neural network architecture to address the FER problem across multiple well-known standard face datasets. Specifically, our network consists of two convolutional layers each followed by max pooling and then four Inception layers. The network is a single component architecture that takes registered facial images as the input and classifies them into either of the six basic or the neutral expressions. We conducted comprehensive experiments on seven publically available facial expression databases, viz. MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of proposed architecture are comparable to or better than the state-of-the-art methods and better than traditional convolutional neural networks and in both accuracy and training time."}, {"bib_hash_or_arxiv_id": "ad9fe9cc0958611d26f6391ba759b39f4c8d3720", "row": 1, "corpus_id": 2482804, "type": "ref", "title": "Learning Social Relation Traits from Face Images", "abstract": "Social relation defines the association, e.g, warm, friendliness, and dominance, between two or more people. Motivated by psychological studies, we investigate if such fine-grained and high-level relation traits can be characterised and quantified from face images in the wild. To address this challenging problem we propose a deep model that learns a rich face representation to capture gender, expression, head pose, and age-related attributes, and then performs pairwise-face reasoning for relation prediction. To learn from heterogeneous attribute sources, we formulate a new network architecture with a bridging layer to leverage the inherent correspondences among these datasets. It can also cope with missing target attribute labels. Extensive experiments show that our approach is effective for fine-grained social relation learning in images and videos."}]}