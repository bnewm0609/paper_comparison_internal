{"tabid": "8f9cda0d-2fe4-4cfb-a1e3-93ade9a7ecde", "table": {"Development": {"239998781": "Exception while querying PaperQA endpoint: '239998781'", "219558583": "The GQA-OOD dataset was developed based on the existing GQA dataset. The dataset features distribution shifts for both validation and test sets. The benchmark also introduced a new set of evaluation metrics to evaluate the reasoning behavior of VQA models. The dataset was constructed using methodologies such as fine-grained reorganization of the GQA dataset, dividing questions into groups according to their contexts, and measuring group imbalance using Shannon entropy. The enhancement of the dataset involved selecting specific answer classes for each question group based on their frequencies.", "62902119": "The methodologies used in the development and enhancement of the datasets include human annotation, data filtering, and the use of consensus scores to measure robustness.", "233168594": "N/A", "252780087": "The development and enhancement of the datasets involved the use of several methodologies and tools. These include mutual information to select shortcut-specific concepts, Shannon entropy to measure group imbalance, Jaccard Similarity Coefficient to analyze the relevance of shortcuts, and the use of OOD test sets for model selection.", "235266220": "The AVQA dataset was developed using a Human-And-Model-in-the-Loop Enabled Training (HAMLET) procedure. This procedure involves iterative data collection rounds where human annotators compete against VQA models to design adversarial examples. The models used in the data collection process were trained on Faster R-CNN region features. The dataset also includes images from different domains, including web images from Conceptual Captions, user-generated images from Fakeddit, and movie images from VCR. The data collection process was conducted on Amazon Mechanical Turk.", "235352921": "The paper mentions the use of Dynabench, a platform for dynamic adversarial data collection and benchmarking, for collecting the AdVQA dataset. The paper also mentions the use of Amazon Mechanical Turk for question collection and validation."}, "Image Source": {"239998781": "Visual Genome", "219558583": "Real-world images", "62902119": "VQA v2.0", "233168594": "VQA v2", "252780087": "N/A", "235266220": "Multiple sources (Conceptual Captions, Fakeddit, VCR)", "235352921": "COCO (Common Objects in Context)"}, "Focus": {"239998781": "Compositional reasoning on vision and commonsense", "219558583": "Evaluating VQA systems on understanding non-visual background knowledge", "62902119": "Challenging commonsense reasoning questions", "233168594": "Evaluating the breadth of knowledge of VQA models", "252780087": "Evaluating models with various metrics", "235266220": "Focused on real-world images and human-posed questions", "235352921": "Measuring the sensitivity of VQA models to language bias"}, "ID": {"239998781": "Identifier of the dataset or evaluation metric", "219558583": "Identifier of the dataset or evaluation metric", "62902119": "Identifier of the dataset or evaluation metric", "233168594": "Identifier of the dataset or evaluation metric", "252780087": "Identifier of the dataset or evaluation metric", "235266220": "Identifier of the dataset or evaluation metric", "235352921": "Identifier of the dataset or evaluation metric"}, "OOD": {"239998781": "Out-of-distribution accuracy", "219558583": "Accuracy on rare OOD samples", "62902119": "Out-of-distribution performance", "233168594": "Performance on OOD evaluation datasets", "252780087": "Performance on OOD test sets", "235266220": "Evaluation metric for OOD split of VQA-CP", "235352921": "Out-of-distribution performance on datasets"}, "Metrics": {"239998781": "Exception while querying PaperQA endpoint: '239998781'", "219558583": "Evaluation Metrics: acc-tail, acc-head, acc-all", "62902119": "Evaluation Metrics: consensus score (CS), VQA accuracy", "233168594": "Evaluation Protocol: VQA-CE (VQA-CounterExamples); Other Methods: VQA-CP, VQA-Rephrasing, GQA-OOD", "252780087": "Evaluation Metrics: Difference between IID and OOD accuracy, empirical risk minimization, overall OOD risk", "235266220": "N/A", "235352921": "N/A"}}, "row_bib_map": [{"bib_hash_or_arxiv_id": "91440bcf751c3b084b1de2a8ea5cb483cc41460d", "row": 0, "corpus_id": 239998781, "type": "ref", "title": "CRIC: A VQA Dataset for Compositional Reasoning on Vision and Commonsense", "abstract": "Alternatively inferring on the visual facts and commonsense is fundamental for an advanced visual question answering (VQA) system. This ability requires models to go beyond the literal understanding of commonsense. The system should not just treat objects as the entrance to query background knowledge, but fully ground commonsense to the visual world and imagine the possible relationships between objects, e.g., \u201cfork, can lift, food\u201d. To comprehensively evaluate such abilities, we propose a VQA benchmark, Compositional Reasoning on vIsion and Commonsense(CRIC), which introduces new types of questions about CRIC, and an evaluation metric integrating the correctness of answering and commonsense grounding. To collect such questions and rich additional annotations to support the metric, we also propose an automatic algorithm to generate question samples from the scene graph associated with the images and the relevant knowledge graph. We further analyze several representative types of VQA models on the CRIC dataset. Experimental results show that grounding the commonsense to the image region and joint reasoning on vision and commonsense are still challenging for current approaches. The dataset is available at https://cricvqa.github.io."}, {"bib_hash_or_arxiv_id": "6c93e7887517b6694cba9fb84a1f527009469c2b", "row": 1, "corpus_id": 219558583, "type": "ref", "title": "Roses are Red, Violets are Blue\u2026 But Should VQA expect Them To?", "abstract": "Models for Visual Question Answering (VQA) are notorious for their tendency to rely on dataset biases, as the large and unbalanced diversity of questions and concepts involved and tends to prevent models from learning to \"reason\", leading them to perform \"educated guesses\" instead. In this paper, we claim that the standard evaluation metric, which consists in measuring the overall in-domain accuracy, is misleading. Since questions and concepts are unbalanced, this tends to favor models which exploit subtle training set statistics. Alternatively, naively introducing artificial distribution shifts between train and test splits is also not completely satisfying. First, the shifts do not reflect real-world tendencies, resulting in unsuitable models; second, since the shifts are handcrafted, trained models are specifically designed for this particular setting, and do not generalize to other configurations. We propose the GQAOOD benchmark designed to overcome these concerns: we measure and compare accuracy over both rare and frequent question-answer pairs, and argue that the former is better suited to the evaluation of reasoning abilities, which we experimentally validate with models trained to more or less exploit biases. In a large-scale study involving 7 VQA models and 3 bias reduction techniques, we also experimentally demonstrate that these models fail to address questions involving infrequent concepts and provide recommendations for future directions of research."}, {"bib_hash_or_arxiv_id": "69ebb504c11b61938cd057076cff80ab5fae880c", "row": 2, "corpus_id": 62902119, "type": "ref", "title": "Cycle-Consistency for Robust Visual Question Answering", "abstract": "Despite significant progress in Visual Question Answer-ing over the years, robustness of today\u2019s VQA models leave much to be desired. We introduce a new evaluation protocol and associated dataset (VQA-Rephrasings) and show that state-of-the-art VQA models are notoriously brittle to linguistic variations in questions. VQA-Rephrasings contains 3 human-provided rephrasings for 40k questions-image pairs from the VQA v2.0 validation dataset. As a step towards improving robustness of VQA models, we propose a model-agnostic framework that exploits cycle consistency. Specifically, we train a model to not only answer a question, but also generate a question conditioned on the answer, such that the answer predicted for the generated question is the same as the ground truth answer to the original question. Without the use of additional supervision, we show that our approach is significantly more robust to linguistic variations than state-of-the-art VQA models, when evaluated on the VQA-Rephrasings dataset. In addition, our approach also outperforms state-of-the-art approaches on the standard VQA and Visual Question Generation tasks on the challenging VQA v2.0 dataset. Code and models will be made publicly available."}, {"bib_hash_or_arxiv_id": "38f1dc4c631f8b84b83f903ff1191d45b6df327b", "row": 3, "corpus_id": 233168594, "type": "ref", "title": "Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering", "abstract": "We introduce an evaluation methodology for visual question answering (VQA) to better diagnose cases of shortcut learning. These cases happen when a model exploits spurious statistical regularities to produce correct answers but does not actually deploy the desired behavior. There is a need to identify possible shortcuts in a dataset and assess their use before deploying a model in the real world. The research community in VQA has focused exclusively on question-based shortcuts, where a model might, for example, answer \"What is the color of the sky\" with \"blue\" by relying mostly on the question-conditional training prior and give little weight to visual evidence. We go a step further and consider multimodal shortcuts that involve both questions and images. We first identify potential shortcuts in the popular VQA v2 training set by mining trivial predictive rules such as co-occurrences of words and visual elements. We then introduce VQA-CounterExamples (VQACE), an evaluation protocol based on our subset of CounterExamples i.e. image-question-answer triplets where our rules lead to incorrect answers. We use this new evaluation in a large-scale study of existing approaches for VQA. We demonstrate that even state-of-the-art models perform poorly and that existing techniques to reduce biases are largely ineffective in this context. Our findings suggest that past work on question-based biases in VQA has only addressed one facet of a complex issue. The code for our method is available at https://github.com/cdancette/detect-shortcuts"}, {"bib_hash_or_arxiv_id": "273f4c4db89bb2137b2a6dda2f679217b7a96abb", "row": 4, "corpus_id": 252780087, "type": "ref", "title": "Language Prior Is Not the Only Shortcut: A Benchmark for Shortcut Learning in VQA", "abstract": "Visual Question Answering (VQA) models are prone to learn the shortcut solution formed by dataset biases rather than the intended solution. To evaluate the VQA models' reasoning ability beyond shortcut learning, the VQA-CP v2 dataset introduces a distribution shift between the training and test set given a question type. In this way, the model cannot use the training set shortcut (from question type to answer) to perform well on the test set. However, VQA-CP v2 only considers one type of shortcut and thus still cannot guarantee that the model relies on the intended solution rather than a solution specific to this shortcut. To overcome this limitation, we propose a new dataset that considers varying types of shortcuts by constructing different distribution shifts in multiple OOD test sets. In addition, we overcome the three troubling practices in the use of VQA-CP v2, e.g., selecting models using OOD test sets, and further standardize OOD evaluation procedure. Our benchmark provides a more rigorous and comprehensive testbed for shortcut learning in VQA. We benchmark recent methods and find that methods specifically designed for particular shortcuts fail to simultaneously generalize to our varying OOD test sets. We also systematically study the varying shortcuts and provide several valuable findings, which may promote the exploration of shortcut learning in VQA."}, {"bib_hash_or_arxiv_id": "a8ed3d3fdb1008c166d5d1cf18dc6a18c6e288cc", "row": 5, "corpus_id": 235266220, "type": "ref", "title": "Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models", "abstract": "Benefiting from large-scale pre-training, we have witnessed significant performance boost on the popular Visual Question Answering (VQA) task. Despite rapid progress, it remains unclear whether these state-of-the-art (SOTA) models are robust when encountering examples in the wild. To study this, we introduce Adversarial VQA, a new large-scale VQA benchmark, collected iteratively via an adversarial human-and-model-in-the-loop procedure. Through this new benchmark, we discover several interesting findings. (i) Surprisingly, we find that during dataset collection, non-expert annotators can easily attack SOTA VQA models successfully. (ii) Both large-scale pre-trained models and adversarial training methods achieve far worse performance on the new benchmark than over standard VQA v2 dataset, revealing the fragility of these models while demonstrating the effectiveness of our adversarial dataset. (iii) When used for data augmentation, our dataset can effectively boost model performance on other robust VQA benchmarks. We hope our Adversarial VQA dataset can shed new light on robustness study in the community and serve as a valuable benchmark for future work."}, {"bib_hash_or_arxiv_id": "49d628f90823e57d23ed397516d62a5670a1c530", "row": 6, "corpus_id": 235352921, "type": "ref", "title": "Human-Adversarial Visual Question Answering", "abstract": "Performance on the most commonly used Visual Question Answering dataset (VQA v2) is starting to approach human accuracy. However, in interacting with state-of-the-art VQA models, it is clear that the problem is far from being solved. In order to stress test VQA models, we benchmark them against human-adversarial examples. Human subjects interact with a state-of-the-art VQA model, and for each image in the dataset, attempt to find a question where the model's predicted answer is incorrect. We find that a wide range of state-of-the-art models perform poorly when evaluated on these examples. We conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions. We hope that this Adversarial VQA (AdVQA) benchmark can help drive progress in the field and advance the state of the art."}]}