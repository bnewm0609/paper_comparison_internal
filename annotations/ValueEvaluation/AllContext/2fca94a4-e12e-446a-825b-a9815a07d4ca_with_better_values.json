{"tabid": "2fca94a4-e12e-446a-825b-a9815a07d4ca", "table": {"Hidden size": {"210180949": "H(h) - 1", "219179333": "NPU: 6, RealNPU: likely 6, NMU: no hidden layers, NALU: no hidden layers"}, "Iterations for one run": {"210180949": "N/A", "219179333": "20,000 training steps"}, "Number of seeds": {"210180949": "N/A", "219179333": "N/A"}, "Learning rates": {"210180949": "N/A", "219179333": "N/A"}, "Division": {"210180949": "Differences in computation of variables 'a' and 'b', data slice handling, interpolation and extrapolation ranges.", "219179333": "Differences in computation of variables 'a' and 'b', data slice handling, interpolation and extrapolation ranges."}, "Regularisation penalty": {"210180949": "sparsity regularizer penalty", "219179333": "L1 regularization penalty"}, "Programming language": {"210180949": "N/A", "219179333": "N/A"}}, "row_bib_map": [{"bib_hash_or_arxiv_id": "058053bbeef88ba98821426866090078baae77a7", "row": 0, "corpus_id": 210180949, "type": "ref", "title": "Neural Arithmetic Units", "abstract": "Neural networks can approximate complex functions, but they struggle to perform exact arithmetic operations over real numbers. The lack of inductive bias for arithmetic operations leaves neural networks without the underlying logic necessary to extrapolate on tasks such as addition, subtraction, and multiplication. We present two new neural network components: the Neural Addition Unit (NAU), which can learn exact addition and subtraction; and the Neural Multiplication Unit (NMU) that can multiply subsets of a vector. The NMU is, to our knowledge, the first arithmetic neural network component that can learn to multiply elements from a vector, when the hidden size is large. The two new components draw inspiration from a theoretical analysis of recently proposed arithmetic components. We find that careful initialization, restricting parameter space, and regularizing for sparsity is important when optimizing the NAU and NMU. Our proposed units NAU and NMU, compared with previous neural units, converge more consistently, have fewer parameters, learn faster, can converge for larger hidden sizes, obtain sparse and meaningful weights, and can extrapolate to negative and small values."}, {"bib_hash_or_arxiv_id": "7b817b53d45abff8867b4ce1b085e3fb224df01a", "row": 1, "corpus_id": 219179333, "type": "ref", "title": "Neural Power Units", "abstract": "Conventional Neural Networks can approximate simple arithmetic operations, but fail to generalize beyond the range of numbers that were seen during training. Neural Arithmetic Units aim to overcome this difficulty, but current arithmetic units are either limited to operate on positive numbers or can only represent a subset of arithmetic operations. We introduce the Neural Power Unit (NPU) that operates on the full domain of real numbers and is capable of learning arbitrary power functions in a single layer. The NPU thus fixes the shortcomings of existing arithmetic units and extends their expressivity. We achieve this by using complex arithmetic without requiring a conversion of the network to complex numbers. A simplification of the unit to the RealNPU yields a highly interpretable model. We show that the NPUs outperform their competitors in terms of accuracy and sparsity on artificial arithmetic datasets, and that the RealNPU can discover the governing equations of a dynamical systems only from data."}]}