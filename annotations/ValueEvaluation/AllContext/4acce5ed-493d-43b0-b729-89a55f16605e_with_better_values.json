{"tabid": "4acce5ed-493d-43b0-b729-89a55f16605e", "table": {"Total epoch": {"47018956": "GCN: 200 epochs, GraphSAGE: 10 epochs, GAT: 30 epochs", "220363476": "GCN: 200 epochs, GAT: 200 epochs, APPNP: 200 epochs, JKNet: 200 epochs, JKNet(Drop): 200 epochs, Incep(Drop): 200 epochs, GCNII: 200 epochs, GCNII*: 200 epochs", "221879032": "N/A", "220647438": "100 epochs for each GNN model", "237497467": "Indicated in the 'Total Epoch' column", "221534325": "400 epochs for each GNN model"}, "Hidden dimension": {"47018956": "Dimensionality of the hidden features in each layer", "220363476": "Number of units in the hidden layers", "221879032": "Size of the feature vector representing each node", "220647438": "Number of dimensions in the feature space of intermediate representations", "237497467": "Size of the feature vector for each node, representing dimensions in each layer", "221534325": "Size of feature vectors representing nodes, determining dimensions in intermediate representations"}}, "row_bib_map": [{"bib_hash_or_arxiv_id": "f2ad407e3aecd7a809c47edc9a4bbf53e130e833", "row": 0, "corpus_id": 47018956, "type": "ref", "title": "Representation Learning on Graphs with Jumping Knowledge Networks", "abstract": "Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of \"neighboring\" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance."}, {"bib_hash_or_arxiv_id": "2363f969d2b5422b7034b70b701679071c323fb5", "row": 1, "corpus_id": 220363476, "type": "ref", "title": "Simple and Deep Graph Convolutional Networks", "abstract": "Graph convolutional networks (GCNs) are a powerful deep learning approach for graph-structured data. Recently, GCNs and subsequent variants have shown superior performance in various application areas on real-world datasets. Despite their success, most of the current GCN models are shallow, due to the {\\em over-smoothing} problem. In this paper, we study the problem of designing and analyzing deep graph convolutional networks. We propose the GCNII, an extension of the vanilla GCN model with two simple yet effective techniques: {\\em Initial residual} and {\\em Identity mapping}. We provide theoretical and empirical evidence that the two techniques effectively relieves the problem of over-smoothing. Our experiments show that the deep GCNII model outperforms the state-of-the-art methods on various semi- and full-supervised tasks. Code is available at this https URL ."}, {"bib_hash_or_arxiv_id": "63b9f25d2f66e4183fb9890585d4c459d29966a7", "row": 2, "corpus_id": 221879032, "type": "ref", "title": "Revisiting Graph Convolutional Network on Semi-Supervised Node Classification from an Optimization Perspective", "abstract": "Graph convolutional networks (GCNs) have achieved promising performance on various graph-based tasks. However they suffer from over-smoothing when stacking more layers. In this paper, we present a quantitative study on this observation and develop novel insights towards the deeper GCN. First, we interpret the current graph convolutional operations from an optimization perspective and argue that over-smoothing is mainly caused by the naive first-order approximation of the solution to the optimization problem. Subsequently, we introduce two metrics to measure the over-smoothing on node-level tasks. Specifically, we calculate the fraction of the pairwise distance between connected and disconnected nodes to the overall distance respectively. Based on our theoretical and empirical analysis, we establish a universal theoretical framework of GCN from an optimization perspective and derive a novel convolutional kernel named GCN+ which has lower parameter amount while relieving the over-smoothing inherently. Extensive experiments on real-world datasets demonstrate the superior performance of GCN+ over state-of-the-art baseline methods on the node classification tasks."}, {"bib_hash_or_arxiv_id": "0812184597718c9fa552a7a3bc59a93d89d2bf75", "row": 3, "corpus_id": 220647438, "type": "ref", "title": "Towards Deeper Graph Neural Networks", "abstract": "Graph neural networks have shown significant success in the field of graph representation learning. Graph convolutions perform neighborhood aggregation and represent one of the most important graph operations. Nevertheless, one layer of these neighborhood aggregation methods only consider immediate neighbors, and the performance decreases when going deeper to enable larger receptive fields. Several recent studies attribute this performance deterioration to the over-smoothing issue, which states that repeated propagation makes node representations of different classes indistinguishable. In this work, we study this observation systematically and develop new insights towards deeper graph neural networks. First, we provide a systematical analysis on this issue and argue that the key factor compromising the performance significantly is the entanglement of representation transformation and propagation in current graph convolution operations. After decoupling these two operations, deeper graph neural networks can be used to learn graph node representations from larger receptive fields. We further provide a theoretical analysis of the above observation when building very deep models, which can serve as a rigorous and gentle description of the over-smoothing issue. Based on our theoretical and empirical analysis, we propose Deep Adaptive Graph Neural Network (DAGNN) to adaptively incorporate information from large receptive fields. A set of experiments on citation, co-authorship, and co-purchase datasets have confirmed our analysis and insights and demonstrated the superiority of our proposed methods."}, {"bib_hash_or_arxiv_id": "b08826852e485ef40f589cbd9c92d16cd49c3fe6", "row": 4, "corpus_id": 237497467, "type": "ref", "title": "Understanding and Resolving Performance Degradation in Deep Graph Convolutional Networks", "abstract": "A Graph Convolutional Network (GCN) stacks several layers and in each layer performs a PROPagation operation~(PROP) and a TRANsformation operation~(TRAN) for learning node representations over graph-structured data. Though powerful, GCNs tend to suffer performance drop when the model gets deep. Previous works focus on PROPs to study and mitigate this issue, but the role of TRANs is barely investigated. In this work, we study performance degradation of GCNs by experimentally examining how stacking only TRANs or PROPs works. We find that TRANs contribute significantly, or even more than PROPs, to declining performance, and moreover that they tend to amplify node-wise feature variance in GCNs, causing variance inflammation that we identify as a key factor for causing performance drop. Motivated by such observations, we propose a variance-controlling technique termed Node Normalization (NodeNorm), which scales each node's features using its own standard deviation. Experimental results validate the effectiveness of NodeNorm on addressing performance degradation of GCNs. Specifically, it enables deep GCNs to outperform shallow ones in cases where deep models are needed, and to achieve comparable results with shallow ones on 6 benchmark datasets. NodeNorm is a generic plug-in and can well generalize to other GNN architectures. Code is publicly available at https://github.com/miafei/NodeNorm."}, {"bib_hash_or_arxiv_id": "22bf8dd00e10591127c2ce7aa815d693ffabe6d9", "row": 5, "corpus_id": 221534325, "type": "ref", "title": "Masked Label Prediction: Unified Massage Passing Model for Semi-Supervised Classification", "abstract": "Graph neural network (GNN) and label propagation algorithm (LPA) are both message passing algorithms, which have achieved superior performance in semi-supervised classification. GNN performs feature propagation by a neural network to make predictions, while LPA uses label propagation across graph adjacency matrix to get results. However, there is still no effective way to directly combine these two kinds of algorithms. To address this issue, we propose a novel Unified Message Passaging Model (UniMP) that can incorporate feature and label propagation at both training and inference time. First, UniMP adopts a Graph Transformer network, taking feature embedding and label embedding as input information for propagation. Second, to train the network without overfitting in self-loop input label information, UniMP introduces a masked label prediction strategy, in which some percentage of input label information are masked at random, and then predicted. UniMP conceptually unifies feature propagation and label propagation and is empirically powerful. It obtains new state-of-the-art semi-supervised classification results in Open Graph Benchmark (OGB)."}]}