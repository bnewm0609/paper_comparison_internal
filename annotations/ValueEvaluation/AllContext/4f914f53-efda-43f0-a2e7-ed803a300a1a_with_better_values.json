{"tabid": "4f914f53-efda-43f0-a2e7-ed803a300a1a", "table": {"ML Algorithm": {"141433678": "ANNs, CNNs, LSTM networks", "3568073": "N/A"}, "Signal Type": {"141433678": "Detects and analyzes 20Gbaud NRZ-OOK, PAM4, PAM8 signals", "3568073": "Detects and analyzes signals related to image quality and variation"}}, "row_bib_map": [{"bib_hash_or_arxiv_id": "51af69d22002448628a46100788db1bb44d6ab4f", "row": 0, "corpus_id": 141433678, "type": "ref", "title": "Intelligent optical performance monitor using multi-task learning based artificial neural network.", "abstract": "An intelligent optical performance monitor using multi-task learning based artificial neural network (MTL-ANN) is designed for simultaneous OSNR monitoring and modulation format identification (MFI). Signals' amplitude histograms (AHs) after constant module algorithm are selected as the input features for MTL-ANN. The results obtained from simulation and experiment of NRZ-OOK, PAM4 and PAM8 signals demonstrate that MTL-ANN could achieve OSNR monitoring and MFI simultaneously with higher accuracy and stability compared with single-task learning based ANNs (STL-ANNs). The results show an MFI accuracy of 100% for the three modulation formats under consideration. Furthermore, OSNR monitoring with mean-square error (MSE) of 0.12 dB and accuracy of 100% is achieved while regarding it as regression problem and classification problem, respectively. In this intelligent optical performance monitor, only a single MTL-ANN is deployed, which enables reduced-complexity optical performance monitor (OPM) devices for multi-parameters estimation in future heterogeneous optical network."}, {"bib_hash_or_arxiv_id": "2107.07338v1", "row": 1, "corpus_id": 3568073, "type": "ours", "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation", "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset."}]}