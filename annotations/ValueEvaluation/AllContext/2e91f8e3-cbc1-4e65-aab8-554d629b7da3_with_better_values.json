{"tabid": "2e91f8e3-cbc1-4e65-aab8-554d629b7da3", "table": {"Dataset": {"250526479": "Cityscapes, ACDC, Dark Zurich, RobotCar Correspondence, CMU Correspondence", "235829267": "Cityscapes, Mapillary Vistas, ADE20K, COCOStuff-10K, ADE20K-Full", "229924195": "ADE20K, Pascal Context, Cityscapes", "247292685": "PASCAL VOC 2012, DRAM", "247996577": "ISPRS Potsdam, iSAID", "234358849": "ISPRS Vaihingen, Potsdam", "226965678": "Kvasir-Instrument, Robust Medical Instrument Segmentation", "252762315": "PAXRay, JSRT", "246276404": "ZeroWaste, TACO, MS-COCO", "257532325": "MFNet, PST900", "254246996": "ADE20k, Pascal Context (P.Cont), Pascal VOC (P.VOC), Cityscapes (Citys.), Berkeley Deep Drive (BDD), CamVid, India Driving Dataset (IDD), KITTI, SUN-RGBD (SUN), ISPRS, SUIM"}, "Year": {"250526479": "N/A", "235829267": "N/A", "229924195": "N/A", "247292685": "N/A", "247996577": "2019, 2020, 2021", "234358849": "N/A", "226965678": "N/A", "252762315": "N/A", "246276404": "N/A", "257532325": "N/A", "254246996": "N/A"}}, "row_bib_map": [{"bib_hash_or_arxiv_id": "ef6ce283028d8a4d4a297951c92d46a8329bd97e", "row": 0, "corpus_id": 250526479, "type": "ref", "title": "Refign: Align and Refine for Adaptation of Semantic Segmentation to Adverse Conditions", "abstract": "Due to the scarcity of dense pixel-level semantic annotations for images recorded in adverse visual conditions, there has been a keen interest in unsupervised domain adaptation (UDA) for the semantic segmentation of such images. UDA adapts models trained on normal conditions to the target adverse-condition domains. Meanwhile, multiple datasets with driving scenes provide corresponding images of the same scenes across multiple conditions, which can serve as a form of weak supervision for domain adaptation. We propose Refign, a generic extension to self-training-based UDA methods which leverages these cross-domain correspondences. Refign consists of two steps: (1) aligning the normal-condition image to the corresponding adverse-condition image using an uncertainty-aware dense matching network, and (2) refining the adverse prediction with the normal prediction using an adaptive label correction mechanism. We design custom modules to streamline both steps and set the new state of the art for domain-adaptive semantic segmentation on several adverse-condition benchmarks, including ACDC and Dark Zurich. The approach introduces no extra training parameters, minimal computational overhead\u2014during training only\u2014and can be used as a drop-in extension to improve any given self-training-based UDA method. Code is available at https://github.com/brdav/refign."}, {"bib_hash_or_arxiv_id": "ee2de69aa5e495edcdc2499d3f4b25e3e076b392", "row": 1, "corpus_id": 235829267, "type": "ref", "title": "Per-Pixel Classification is Not All You Need for Semantic Segmentation", "abstract": "Modern approaches typically formulate semantic segmentation as a per-pixel classification task, while instance-level segmentation is handled with an alternative mask classification. Our key insight: mask classification is sufficiently general to solve both semantic- and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure. Following this observation, we propose MaskFormer, a simple mask classification model which predicts a set of binary masks, each associated with a single global class label prediction. Overall, the proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results. In particular, we observe that MaskFormer outperforms per-pixel classification baselines when the number of classes is large. Our mask classification-based method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K) and panoptic segmentation (52.7 PQ on COCO) models."}, {"bib_hash_or_arxiv_id": "ca54e20c608d8c0d16117dffdeaa43d946f47cd5", "row": 2, "corpus_id": 229924195, "type": "ref", "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers", "abstract": "Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission."}, {"bib_hash_or_arxiv_id": "f228110baefd2b980767980dcc7b0737c66c5c42", "row": 3, "corpus_id": 247292685, "type": "ref", "title": "Semantic Segmentation in Art Paintings", "abstract": "Semantic segmentation is a difficult task even when trained in a supervised manner on photographs. In this paper, we tackle the problem of semantic segmentation of artistic paintings, an even more challenging task because of a much larger diversity in colors, textures, and shapes and because there are no ground truth annotations available for segmentation. We propose an unsupervised method for semantic segmentation of paintings using domain adaptation. Our approach creates a training set of pseudo\u2010paintings in specific artistic styles by using style\u2010transfer on the PASCAL VOC 2012 dataset, and then applies domain confusion between PASCAL VOC 2012 and real paintings. These two steps build on a new dataset we gathered called DRAM (Diverse Realism in Art Movements) composed of figurative art paintings from four movements, which are highly diverse in pattern, color, and geometry. To segment new paintings, we present a composite multi\u2010domain adaptation method that trains on each sub\u2010domain separately and composes their solutions during inference time. Our method provides better segmentation results not only on the specific artistic movements of DRAM, but also on other, unseen ones. We compare our approach to alternative methods and show applications of semantic segmentation in art paintings. The code and models for our approach are publicly available at: https://github.com/Nadavc220/SemanticSegmentationInArtPaintings."}, {"bib_hash_or_arxiv_id": "71eeb0123f1badc42d82bf1499f8b04048ef2f26", "row": 4, "corpus_id": 247996577, "type": "ref", "title": "An Empirical Study of Remote Sensing Pretraining", "abstract": "Deep learning has largely reshaped remote sensing (RS) research for aerial image understanding and made a great success. Nevertheless, most of the existing deep models are initialized with the ImageNet pretrained weights. Since natural images inevitably present a large domain gap relative to aerial images, probably limiting the finetuning performance on downstream aerial scene tasks. This issue motivates us to conduct an empirical study of remote sensing pretraining (RSP) on aerial images. To this end, we train different networks from scratch with the help of the largest RS scene recognition dataset up to now \u2014 MillionAID, to obtain a series of RS pretrained backbones, including both convolutional neural networks (CNN) and vision transformers such as Swin and ViTAE, which have shown promising performance on computer vision tasks. Then, we investigate the impact of RSP on representative downstream tasks including scene recognition, semantic segmentation, object detection, and change detection using these CNN and vision transformer backbones. Empirical study shows that RSP can help deliver distinctive performances in scene recognition tasks and in perceiving RS related semantics such as \u201cBridge\u201d and \u201cAirplane\u201d. We also find that, although RSP mitigates the data discrepancies of traditional ImageNet pretraining on RS images, it may still suffer from task discrepancies, where downstream tasks require different representations from scene recognition tasks. These findings call for further research efforts on both large-scale pretraining datasets and effective pretraining methods. The codes and pretrained models will be released at https://github.com/ViTAE-Transformer/ViTAE-Transformer-Remote-Sensing."}, {"bib_hash_or_arxiv_id": "7a803f86bde02a0151ad1887eafe8a962877e998", "row": 5, "corpus_id": 234358849, "type": "ref", "title": "A Novel Transformer Based Semantic Segmentation Scheme for Fine-Resolution Remote Sensing Images", "abstract": "The fully convolutional network (FCN) with an encoder-decoder architecture has been the standard paradigm for semantic segmentation. The encoder-decoder architecture utilizes an encoder to capture multilevel feature maps, which are incorporated into the final prediction by a decoder. As the context is crucial for precise segmentation, tremendous effort has been made to extract such information in an intelligent fashion, including employing dilated/atrous convolutions or inserting attention modules. However, these endeavors are all based on the FCN architecture with ResNet or other backbones, which cannot fully exploit the context from the theoretical concept. By contrast, we introduce the Swin Transformer as the backbone to extract the context information and design a novel decoder of densely connected feature aggregation module (DCFAM) to restore the resolution and produce the segmentation map. The experimental results on two remotely sensed semantic segmentation datasets demonstrate the effectiveness of the proposed scheme."}, {"bib_hash_or_arxiv_id": "2b4556b56899a6cdbeb3ef9a833e7c451ace7515", "row": 6, "corpus_id": 226965678, "type": "ref", "title": "Kvasir-Instrument: Diagnostic and therapeutic tool segmentation dataset in gastrointestinal endoscopy", "abstract": "Gastrointestinal (GI) pathologies are periodically screened, biopsied, and resected using surgical tools. Usually, the procedures and the treated or resected areas are not specifically tracked or analysed during or after colonoscopies. Information regarding disease borders, development and amount and size of the resected area get lost.This can lead to poor follow-up and bothersome reassessment difficulties post-treatment. To improve the current standard and also to foster more research on the topic we have released the \"Kvasir-Instrument\" dataset which consists of 590 annotated frames containing GI procedure tools such as snares, balloons and biopsy forceps, etc. Beside of the images, the dataset includes ground truth masks and bounding boxes and has been verified by two expert GI endoscopists. Additionally, we provide a baseline for the segmentation of the GI tools to promote research and algorithm development. We obtained a dice coefficient score of 0.9158 and a Jaccard index of 0.8578 using a classical U-Net architecture. A similar dice coefficient score was observed for DoubleUNet. The qualitative results showed that the model did not work for the images with specularity and the frames with multiple instruments, while the best result for both methods was observed on all other types of images. Both, qualitative and quantitative results show that the model performs reasonably good, but there is a large potential for further improvements. Benchmarking using the dataset provides an opportunity for researchers to contribute to the field of automatic endoscopic diagnostic and therapeutic tool segmentation for GI endoscopy."}, {"bib_hash_or_arxiv_id": "6daea1ba7fd1ee1f8956230c41937d7aa9fce9fe", "row": 7, "corpus_id": 252762315, "type": "ref", "title": "Detailed Annotations of Chest X-Rays via CT Projection for Report Understanding", "abstract": "In clinical radiology reports, doctors capture important information about the patient's health status. They convey their observations from raw medical imaging data about the inner structures of a patient. As such, formulating reports requires medical experts to possess wide-ranging knowledge about anatomical regions with their normal, healthy appearance as well as the ability to recognize abnormalities. This explicit grasp on both the patient's anatomy and their appearance is missing in current medical image-processing systems as annotations are especially difficult to gather. This renders the models to be narrow experts e.g. for identifying specific diseases. In this work, we recover this missing link by adding human anatomy into the mix and enable the association of content in medical reports to their occurrence in associated imagery (medical phrase grounding). To exploit anatomical structures in this scenario, we present a sophisticated automatic pipeline to gather and integrate human bodily structures from computed tomography datasets, which we incorporate in our PAXRay: A Projected dataset for the segmentation of Anatomical structures in X-Ray data. Our evaluation shows that methods that take advantage of anatomical information benefit heavily in visually grounding radiologists' findings, as our anatomical segmentations allow for up to absolute 50% better grounding results on the OpenI dataset as compared to commonly used region proposals. The PAXRay dataset is available at https://constantinseibold.github.io/paxray/."}, {"bib_hash_or_arxiv_id": "8ba6ef8374033bcf1f5fe44eb86a097a9e684fc6", "row": 8, "corpus_id": 246276404, "type": "ref", "title": "ZeroWaste Dataset: Towards Deformable Object Segmentation in Cluttered Scenes", "abstract": "Less than 35% of recyclable waste is being actually recycled in the US [2], which leads to increased soil and sea pollution and is one of the major concerns of environmental researchers as well as the common public. At the heart of the problem are the inefficiencies of the waste sorting process (separating paper, plastic, metal, glass, etc.) due to the extremely complex and cluttered nature of the waste stream. Recyclable waste detection poses a unique computer vision challenge as it requires detection of highly deformable and often translucent objects in cluttered scenes without the kind of context information usually present in human-centric datasets. This challenging computer vision task currently lacks suitable datasets or methods in the available literature. In this paper, we take a step towards computer-aided waste detection and present the first in-the-wild industrial-grade waste detection and segmentation dataset, ZeroWaste. We believe that ZeroWaste will catalyze research in object detection and semantic segmentation in extreme clutter as well as applications in the recycling domain. Our project page can be found at http://ai.bu.edu/zerowaste/"}, {"bib_hash_or_arxiv_id": "f3a5df43e1baf36c5e5c2987da96eba7c61aa21f", "row": 9, "corpus_id": 257532325, "type": "ref", "title": "SpiderMesh: Spatial-aware Demand-guided Recursive Meshing for RGB-T Semantic Segmentation", "abstract": "For semantic segmentation in urban scene understanding, RGB cameras alone often fail to capture a clear holistic topology in challenging lighting conditions. Thermal signal is an informative additional channel that can bring to light the contour and fine-grained texture of blurred regions in low-quality RGB image. Aiming at practical RGB-T (thermal) segmentation, we systematically propose a Spatial-aware Demand-guided Recursive Meshing (SpiderMesh) framework that: 1) proactively compensates inadequate contextual semantics in optically-impaired regions via a demand-guided target masking algorithm; 2) refines multimodal semantic features with recursive meshing to improve pixel-level semantic analysis performance. We further introduce an asymmetric data augmentation technique M-CutOut, and enable semi-supervised learning to fully utilize RGB-T labels only sparsely available in practical use. Extensive experiments on MFNet and PST900 datasets demonstrate that SpiderMesh achieves state-of-the-art performance on standard RGB-T segmentation benchmarks."}, {"bib_hash_or_arxiv_id": "73a41a17fd0de509b3f84d0699b5f71601611673", "row": 10, "corpus_id": 254246996, "type": "ref", "title": "Location-Aware Self-Supervised Transformers", "abstract": "Pixel-level labels are particularly expensive to acquire. Hence, pretraining is a critical step to improve models on a task like semantic segmentation. However, prominent algorithms for pretraining neural networks use image-level objectives, e.g. image classification, image-text alignment a la CLIP, or self-supervised contrastive learning. These objectives do not model spatial information, which might be sub-optimal when finetuning on downstream tasks with spatial reasoning. In this work, we pretrain network with a location-aware (LOCA) self-supervised method which fosters the emergence of strong dense features. Specifically, we use both a patch-level clustering scheme to mine dense pseudo-labels and a relative location prediction task to encourage learning about object parts and their spatial arrangements. Our experiments show that LOCA pretraining leads to representations that transfer competitively to challenging and diverse semantic segmentation datasets."}]}