{"tabid": "5a000529-a230-4126-8210-1b3c99df3e0c", "table": {"# Images": {"173991173": "14,031 unique images", "214612106": "4,998 unique pathology images", "231951663": "642 unique images"}, "# QA pairs": {"173991173": "DAQUAR: 12,468, Visual Madlibs: 360,001, Visual 7W: 327,939, VQA (v2): 1.1M, MovieQA: 14,944, CLEVR: 999,968, KB-VQA: 2,402, FVQA: 5,826, OK-VQA: 14,055", "214612106": "DAQUAR: 12,468, VQA: 614,000, VQA v2: 1,000,000", "231951663": "14,028"}, "Source of imagesand content": {"173991173": "Datasets vary, including COCO, visual and textual story comprehension, with data types like scene, objects, person, and various forms of visual reasoning.", "214612106": "VQA-Med dataset includes 4,200 radiology images and 15,292 question-answer pairs; VQA-RAD dataset features clinician-generated questions and answers on radiology images.", "231951663": "Visual data includes CT scans, MRIs, X-Rays; textual data comprises semantic labels for organs and diseases, and doctor-generated questions."}, "QACreation": {"173991173": "Question-answer pairs generated through two rounds of labeling on Amazon Mechanical Turk, requiring external knowledge.", "214612106": "Question-answer pairs generated from captions in pathology textbooks using NLP techniques, manually checked for correctness.", "231951663": "Question-answer pairs created by experienced doctors using pre-defined templates and clinical experience, sourced from labeled radiology images."}, "Question Category": {"173991173": "Covers a wide range of knowledge categories including vehicles, brands, geography, history, and more, requiring reasoning based on external knowledge.", "214612106": "Focuses on pathology, requiring deep understanding of visual content in pathology images and knowledge of pathological findings.", "231951663": "Includes vision-only and knowledge-based questions, requiring both visual reasoning from radiology images and external medical knowledge."}}, "row_bib_map": [{"bib_hash_or_arxiv_id": "6dfd5792688a8560a7f26036a8e45c15f426bdd8", "row": 0, "corpus_id": 173991173, "type": "ref", "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge", "abstract": "Visual Question Answering (VQA) in its ideal form lets us study reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most VQA benchmarks to date are focused on questions such as simple counting, visual attributes, and object detection that do not require reasoning or knowledge beyond what is in the image. In this paper, we address the task of knowledge-based visual question answering and provide a benchmark, called OK-VQA, where the image content is not sufficient to answer the questions, encouraging methods that rely on external knowledge resources. Our new dataset includes more than 14,000 questions that require external knowledge to answer. We show that the performance of the state-of-the-art VQA models degrades drastically in this new setting. Our analysis shows that our knowledge-based VQA task is diverse, difficult, and large compared to previous knowledge-based VQA datasets. We hope that this dataset enables researchers to open up new avenues for research in this domain."}, {"bib_hash_or_arxiv_id": "cad8f1bb7e8d5dc7d502d1cc02c5a919f3c274c0", "row": 1, "corpus_id": 214612106, "type": "ref", "title": "PathVQA: 30000+ Questions for Medical Visual Question Answering", "abstract": "Is it possible to develop an \"AI Pathologist\" to pass the board-certified examination of the American Board of Pathology? To achieve this goal, the first step is to create a visual question answering (VQA) dataset where the AI agent is presented with a pathology image together with a question and is asked to give the correct answer. Our work makes the first attempt to build such a dataset. Different from creating general-domain VQA datasets where the images are widely accessible and there are many crowdsourcing workers available and capable of generating question-answer pairs, developing a medical VQA dataset is much more challenging. First, due to privacy concerns, pathology images are usually not publicly available. Second, only well-trained pathologists can understand pathology images, but they barely have time to help create datasets for AI research. To address these challenges, we resort to pathology textbooks and online digital libraries. We develop a semi-automated pipeline to extract pathology images and captions from textbooks and generate question-answer pairs from captions using natural language processing. We collect 32,799 open-ended questions from 4,998 pathology images where each question is manually checked to ensure correctness. To our best knowledge, this is the first dataset for pathology VQA. Our dataset will be released publicly to promote research in medical VQA."}, {"bib_hash_or_arxiv_id": "29f7abceef54decdb49c568cecc5a74e4b9f8782", "row": 2, "corpus_id": 231951663, "type": "ref", "title": "Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering", "abstract": "Medical visual question answering (Med-VQA) has tremendous potential in healthcare. However, the development of this technology is hindered by the lacking of publicly-available and high-quality labeled datasets for training and evaluation. In this paper, we present a large bilingual dataset, SLAKE, with comprehensive semantic labels annotated by experienced physicians and a new structural medical knowledge base for Med-VQA. Besides, SLAKE includes richer modalities and covers more human body parts than the currently available dataset. We show that SLAKE can be used to facilitate the development and evaluation of Med-VQA systems. The dataset can be downloaded from http://www.med-vqa.com/slake."}]}