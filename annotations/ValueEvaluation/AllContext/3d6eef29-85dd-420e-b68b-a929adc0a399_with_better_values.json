{"tabid": "3d6eef29-85dd-420e-b68b-a929adc0a399", "table": {"Name": {"6875312": "Dueling Double DQN, Gorila", "16326763": "DQN, NFQCA, DPG", "16046818": "Deep Q-learning, UCC-I, CEM, CMA, Natural Gradient, Empirical FIM", "28695052": "TRPO, CEM, A2C, ACER", "28202810": "DDPG, PPO, SQL, TD3"}, "Approach": {"6875312": "Asynchronous actor-learners, value-based and policy-based methods, gradient ascent, baseline variance reduction, parallel exploration", "16326763": "Actor-critic methods, deep neural network function approximators", "16046818": "Policy-based RL, TRPO, natural policy gradient, CEM, CMA, empirical FIM, Max KL", "28695052": "Policy-based RL, deep Q-learning, vanilla policy gradient, TRPO, A2C, CEM", "28202810": "Actor-critic architectures, on-policy and off-policy learning, maximum entropy RL, Q-learning, policy gradient, value function approximation, stochastic policy optimization"}}, "row_bib_map": [{"bib_hash_or_arxiv_id": "8a4c4505ccc903ca8ae9403ab1d844ff6b3a9e7f", "row": 0, "corpus_id": 6875312, "type": "ref", "title": "Asynchronous Methods for Deep Reinforcement Learning", "abstract": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input."}, {"bib_hash_or_arxiv_id": "17b00f52916b19ddf3becdd073a8255f99bdf302", "row": 1, "corpus_id": 16326763, "type": "ref", "title": "Continuous control with deep reinforcement learning", "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs."}, {"bib_hash_or_arxiv_id": "d50aa4f58b88e7f126a238b0ed184e0fa92836bb", "row": 2, "corpus_id": 16046818, "type": "ref", "title": "Trust Region Policy Optimization", "abstract": "We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters."}, {"bib_hash_or_arxiv_id": "ed3ec26eedd8f832f600985b0d0ceff581cf7b71", "row": 3, "corpus_id": 28695052, "type": "ref", "title": "Proximal Policy Optimization Algorithms", "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time."}, {"bib_hash_or_arxiv_id": "82635edbff7d314b6effe970a43cf9d1d085e116", "row": 4, "corpus_id": 28202810, "type": "ref", "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor", "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds."}]}