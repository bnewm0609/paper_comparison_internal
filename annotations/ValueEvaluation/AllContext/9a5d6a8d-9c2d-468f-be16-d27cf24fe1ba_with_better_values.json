{"tabid": "9a5d6a8d-9c2d-468f-be16-d27cf24fe1ba", "table": {"Key Contributions": {"235614375": "Introduced Mean Augmented Federated Learning (MAFL) and FedMix algorithm, enhancing privacy and performance in non-iid settings.", "195776413": "Introduced Astraea framework, data augmentation, mediators for training rescheduling, and demonstrated improved accuracy on imbalanced datasets.", "203951869": "Introduced transfer learning and knowledge distillation in federated learning, significantly improving local model performance.", "211082601": "Investigated incentives for federated learning participation, introduced local adaptation techniques, and analyzed privacy and robustness effects on accuracy.", "224725746": "Introduced Federated Contrastive Averaging (FedCA) with dictionary and alignment modules, enhancing representation consistency and alignment in FURL.", "259991420": "Outlined data-level methods in federated learning: Astraea's mediators, FAug's GAN for data generation, FedMix's loss function approximation, and FedProc's global prototype contrastive loss."}, "Limitations": {"235614375": "Privacy violation, communication cost, loss of discriminative characteristics", "195776413": "N/A", "203951869": "N/A", "211082601": "N/A", "224725746": "N/A", "259991420": "Private Data Processing: Significant manual effort, may not address statistical heterogeneity, additional computation costs, may not guarantee data security; External Data Utilization: Increases communication overhead, may not address model heterogeneity, requires additional computational resources, may not address statistical heterogeneity"}}, "row_bib_map": [{"bib_hash_or_arxiv_id": "d60770569afba297a756e2405628e4ca615657bc", "row": 0, "corpus_id": 235614375, "type": "ref", "title": "FedMix: Approximation of Mixup under Mean Augmented Federated Learning", "abstract": "Federated learning (FL) allows edge devices to collectively learn a model without directly sharing data within each device, thus preserving privacy and eliminating the need to store data globally. While there are promising results under the assumption of independent and identically distributed (iid) local data, current state-of-the-art algorithms suffer from performance degradation as the heterogeneity of local data across clients increases. To resolve this issue, we propose a simple framework, Mean Augmented Federated Learning (MAFL), where clients send and receive averaged local data, subject to the privacy requirements of target applications. Under our framework, we propose a new augmentation algorithm, named FedMix, which is inspired by a phenomenal yet simple data augmentation method, Mixup, but does not require local raw data to be directly shared among devices. Our method shows greatly improved performance in the standard benchmark datasets of FL, under highly non-iid federated settings, compared to conventional algorithms."}, {"bib_hash_or_arxiv_id": "0f9b6c910adf25ebc6fc00e5ee7e775047ba0333", "row": 1, "corpus_id": 195776413, "type": "ref", "title": "Astraea: Self-Balancing Federated Learning for Improving Classification Accuracy of Mobile Deep Learning Applications", "abstract": "Federated learning (FL) is a distributed deep learning method which enables multiple participants, such as mobile phones and IoT devices, to contribute a neural network model while their private training data remains in local devices. This distributed approach is promising in the edge computing system where have a large corpus of decentralized data and require high privacy. However, unlike the common training dataset, the data distribution of the edge computing system is imbalanced which will introduce biases in the model training and cause a decrease in accuracy of federated learning applications. In this paper, we demonstrate that the imbalanced distributed training data will cause accuracy degradation in FL. To counter this problem, we build a self-balancing federated learning framework call Astraea, which alleviates the imbalances by 1) Global data distribution based data augmentation, and 2) Mediator based multi-client rescheduling. The proposed framework relieves global imbalance by runtime data augmentation, and for averaging the local imbalance, it creates the mediator to reschedule the training of clients based on Kullback-Leibler divergence (KLD) of their data distribution. Compared with FedAvg, the state-of-the-art FL algorithm, Astraea shows +5.59% and +5.89% improvement of top-1 accuracy on the imbalanced EMNIST and imbalanced CINIC-10 datasets, respectively. Meanwhile, the communication traffic of Astraea can be 92% lower than that of FedAvg."}, {"bib_hash_or_arxiv_id": "96bec4f0c0ff5a649ef2b1aac012bb76db2d79c1", "row": 2, "corpus_id": 203951869, "type": "ref", "title": "FedMD: Heterogenous Federated Learning via Model Distillation", "abstract": "Federated learning enables the creation of a powerful centralized model without compromising data privacy of multiple participants. While successful, it does not incorporate the case where each participant independently designs its own model. Due to intellectual property concerns and heterogeneous nature of tasks and data, this is a widespread requirement in applications of federated learning to areas such as health care and AI as a service. In this work, we use transfer learning and knowledge distillation to develop a universal framework that enables federated learning when each agent owns not only their private data, but also uniquely designed models. We test our framework on the MNIST/FEMNIST dataset and the CIFAR10/CIFAR100 dataset and observe fast improvement across all participating models. With 10 distinct participants, the final test accuracy of each model on average receives a 20% gain on top of what's possible without collaboration and is only a few percent lower than the performance each model would have obtained if all private datasets were pooled and made directly available for all participants."}, {"bib_hash_or_arxiv_id": "fe60cf124d3bd1ea62bdc1f624ca5621c8c08920", "row": 3, "corpus_id": 211082601, "type": "ref", "title": "Salvaging Federated Learning by Local Adaptation", "abstract": "Federated learning (FL) is a heavily promoted approach for training ML models on sensitive data, e.g., text typed by users on their smartphones. FL is expressly designed for training on data that are unbalanced and non-iid across the participants. To ensure privacy and integrity of the federated model, latest FL approaches use differential privacy or robust aggregation to limit the influence of \"outlier\" participants. \nFirst, we show that on standard tasks such as next-word prediction, many participants gain no benefit from FL because the federated model is less accurate on their data than the models they can train locally on their own. Second, we show that differential privacy and robust aggregation make this problem worse by further destroying the accuracy of the federated model for many participants. \nThen, we evaluate three techniques for local adaptation of federated models: fine-tuning, multi-task learning, and knowledge distillation. We analyze where each technique is applicable and demonstrate that all participants benefit from local adaptation. Participants whose local models are poor obtain big accuracy improvements over conventional FL. Participants whose local models are better than the federated model and who have no incentive to participate in FL today improve less, but sufficiently to make the adapted federated model better than their local models."}, {"bib_hash_or_arxiv_id": "6cacc96eacf79670c8068a0d69edc8073c56869b", "row": 4, "corpus_id": 224725746, "type": "ref", "title": "Federated unsupervised representation learning", "abstract": "To leverage the enormous amount of unlabeled data on distributed edge devices, we formulate a new problem in federated learning called federated unsupervised representation learning (FURL) to learn a common representation model without supervision while preserving data privacy. FURL poses two new challenges: (1) data distribution shift (non-independent and identically distributed, non-IID) among clients would make local models focus on different categories, leading to the inconsistency of representation spaces; (2) without unified information among the clients in FURL, the representations across clients would be misaligned. To address these challenges, we propose the federated contrastive averaging with dictionary and alignment (FedCA) algorithm. FedCA is composed of two key modules: a dictionary module to aggregate the representations of samples from each client which can be shared with all clients for consistency of representation space and an alignment module to align the representation of each client on a base model trained on public data. We adopt the contrastive approach for local model training. Through extensive experiments with three evaluation protocols in IID and non-IID settings, we demonstrate that FedCA outperforms all baselines with significant margins."}, {"bib_hash_or_arxiv_id": "2307.10616v2", "row": 5, "corpus_id": 259991420, "type": "ours", "title": "Heterogeneous Federated Learning: State-of-the-art and Research Challenges", "abstract": "Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications. Existing FL works mainly focus on model homogeneous settings. However, practical FL typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients. Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex. Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential. In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges. In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons. We classify existing methods from three different levels according to the HFL procedure: data-level, model-level, and server-level. Finally, several critical and promising future research directions in HFL are discussed, which may facilitate further developments in this field. A periodically updated collection on HFL is available at https://github.com/marswhu/HFL_Survey."}]}