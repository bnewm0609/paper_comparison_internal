{"tabid": "6eb5a621-978e-4e5b-b6db-ff7da21dac77", "table": {"Task": {"4719239": "Keyword spotting, speech recognition, background noise detection", "10475843": "Speaker identification and verification", "3697399": "Note reconstruction, instrument interpolation, pitch interpolation", "233407598": "Inverse synthesis, inferring macro-parameters of synthesizers, audio-to-MIDI conversion, crafting perceptually motivated auditory representations"}, "# of classes": {"4719239": "Number of different categories or labels in the dataset", "10475843": "Number of different speakers or classes in the dataset", "3697399": "Number of different classes or categories in the dataset", "233407598": "Number of distinct categories or labels in the dataset"}, "# of examples": {"4719239": "105,829 utterances", "10475843": "100,000+ utterances", "3697399": "306,043 musical notes", "233407598": "31,000 and 2,100 instances"}}, "row_bib_map": [{"bib_hash_or_arxiv_id": "cbd1cf3e707d6d25c19595d570351afdb3a53845", "row": 0, "corpus_id": 4719239, "type": "ref", "title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition", "abstract": "Describes an audio dataset of spoken words designed to help train and evaluate keyword spotting systems. Discusses why this task is an interesting challenge, and why it requires a specialized dataset that is different from conventional datasets used for automatic speech recognition of full sentences. Suggests a methodology for reproducible and comparable accuracy metrics for this task. Describes how the data was collected and verified, what it contains, previous versions and properties. Concludes by reporting baseline results of models trained on this dataset."}, {"bib_hash_or_arxiv_id": "fc1f36da3d532e0c5bb0b65b63df741f798a59ce", "row": 1, "corpus_id": 10475843, "type": "ref", "title": "VoxCeleb: A Large-Scale Speaker Identification Dataset", "abstract": "Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and are usually hand-annotated, hence limited in size. The goal of this paper is to generate a large scale text-independent speaker identification dataset collected 'in the wild'. We make two contributions. First, we propose a fully automated pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains hundreds of thousands of 'real world' utterances for over 1,000 celebrities. Our second contribution is to apply and compare various state of the art speaker identification techniques on our dataset to establish baseline performance. We show that a CNN based architecture obtains the best performance for both identification and verification."}, {"bib_hash_or_arxiv_id": "4d6b0d5794b642a4201db5f2180cc046ab42166e", "row": 2, "corpus_id": 3697399, "type": "ref", "title": "Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders", "abstract": "Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive."}, {"bib_hash_or_arxiv_id": "22c3b54f69c771df529f4f5778bde8e4da495055", "row": 3, "corpus_id": 233407598, "type": "ref", "title": "One Billion Audio Sounds from GPU-Enabled Modular Synthesis", "abstract": "We release synth1B1, a multi-modal audio corpus consisting of 1 billion 4-second synthesized sounds, paired with the synthesis parameters used to generate them. The dataset is 100x larger than any audio dataset in the literature. We also introduce torchsynth, an open source modular synthesizer that generates the synth 1B1 samples on-the-fly at 16200x faster than real-time (714MHz) on a single GPU. Finally, we release two new audio datasets: FM synth timbre and subtractive synth pitch. Using these datasets, we demonstrate new rank-based evaluation criteria for existing audio representations. Finally, we propose a novel approach to synthesizer hyperparameter optimization."}]}