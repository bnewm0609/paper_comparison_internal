{"tabid": "41b0fd65-b08d-438b-a088-27e8a49a92ab", "table": {"Category": {"5890185": "Word-level explanations, Sentence-level explanations", "4052735": "Visual justification through attention visualization, Textual explanations, Detailed specifications", "53955763": "Semantic parsing, Information retrieval", "261081574": "Agnostic, Template-based, Neural-based", "222379195": "Text-only, Vision-only, Visual-textual", "225068216": "Visual Question Answering (VQA), Image Captioning, Visual Commonsense Reasoning (VCR), Generative models", "252383606": "Learning from instruction, Learning from explanations, Few-shot learning", "256504063": "Chain-of-Thought (CoT) reasoning, Few-Shot-CoT, Manual-CoT, Auto-CoT, Zero-Shot-CoT, Program-of-Thoughts (PoT), Self-Consistency Decoding, Randomness in the Input Space", "258546810": "CoT prompting, Zero-shot prompting, Plan-based CoT, Data mixture strategy"}, "Type of Texts": {"5890185": "Word-level explanations, Sentence-level explanations", "4052735": "Clues to justify the answer, Complementary delineation, Detailed specification", "53955763": "Image description, Supporting facts retrieval, Dense captions", "261081574": "Agnostic, Template-based, Neural-based", "222379195": "Text-only, Vision-only, Visual-textual", "225068216": "Answer generation, Rationale generation", "252383606": "Lecture, Explanation", "256504063": "Commonsense mistakes, Logical mistakes, Others", "258546810": "N/A"}}, "row_bib_map": [{"bib_hash_or_arxiv_id": "97845a683573658e34a58eeade58d6cc4718a198", "row": 0, "corpus_id": 5890185, "type": "ref", "title": "Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions", "abstract": "In Visual Question Answering, most existing approaches adopt the pipeline of representing an image via pre-trained CNNs, and then using the uninterpretable CNN features in conjunction with the question to predict the answer. Although such end-to-end models might report promising performance, they rarely provide any insight, apart from the answer, into the VQA process. In this work, we propose to break up the end-to-end VQA into two steps: explaining and reasoning, in an attempt towards a more explainable VQA by shedding light on the intermediate results between these two steps. To that end, we first extract attributes and generate descriptions as explanations for an image. Next, a reasoning module utilizes these explanations in place of the image to infer an answer. The advantages of such a breakdown include: (1) the attributes and captions can reflect what the system extracts from the image, thus can provide some insights for the predicted answer; (2) these intermediate results can help identify the inabilities of the image understanding or the answer inference part when the predicted answer is wrong. We conduct extensive experiments on a popular VQA dataset and our system achieves comparable performance with the baselines, yet with added benefits of explanability and the inherent ability to further improve with higher quality explanations."}, {"bib_hash_or_arxiv_id": "7d8ec95fb2d8610e9706e95c05585ed1f10126ba", "row": 1, "corpus_id": 4052735, "type": "ref", "title": "VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions", "abstract": "Most existing works in visual question answering (VQA) are dedicated to improving the accuracy of predicted answers, while disregarding the explanations. We argue that the explanation for an answer is of the same or even more importance compared with the answer itself, since it makes the question and answering process more understandable and traceable. To this end, we propose a new task of VQA-E (VQA with Explanation), where the computational models are required to generate an explanation with the predicted answer. We first construct a new dataset, and then frame the VQA-E problem in a multi-task learning architecture. Our VQA-E dataset is automatically derived from the VQA v2 dataset by intelligently exploiting the available captions. We have conducted a user study to validate the quality of explanations synthesized by our method. We quantitatively show that the additional supervision from explanations can not only produce insightful textual sentences to justify the answers, but also improve the performance of answer prediction. Our model outperforms the state-of-the-art methods by a clear margin on the VQA v2 dataset."}, {"bib_hash_or_arxiv_id": "e51c2e4bac1587b4a8609cbabe4c5f74e0abc167", "row": 2, "corpus_id": 53955763, "type": "ref", "title": "Visual Question Answering as Reading Comprehension", "abstract": "Visual question answering (VQA) demands simultaneous comprehension of both the image visual content and natural language questions. In some cases, the reasoning needs the help of common sense or general knowledge which usually appear in the form of text. Current methods jointly embed both the visual information and the textual feature into the same space. Nevertheless, how to model the complex interactions between the two different modalities is not an easy work. In contrast to struggling on multimodal feature fusion, in this paper, we propose to unify all the input information by natural language so as to convert VQA into a machine reading comprehension problem. With this transformation, our method not only can tackle VQA datasets that focus on observation based questions, but can also be naturally extended to handle knowledge-based VQA which requires to explore large-scale external knowledge base. It is a step towards being able to exploit large volumes of text and natural language processing techniques to address VQA problem. Two types of models are proposed to deal with open-ended VQA and multiple-choice VQA respectively. We evaluate our models on three VQA benchmarks. The comparable performance with the state-of-the-art demonstrates the effectiveness of the proposed method."}, {"bib_hash_or_arxiv_id": "199b9fc3f16b3e4af831f5db8ca60d6b8633402f", "row": 3, "corpus_id": 261081574, "type": "ref", "title": "From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models", "abstract": "Large language models (LLMs) have demonstrated excellent zero-shot generalization to new language tasks. However, effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging, primarily due to the modality disconnect and task disconnect between the LLM and VQA tasks. End-to-end training on multimodal data may bridge the disconnects, but is inflexible and computationally expensive. To address this issue, we propose Img2LLM, a plug-and-play module that provides LLM prompts to enable LLMs to perform zeroshot VQA tasks without end-to-end training. We develop LLM-agnostic models describe image content as exemplar question-answer pairs, which prove to be effective LLM prompts. Img2LLM offers the following benefits: 1) It achieves comparable or better performance than methods relying on end-to-end training. For example, we outperform Flamingo [3] by 5.6% on VQAv2. On the challenging A-OKVQA dataset, our method outperforms few-shot methods by as much as 20%. 2) It flexibly interfaces with a wide range of LLMs to perform VQA. 3) It eliminates the need to specialize LLMs using end-to-end finetuning and serve highly specialized LLMs to end users, thereby reducing cost. Code is available via the LAVIS [28] framework at https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa."}, {"bib_hash_or_arxiv_id": "c9a6189d2d78588f273706e9369c58cc0e3000cd", "row": 4, "corpus_id": 222379195, "type": "ref", "title": "Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs", "abstract": "Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first study focused on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. The key challenge of accurate rationalization is comprehensive image understanding at all levels: not just their explicit content at the pixel level, but their contextual contents at the semantic and pragmatic levels. We present Rationale^VT Transformer, an integrated model that learns to generate free-text rationales by combining pretrained language models with object recognition, grounded visual semantic frames, and visual commonsense graphs. Our experiments show that the base pretrained language model benefits from visual adaptation and that free-text rationalization is a promising research direction to complement model interpretability for complex visual-textual reasoning tasks."}, {"bib_hash_or_arxiv_id": "9b3be3913d9901e4cc4a3c02a3e8a5fb5874103e", "row": 5, "corpus_id": 225068216, "type": "ref", "title": "Beyond VQA: Generating Multi-word Answers and Rationales to Visual Questions", "abstract": "Visual Question Answering is a multi-modal task that aims to measure high-level visual understanding. Contemporary VQA models are restrictive in the sense that answers are obtained via classification over a limited vocabulary (in the case of open-ended VQA), or via classification over a set of multiple-choice-type answers. In this work, we present a completely generative formulation where a multi-word answer is generated for a visual query. To take this a step forward, we introduce a new task: ViQAR (Visual Question Answering and Reasoning), wherein a model must generate the complete answer and a rationale that seeks to justify the generated answer We propose an end-to-end architecture to solve this task and describe how to evaluate it. We show that our model generates strong answers and rationales through qualitative and quantitative evaluation, as well as through a human Turing Test."}, {"bib_hash_or_arxiv_id": "647736a3a0132d609476fa7065a1b0ff0f3fb132", "row": 6, "corpus_id": 252383606, "type": "ref", "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering", "abstract": "When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40% of the data. The data and code are available at https://scienceqa.github.io."}, {"bib_hash_or_arxiv_id": "ccc8b680219b2b13c65241c2ca0849daae260417", "row": 7, "corpus_id": 256504063, "type": "ref", "title": "Multimodal Chain-of-Thought Reasoning in Language Models", "abstract": "Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. With Multimodal-CoT, our model under 1 billion parameters outperforms the previous state-of-the-art LLM (GPT-3.5) by 16 percentage points (75.17%->91.68% accuracy) on the ScienceQA benchmark and even surpasses human performance. Code is publicly available available at https://github.com/amazon-science/mm-cot."}, {"bib_hash_or_arxiv_id": "82a4986324a6d58306b2a64d726972a36a262354", "row": 8, "corpus_id": 258546810, "type": "ref", "title": "T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering", "abstract": "Large Language Models (LLMs) have recently demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. They have also shown the ability to perform chain-of-thought (CoT) reasoning to solve complex problems. Recent studies have explored CoT reasoning in complex multimodal scenarios, such as the science question answering task, by fine-tuning multimodal models with high-quality human-annotated CoT rationales. However, collecting high-quality COT rationales is usually time-consuming and costly. Besides, the annotated rationales are hardly accurate due to the external essential information missed. To address these issues, we propose a novel method termed T-SciQ that aims at teaching science question answering with LLM signals. The T-SciQ approach generates high-quality CoT rationales as teaching signals and is advanced to train much smaller models to perform CoT reasoning in complex modalities. Additionally, we introduce a novel data mixing strategy to produce more effective teaching data samples for simple and complex science question answer problems. Extensive experimental results show that our T-SciQ method achieves a new state-of-the-art performance on the ScienceQA benchmark, with an accuracy of 96.18%. Moreover, our approach outperforms the most powerful fine-tuned baseline by 4.5%. The code is publicly available at https://github.com/T-SciQ/T-SciQ."}]}