{"tabid": "0c1dc5de-bbb8-47f6-a91a-01807c9e6153", "table": {"Known Info": {"5258236": ["3D point cloud, Rendered images from 3D model with texture"], "116997379": ["3D point cloud"], "4392433": ["3D point cloud or rendered RGB-D images with pose"]}, "Key idea": {"5258236": ["Find Correspondences between 2D points and 3D points, and use PNP methods"], "116997379": ["Find 3D Correspondence through random hypothesis or 3D descriptors, and results are refined using ICP"], "4392433": ["Represent pose suitable for CNN"]}}, "row_bib_map": [{"bib_hash_or_arxiv_id": "58c7b1004f825a17b0322d28e2cce8d2a1063e4a", "row": 0, "corpus_id": 5258236, "type": "ref", "title": "Object recognition from local scale-invariant features", "abstract": "An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds."}, {"bib_hash_or_arxiv_id": "5a6a7ed29db9608dd99955e8f1ca1ecdb627a563", "row": 1, "corpus_id": 116997379, "type": "ref", "title": "Spin-Images: A Representation for 3-D Surface Matching", "abstract": "Surface matching is the process that compares surfaces and decides whether they are similar. In three-dimensional (3-D) computer vision, surface matching plays a prominent role. Surface matching can be used for object recognition; by comparing two surfaces, an association between a known object and sensed data is established. By computing the 3-D transformation that aligns two surfaces, surface matching can also be used for surface registration. Surface matching is difficult because the coordinate system in which to compare two surfaces is undefined. The typical approach to surface matching is to transform the surfaces being compared into representations where comparison of surfaces is straightforward. Surface matching is further complicated by characteristics of sensed data, including clutter, occlusion and sensor noise. This thesis describes a data level representation of surfaces used for surface matching. In our representation, surface shape is described by a dense collection of oriented points, 3-D points with surface normal. Using a single point basis constructed from an oriented point, the position of other points on the surface can be described by two parameters. The accumulation of these parameters for many points on the surface results in an image at each oriented point. These images, localized descriptions of the global shape of the surface, are invariant to rigid transformations. Through correlation of images, point correspondences between two surfaces can be established. When two surfaces have many point correspondences, they match. Taken together, the oriented points and associated images make up our surface representation. Because the image generation process can be visualized as a sheet spinning about the normal of a point, the images in our representation are called spin-images . Spin-images combine the descriptive nature of global object properties with the robustness to partial views and clutter of local shape descriptions. Through adjustment of spin-image generation parameters, spin-images can be smoothly transformed from global to local representations. Since they are object-centered representations, spin-images can be compared without alignment of surfaces. However, spin-images are constructed with respect to specific surface points, so they can also be used to align surfaces. Because spin-images are constructed without surface fitting or optimization, they are simple to construct and analyze. We demonstrate the usefulness of spin-images by applying them to two problems in 3-D computer vision. First we show that surface registration using spin-images is accurate enough to build complete models of objects from multiple range images. Without a calibrated image acquisition system, we have built twenty models of objects with complicated shapes. We also apply spin-images to the problem of recognizing complete 3-D models of objects in partial scenes containing clutter and occlusion. Using spin-images, we have simultaneously recognized multiple objects from a library containing twenty models. We also verify experimentally that spin-image matching is robust to scene clutter by recognizing objects in 100 scenes containing clutter and occlusion."}, {"bib_hash_or_arxiv_id": "384a7a39145a15ba352b2bcd79f98f2608f6f184", "row": 2, "corpus_id": 4392433, "type": "ref", "title": "BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth", "abstract": "We introduce a novel method for 3D object detection and pose estimation from color images only. We first use segmentation to detect the objects of interest in 2D even in presence of partial occlusions and cluttered background. By contrast with recent patch-based methods, we rely on a \u201cholistic\u201d approach: We apply to the detected objects a Convolutional Neural Network (CNN) trained to predict their 3D poses in the form of 2D projections of the corners of their 3D bounding boxes. This, however, is not sufficient for handling objects from the recent T-LESS dataset: These objects exhibit an axis of rotational symmetry, and the similarity of two images of such an object under two different poses makes training the CNN challenging. We solve this problem by restricting the range of poses used for training, and by introducing a classifier to identify the range of a pose at run-time before estimating it. We also use an optional additional step that refines the predicted poses. We improve the state-of-the-art on the LINEMOD dataset from 73.7% [2] to 89.3% of correctly registered RGB frames. We are also the first to report results on the Occlusion dataset [1 ] using color images only. We obtain 54% of frames passing the Pose 6D criterion on average on several sequences of the T-LESS dataset, compared to the 67% of the state-of-the-art [10] on the same sequences which uses both color and depth. The full approach is also scalable, as a single network can be trained for multiple objects simultaneously."}], "decontext_schema": {"Known Info": "In the context of the given table from the scientific paper, \"Known Info\" refers to the available information or input data used in each study, which includes 3D point clouds and rendered images with texture or pose for the corresponding key ideas.", "Key idea": "The Key idea column in the table refers to the novel contribution or method used in each study related to finding correspondences between 2D and 3D points using different techniques such as PNP methods, random hypotheses, or 3D descriptors, and refining the results using Iterative Closest Point (ICP) algorithm."}}