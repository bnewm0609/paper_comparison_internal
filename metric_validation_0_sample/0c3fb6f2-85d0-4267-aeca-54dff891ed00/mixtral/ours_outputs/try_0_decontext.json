[{"id": 466, "tabid": "0c3fb6f2-85d0-4267-aeca-54dff891ed00", "schema": ["Task Allocation Mechanisms", "Types of Crowdsourcing Platforms"], "table": {"Task Allocation Mechanisms": {"paper_1": ["Quota system and piece rate system"], "paper_2": ["Dynamic pricing mechanism with a base payment and optional bonuses"], "paper_3": ["QuikTurkit, an intelligent recruitment system for human workers"], "paper_4": ["Retainer model and rapid refinement techniques for on-demand, realtime crowdsourcing"], "paper_5": ["Retainer model for realtime crowdsourcing"], "paper_6": ["Proper scoring rule for self-fulfilling prophecy in predicting own actions"], "paper_7": ["Scoring rules for worker and manager to report information and best effort in task completion"], "paper_8": ["Equitable payment for collaborative crowdsourcing tasks"], "paper_9": ["N/A"], "paper_10": ["N/A"], "paper_11": ["N/A"], "paper_12": ["FROG: Fast and Reliable crOwdsourcinG framework with task scheduler and notification modules"], "paper_13": ["N/A"], "paper_14": ["All-pay auctions model for crowdsourcing systems"]}, "Types of Crowdsourcing Platforms": {"paper_1": ["Amazon's Mechanical Turk (AMT)"], "paper_2": ["N/A"], "paper_3": ["VizWiz: a talking application for mobile phones"], "paper_4": ["N/A"], "paper_5": ["N/A"], "paper_6": ["N/A"], "paper_7": ["N/A"], "paper_8": ["Amazon Mechanical Turk (AMT)"], "paper_9": ["N/A"], "paper_10": ["Quizz: a gamified crowdsourcing system"], "paper_11": ["Amazon's Mechanical Turk (AMT)"], "paper_12": ["FROG platform"], "paper_13": ["Galaxy Zoo, a citizen science project"], "paper_14": ["Taskcn.com"]}}, "caption": "Crowdsourcing", "gold_col": 2, "predicted_col_num": 2, "type": "mixtral_5_single_call_multiple", "error_counts": {"length_error": 1, "json_error": 0, "paper_num_error": 0, "column_num_error": 0, "scheme_length_error": 1, "scheme_json_error": 0, "scheme_unknown_error": 0, "over_max_length_error": false, "have_length_error": true}, "decontext_schema": {"Task Allocation Mechanisms": "Task Allocation Mechanisms refer to the methods used in crowdsourcing platforms to assign tasks to human workers.", "Types of Crowdsourcing Platforms": "Types of Crowdsourcing Platforms refers to the specific crowdsourcing platforms mentioned in each paper."}}, {"id": 466, "tabid": "0c3fb6f2-85d0-4267-aeca-54dff891ed00", "schema": ["Financial Incentives", "Scoring Rules"], "table": {"Financial Incentives": {"paper_1": ["quota and piece rate systems"], "paper_2": ["dynamic pricing mechanism with optional bonuses"], "paper_3": ["payment for services"], "paper_4": ["retainer model, paying workers a small wage"], "paper_5": ["retainer model with push notifications and precruitment"], "paper_6": ["proper scoring rule based mechanism", "strictly proper scoring rule"], "paper_7": ["scoring rules to encourage truthful reporting"], "paper_8": ["equitable payment based on contribution"], "paper_9": [" entertainments/rewards for accuracy"], "paper_10": ["ad-targeting budget"], "paper_11": ["micro-diversions"], "paper_12": ["incentives for completing tasks quickly and accurately"], "paper_13": ["payment based on engagement prediction"], "paper_14": ["contests offering various rewards"]}, "Scoring Rules": {"paper_1": ["quantity and quality of work"], "paper_2": ["quality of work, learnt using time sequence model"], "paper_3": ["correctness of answers"], "paper_4": ["early signs of agreement, agreement quality"], "paper_5": ["performance"], "paper_6": ["accuracy of prediction"], "paper_7": ["truthful reporting, best effort, and collusion prevention"], "paper_8": ["equitable payment based on contribution"], "paper_9": ["accuracy of solutions"], "paper_10": ["user competence"], "paper_11": ["work quality"], "paper_12": ["task accuracy, task latency"], "paper_13": ["engagement prediction"], "paper_14": ["users' participation levels"]}}, "caption": "Incentive mechanisms", "gold_col": 2, "predicted_col_num": 2, "type": "mixtral_5_single_call_multiple", "error_counts": {"length_error": 0, "json_error": 0, "paper_num_error": 0, "column_num_error": 0, "scheme_length_error": 1, "scheme_json_error": 0, "scheme_unknown_error": 0, "over_max_length_error": false, "have_length_error": false}, "decontext_schema": {"Financial Incentives": "Financial Incentives refer to monetary or non-monetary rewards given to individuals or groups to encourage desired behaviors or outcomes, as shown in the given table through various payment systems, retainer models, dynamic pricing mechanisms, and contests.", "Scoring Rules": "In the context of the table from the scientific paper, Scoring Rules refer to the specific methods used to evaluate or determine the outcome or performance in each study, based on various factors such as accuracy, engagement prediction, truthful reporting, work quality, and equitable payment based on contribution."}}, {"id": 466, "tabid": "0c3fb6f2-85d0-4267-aeca-54dff891ed00", "schema": ["Quality Estimation", "Bonus Delivery"], "table": {"Quality Estimation": {"paper_1": ["Anchoring effect: increased financial incentives increase the quantity, but not the quality"], "paper_2": ["N/A"], "paper_3": ["Intelligent recruitment of human workers in advance called quikTurkit for answering questions quickly"], "paper_4": ["Synchronous crowds for on-demand, realtime crowdsourcing to lower crowd latency"], "paper_5": ["Realtime crowdsourcing techniques to recruit paid crowds within seconds and minimize cost"], "paper_6": ["Mechanism to incentivize agents who predict their own future actions and truthfully declare their predictions"], "paper_7": ["Scoring rules for worker and manager to provide truthful information and best effort in completing tasks as quickly as possible"], "paper_8": ["Equitable payment schemes for collaborative crowdsourcing tasks to motivate workers"], "paper_9": ["Games with a purpose that generate data as a side effect of game play"], "paper_10": ["Quizz, a gamified crowdsourcing system that estimates user competence and acquires new knowledge"], "paper_11": ["Micro-diversions to improve crowd workers' experiences during long sequences of monotonous tasks"], "paper_12": ["FROG framework for fast and reliable crowdsourcing with task scheduler and notification modules"], "paper_13": ["Statistical models for predicting the forthcoming engagement of volunteers in online citizen science projects"], "paper_14": ["Crowdsourcing system modeled as all-pay auctions with incomplete information to demonstrate incentives and participation levels"]}, "Bonus Delivery": {"paper_1": ["N/A"], "paper_2": ["Dynamic pricing mechanism, named CrowdPricer for incentively delivering bonuses to crowd workers"], "paper_3": ["N/A"], "paper_4": ["N/A"], "paper_5": ["Push notifications, shared retainer pools, and precruitment to improve performance"], "paper_6": ["Mechanism using strictly proper scoring rules to incentivize agents to predict their own actions"], "paper_7": ["N/A"], "paper_8": ["Equitable payment schemes for collaborative crowdsourcing tasks"], "paper_9": ["N/A"], "paper_10": ["Quizz, a gamified crowdsourcing system that acquires new knowledge"], "paper_11": ["N/A"], "paper_12": ["FROG framework for fast and reliable crowdsourcing"], "paper_13": ["N/A"], "paper_14": ["Crowdsourcing system modeled as all-pay auctions with incomplete information"]}}, "caption": "Quality assessment and control", "gold_col": 2, "predicted_col_num": 2, "type": "mixtral_5_single_call_multiple", "error_counts": {"length_error": 0, "json_error": 0, "paper_num_error": 0, "column_num_error": 0, "scheme_length_error": 1, "scheme_json_error": 0, "scheme_unknown_error": 0, "over_max_length_error": false, "have_length_error": false}, "decontext_schema": {"Quality Estimation": "In the context of the given table from the scientific papers, Quality Estimation refers to the estimation or measurement of the quality of the results or outputs obtained from crowdsourcing tasks.", "Bonus Delivery": "In the context of the given table from the scientific papers, Bonus Delivery refers to a dynamic pricing mechanism, named CrowdPricer, for incentively delivering bonuses to crowd workers (as mentioned in paper_2)."}}]