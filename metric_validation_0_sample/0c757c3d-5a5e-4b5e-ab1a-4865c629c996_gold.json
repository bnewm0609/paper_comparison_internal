{"tabid": "0c757c3d-5a5e-4b5e-ab1a-4865c629c996", "table": {"Method": {"202773977": ["Derivative-based approach to exploit chromatic aberration present in a single multispectral image"], "202784436": ["Joint deep neural network architecture for registered 2D infrared images and 3D point clouds"]}, "Platform": {"202773977": ["Multispectral camera"], "202784436": ["Infrared & Point Cloud from Time-of-Flight Cameras"]}, "Spectral Range": {"202773977": ["450nm-700nm"], "202784436": ["700nm-1mm"]}, "Evaluation Metric": {"202773977": ["Mean Square Error(MSE) and Standard Deviation(SD) to evaluate the accuracy of the estimated depth with ground-truth data."], "202784436": ["Average Error(AE) and Mean Average Precision(MAE) to evaluate the accuracy of prediction of pose estimation compared with ground-truth data and other methods."]}}, "row_bib_map": [{"bib_hash_or_arxiv_id": "2daf6cb832b43176fcc84a5f489b539812ec9961", "row": 0, "corpus_id": 202773977, "type": "ref", "title": "Depth from Spectral Defocus Blur", "abstract": "This paper proposes a method for depth estimation from a single multispectral image by using a lens property known as a chromatic aberration. The chromatic aberration cause that the light passing through the lens is refracted depending on the wavelength. The refraction cause that rays vary their angle depending on the wavelength and generate a change in focal length which leads to a defocus blur for different wavelengths.We show that the chromatic aberration provides clues to recover depth maps from a single multispectral image if we assume that the defocus blur is Gaussian. The proposed method needs only a standard wide-aperture lens which naturally exhibits the chromatic aberration and a multispectral camera. Moreover, we use a simple yet effective depth of field synthesis method to calculate the derivatives and obtain all-in-focus images necessary to approximate spectral derivatives. We verified the effectiveness of the proposed method on various real-world scenes."}, {"bib_hash_or_arxiv_id": "303b62c2e74eb3bfca0a24d0dfac7dfb61d0b50b", "row": 1, "corpus_id": 202784436, "type": "ref", "title": "3D Driver Pose Estimation Based on Joint 2d-3d Network", "abstract": "3D driver pose estimation is a promising and challenging problem for computer-human interaction. Recently convolutional neural networks (CNNs) have been introduced into 3D pose estimation, but these methods have the problem of slow running speed and are not suitable for driving scenario. In this paper, our method is based on two type of inputs, IR image and point cloud obtained from TOF camera. We propose a Joint 2D-3D network incorporating image-based and point-based feature to promote the performance of 3D human pose estimation and run in a high speed. For point cloud with invalid points, we firstly do preprocess and then design a denoising module to handle this problem. Experiments on private Driver dataset and public ITOP dataset show that our method achieves efficient and competitive performance on 3D human pose estimation."}], "decontext_schema": {"Method": "The Method column in the table refers to the specific approach or technique used in each study to analyze or process the data, as described in the corresponding scientific paper. In the given examples, the first paper uses a derivative-based approach to exploit chromatic aberration in a multispectral image, while the second paper employs a joint deep neural network architecture for registered 2D infrared images and 3D point clouds.", "Platform": "The Platform column in the table refers to the specific type of hardware or sensor system used in each study, including a multispectral camera for the first entry and infrared & point cloud from time-of-flight cameras for the second entry.", "Spectral Range": "The Spectral Range in the table refers to the wavelength or frequency range covered by the specific methods or platforms used in each scientific study.", "Evaluation Metric": "The Evaluation Metric refers to the quantitative measure used to assess the accuracy and performance of the methods in the given scientific paper, as indicated by the specific values mentioned in each row of the table. For example, in the first row, Mean Square Error (MSE) and Standard Deviation (SD) are used to evaluate the accuracy of the estimated depth with ground-truth data in the multispectral camera method. Similarly, in the second row, Average Error (AE) and Mean Average Precision (MAE) are used to evaluate the accuracy of prediction of pose estimation compared with ground-truth data and other methods in the infrared and point cloud from time-of-flight cameras method."}}