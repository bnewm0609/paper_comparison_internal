{"tabid": "0c3fb6f2-85d0-4267-aeca-54dff891ed00", "table": {"Category": {"203698": ["Monetary incentives"], "249627246": ["-"], "52804681": ["-"], "6921166": ["-"], "14429309": ["-"], "10037977": ["-"], "1869524": ["-"], "207960449": ["-"], "11959487": ["Non-monetary incentives"], "15831640": ["-"], "14320037": ["-"], "64740126": ["Worker behavior prediction"], "13633562": ["-"], "16926268": ["Crowdsourcing contests"]}, "Description": {"203698": ["Increasing monetary incentives"], "249627246": ["Dynamic pricing of tasks"], "52804681": ["Recruiting workers for real-time service"], "6921166": ["Recruiting workers for real-time service"], "14429309": ["Estimating waiting time by queueing theory"], "10037977": ["Giving incentive to work during declared period"], "1869524": ["Having workers declare their effort and completion time"], "207960449": ["Rewarding collaborative work"], "11959487": ["Introducing games with a purpose"], "15831640": ["Recruiting volunteer workers through advertisements"], "14320037": ["Providing occasional entertainments for workers"], "64740126": ["Worker availability prediction using machine learning"], "13633562": ["Predicting volunteer worker participation"], "16926268": ["Relationship between rewards and participation in contests"]}}, "row_bib_map": [{"bib_hash_or_arxiv_id": "5b989410a19114071e0568deb51577df9b8478c2", "row": 0, "corpus_id": 203698, "type": "ref", "title": "Financial incentives and the \"performance of crowds\"", "abstract": "The relationship between financial incentives and performance, long of interest to social scientists, has gained new relevance with the advent of web-based \"crowd-sourcing\" models of production. Here we investigate the effect of compensation on performance in the context of two experiments, conducted on Amazon's Mechanical Turk (AMT). We find that increased financial incentives increase the quantity, but not the quality, of work performed by participants, where the difference appears to be due to an \"anchoring\" effect: workers who were paid more also perceived the value of their work to be greater, and thus were no more motivated than workers paid less. In contrast with compensation levels, we find the details of the compensation scheme do matter---specifically, a \"quota\" system results in better work for less pay than an equivalent \"piece rate\" system. Although counterintuitive, these findings are consistent with previous laboratory studies, and may have real-world analogs as well."}, {"bib_hash_or_arxiv_id": "44b0b7427ddb34e03ab17efdd8ed26bf43c4827d", "row": 1, "corpus_id": 249627246, "type": "ref", "title": "On Dynamically Pricing Crowdsourcing Tasks", "abstract": "Crowdsourcing techniques have been extensively explored in the past decade, including task allocation, quality assessment, and so on. Most of professional crowdsourcing platforms adopt the fixed pricing scheme to offer a fixed price for crowd tasks. It is neither incentive for crowd workers to produce good performance, nor profitable for the requester to gain high utility with low budget. In this article, we study the problem of pricing crowdsourcing tasks with optional bonuses. We propose a dynamic pricing mechanism, named CrowdPricer for incentively delivering bonuses to the crowd workers of completing tasks, in addition to offering a base payment for completing a task. We leverage a deep time sequence model to learn the effect of bonuses on workers\u2019 quality for crowd tasks. CrowdPricer makes decisions on whether to provide bonuses on workers, so as to maximize the requester\u2019s utility in expectation. We present an efficient bonus delivery algorithm under the help of beam search technique, in order to efficiently solve the decision making problem. Extensive experiments using both a real crowdsourcing platform and simulations demonstrate that CrowdPricer yields the higher utility for the requester. It also obtains more correct crowd answers than the state-of-the-art pricing methods."}, {"bib_hash_or_arxiv_id": "a688cfd63742c4c08bf2f1a74ba76fc76c17448c", "row": 2, "corpus_id": 52804681, "type": "ref", "title": "VizWiz: nearly real-time answers to visual questions", "abstract": "The lack of access to visual information like text labels, icons, and colors can cause frustration and decrease independence for blind people. Current access technology uses automatic approaches to address some problems in this space, but the technology is error-prone, limited in scope, and quite expensive. In this paper, we introduce VizWiz, a talking application for mobile phones that offers a new alternative to answering visual questions in nearly real-time - asking multiple people on the web. To support answering questions quickly, we introduce a general approach for intelligently recruiting human workers in advance called quikTurkit so that workers are available when new questions arrive. A field deployment with 11 blind participants illustrates that blind people can effectively use VizWiz to cheaply answer questions in their everyday lives, highlighting issues that automatic approaches will need to address to be useful. Finally, we illustrate the potential of using VizWiz as part of the participatory design of advanced tools by using it to build and evaluate VizWiz::LocateIt, an interactive mobile tool that helps blind people solve general visual search problems."}, {"bib_hash_or_arxiv_id": "e5be4a978cb8009c1f1b034aaa13a6f9f70fd814", "row": 3, "corpus_id": 6921166, "type": "ref", "title": "Crowds in two seconds: enabling realtime crowd-powered interfaces", "abstract": "Interactive systems must respond to user input within seconds. Therefore, to create realtime crowd-powered interfaces, we need to dramatically lower crowd latency. In this paper, we introduce the use of synchronous crowds for on-demand, realtime crowdsourcing. With synchronous crowds, systems can dynamically adapt tasks by leveraging the fact that workers are present at the same time. We develop techniques that recruit synchronous crowds in two seconds and use them to execute complex search tasks in ten seconds. The first technique, the retainer model, pays workers a small wage to wait and respond quickly when asked. We offer empirically derived guidelines for a retainer system that is low-cost and produces on-demand crowds in two seconds. Our second technique, rapid refinement, observes early signs of agreement in synchronous crowds and dynamically narrows the search space to focus on promising directions. This approach produces results that, on average, are of more reliable quality and arrive faster than the fastest crowd member working alone. To explore benefits and limitations of these techniques for interaction, we present three applications: Adrenaline, a crowd-powered camera where workers quickly filter a short video down to the best single moment for a photo; and Puppeteer and A|B, which examine creative generation tasks, communication with workers, and low-latency voting."}, {"bib_hash_or_arxiv_id": "15e51770b5ee1a8a76d6479362dbdee35cc34edf", "row": 4, "corpus_id": 14429309, "type": "ref", "title": "Analytic Methods for Optimizing Realtime Crowdsourcing", "abstract": "Realtime crowdsourcing research has demonstrated that it is possible to recruit paid crowds within seconds by managing a small, fast-reacting worker pool. Realtime crowds enable crowd-powered systems that respond at interactive speeds: for example, cameras, robots and instant opinion polls. So far, these techniques have mainly been proof-of-concept prototypes: research has not yet attempted to understand how they might work at large scale or optimize their cost/performance trade-offs. In this paper, we use queueing theory to analyze the retainer model for realtime crowdsourcing, in particular its expected wait time and cost to requesters. We provide an algorithm that allows requesters to minimize their cost subject to performance requirements. We then propose and analyze three techniques to improve performance: push notifications, shared retainer pools, and precruitment, which involves recalling retainer workers before a task actually arrives. An experimental validation finds that precruited workers begin a task 500 milliseconds after it is posted, delivering results below the one-second cognitive threshold for an end-user to stay in flow."}, {"bib_hash_or_arxiv_id": "b2cf2df0232e9cb7e721192a49a91652801eb562", "row": 5, "corpus_id": 10037977, "type": "ref", "title": "Predicting Own Action: Self-Fulfilling Prophecy Induced by Proper Scoring Rules", "abstract": "\n \n This paper studies a mechanism to incentivize agents who predict their own future actions and truthfully declare their predictions. In a crowdsouring setting (e.g., participatory sensing), obtaining an accurate prediction of the actions of workers/agents is valuable for a requester who is collecting real-world information from the crowd. If an agent predicts an external event that she cannot control herself (e.g., tomorrow's weather), any proper scoring rule can give an accurate incentive. In our problem setting, an agent needs to predict her own action (e.g., what time tomorrow she will take a photo of a specific place) that she can control to maximize her utility. Also, her (gross) utility can vary based on an eternal event. We first prove that a mechanism can satisfy our goal if and only if it utilizes a strictly proper scoring rule, assuming that an agent can find an optimal declaration that maximizes her expected utility. This declaration is self-fulfilling; if she acts to maximize her utility, the probabilistic distribution of her action matches her declaration, assuming her prediction about the external event is correct. Furthermore, we develop a heuristic algorithm that efficiently finds a semi-optimal declaration, and show that this declaration is still self-fulfilling. We also examine our heuristic algorithm's performance and describe how an agent acts when she faces an unexpected scenario.\n \n"}, {"bib_hash_or_arxiv_id": "38433d3cbcedc4288962c8570361fbea4f8ecf87", "row": 6, "corpus_id": 1869524, "type": "ref", "title": "Predicting your own effort", "abstract": "We consider a setting in which a worker and a manager may each have information about the likely completion time of a task, and the worker also affects the completion time by choosing a level of effort. The task itself may further be composed of a set of subtasks, and the worker can also decide how many of these subtasks to split out into an explicit prediction task. In addition, the worker can learn about the likely completion time of a task as work on subtasks completes. We characterize a family of scoring rules for the worker and manager that provide three properties: information is truthfully reported; best effort is exerted by the worker in completing tasks as quickly as possible; and collusion is not possible. We also study the factors influencing when a worker will split a task into subtasks, each forming a separate prediction target."}, {"bib_hash_or_arxiv_id": "6d239c2a6ba042683bb7b6eb74e09f94c17f2a0f", "row": 7, "corpus_id": 207960449, "type": "ref", "title": "Paying Crowd Workers for Collaborative Work", "abstract": "Collaborative crowdsourcing tasks allow crowd workers to solve problems that they could not handle alone, but worker motivation in these tasks is not well understood. In this paper, we study how to motivate groups of workers by paying them equitably. To this end, we characterize existing collaborative tasks based on the types of information available to crowd workers. Then, we apply concepts from equity theory to show how fair payments relate to worker motivation, and we propose two theoretically grounded classes of fair payments. Finally, we run two experiments using an audio transcription task on Amazon Mechanical Turk to understand how workers perceive these payments. Our results show that workers recognize fair and unfair payment divisions, but are biased toward payments that reward them more. Additionally, our data suggests that fair payments could lead to a small increase in worker effort. These results inform the design of future collaborative crowdsourcing tasks."}, {"bib_hash_or_arxiv_id": "76073417360c31d4b748220bdd24acbe180107db", "row": 8, "corpus_id": 11959487, "type": "ref", "title": "Designing games with a purpose", "abstract": "Data generated as a side effect of game play also solves computational problems and trains AI algorithms."}, {"bib_hash_or_arxiv_id": "4763a1f84d3eb9ae237a2045216e0e9380106e8c", "row": 9, "corpus_id": 15831640, "type": "ref", "title": "Quizz: Targeted crowdsourcing with a billion (potential) users", "abstract": "We describe Quizz, a gamified crowdsourcing system that simultaneously assesses the knowledge of users and acquires new knowledge from them. Quizz operates by asking users to complete short quizzes on specific topics; as a user answers the quiz questions, Quizz estimates the user's competence. To acquire new knowledge, Quizz also incorporates questions for which we do not have a known answer; the answers given by competent users provide useful signals for selecting the correct answers for these questions. Quizz actively tries to identify knowledgeable users on the Internet by running advertising campaigns, effectively leveraging the targeting capabilities of existing, publicly available, ad placement services. Quizz quantifies the contributions of the users using information theory and sends feedback to the advertisingsystem about each user. The feedback allows the ad targeting mechanism to further optimize ad placement. Our experiments, which involve over ten thousand users, confirm that we can crowdsource knowledge curation for niche and specialized topics, as the advertising network can automatically identify users with the desired expertise and interest in the given topic. We present controlled experiments that examine the effect of various incentive mechanisms, highlighting the need for having short-term rewards as goals, which incentivize the users to contribute. Finally, our cost-quality analysis indicates that the cost of our approach is below that of hiring workers through paid-crowdsourcing platforms, while offering the additional advantage of giving access to billions of potential users all over the planet, and being able to reach users with specialized expertise that is not typically available through existing labor marketplaces."}, {"bib_hash_or_arxiv_id": "e5dfbd311a0071413a81d96372855c5b4ada47aa", "row": 10, "corpus_id": 14320037, "type": "ref", "title": "And Now for Something Completely Different: Improving Crowdsourcing Workflows with Micro-Diversions", "abstract": "Crowdsourcing has become a popular and indispensable component of many problem-solving pipelines in the research literature, with crowd workers often treated as computational resources that can reliably solve problems that computers have trouble with, such as image labeling/classification, natural language processing, or document writing. Yet, obviously crowd workers are human, and long sequences of the same monotonous tasks might intuitively reduce the amount of good quality work done by the workers. Here we propose an investigation into how we can use diversions containing small amounts of entertainment to improve crowd workers' experiences. We call these small period of entertainment ``micro-diversions\", which we hypothesize to provide timely relief to workers during long sequences of micro-tasks. We hope to improve productivity by retaining workers to work on our tasks longer and to either improve or retain the quality of work. We experimentally test micro-diversions on Amazon's Mechanical Turk, a large paid-crowdsourcing platform. We find that micro-diversions can significantly improve worker retention rate while retaining the same work quality."}, {"bib_hash_or_arxiv_id": "7e51b4b0bdf7761438ea2945fdb8e1428eb3a082", "row": 11, "corpus_id": 64740126, "type": "ref", "title": "FROG: A Fast and Reliable Crowdsourcing Framework", "abstract": "For decades, the crowdsourcing has gained much attention from both academia and industry, which outsources a number of tasks to human workers. Typically, existing crowdsourcing platforms include CrowdFlower, Amazon Mechanical Turk (AMT), and so on, in which workers can autonomously select tasks to do. However, due to the unreliability of workers or the difficulties of tasks, workers may sometimes finish doing tasks either with incorrect/incomplete answers or with significant time delays. Existing studies considered improving the task accuracy through voting or learning methods, they usually did not fully take into account reducing the latency of the task completion. This is especially critical, when a task requester posts a group of tasks (e.g., sentiment analysis), and one can only obtain answers of all tasks after the last task is accomplished. As a consequence, the time delay of even one task in this group could delay the next step of the task requester\u2019s work from minutes to days, which is quite undesirable for the task requester. Inspired by the importance of the task accuracy and latency, in this paper, we will propose a novel crowdsourcing framework, namely Fast and Reliable crOwdsourcinG framework (FROG), which intelligently assigns tasks to workers, such that the latencies of tasks are reduced and the expected accuracies of tasks are met. Specifically, our FROG framework consists of two important components, task scheduler and notification modules. For the task scheduler module, we formalize a FROG task scheduling (FROG-TS) problem, in which the server actively assigns workers to tasks to achieve high task reliability and low task latency. We prove that the FROG-TS problem is NP-hard. Thus, we design two heuristic approaches, request-based and batch-based scheduling. For the notification module, we define an efficient worker notifying (EWN) problem, which only sends task invitations to those workers with high probabilities of accepting the tasks. To tackle the EWN problem, we propose a smooth kernel density estimation approach to estimate the probability that a worker accepts the task invitation. Through extensive experiments, we demonstrate the effectiveness and efficiency of our proposed FROG platform on both real and synthetic data sets."}, {"bib_hash_or_arxiv_id": "6138de73e10d2ee9c4e1e0682170bd08c4374860", "row": 12, "corpus_id": 13633562, "type": "ref", "title": "Why Stop Now? Predicting Worker Engagement in Online Crowdsourcing", "abstract": "\n \n We present studies of the attention and time, or engagement, invested by crowd workers on tasks. Consideration of worker engagement is especially important in volunteer settings such as online citizen science. Using data from Galaxy Zoo, a prominent citizen science project, we design and construct statistical models that provide predictions about the forthcoming engagement of volunteers. We characterize the accuracy of predictions with respect to different sets of features that describe user behavior and study the sensitivity of predictions to variations in the amount of data and retraining. We design our model for guiding system actions in real-time settings, and discuss the prospect for harnessing predictive models of engagement to enhance user attention and effort on volunteer tasks.\n \n"}, {"bib_hash_or_arxiv_id": "ce9eaadec0f8c8d25031507bede8bb8b481de7a6", "row": 13, "corpus_id": 16926268, "type": "ref", "title": "Crowdsourcing and all-pay auctions", "abstract": "In this paper we present and analyze a model in which users select among, and subsequently compete in, a collection of contests offering various rewards. The objective is to capture the essential features of a crowdsourcing system, an environment in which diverse tasks are presented to a large community. We aim to demonstrate the precise relationship between incentives and participation in such systems.\n We model contests as all-pay auctions with incomplete information; as a consequence of revenue equivalence, our model may also be interpreted more broadly as one in which users select among auctions of heterogeneous goods. We present two regimes in which we find an explicit correspondence in equilibrium between the offered rewards and the users' participation levels. The regimes respectively model situations in which different contests require similar or unrelated skills. Principally, we find that rewards yield logarithmically diminishing returns with respect to participation levels. We compare these results to empirical data from the crowdsourcing site Taskcn.com; we find that as we condition the data on more experienced users, the model more closely conforms to the empirical data."}], "decontext_schema": {"Category": "In the context of the given table from the scientific paper, Category refers to the type or nature of the incentives or approaches used in crowdsourcing tasks.", "Description": "In the context of the given table from the scientific paper, the term \"Description\" refers to a brief explanation or label for each row's category."}}