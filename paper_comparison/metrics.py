from typing import Any, Optional

# from bert_score import BERTScorer
import numpy as np
from sacrebleu.metrics import BLEU
from omegaconf import DictConfig

from paper_comparison.types import Table
from paper_comparison.metrics_utils import get_p_r_f1, get_similar_sentence, align_schema, decontextualize_table


class BaseMetric:
    """Calculates and stores information for evaluation metrics.

    Attributes:
        name: The name of the metric.
        requires_metadata: True if calculating the metric requires information outside of just the
            (prediction, target) pairs (e.g. the inputs).
    """

    name: str
    requires_metadata: bool

    def __init__(self, args):
        """Initialize the metric.

        By default, per-example scores are stored in `self.scores` and `self.requires_metadata` is False
        """

        self.scores = []
        self.requires_metadata = False
        self.reset()

    def add(self, prediction: Table, target: Table, metadata: Optional[Any] = None):
        """Calculate the score for the the given (prediction, target) pair

        and add it to the list of all scores.

        Args:
            prediction (str): The string generated by the model.
            target (str): The gold string.
            metadata (Optional[Any]): If required, the additional information needed for calculating the score.
        """

        raise NotImplementedError()

    def process_scores(self) -> dict[str, Any]:
        """Convert the list of scores into summary statistics which are returned

        Returns:
            A dict mapping from summary statistic name to the statistic. This is flexible, so even the entire
            scores list can be returned here.
        """
        raise NotImplementedError()

    def evaluate(
        self,
        predictions: Optional[list[Table]] = None,
        targets: Optional[list[Table]] = None,
        metadata: Optional[list[Any]] = None,
    ):
        """Evaluate the predictions against the targets.

        Wrap the `self.add` and `self.process_scores` methods so the metric can be called with multiple
        predictions and targets at once. If predictions and targets are not provided, simply process the
        scores (that likely have already been computed).

        Args:
            predictions (Optional[list[str]]): list of model-generated strings.
            targets (Optional[list[str]]): list of gold target strings. `predictions[i]` is the prediction for
                `targets[i]`.
            metadata (Optional[list[Any]]): list of additional information if needed.

        Returns:
            The result of `self.process_scores()`.
        """
        if predictions is not None and targets is not None:
            if metadata is None:
                for prediction, target in zip(predictions, targets):
                    self.add(prediction, target)
            else:
                for prediction, target, metadatum in zip(predictions, targets, metadata):
                    self.add(prediction, target, metadatum)

        return self.process_scores()

    def reset(self) -> None:
        """Reset the accumulated scores."""
        self.scores = []


class AnswerCorrectnessMetric(BaseMetric):
    def __init__(self, args):
        super().__init__(args)

    def add(self, prediction: Table, target: Table, metadata: Any | None = None):
        return super().add(prediction, target, metadata)

    def process_scores(self) -> dict[str, Any]:
        return super().process_scores()

    def reset(self) -> None:
        return super().reset()


class SchemaRecallMetric(BaseMetric):
    """
    Calculates the recall of the produced schema.

    Namely, the tables we produce might have many more columns than the gold
    tables. This metric computes how many of the columns in the predicted
    tables are also in the gold table as a proportion of the number of columns
    in the gold table.

    This calculation requires aligning the columns in some way, and there are
    a few ways this can be done:
        1. Using just the schema (ie column headers)
        2. Using the values in the table

    I think the best would be to first decontextualize the table and align, but
    we should try implementing a few different methods.
    """

    def __init__(
        self,
        args,
        should_decontext: bool = True,
        sim_threshold=0.4,
        align_method_name: str = "sentence-transformers",
        **align_method_config,
    ):
        """
        sim_method [str]: indicates what method to sue to align the columns
        """
        super().__init__(args)
        self.should_decontext = should_decontext
        self.sim_threshold = sim_threshold
        self.align_method_name = align_method_name
        if not align_method_config:
            self.align_method_config = {}
        else:
            self.align_method_config = align_method_config

    def add(self, prediction: Table, target: Table, metadata: Any | None = None):
        # optionally decontextualize the target
        if self.should_decontext:
            target = decontextualize_table(target, metadata)

        # align the columns of the prediction and the target tables
        schema_alignments, alignment_scores = align_schema(
            target.values,
            prediction.values,
            self.align_method_name,
            self.sim_threshold,
            **self.align_method_config,
        )

        false_negs = [gold_col for (gold_col, _), pred_cols in schema_alignments.items() if not pred_cols]
        true_pos = [gold_col for gold_col, _ in schema_alignments if gold_col not in false_negs]

        # next calculate the recall between the aligned columns
        if len(schema_alignments) == 0:
            recall = 0
        else:
            recall = len(true_pos) / (len(schema_alignments))

        self.scores["recalls"].append(recall)
        self.scores["alignment_scores"].append(alignment_scores.tolist())
        self.scores["alignments"].append({k1: vals for (k1, _), vals in schema_alignments.items()})

    def reset(self) -> None:
        self.scores = {
            "recalls": [],
            "alignment_scores": [],
            "alignments": [],
        }

    def process_scores(self) -> dict[str, Any]:
        return {
            repr(self): {
                "recall": np.mean(self.scores["recalls"]),
                "recalls": self.scores["recalls"],
                "alignments": self.scores["alignments"],
                "alignment_scores": self.scores["alignment_scores"],
            }
        }

    def __repr__(self) -> str:
        args = [
            f"should_decontext={self.should_decontext}",
            f"sim_threshold={self.sim_threshold}",
            f"align_method_name={self.align_method_name}",
        ]
        for k, v in self.align_method_config.items():
            args.append(f"{k}={v}")
        return f"SchemaRecallMetric({', '.join(args)})"


class SchemaDiversityMetric(BaseMetric):
    """Fix a number of schema and see how much repetition there is. This does not need a target.

    Some ways to do this:
    - Self-bleu
    - Cluster the schema by similarity using some kind of unsupervised clustering thing?
    """

    def __init__(self, args):
        super().__init__(args)
        self.reset()

        # Enbale effective_order for sentence-level BLEU
        self.bleu = BLEU(effective_order=True)

    def add(self, prediction: Table, target: Table, metadata: Any | None = None):
        # 1. Calculate Self-BLEU
        pred_schemata = list(prediction.schema)

        scores = []
        for i in range(len(pred_schemata)):
            scores.append(
                self.bleu.sentence_score(pred_schemata[i], pred_schemata[:i] + pred_schemata[i + 1 :]).score
            )

        self.scores["self-bleu"].append(np.mean(scores))

    def process_scores(self) -> dict[str, Any]:
        return {"diversity": {"self-bleu": np.mean(self.scores["self-bleu"])}}

    def reset(self) -> None:
        self.scores = {
            "self-bleu": [],
        }


class ValueRecallMetric(BaseMetric):
    def __init__(self, args):
        super().__init__(args)

    def add(self, prediction: Table, target: Table, metadata: Any | None = None):
        return super().add(prediction, target, metadata)

    def process_scores(self) -> dict[str, Any]:
        return super().process_scores()

    def reset(self) -> None:
        return super().reset()


class GPT4Preference(BaseMetric):
    def __init__(self, args):
        super().__init__(args)

    def add(self, prediction: Table, target: Table, metadata: Any | None = None):
        return super().add(prediction, target, metadata)

    def process_scores(self) -> dict[str, Any]:
        return super().process_scores()

    def reset(self) -> None:
        return super().reset()


class Evaluator:
    def __init__(self, metrics: list[BaseMetric]):
        self.metrics = metrics

    def __call__(self, args, tables: list[Table], data) -> dict:
        for i in range(len(tables)):
            for metric in self.metrics:
                target = data[i][data.y_label]
                metric.add(tables[i], target)

        all_scores: dict = {}
        for metric in self.metrics:
            all_scores |= metric.process_scores()
        return all_scores


METRIC_MAP = {
    "diversity": SchemaDiversityMetric,
    "schema_recall": SchemaRecallMetric,
}


def load_metrics(args: DictConfig):
    metrics: list[BaseMetric] = []

    for metric_param in args.eval.metrics:
        if isinstance(metric_param, DictConfig):
            metric_name = metric_param["name"]
            metric_param_dict = {k: v for k, v in metric_param.items() if k != "name"}
        else:
            metric_name = metric_param
            metric_param_dict = {}

        metrics.append(METRIC_MAP[metric_name](args=args, **metric_param_dict))

    return Evaluator(metrics)
