from typing import Any, Optional

# from bert_score import BERTScorer
import numpy as np
from sacrebleu.metrics import BLEU
from omegaconf import DictConfig

from paper_comparison.types import Table
from paper_comparison.metrics_utils import get_p_r_f1, get_similar_sentence


class BaseMetric:
    """Calculates and stores information for evaluation metrics.

    Attributes:
        name: The name of the metric.
        requires_metadata: True if calculating the metric requires information outside of just the
            (prediction, target) pairs (e.g. the inputs).
    """

    name: str
    requires_metadata: bool

    def __init__(self, args):
        """Initialize the metric.

        By default, per-example scores are stored in `self.scores` and `self.requires_metadata` is False
        """

        self.scores = []
        self.requires_metadata = False
        self.reset()

    def add(self, prediction: Table, target: Table, metadata: Optional[Any] = None):
        """Calculate the score for the the given (prediction, target) pair

        and add it to the list of all scores.

        Args:
            prediction (str): The string generated by the model.
            target (str): The gold string.
            metadata (Optional[Any]): If required, the additional information needed for calculating the score.
        """

        raise NotImplementedError()

    def process_scores(self) -> dict[str, Any]:
        """Convert the list of scores into summary statistics which are returned

        Returns:
            A dict mapping from summary statistic name to the statistic. This is flexible, so even the entire
            scores list can be returned here.
        """
        raise NotImplementedError()

    def evaluate(
        self,
        predictions: Optional[list[Table]] = None,
        targets: Optional[list[Table]] = None,
        metadata: Optional[list[Any]] = None,
    ):
        """Evaluate the predictions against the targets.

        Wrap the `self.add` and `self.process_scores` methods so the metric can be called with multiple
        predictions and targets at once. If predictions and targets are not provided, simply process the
        scores (that likely have already been computed).

        Args:
            predictions (Optional[list[str]]): list of model-generated strings.
            targets (Optional[list[str]]): list of gold target strings. `predictions[i]` is the prediction for
                `targets[i]`.
            metadata (Optional[list[Any]]): list of additional information if needed.

        Returns:
            The result of `self.process_scores()`.
        """
        if predictions is not None and targets is not None:
            if metadata is None:
                for prediction, target in zip(predictions, targets):
                    self.add(prediction, target)
            else:
                for prediction, target, metadatum in zip(predictions, targets, metadata):
                    self.add(prediction, target, metadatum)

        return self.process_scores()

    def reset(self) -> None:
        """Reset the accumulated scores."""
        self.scores = []


class BasePairwiseComparisonMetric(BaseMetric):
    def __init__(self, args):
        super().__init__(args)

    def add(self, prediction: Table, target: Table, metadata: Any | None = None):
        return super().add(prediction, target, metadata)

    def process_scores(self) -> dict[str, Any]:
        return super().process_scores()

    def reset(self) -> None:
        return super().reset()


class AnswerCoherenceMetric(BaseMetric):
    def __init__(self, args):
        super().__init__(args)

    def add(self, prediction: Table, target: Table, metadata: Any | None = None):
        return super().add(prediction, target, metadata)

    def process_scores(self) -> dict[str, Any]:
        return super().process_scores()

    def reset(self) -> None:
        return super().reset()


class AnswerCorrectnessMetric(BaseMetric):
    def __init__(self, args):
        super().__init__(args)

    def add(self, prediction: Table, target: Table, metadata: Any | None = None):
        return super().add(prediction, target, metadata)

    def process_scores(self) -> dict[str, Any]:
        return super().process_scores()

    def reset(self) -> None:
        return super().reset()


class SchemaPrecisionRecallMetric(BaseMetric):
    """
    Calculates the precision and recall of the produced schema
    Precision:
    Recall: Of all the possible things we could compare from a paper, how many do we capture.

    This assumes we have (at least) some examples of really detailed human authored tables.
    """

    def __init__(self, args, sim_threshold=0.4, sim_method="jaccard_keywords"):
        super().__init__(args)
        self.sim_threshold = sim_threshold
        self.sim_method = sim_method

    def add(self, prediction: Table, target: Table, metadata: Any | None = None):
        # Check if the
        target_covered = {attribute: False for attribute in target.schema}
        pred_correct = {attribute: 0.0 for attribute in prediction.schema}

        for pred_attr in prediction.schema:
            closest_match, match_score = get_similar_sentence(
                pred_attr, list(target.schema), method=self.sim_method
            )
            if match_score > self.sim_threshold:
                target_covered[closest_match] = True
            pred_correct[pred_attr] = match_score

        tp = sum([int(x > self.sim_threshold) for x in pred_correct.values()])
        fp = len(pred_correct) - tp
        fn = len(target_covered) - sum(target_covered.values())

        p, r, f1 = get_p_r_f1(tp, fp, fn)
        self.scores["precision"].append(p)
        self.scores["recall"].append(r)
        self.scores["f1"].append(f1)
        self.scores["preds"].append(pred_correct)
        self.scores["targets"].append(target_covered)

    def process_scores(self) -> dict[str, Any]:
        return {
            "precision": np.mean(self.scores["precision"]),
            "recall": np.mean(self.scores["recall"]),
            "f1": np.mean(self.scores["f1"]),
            "preds": self.scores["preds"],
            "targets": self.scores["targets"],
        }

    def reset(self) -> None:
        self.scores = {
            "precision": [],
            "recall": [],
            "f1": [],
            "preds": [],
            "targets": [],
        }


class SchemaDiversityMetric(BaseMetric):
    """Fix a number of schema and see how much repetition there is. This does not need a target.

    Some ways to do this:
    - Self-bleu
    - Cluster the schema by similarity using some kind of unsupervised clustering thing?
    """

    def __init__(self, args):
        super().__init__(args)
        self.reset()

        # Enbale effective_order for sentence-level BLEU
        self.bleu = BLEU(effective_order=True)

    def add(self, prediction: Table, target: Table, metadata: Any | None = None):
        # 1. Calculate Self-BLEU
        pred_schemata = list(prediction.schema)

        scores = []
        for i in range(len(pred_schemata)):
            scores.append(
                self.bleu.sentence_score(pred_schemata[i], pred_schemata[:i] + pred_schemata[i + 1 :]).score
            )

        self.scores["self-bleu"].append(np.mean(scores))

    def process_scores(self) -> dict[str, Any]:
        return {"diversity": {"self-bleu": np.mean(self.scores["self-bleu"])}}

    def reset(self) -> None:
        self.scores = {
            "self-bleu": [],
        }


class SchemaComparisonMetric(BasePairwiseComparisonMetric):
    """Interleave top-k schema"""

    def __init__(self, args):
        super().__init__(args)

    def add(self, prediction: Table, target: Table, metadata: Any | None = None):
        return super().add(prediction, target, metadata)

    def process_scores(self) -> dict[str, Any]:
        return super().process_scores()

    def reset(self) -> None:
        return super().reset()


class Evaluator:
    def __init__(self, metrics: list[BaseMetric]):
        self.metrics = metrics

    def __call__(self, args, tables: list[Table], data) -> dict:
        for i in range(len(tables)):
            for metric in self.metrics:
                target = data[i][data.y_label]
                metric.add(tables[i], target)

        all_scores: dict = {}
        for metric in self.metrics:
            all_scores |= metric.process_scores()
        return all_scores


METRIC_MAP = {
    "PRF": SchemaPrecisionRecallMetric,
    "diversity": SchemaDiversityMetric,
}


def load_metrics(args: DictConfig):
    metrics: list[BaseMetric] = []

    for metric_param in args.eval.metrics:
        if isinstance(metric_param, DictConfig):
            metric_name = metric_param["name"]
            metric_param_dict = {k: v for k, v in metric_param.items() if k != "name"}
        else:
            metric_name = metric_param
            metric_param_dict = {}

        metrics.append(METRIC_MAP[metric_name](args=args, **metric_param_dict))

    return Evaluator(metrics)
