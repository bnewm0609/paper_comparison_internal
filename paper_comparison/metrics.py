from typing import Any, Optional

# from bert_score import BERTScorer
import numpy as np
from sacrebleu.metrics import BLEU
from omegaconf import DictConfig

from paper_comparison.types import Table

class BaseMetric:
    """Calculates and stores information for evaluation metrics.

    Attributes:
        name: The name of the metric.
        requires_metadata: True if calculating the metric requires information outside of just the
            (prediction, target) pairs (e.g. the inputs).
    """

    name: str
    requires_metadata: bool

    def __init__(self):
        """Initialize the metric.

        By default, per-example scores are stored in `self.scores` and `self.requires_metadata` is False
        """

        self.scores = {}
        self.requires_metadata = False
        self.reset()

    def add(self, prediction: Table, target: Table, metadata: Optional[Any] = None):
        """Calculate the score for the the given (prediction, target) pair

        and add it to the list of all scores.

        Args:
            prediction (str): The string generated by the model.
            target (str): The gold string.
            metadata (Optional[Any]): If required, the additional information needed for calculating the score.
        """

        raise NotImplementedError()

    def process_scores(self) -> dict[str, Any]:
        """Convert the list of scores into summary statistics which are returned

        Returns:
            A dict mapping from summary statistic name to the statistic. This is flexible, so even the entire
            scores list can be returned here.
        """
        raise NotImplementedError()

    def evaluate(
        self,
        predictions: Optional[list[Table]] = None,
        targets: Optional[list[Table]] = None,
        metadata: Optional[list[Any]] = None,
    ):
        """Evaluate the predictions against the targets.

        Wrap the `self.add` and `self.process_scores` methods so the metric can be called with multiple
        predictions and targets at once. If predictions and targets are not provided, simply process the
        scores (that likely have already been computed).

        Args:
            predictions (Optional[list[Table]]): list of model-generated tables.
            targets (Optional[list[Table]]): list of gold target tables. `predictions[i]` is the prediction for
                `targets[i]`.
            metadata (Optional[list[Any]]): list of additional information if needed.

        Returns:
            The result of `self.process_scores()`.
        """
        if predictions is not None and targets is not None:
            if metadata is None:
                for prediction, target in zip(predictions, targets):
                    self.add(prediction, target)
            else:
                for prediction, target, metadatum in zip(predictions, targets, metadata):
                    self.add(prediction, target, metadatum)

        return self.process_scores()

    def reset(self) -> None:
        """Reset the accumulated scores."""
        self.scores = {}


class SchemaRecallMetric(BaseMetric):
    """
    Calculates the recall of the produced schema.

    Namely, the tables we produce might have many more columns than the gold
    tables. This metric computes how many of the columns in the predicted
    tables are also in the gold table as a proportion of the number of columns
    in the gold table. It allows multiple columns from the predicted table to
    match a single column in the gold table.

    This calculation requires featurizing and aligning the columns in some way.
    metrics_utils contains three featurizers (column names, names+values, decontextualization)
    and four alignment methods (exact match, jaccard similarity, edit distance, embedding similarity).
    Pass appropriate featurizers and alignment methods when initializing the metric.
    """

    def __init__(
        self,
        featurizer,
        alignment_scorer,
        sim_threshold=0.4,
    ):
        super().__init__()
        self.requires_metadata = True
        self.featurizer = featurizer
        self.alignment_scorer = alignment_scorer
        self.sim_threshold = sim_threshold
        # Initialize some variables to maintain track of metrics computed so far
        self.scores["recall"] = {}
        self.scores["alignment_scores"] = {}
        self.scores["alignments"] = {}

    def add(self, prediction: Table, target: Table, metadata: Any | None = None):
        # compute alignment matrix between the prediction and target tables
        alignment_matrix = self.alignment_scorer.score_schema_alignments(
            prediction, 
            target, 
            featurizer=self.featurizer
        )

        # choose an aligment from the alignment matrix using similarity threshold
        alignment = {k:v for k,v in alignment_matrix.items() if v >= self.sim_threshold}

        # compute how many gold columns have been matched
        matched_gold_col_num = len(set([col_pair[0] for col_pair in alignment]))
        total_gold_col_num = len(target.schema)

        # next calculate the recall between the aligned columns
        if matched_gold_col_num == 0:
            recall = 0.0
        else:
            recall = float(matched_gold_col_num) / total_gold_col_num

        # store recall score, alignment matrix and final alignments
        # TODO: Maybe some of these variables can be dropped?
        if not target.tabid in self.scores["recall"]:
            self.scores["recall"][target.tabid] = []
            self.scores["alignment_scores"][target.tabid] = []
            self.scores["alignments"][target.tabid] = []

        self.scores["recall"][target.tabid].append(recall)
        self.scores["alignment_scores"][target.tabid].append(alignment_matrix)
        self.scores["alignments"][target.tabid].append(alignment)

    def reset(self) -> None:
        self.scores = {
            "recall": {},
            "alignment_scores": {},
            "alignments": {},
        }

    def process_scores(self) -> dict[str, Any]:
        return {
            repr(self): {
                "recall": np.mean([max(recall_list) for tab_id, recall_list in self.scores["recall"].items()]),
                # "recalls": self.scores["recalls"],
                # "alignments": self.scores["alignments"],
                # "alignment_scores": self.scores["alignment_scores"],
            }
        }

    def __repr__(self) -> str:
        args = [
            f"featurizer={self.featurizer.name}",
            f"alignment_scorer={self.alignment_scorer.name}",
            f"sim_threshold={self.sim_threshold}",
        ]
        return f"SchemaRecallMetric({', '.join(args)})"
    

# TODO: Add this back if we want to report more than one metric later
# class Evaluator:
#     def __init__(self, metrics: list[BaseMetric]):
#         self.metrics = metrics

#     def __call__(self, args, tables: list[Table], data) -> dict:
#         for i in range(len(tables)):
#             for metric in self.metrics:
#                 target = data[i][data.y_label]
#                 metric.add(tables[i], target)

#         all_scores: dict = {}
#         for metric in self.metrics:
#             all_scores |= metric.process_scores()
#         return all_scores


# METRIC_MAP = {
#     "diversity": SchemaDiversityMetric,
#     "schema_recall": SchemaRecallMetric,
# }


# def load_metrics(args: DictConfig):
#     metrics: list[BaseMetric] = []

#     for metric_param in args.eval.metrics:
#         if isinstance(metric_param, DictConfig):
#             metric_name = metric_param["name"]
#             metric_param_dict = {k: v for k, v in metric_param.items() if k != "name"}
#         else:
#             metric_name = metric_param
#             metric_param_dict = {}

#         metrics.append(METRIC_MAP[metric_name](args=args, **metric_param_dict))

#     return Evaluator(metrics)
    
# TODO: Do we want to keep some format of the diversity metric around?
# class SchemaDiversityMetric(BaseMetric):
#     """Fix a number of schema and see how much repetition there is. This does not need a target.

#     Some ways to do this:
#     - Self-bleu
#     - Cluster the schema by similarity using some kind of unsupervised clustering thing?
#     """

#     def __init__(self, args):
#         super().__init__(args)
#         self.reset()

#         # Enbale effective_order for sentence-level BLEU
#         self.bleu = BLEU(effective_order=True)

#     def add(self, prediction: Table, target: Table, metadata: Any | None = None):
#         # 1. Calculate Self-BLEU
#         pred_schemata = list(prediction.schema)

#         scores = []
#         for i in range(len(pred_schemata)):
#             scores.append(
#                 self.bleu.sentence_score(pred_schemata[i], pred_schemata[:i] + pred_schemata[i + 1 :]).score
#             )

#         self.scores["self-bleu"].append(np.mean(scores))

#     def process_scores(self) -> dict[str, Any]:
#         return {"diversity": {"self-bleu": np.mean(self.scores["self-bleu"])}}

#     def reset(self) -> None:
#         self.scores = {
#             "self-bleu": [],
#         }


# ---------- TO BE POTENTIALLY DELETED AFTER FINALIZING CODE REFACTOR -----------
# class AnswerCorrectnessMetric(BaseMetric):
#     def __init__(self, args):
#         super().__init__(args)

#     def add(self, prediction: Table, target: Table, metadata: Any | None = None):
#         return super().add(prediction, target, metadata)

#     def process_scores(self) -> dict[str, Any]:
#         return super().process_scores()

#     def reset(self) -> None:
#         return super().reset()

# class ValueRecallMetric(BaseMetric):
#     def __init__(self, args):
#         super().__init__(args)

#     def add(self, prediction: Table, target: Table, metadata: Any | None = None):
#         return super().add(prediction, target, metadata)

#     def process_scores(self) -> dict[str, Any]:
#         return super().process_scores()

#     def reset(self) -> None:
#         return super().reset()

# class GPT4Preference(BaseMetric):
#     def __init__(self, args):
#         super().__init__(args)

#     def add(self, prediction: Table, target: Table, metadata: Any | None = None):
#         return super().add(prediction, target, metadata)

#     def process_scores(self) -> dict[str, Any]:
#         return super().process_scores()

#     def reset(self) -> None:
#         return super().reset()